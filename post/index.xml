<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on YoungStatS</title>
    <link>https://youngstats.github.io/post/</link>
    <description>Recent content in Posts on YoungStatS</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 Jul 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://youngstats.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Depth Quantile Functions</title>
      <link>https://youngstats.github.io/post/2021/07/01/depth-quantile-functions/</link>
      <pubDate>Thu, 01 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/07/01/depth-quantile-functions/</guid>
      <description>Figure 1: Depth quantile functions for the wine data (d=13), class 2 vs class 3. Blue curves correspond to between class comparisons, red/pink correspond to within class comparisons.
A common technique in modern statistics is the so-called kernel trick, where data is mapped into a (usually) infinite-dimensional feature space, where various statistical tasks can be carried out. Relatedly, we introduce the depth quantile function (DQF), \(q_{ij}(\alpha)\) which similarly maps observations into an infinite dimensional space (the double index will become clear below), though in this case, these new representations of the data are functions of a one-dimensional variable \(\alpha\) which allows plotting.</description>
    </item>
    
    <item>
      <title>Concentration Inequalities in Machine Learning</title>
      <link>https://youngstats.github.io/post/2021/06/30/concentration-inequalities-in-machine-learning/</link>
      <pubDate>Wed, 30 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/06/30/concentration-inequalities-in-machine-learning/</guid>
      <description>The fifth “One World webinar” organized by YoungStatS will take place on September 15th, 2021. Selected young European researchers active in the areas of probability and machine learning will present their recent contributions. The webinar is joint cooperation between the Young Researchers Committee of the Bernoulli Society and the YoungStatS project.
When &amp;amp; Where:
Wednesday, September 15th, 17:00 CESTOnline, via Zoom. The registration form will be provided on the YoungStatS website.</description>
    </item>
    
    <item>
      <title>Optional stopping with Bayes factors: possibilities and limitations</title>
      <link>https://youngstats.github.io/post/2021/06/10/optional-stopping-with-bayes-factors-possibilities-and-limitations/</link>
      <pubDate>Thu, 10 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/06/10/optional-stopping-with-bayes-factors-possibilities-and-limitations/</guid>
      <description>In recent years, a surprising number of scientific results have failedto hold up to continued scrutiny. Part of this ‘replicability crisis’may be caused by practices that ignore the assumptions of traditional(frequentist) statistical methods (John, Loewenstein, and Prelec 2012). One ofthese assumptions is that the experimental protocol should be completelydetermined upfront. In practice, researchers often adjust the protocoldue to unforeseen circumstances or collect data until a point has beenproven.</description>
    </item>
    
    <item>
      <title>Spatiotemporal modeling and real-time prediction of origin-destination traffic demand</title>
      <link>https://youngstats.github.io/post/2021/06/09/spatiotemporal-modeling-and-real-time-prediction-of-origin/</link>
      <pubDate>Wed, 09 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/06/09/spatiotemporal-modeling-and-real-time-prediction-of-origin/</guid>
      <description>Introduction
In the past decades, intelligent transportation system (ITS) has broughtadvanced technology that enables a data-rich environment andunprecedented opportunities for traffic prediction, which is consideredas one of the most prevalent issues facing ITS (Li et al., 2015). Wediscuss the online prediction of the origin-destination (OD) demandcount in traffic networks, which represents the number of trips betweencertain combinations of an origin and a destination.</description>
    </item>
    
    <item>
      <title>Recent Advances in Functional Data Analysis</title>
      <link>https://youngstats.github.io/post/2021/04/29/fda-webinar/</link>
      <pubDate>Thu, 29 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/04/29/fda-webinar/</guid>
      <description>The fourth &amp;ldquo;One World webinar&amp;rdquo; organized by YoungStatS will take place on June 30th, 2021. The topic of this webinar is on Functional Data Analysis. Selected young European researchers active in this area of research will present their contributions on spherical functional autoregressions, additive models, and clustering methods for functional data, with the focus on both theoretical developments and applications.
When &amp;amp; Where:
  Wednesday, June 30th, 16:30 CEST</description>
    </item>
    
    <item>
      <title>Composite-Based Structural Equation Modeling: Developments and Perspectives</title>
      <link>https://youngstats.github.io/post/2021/04/28/csem-webinar/</link>
      <pubDate>Wed, 28 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/04/28/csem-webinar/</guid>
      <description>The third &amp;ldquo;One World webinar&amp;rdquo; organized by YoungStatS will take place on May 19th, 2021. The focus of this webinar will be on composite-based structural equation modeling, particularly on partial least squares path modeling (Wold, 1982; Lohmöller, 1989) and approaches to assess composite models. The webinar will present some of the most interesting and recent theoretical developments and applications from younger scholars active in this area of research.
When &amp;amp; Where:</description>
    </item>
    
    <item>
      <title>A small step to understand Generative Adversarial Networks</title>
      <link>https://youngstats.github.io/post/2021/04/27/a-small-step-to-understand-gda/</link>
      <pubDate>Tue, 27 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/04/27/a-small-step-to-understand-gda/</guid>
      <description>IntroductionIn the last decade, there have been spectacular advances on the practical side of machine learning.One of the most impressive may be the success of Generative Adversarial Networks (GANs) for image generation (Goodfellow et al. 2014).State of the art models are capable of producing portraits of fake persons that look perfectly authentic to you and me (see e.g. (Salimans et al. 2016) and (Karras et al.</description>
    </item>
    
    <item>
      <title>Generalizing the Neyman-Pearson Lemma for multiple hypothesis testing problems</title>
      <link>https://youngstats.github.io/post/2021/04/13/generalizing-the-neyman-pearson-lemma-for-multiple-hypothesis-testing-problems/</link>
      <pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/04/13/generalizing-the-neyman-pearson-lemma-for-multiple-hypothesis-testing-problems/</guid>
      <description>IntroductionLet us start by considering the optimal rejection policy for a single hypothesis testing problem. There are three elements to the problem. The objective: to maximize the power to reject the null hypothesis; The constraint: to control the type I error probability, so that it is at most a predefined \(\alpha\); The decision policy: for every realized sample define the decision \(D\in \{0,1\}\), where the null hypothesis is rejected if \(D=1\) and retained otherwise.</description>
    </item>
    
    <item>
      <title>Developments in Bayesian Nonparametrics (updated with slides)</title>
      <link>https://youngstats.github.io/post/2021/04/06/bnp-webinar/</link>
      <pubDate>Tue, 06 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/04/06/bnp-webinar/</guid>
      <description>The second &amp;ldquo;One World webinar&amp;rdquo; organized by YoungStatS will take place on April 21st. The focus of this webinar will be on illustrating modern advances in Bayesian Nonparametrics data analysis, discussing challenging theoretical problems and stimulating case-studies within this active area of research.
When &amp;amp; Where:
 Wednesday, April 21st, 16:30 CEST Online, via Zoom. The registration form is available here. Further details and the Zoom link will be sent to the registered addresses only.</description>
    </item>
    
    <item>
      <title>Analysis of a Two-Layer Neural Network via Displacement Convexity</title>
      <link>https://youngstats.github.io/post/2021/03/14/analysis-of-a-two-layer-neural-network-via-displacement-convexity/</link>
      <pubDate>Sun, 14 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/03/14/analysis-of-a-two-layer-neural-network-via-displacement-convexity/</guid>
      <description>AbstractWe consider the problem of learning a function defined on a compact domain, using linear combinationsof a large number of “bump-like” components (neurons). This idea lies at the core of a variety of methodsfrom two-layer neural networks to kernel regression, to boosting. In general, the resulting risk minimizationproblem is non-convex and is solved by gradient descent or its variants. Nonetheless, little is known aboutglobal convergence properties of these approaches.</description>
    </item>
    
  </channel>
</rss>
