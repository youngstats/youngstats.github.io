<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on YoungStatS</title>
    <link>https://youngstats.github.io/post/</link>
    <description>Recent content in Posts on YoungStatS</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 13 Apr 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://youngstats.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Generalizing the Neyman-Pearson Lemma for multiple hypothesis testing problems</title>
      <link>https://youngstats.github.io/post/2021/04/13/generalizing-the-neyman-pearson-lemma-for-multiple-hypothesis-testing-problems/</link>
      <pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/04/13/generalizing-the-neyman-pearson-lemma-for-multiple-hypothesis-testing-problems/</guid>
      <description>IntroductionLet us start by considering the optimal rejection policy for a single hypothesis testing problem. There are three elements to the problem. The objective: to maximize the power to reject the null hypothesis; The constraint: to control the type I error probability, so that it is at most a predefined \(\alpha\); The decision policy: for every realized sample define the decision \(D\in \{0,1\}\), where the null hypothesis is rejected if \(D=1\) and retained otherwise.</description>
    </item>
    
    <item>
      <title>Developments in Bayesian Nonparametrics</title>
      <link>https://youngstats.github.io/post/2021/04/06/bnp-webinar/</link>
      <pubDate>Tue, 06 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/04/06/bnp-webinar/</guid>
      <description>The second &amp;ldquo;One World webinar&amp;rdquo; organized by YoungStatS will take place on April 21st. The focus of this webinar will be on illustrating modern advances in Bayesian Nonparametrics data analysis, discussing challenging theoretical problems and stimulating case-studies within this active area of research.
When &amp;amp; Where:
 Wednesday, April 21st, 16:30 CEST Online, via Zoom. The registration form is available here. Further details and the Zoom link will be sent to the registered addresses only.</description>
    </item>
    
    <item>
      <title>Analysis of a Two-Layer Neural Network via Displacement Convexity</title>
      <link>https://youngstats.github.io/post/2021/03/14/analysis-of-a-two-layer-neural-network-via-displacement-convexity/</link>
      <pubDate>Sun, 14 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/03/14/analysis-of-a-two-layer-neural-network-via-displacement-convexity/</guid>
      <description>AbstractWe consider the problem of learning a function defined on a compact domain, using linear combinationsof a large number of “bump-like” components (neurons). This idea lies at the core of a variety of methodsfrom two-layer neural networks to kernel regression, to boosting. In general, the resulting risk minimizationproblem is non-convex and is solved by gradient descent or its variants. Nonetheless, little is known aboutglobal convergence properties of these approaches.</description>
    </item>
    
    <item>
      <title>A Scalable Empirical Bayes Approach to Variable Selection in Generalized Linear Models</title>
      <link>https://youngstats.github.io/post/2021/03/13/a-scalable-empirical-bayes-approach-to-variable-selection-in-generalized-linear-models/</link>
      <pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/03/13/a-scalable-empirical-bayes-approach-to-variable-selection-in-generalized-linear-models/</guid>
      <description>In the toolbox of most scientists over the past century, there have been few methods as powerful and as versatile as linear regression. The introduction of the generalized linear model (GLM) framework in the 1970’s extended the inferential and predictive capabilities to binary or count data. While the effect of this ‘Swiss Army knife’ of scientific research cannot be overstated, rapid (and amazing) technological advances in other areas have pushed it beyond its theoretical capacity.</description>
    </item>
    
    <item>
      <title>Compositional scalar-on-function regression as a tool (not only) for geological data</title>
      <link>https://youngstats.github.io/post/2021/03/10/compositional-scalar-on-function-regression-as-a-tool-not-only-for-geological-data/</link>
      <pubDate>Wed, 10 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/03/10/compositional-scalar-on-function-regression-as-a-tool-not-only-for-geological-data/</guid>
      <description>Compositional data are characterized by the fact that the relevant information is contained not necessarily in the absolute values but rather in the relative proportions between particular components. As an example, take household expenditures for different purposes (housing, groceries, travel etc.) or geochemical composition of a certain soil sample. In the latter case, the resulting composition of chemical elements is determined strongly by the particle size distribution (PSD, i.</description>
    </item>
    
    <item>
      <title>Higher Order Targeted Maximum Likelihood Estimation</title>
      <link>https://youngstats.github.io/post/2021/03/10/higher-order-targeted-maximum-likelihood-estimation/</link>
      <pubDate>Wed, 10 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/03/10/higher-order-targeted-maximum-likelihood-estimation/</guid>
      <description>Summary
We propose a higher order targeted maximum likelihood estimation (TMLE) that only relies on a sequentially and recursively defined set of data-adaptive fluctuations. Without the need to assume the often too stringent higher order pathwise differentiability, the method is practical for implementation and has the potential to be fully computerized.
Background Targeted Maximum Likelihood Estimation (TMLE) It has been particularly of interest for semiparametric theories and real world practices to make efficient and substitution-based estimation for target quantities that are functions of data distribution.</description>
    </item>
    
    <item>
      <title>Give me an adequate correlation: assessing relationships in percentage (or proportional) data</title>
      <link>https://youngstats.github.io/post/2021/02/04/give-me-an-adequate-correlation-assessing-relationships-in-percentage-or-proportional-data/</link>
      <pubDate>Thu, 04 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/02/04/give-me-an-adequate-correlation-assessing-relationships-in-percentage-or-proportional-data/</guid>
      <description>Correlations and negative bias We assume that you are quite familiar with the following problem. Consider a data set where the information is expressed in percentages or proportions. An example are household expenditures, given as average amounts (in Euros) the households are spending on food, housing, transportation, etc. Since the expenditures would not be comparable among countries with very different economic level, it makes sense to express the data as proportions (or percentages) of the single categories on the total expenditures.</description>
    </item>
    
    <item>
      <title>Recent Advances in COVID-19 modelling</title>
      <link>https://youngstats.github.io/post/2021/02/04/covid-webinar/</link>
      <pubDate>Thu, 04 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/02/04/covid-webinar/</guid>
      <description>YoungStatS project of Young Statisticians Europe, FENStatS, proudly announces our first One World YoungStatS webinar. With four young scholars, we will discuss Recent Advances in the Modelling of COVID-19, presenting novel statistical models, interesting advancements and applications of mechanistic models, as well as their combinations.
When: Wednesday, February 10th, 16:00 (Central European Time).
Speakers:
 Cécile Tran Kiem (Institute Pasteur, Paris, France) Pierfrancesco Alaimo Di Loro and Marco Mingione (StatGroup-19, Italy) Kevin Kunzmann (University of Cambridge, United Kingdom)  Discussant:</description>
    </item>
    
    <item>
      <title>Locally adapative k-nearest neighbour classification</title>
      <link>https://youngstats.github.io/post/2021/01/31/local-knn/</link>
      <pubDate>Sun, 31 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/01/31/local-knn/</guid>
      <description>Abstract Binary classification is one of the cornerstones of modern data science, but, until recently, our understanding of classical methods such as the k-nn algorithm was limited to settings where feature vectors were compactly supported. Based on a new analysis of this classifier, we propose a variant with significantly lower risk for heavy-tailed distributions.
 The \(k\)-nearest neighbour classifier The basic classifier that we consider here was introduced by Fix and Hodges (1951), and is arguably the simplest and most intuitive nonparametric classifier.</description>
    </item>
    
    <item>
      <title>PLS for Big Data: A unified parallel algorithm for regularised group PLS</title>
      <link>https://youngstats.github.io/post/2021/01/28/pls-for-big-data/</link>
      <pubDate>Thu, 28 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/01/28/pls-for-big-data/</guid>
      <description>We look at the problem of learning latent structure between two blocks of data through the partial least squares (PLS) approach. These methods include approaches for supervised and unsupervised statistical learning. We review these methods and present approaches to decrease the computation time and scale the method to big data
Given two blocks of data, the PLS approach seeks latent variables which are constructed as linear combinations of the original datasets.</description>
    </item>
    
  </channel>
</rss>
