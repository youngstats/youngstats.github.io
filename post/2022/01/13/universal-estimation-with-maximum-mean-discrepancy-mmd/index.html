<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>Universal estimation with Maximum Mean Discrepancy (MMD) | YoungStatS</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <link href="//cdn.bootcss.com/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">

  </head>

  <body class="page">
    <nav class="header">
      <div class="banner">
<a href="/" class="text">
<div class="yourfancytitle"> YoungStatS </div> 
</a>
</div>

      <div class="head-meta">
      
        <span><a href="/">&larr; Back to Home</a></span>
       <span>The blog of Young Statisticians Europe (YSE)</span>
        
        
        
          
        
        
        
        
      
      </div>
    </nav>
    
    <footer>
<script src="//yihui.org/js/math-code.js"></script>
<script async
src="cdnjs.cloudflare.com/ajax/libs/mathjax/3.1.0?config=TeX-MML-AM_CHTML">
</script>
<div class="footer">
  <ul class="menu">
    
    <li><a href="/"><span data-hover="Home">Home</span></a></li>
    
    <li><a href="/categories/webinars"><span data-hover="Webinars">Webinars</span></a></li>
    
    <li><a href="/about/"><span data-hover="About">About</span></a></li>
    
    <li><a href="/categories/"><span data-hover="Categories">Categories</span></a></li>
    
  </ul>
  
</div>
</footer>

<div class="container">
<article>
<div class="article-meta">

  <div class="categories">
  
    <a href="/categories/robust-statistics">robust-statistics</a>
  
  </div>

  <h1><span class="title">Universal estimation with Maximum Mean Discrepancy (MMD)</span></h1>

  
  <h3 class="author">Pierre Alquier
</h3>
  

  
  

</div>



<main>

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p><em>This is an updated version of a blog post on RIKEN AIP Approximate Bayesian Inference team webpage:</em> <a href="https://team-approx-bayes.github.io/blog/mmd/" class="uri">https://team-approx-bayes.github.io/blog/mmd/</a></p>
<p><img src="/post/2022-01-13-universal-estimation-with-maximum-mean-discrepancy-mmd_files/cover2.png" /></p>
<div id="introduction" class="section level2">
<h2>INTRODUCTION</h2>
<p>A very old and yet very exciting problem in statistics is the definition of a universal estimator <span class="math inline">\(\hat{\theta}\)</span>. An estimation procedure that would work all the time. Close your eyes, push the button, it works, for any model, in any context.</p>
<p>Formally speaking, we want that for some metric <span class="math inline">\(d\)</span> on probability distributions, for any statistical model <span class="math inline">\((P_\theta,\theta\in\Theta)\)</span>, given <span class="math inline">\(X_1,\dots,X_n\)</span> drawn i.i.d from some <span class="math inline">\(P^0\)</span> <em>not necessarily in the model</em>,</p>
<p><span class="math display">\[
d\left(P_{\hat{\theta}},P^0 \right) \leq \inf_{\theta\in\Theta} d\left(P_{\theta},P^0 \right) + r_n(\Theta),
\]</span></p>
<p>where <span class="math inline">\(r_n(\Theta) \rightarrow 0\)</span> when <span class="math inline">\(n\rightarrow \infty\)</span> holds, either in expectation or with large probability.</p>
<p>Why would this be nice? Well, first, if the model is well specified, that is, <span class="math inline">\(P^0 = P_{\theta^0}\)</span> for some <span class="math inline">\(\theta^0\in\Theta\)</span>, we would have:</p>
<p><span class="math display">\[
d\left(P_{\hat{\theta}},P^0 \right) \leq  r_n(\Theta) \rightarrow 0,
\]</span></p>
<p>so the estimator is consistent. But the ill-specified case is also relevant. Remember that “<em>all models are wrong, some models are useful</em>”. A very interesting case is Huber’s contamination model: assume that the data is drawn from the model, but might be corrupted with small probability. That is, <span class="math inline">\(P^0 = (1-\varepsilon) P_{\theta^0} + \varepsilon Q\)</span> where <span class="math inline">\(\varepsilon\in[0,1]\)</span> is small and <span class="math inline">\(Q\)</span>, the contamination distribution, can be absolutely whatever. Then we would have:</p>
<p><span class="math display">\[
d\left(P_{\hat{\theta}},P^0 \right) \leq d(P_{\theta^0},P^0) + r_n(\Theta) = d(P_{\theta^0},(1-\varepsilon) P_{\theta^0} + \varepsilon Q) + r_n(\Theta).
\]</span></p>
<p>If the metric <span class="math inline">\(d\)</span> is such that <span class="math inline">\(d(P_{\theta^0},(1-\varepsilon) P_{\theta^0} + \varepsilon Q) \leq C \varepsilon\)</span> for some constant <span class="math inline">\(C\)</span>, we end up with</p>
<p><span class="math display">\[
d\left(P_{\hat{\theta}},P^0 \right) \leq C \varepsilon +  r_n(\Theta),
\]</span></p>
<p>which means that, as long as <span class="math inline">\(\varepsilon\)</span> remains quite small, the estimator is still not too bad. That is, the estimator is robust to contamination.</p>
</div>
<div id="the-mle-does-not-work" class="section level2">
<h2>THE MLE DOES NOT WORK</h2>
<p>In case you believe that popular estimators such as Maximum Likelihood Estimator (MLE) is universal, surprise: it’s not.</p>
<p>First, if the model does not satisfy regularity assumptions, the MLE is not even defined. For example, consider a location model, that is, <span class="math inline">\(P_\theta\)</span> has the density
<span class="math display">\[ p_\theta(x) = g(x-\theta) \]</span>
and consider
<span class="math display">\[ g(x) = \frac{\exp(-|x|)}{2\sqrt{\pi|x|}} . \]</span>
Obviously, the likelihood is infinite at each data point, that is, as soon as <span class="math inline">\(\theta\in\{X_1,\dots,X_n\}\)</span>, we have
<span class="math display">\[ \prod_{i=1}^n p_\theta(X_i) = +\infty. \]</span>
Even when the MLE is well defined, there are examples where it is known to be inconsistent [8]. It is also well known to be non robust to contamination in the data.</p>
</div>
<div id="some-examples-of-universal-estimators" class="section level2">
<h2>SOME EXAMPLES OF UNIVERSAL ESTIMATORS</h2>
<p>The first example of universal estimator we are aware of: Yatracos’ estimator [7], with <span class="math inline">\(d\)</span> being the total variation distance. It works with the rate <span class="math inline">\(r_n(\Theta) = [\mathrm{dim}(\Theta)/n]^{\frac{1}{2}}\)</span> when <span class="math inline">\(\Theta\)</span> is in some finite dimensional space. And it doesn’t work for nonparametric estimation. Still, it’s nice, and the paper is beautiful (and short). Equally beautiful is the book by Devroye and Lugosi [9] which studies many estimations methods for the total variation distance, including variants of Yatracos’ estimator.</p>
<p>Another example is Birgé, Barraud and Sart’s <span class="math inline">\(\rho\)</span>-estimator [8], which satisfies a similar result for the Hellinger distance. If you want to read the paper, be aware: this is extremely difficult to prove! It is also very nice, because the Hellinger distance looks locally very similar to the KL in many models. In some sense, the <span class="math inline">\(\rho\)</span>-estimator actually does what the MLE should do.</p>
<p>By the way. We live in the big data era, high dimensional data, big networks, more layers, you know. So it must be said that Yatracos’ estimator, and the <span class="math inline">\(\rho\)</span>-estimators, cannot be used in practice. They require an exhaustive search in a fine discretization of the parameter space, don’t expect to do that with a deep NN. Don’t expect to do it either for a very shallow NN, not even for a linear regression in dimension 50 (as discussed later, a variant of Yatracos’ estimator by Devroye and Lugosi might be feasible, though).</p>
</div>
<div id="mmd-estimation" class="section level2">
<h2>MMD-ESTIMATION</h2>
<p>Let us now describe yet another metric, and another estimator. This metric is based on kernels. So, let <span class="math inline">\(K\)</span> be a kernel on the observations space, <span class="math inline">\(\mathcal{X}\)</span>. This means that there is an Hilbert space <span class="math inline">\(\mathcal{H}\)</span>, equipped with a norm <span class="math inline">\(\left\lVert\cdot\right\rVert _{\mathcal{H}}\)</span>, and a continuous map <span class="math inline">\(\Phi:\mathcal{X}\rightarrow \mathcal{H}\)</span> such that <span class="math inline">\(K(x,x&#39;) = \left&lt;\Phi(x),\Phi(x&#39;)\right&gt;\)</span>.</p>
<p>Given a probability distribution <span class="math inline">\(P\)</span> on <span class="math inline">\(\mathcal{X}\)</span>, let us define the kernel mean embedding</p>
<p><span class="math display">\[
 \mu(P) = \int \Phi(x) P(\mathrm{d} x) = \mathbb{E} _{X \sim P }[\Phi(X)].
\]</span></p>
<p>Wait. Of course, this is not always defined! It is, say, if <span class="math inline">\(\int \left\lVert\Phi(x)\right\rVert_{\mathcal{H}} P(\mathrm{d} x) &lt;+\infty\)</span>.</p>
<p>Now, it appears that some kernels <span class="math inline">\(k\)</span> are known such that:</p>
<ol style="list-style-type: lower-roman">
<li><p><span class="math inline">\(k(x,x) = \left\| \Phi(x) \right\|_{\mathcal{H}}^2 \leq 1\)</span> for any <span class="math inline">\(x\)</span>, which in turn ensures that <span class="math inline">\(\mu(P)\)</span> is well defined for any <span class="math inline">\(P\)</span>.</p></li>
<li><p><span class="math inline">\(P\mapsto \mu(P)\)</span> is one to one.</p></li>
</ol>
<p>For example, when <span class="math inline">\(\mathcal{X}=\mathbb{R}^d\)</span>, the Gaussian kernel</p>
<p><span class="math display">\[
k(x,x&#39;) = \exp\left(- \frac{\left\lVert x-x&#39;\right\rVert^2}{\gamma^2} \right),
\]</span></p>
<p>for some <span class="math inline">\(\gamma&gt;0\)</span>, satisfies (i) and (ii).</p>
<p>For any such kernel,</p>
<p><span class="math display">\[
d_{k}(P,Q)=\left\lVert \mu(P)-\mu(Q)\right\rVert_{\mathcal{H}}
\]</span></p>
<p>is a distance between probability distributions. We can now define the MMD-estimator. Let <span class="math inline">\(\hat{P}_n\)</span> denote the empirical probability distribution, that is,</p>
<p><span class="math display">\[
\hat{P} _n
= \frac{1}{n}\sum_{i=1}^{n} \delta_{X_i}.
\]</span></p>
<p>The estimator <span class="math inline">\(\hat{\theta}\)</span> is defined by</p>
<p><span class="math display">\[
\hat{\theta} = \arg\min_{\theta\in\Theta}d_k(P_\theta,\hat{P}_n).
\]</span></p>
<p>Note: [12] below provide some conditions ensuring that the minimizer indeed exists. But actually, if if does not exist, just take any <span class="math inline">\(\varepsilon\)</span>-minimizer for <span class="math inline">\(\varepsilon\)</span> small enough, and all the good properties discussed below still hold.</p>
<p>Note: if you believed the statement “<em>Close your eyes, push the button, it works</em>” above, you will of course be disappointed. Life is not that simple. The choice of the kernel <span class="math inline">\(k\)</span> is far from easy, and is of course context dependent.</p>
</div>
<div id="the-shortest-consistency-proof-ever" class="section level2">
<h2>THE SHORTEST CONSISTENCY PROOF EVER?</h2>
<p>We now prove that, as long as the kernel satisfies (i) and (ii) above, for any statistical model <span class="math inline">\((P_\theta,\theta\in\Theta)\)</span> (parametric, or not!), given <span class="math inline">\(X_1,\dots,X_n\)</span> drawn i.i.d from some <span class="math inline">\(P^0\)</span>,</p>
<p><span class="math display">\[
\mathbb{E} \left[ d_k\left(P_{\hat{\theta}},P^0 \right) \right] \leq \inf_{\theta\in\Theta} d_k\left(P_{\theta},P^0 \right) + \frac{2}{\sqrt{n}}.
\]</span></p>
<p>This is taken from our paper [1]. (Note that the expectation is with respect to the sample: <span class="math inline">\(X_1,\dots,X_n\)</span> as <span class="math inline">\(\hat{\theta}=\hat{\theta}(X_1,\dots,X_n)\)</span>, the dependence with respect to the sample is always dropped from the notation in satistics and in machine learning).</p>
<p>First, for any <span class="math inline">\(\theta\in\Theta\)</span>,</p>
<p><span class="math display">\[
d_k\left(P_{\hat{\theta}_n},P^0 \right) \leq d_k\left(P_{\hat{\theta}_n},\hat{P}_n \right) +  d_k\left(\hat{P}_n,P^0 \right)
\]</span></p>
<p>by the triangle inequality. Using the defining property of <span class="math inline">\(\hat{\theta}\)</span>, that is, that it mimizes the first term in the right-hand side,</p>
<p><span class="math display">\[
d_k\left(P_{\hat{\theta}_n},P^0 \right) \leq d_k\left(P_{\theta},\hat{P}_n \right) +  d_k\left(\hat{P}_n,P^0 \right),
\]</span></p>
<p>and using again the triangle inequality,</p>
<p><span class="math display">\[
d_k\left(P_{\hat{\theta}_n},P^0 \right) \leq d_k \left(P_{\theta},P^0 \right) + 2 d_k\left(\hat{P}_n,P^0 \right).
\]</span></p>
<p>Take the expectation of both sides, and keeping in mind that this holds for any <span class="math inline">\(\theta\)</span>, this gives:</p>
<p><span class="math display">\[
\mathbb{E} \left[ d_k\left(P_{\hat{\theta}},P^0 \right) \right] \leq \inf_{\theta\in\Theta} d_k\left(P_{\theta},P^0 \right) + 2 \mathbb{E} \left[d_k\left(\hat{P}_n,P^0 \right) \right].
\]</span></p>
<p>So, it all boils down to a control of the expectation of <span class="math inline">\(d_k(\hat{P}_n,P^0 )\)</span> in the right-hand side. Using Jensen’s inequality,</p>
<p><span class="math display">\[
\mathbb{E} \left[ d_k\left(\hat{P}_n,P^0 \right)\right] \leq \sqrt{\mathbb{E} \left[d_k^2\left(\hat{P}_n,P^0 \right)\right]}
\]</span></p>
<p>so let us just focus on bounding the expected square distance. Using the definition of the MMD distance,</p>
<p><span class="math display">\[
\mathbb{E} \left[d_k^2\left(\hat{P}_n,P^0 \right)\right] = \mathbb{E} \left[ \left\lVert \frac{1}{n} \sum_{i=1}^n \Phi(X_i)-\mathbb{E}_{X\sim P^0}[\Phi(X)]  \right\rVert_{\mathcal{H}}^2 \right] = \mathrm{Var}\left(\frac{1}{n} \sum_{i=1}^n \Phi(X_i) \right)
\]</span></p>
<p>and so, as <span class="math inline">\(X_1,\dots,X_n\)</span> are i.i.d,</p>
<p><span class="math display">\[
\mathbb{E} \left[d_k^2\left(\hat{P}_n,P^0 \right)\right] = \frac{1}{n} \mathrm{Var}[\Phi(X_1)] \leq \frac{1}{n} \mathbb{E}\left[ \left\lVert\Phi(X_1) \right\rVert_\mathcal{H}^2\right] \leq \frac{1}{n}.
\]</span></p>
</div>
<div id="how-to-compute-the-mmd-estimator" class="section level2">
<h2>HOW TO COMPUTE THE MMD-ESTIMATOR?</h2>
<p>Now, of course, one question remains: is it easier to compute <span class="math inline">\(\hat{\theta}\)</span> than Yatracos’ estimator?</p>
<p>This question is discussed in depth in [1] and [12]. The main message is that the minimization of <span class="math inline">\(d_k^2(P_\theta,\hat{P}_{n})\)</span> is usually a smooth, but non-convex problem (in [1] we exhibit one model for which the problem is convex, though). An unbiased estimate of the gradient of this quantity is easy to build. So, it is possible to use a stochastic gradient algorithm (SGA), but because of the non-convexity of the problem, it is not possible to show that this will lead to a global minimum. Still, in practice, the performances of the estimator obtained by using the SGA are excellent, see [1,12].</p>
</div>
<div id="historical-references" class="section level2">
<h2>HISTORICAL REFERENCES</h2>
<p>The idea to use an estimator of the form</p>
<p><span class="math display">\[
\hat{\theta} = \arg\min_{\theta\in\Theta} d(P_\theta,\hat{P}_n)
\]</span></p>
<p>goes back to the 50s, see [5], under the name “minimum distance estimation” (MDE). The paper [6] is followed by a discussion by Sture Holm who argues that this leads to robust estimators when the distance <span class="math inline">\(d\)</span> is bounded. The reader can try for example the Kolmogorov-Smirnov distance defined for <span class="math inline">\(\mathcal{X}=\mathbb{R}\)</span>,</p>
<p><span class="math display">\[
d(P,Q) = \sup_{a\in\mathbb{R}} |P(X\leq a) - Q(X\leq a)|.
\]</span></p>
<p>Another example is the total variation distance. Note that the initial procedure proposed by Yatracos [7] is <em>not</em> the MDE with the TV distance (but it is an MDE with respect to another, model dependent semi-metric).</p>
<p>Also, we mention that the procedure used by Barraud, Birgé and Sart in [8] <em>cannot</em> be interpreted as minimum distance estimation.</p>
<p>The MMD distance has been used in kernel methods for years, we refer the reader to the excellent tutorial [10]. However, up to your knowledge, the first time it was used as described in this blog post was in [11] where the authors used this technique to estimate <span class="math inline">\(\theta\)</span> in a special type of model <span class="math inline">\((P_\theta,\theta\in\Theta)\)</span> called Generative Adversarial Network (GAN, I guess you already heard about it). The first general study of MMD-estimation is [12], where the authors study the consistency and asymptotic normality of <span class="math inline">\(\hat{\theta}\)</span> (among others!).</p>
</div>
<div id="advertisement-our-recent-works-on-mmd" class="section level2">
<h2>ADVERTISEMENT: OUR RECENT WORKS ON MMD</h2>
<p>Our preprint on MMD-estimation and robustness study specifically the robustness properties of <span class="math inline">\(\hat{\theta}\)</span>. Namely, in Huber’s contamination model, we study in detail the dependence of <span class="math inline">\(\mathbb{E}[d(P_{\hat{\theta}},P^0)]\)</span> with respect to <span class="math inline">\(n\)</span>, the sample size, and <span class="math inline">\(\varepsilon\)</span>, the level of contamination. Moreover, in this paper, we also extend the consistency proof to the case where the observations are not independent.</p>
<p>[1] <a href="https://doi.org/10.3150/21-BEJ1338">B.-E. Chérief-Abdellatif and P. Alquier (2019). Finite Sample Properties of Parametric MMD Estimation: Robustness to Misspecification and Dependence. Bernoulli, 2022, vol. 28(1), no. 1, pp. 181-213.</a></p>
<p>In the short paper on MMD-Bayes, we study a generalized Bayes procedure using the MMD distance:</p>
<p><span class="math display">\[
\pi(\theta|X_1,\dots,X_n) \propto \exp\left(- \eta d_k^2(P_\theta,\hat{P}_n) \right) \pi(\theta)
\]</span></p>
<p>where <span class="math inline">\(\pi\)</span> is some prior distribution and <span class="math inline">\(\eta&gt;0\)</span> some tuning parameter. This leads to robust Bayesian estimators.</p>
<p>[2] <a href="http://proceedings.mlr.press/v118/cherief-abdellatif20a.html">B.-E. Chérief-Abdellatif and P. Alquier (2020). MMD-Bayes: Robust Bayesian Estimation via Maximum Mean Discrepancy. Proceedings of The 2nd Symposium on Advances in Approximate Bayesian Inference (AABI), Proceedings of Machine Learning Research, vol. 118, pp. 1-21.</a></p>
<p>We finally have two recent preprints on MMD-estimation in regression models and in copulas models, respectively. These models have in common that they are semi-parametric: we want to estimate a parameter that does not completely define the distribution of the data. For example, in the linear regression model <span class="math inline">\(Y_i = \left&lt;\theta,X_i\right&gt; + \varepsilon_i\)</span>, we usually only specify the distribution of <span class="math inline">\(\varepsilon\)</span>, say <span class="math inline">\(\mathcal{N}(0,\sigma^2)\)</span>. However, in order to use the MMD-estimator as defined above, one should also specify the distribution of the <span class="math inline">\(X_i\)</span>’s. In [4], we propose a trick to avoid that, but the analysis becomes immediately much more complex.</p>
<p>[3] <a href="https://doi.org/10.1080/01621459.2021.2024836">P. Alquier, B.-E. Chérief-Abdellatif, A. Derumigny and J.-D. Fermanian. Estimation of Copulas via Maximum Mean Discrepancy, to appear in Journal of the American Statistical Association.</a> The paper comes with the R package: <a href="https://cran.r-project.org/web/packages/MMDCopula/">MMDCopula</a>.</p>
<p>[4] <a href="http://arxiv.org/abs/2006.00840">P. Alquier and M. Gerber (2020). Universal Robust Regression via Maximum Mean Discrepancy. Preprint arxiv:2006.00840.</a> We are currently working on the corresponding R package.</p>
</div>
<div id="references" class="section level2">
<h2>REFERENCES</h2>
<p>[5] <a href="https://projecteuclid.org/download/pdf_1/euclid.aoms/1177707038">J. Wolfowitz (1957). The minimum distance method. The Annals of Mathematical Statistics, 28(1), 75-88.</a></p>
<p>[6] P. J. Bickel (1976). Another Look at Robustness: A Review of Reviews and Some New Developments. Scandinavian Journal of Statistics 3, 145-168.</p>
<p>[7] <a href="https://projecteuclid.org/download/pdf1/euclid.aos/1176349553">Y. G. Yatracos (1985). Rates of convergence of minimum distance estimators and Kolmogorov’s entropy. The Annals of Statistics, 13(2), 768-774.</a></p>
<p>[8] <a href="https://link.springer.com/article/10.1007/s00222-016-0673-5">Y. Baraud, L. Birgé and M. Sart (2017). A new method for estimation and model selection: <span class="math inline">\(\rho\)</span>-estimation. Inventiones mathematicae, 207, 425-517.</a></p>
<p>[9] <a href="https://www.springer.com/gp/book/9780387951171">L. Devroye and G. Lugosi (2001). Combinatorial methods in density estimation. Springer.</a></p>
<p>[10] <a href="https://www.nowpublishers.com/article/Details/MAL-060">K. Muandet, K. Fukumizu, B. Sriperumbudur and B. Schölkopf, (2017). Kernel mean embedding of distributions: A review and beyond. Foundations and Trends in Machine Learning, 10(1-2), 1-141.</a></p>
<p>[11] <a href="http://auai.org/uai2015/proceedings/papers/230.pdf">G. K. Dziugaite, D. M. Roy and Z. Ghahramani (2015). Training generative neural networks via maximum mean discrepancy optimization. UAI’15: Proceedings of the Thirty-First Conference on Uncertainty in Artificial IntelligenceJuly 2015, 258-267.</a></p>
<p>[12] <a href="http://arxiv.org/abs/1906.05944">F.-X. Briol, A. Barp, A. B. Duncan and M. Girolami (2019). Statistical Inference for Generative Models via Maximum Mean Discrepancy. Preprint arXiv:1906.05944.</a></p>
</div>
<div id="about-the-author" class="section level2">
<h2>ABOUT THE AUTHOR</h2>
<p><a href="https://pierrealquier.github.io/">Pierre Alquier</a></p>
<p>Please visit the webpage of my co-authors on this topic:</p>
<p><a href="https://badreddinecheriefabdellatif.github.io/">Badr-Eddine Chérief-Abdellatif</a></p>
<p><a href="https://alexisderumigny.wordpress.com/">Alexis Derumigny</a></p>
<p><a href="http://www.crest.fr/pagesperso.php?user=2975">Jean-David Fermanian</a></p>
<p><a href="https://research-information.bris.ac.uk/en/persons/mathieu-gerber">Mathieu Gerber</a></p>
</div>

</main>


















</article>
</div>

<script async src="//yihui.name/js/center-img.js"></script>
 


<script src="//yihui.name/js/math-code.js"></script>
<script async src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>






</body>
</html>

