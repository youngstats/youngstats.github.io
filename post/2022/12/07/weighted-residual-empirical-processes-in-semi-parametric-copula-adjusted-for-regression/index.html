<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>Weighted residual empirical processes in semi-parametric copula adjusted for regression | YoungStatS</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <link href="//cdn.bootcss.com/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">

  </head>

  <body class="page">
    <nav class="header">
      <div class="banner">
<a href="/" class="text">
<div class="yourfancytitle"> YoungStatS </div> 
</a>
</div>

      <div class="head-meta">
      
        <span><a href="/">&larr; Back to Home</a></span>
       <span>The blog of Young Statisticians Europe (YSE)</span>
        
        
        
          
        
        
        
        
      
      </div>
    </nav>
    
    <footer>
<script src="//yihui.org/js/math-code.js"></script>
<script async
src="cdnjs.cloudflare.com/ajax/libs/mathjax/3.1.0?config=TeX-MML-AM_CHTML">
</script>
<div class="footer">
  <ul class="menu">
    
    <li><a href="/"><span data-hover="Home">Home</span></a></li>
    
    <li><a href="/categories/webinars"><span data-hover="Webinars">Webinars</span></a></li>
    
    <li><a href="/about/"><span data-hover="About">About</span></a></li>
    
    <li><a href="/categories/"><span data-hover="Categories">Categories</span></a></li>
    
  </ul>
  
</div>
</footer>

<div class="container">
<article>
<div class="article-meta">

  <div class="categories">
  
    <a href="/categories/copula-models">copula-models</a>
  
     &hercon; <a href="/categories/semiparametric-statistics">semiparametric-statistics</a>
  
     &hercon; <a href="/categories/empirical-processes">empirical-processes</a>
  
     &hercon; <a href="/categories/regression">regression</a>
  
  </div>

  <h1><span class="title">Weighted residual empirical processes in semi-parametric copula adjusted for regression</span></h1>

  
  <h3 class="author">Yue Zhao,  Irène Gijbels and Ingrid Van Keilegom
</h3>
  

  
  

</div>



<main>



<div id="overview" class="section level1">
<h1>Overview</h1>
<p>In this post we first review the concept of semi-parametric copula and
the accompanying estimation procedure of pseudo-likelihood estimation
(PLE). We then generalize the estimation problem to the setting where
the copula signal is hidden in a semi- or non-parametric regression
model. Under this setting we have to base the PLE on the residuals. The
particular challenge of the diverging score function is handled via the
technique of the weighted residual empirical processes.</p>
</div>
<div id="the-semi-parametric-copula-model" class="section level1">
<h1>The semi-parametric copula model</h1>
<p>Copula has been a popular method to model multivariate dependence
structure since its introduction in <span class="citation">Sklar (<a href="#ref-Sklar1959" role="doc-biblioref">1959</a>)</span>. Consider a random vector
<span class="math inline">\(\mathbf{E}=(E_1,\dots,E_p)^\top\in\mathbb{R}^p\)</span> with joint distribution function
<span class="math inline">\(H\)</span>; we assume throughout that <span class="math inline">\(E_k\)</span>, <span class="math inline">\(k\in\{1,\dots,p\}\)</span> has absolutely
continuous marginal distribution function <span class="math inline">\(F_k\)</span>. Then the <em>copula</em> <span class="math inline">\(C\)</span>
associated with <span class="math inline">\(\mathbf{E}\)</span> is the joint distribution function of the
marginally transformed random vector <span class="math inline">\((F_1(E_1),\dots,F_p(E_p))^\top\)</span>.
It is clear from this definition that <span class="math inline">\(C\)</span> itself is always a
distribution supported on the unit hypercube <span class="math inline">\([0,1]^p\)</span>, and <span class="math inline">\(C\)</span> always
has <em>uniform marginals</em> supported on <span class="math inline">\([0,1]\)</span> whatever the marginals of
<span class="math inline">\(\mathbf{E}\)</span> may be. (The explicit form of <span class="math inline">\(C\)</span> follows from the Sklar’s
theorem, for instance Corollary 2.10.10 in <span class="citation">Nelsen (<a href="#ref-Nelsen1999" role="doc-biblioref">2006</a>)</span>:
<span class="math inline">\(C(\mathbf{u}) = H(F_1^{\leftarrow}(u_1),\dots,F_p^{\leftarrow}(u_p))\)</span> for
<span class="math inline">\(F_k^{\leftarrow}\)</span> the inverse of <span class="math inline">\(F_k\)</span> and
<span class="math inline">\(\mathbf{u} = (u_1,\dots,u_p)^\top\in[0,1]^p\)</span>.) Furthermore, by the <em>invariance
property</em>, if <span class="math inline">\(g_1,\dots,g_p\)</span> are univariate strictly increasing
functions, then <span class="math inline">\(\mathbf{E}\)</span> and its marginally transformed version
<span class="math inline">\((g_1(E_1),\dots,g_p(E_p))^\top\)</span> will admit the copula.</p>
<p>Thus, copula is a <em>margin-free</em> measure of multivariate dependence.
Applied in the opposite direction, one could also start from a copula
and couple the copula with arbitrary marginals to create multivariate
distributions in a flexible manner. For instance, beyond the usual
applications in finance and economy, copulas could be used in the latter
manner to model the dependence among the repeated observations in
longitudinal data (<span class="citation">Sun, Frees, and Rosenberg (<a href="#ref-SunFreesRosenberg2008" role="doc-biblioref">2008</a>)</span>).</p>
<p>In this post we will focus on the <em>semi-parametric</em> copula model that
serves as a middle ground between a totally non-parametric approach to
copula modelling (via the so called <em>empirical copula</em>, see for instance
<span class="citation">Fermanian, Radulović, and Wegkamp (<a href="#ref-FermanianRadulovicWegkamp2004" role="doc-biblioref">2004</a>)</span> and <span class="citation">Berghaus, Bücher, and Volgushev (<a href="#ref-BerghausBucherVolgushev2017" role="doc-biblioref">2017</a>)</span>) and a
totally parametric modelling of the random vector <span class="math inline">\(\mathbf{E}\)</span>. In the
semi-parametric copula model, we consider a collection of possible
distributions of <span class="math inline">\(\mathbf{E}\)</span> where the copulas <span class="math inline">\(C=C(\mathbf{\cdot};\mathbf{\theta})\)</span> are
constrained to be parametrized by an Euclidean <em>copula parameter</em>
<span class="math inline">\(\mathbf{\theta}=(\theta_1,\dots,\theta_d)^\top\)</span>, but where the marginals
<span class="math inline">\(F_1,\dots,F_p\)</span> of <span class="math inline">\(\mathbf{E}\)</span> could range over all <span class="math inline">\(p\)</span>-tuples of absolutely
continuous univariate distribution functions.</p>
<!-------------------------------------------------------------------------------->
</div>
<div id="the-pseudo-likelihood-method" class="section level1">
<h1>The pseudo-likelihood method</h1>
<p>In the semi-parametric copula model, the primary interest is often the
true value <span class="math inline">\(\mathbf{\theta}^*\)</span> of the copula parameter that determines the
multivariate dependence. An obvious challenge in estimating
<span class="math inline">\(\mathbf{\theta}^*\)</span> in the copula setting is how to handle the unknown
marginals <span class="math inline">\(F_1,\dots,F_p\)</span>. The canonical solution is the
pseudo-likelihood estimation (PLE) introduced in <span class="citation">Oakes (<a href="#ref-Oakes1994" role="doc-biblioref">1994</a>)</span> and
<span class="citation">Genest, Ghoudi‬, and Rivest (<a href="#ref-GenestGhoudiRivest1995" role="doc-biblioref">1995</a>)</span> that we now describe.</p>
<p>Let <span class="math inline">\(g_1(\mathbf{\cdot};\mathbf{\theta}),\dots,g_d(\mathbf{\cdot};\mathbf{\theta})\)</span> be a collection of
appropriate <em>score functions</em> such that the population <em>estimating
equation</em> <span class="math inline">\(\mathbb{E} g_m(F_1(E_1),\dots,F_p(E_p);\mathbf{\theta})=0\)</span> holds only when
<span class="math inline">\(\mathbf{\theta}=\mathbf{\theta}^*\)</span>, for all <span class="math inline">\(m\in\{1,\dots,d\}\)</span>. In principle one can
always choose the score functions to be the ones in the maximum
likelihood estimation, namely
<span class="math inline">\(g_m(\cdot;\mathbf{\theta})=\frac{\partial}{\partial \theta_m} \log c(\mathbf{\cdot};\mathbf{\theta})\)</span>
where <span class="math inline">\(c(\mathbf{\cdot};\mathbf{\theta})\)</span> is the density of the copula
<span class="math inline">\(C(\mathbf{\cdot};\mathbf{\theta})\)</span>. Thus, if <span class="math inline">\(F_1,\dots,F_p\)</span> were known, to estimate
<span class="math inline">\(\mathbf{\theta}^*\)</span> empirically based on a sample
<span class="math inline">\(\mathbf{E}_i=(E_{i,1},\dots,E_{i,p})^\top\)</span>, <span class="math inline">\(i\in\{1,\dots,n\}\)</span> of <span class="math inline">\(\mathbf{E}\)</span>, one
could simply “find the zero” of the empirical version of the estimating
equation, that is to estimate <span class="math inline">\(\mathbf{\theta}^*\)</span> by
<span class="math inline">\(\mathbf{\hat{\theta}}^{\text{parametric}}\)</span> that solves, for all
<span class="math inline">\(m\in\{1,\dots,d\}\)</span>, <span class="math display">\[\begin{align*}
    \frac{1}{n} \sum_{i=1}^n g_m(F_1(E_{i,1}),\dots,F_p(E_{i,p});\hat{\theta}^{\text{parametric}}) =0 .
\end{align*}\]</span> (The superscript “parametric” points to the fact that when
<span class="math inline">\(F_1,\dots,F_p\)</span> are known, we are basically solving a parametric
problem.) However, in semi-parametric copula modelling we commonly avoid
setting <span class="math inline">\(F_1,\dots,F_p\)</span> to some particular form. The PLE method solves
this problem by replacing the unknown <span class="math inline">\(F_k\)</span>, <span class="math inline">\(k\in\{1,\dots,p\}\)</span> by its
empirical counterpart, namely the empirical distribution function
<span class="math inline">\(F_{n,k}(t) = \frac{1}{n+1} \sum_{i=1}^n \mathbf{1}\{E_{i,k} \le t \}\)</span>. The
<em>oracle</em> PLE estimator <span class="math inline">\(\mathbf{\hat{\theta}}^{\text{oracle}}\)</span> of <span class="math inline">\(\mathbf{\theta}^*\)</span> is
then the one that solves the following, revised estimating equation: for
all <span class="math inline">\(m\in\{1,\dots,d\}\)</span>, <span class="math display">\[\begin{align}
    $\frac{1}{n} \sum_{i=1}^n g_m(F_{n,1}(E_{i,1}),\dots,F_{n,p}(E_{i,p});\mathbf{\hat{\theta}}^{\text{oracle}}) = \int g_m(\mathbf{u};\mathbf{\hat{\theta}}^{\text{oracle}})\mathrm{d}C_n(\mathbf{u}) = 0.
    \label{eq1}\tag{1}
\end{align}\]</span> All integrals in this post are over <span class="math inline">\([0,1]^p\)</span>. To simplify
our expression above and later, we have introduced the empirical copula
<span class="math inline">\(C_n\)</span> that is a multivariate distribution function on <span class="math inline">\([0,1]^p\)</span> with a
mass of <span class="math inline">\(1/n\)</span> at each of
<span class="math inline">\((F_{n,1}(E_{i,1}),\dots,F_{n,p}(E_{i,p}))^\top\)</span>, <span class="math inline">\(i\in\{1,\dots,n\}\)</span>
(precisely,
<span class="math inline">\(C_n(\mathbf{u}) = \frac{1}{n} \sum_{i=1}^n \mathbf{1}\left\{ F_{n,1}(E_{i,1})\le u_1, \dots, F_{n,p}(E_{i,p})\le u_p \right\}\)</span>).
The qualifier “oracle” in <span class="math inline">\(\mathbf{\hat{\theta}}^{\text{oracle}}\)</span> is used to
distinguish the current case when we can still directly observe the
copula sample <span class="math inline">\(\mathbf{E}_i\)</span>, <span class="math inline">\(i\in\{1,\dots,n\}\)</span> (albeit without knowing
<span class="math inline">\(F_1,\dots,F_p\)</span>), from the case when even that sample will be subject to
perturbation which we now turn to.</p>
<!-------------------------------------------------------------------------------->
</div>
<div id="residual-based-pseudo-likelihood-for-semi-parametric-copula-adjusted-for-regression" class="section level1">
<h1>Residual-based pseudo-likelihood for semi-parametric copula adjusted for regression</h1>
<p>From now on we suppose that the copula signal <span class="math inline">\(\mathbf{E}=\mathbf{E}_{\mathbf{\theta}^*}\)</span> is
“hidden” in a multivariate response semi- or non-parametric regression
model, a setting considered in our recent work
(<span class="citation">Zhao, Gijbels, and Van Keilegom (<a href="#ref-ZhaoGijbelsKeilegom2022" role="doc-biblioref">2022</a>)</span>): for a covariate <span class="math inline">\(\mathbf{X}\in\mathbb{R}^q\)</span> (independent
of <span class="math inline">\(\mathbf{E}\)</span>) and a response <span class="math inline">\(\mathbf{Y}=(Y_1,\dots,Y_p)^\top\in\mathbb{R}^p\)</span>,
<span class="math display">\[\begin{align*}
    Y_1 &amp;= m_1(\mathbf{X}) + E_1 , \\
    &amp;\quad \vdots \\
    Y_p &amp;= m_p(\mathbf{X}) + E_p .
\end{align*}\]</span> In its raw form, the model above is a purely non-parametric
regression model; by specifying particular forms of the regression
function <span class="math inline">\(m_k\)</span>, the above will accommodate a wide range of popular non-
and semi-parametric regression variants such as the partly linear
regression model and the additive model. (It’s not much more difficult
to consider a more flexible, heteroscedastic model
<span class="math inline">\(Y_k = m_k(\mathbf{X}) + \sigma_k(\mathbf{X})E_k\)</span>, though we refrain from doing so in this
post.) <span class="citation">Gijbels, Omelka, and Veraverbeke (<a href="#ref-GijbelsOmelkaVeraverbeke2015" role="doc-biblioref">2015</a>)</span> considered a similar model and
studied the resulting <em>empirical copula process</em>.</p>
<p>Under this regression model, we can observe an i.i.d. sample
<span class="math inline">\((\mathbf{Y}_1,\mathbf{X}_1),\dots,(\mathbf{Y}_n,\mathbf{X}_n)\)</span> of <span class="math inline">\((\mathbf{Y},\mathbf{X})\)</span>, but crucially <em>not</em>
the copula sample <span class="math inline">\(\mathbf{E}_1, \dots, \mathbf{E}_n\)</span>. To eventually arrive at our
estimator for <span class="math inline">\(\mathbf{\theta}^*\)</span> in this setting, we will first form our
empirical copula <span class="math inline">\(\hat{C}_n\)</span> based on the residuals of the regression as
follows. Let <span class="math inline">\(\hat{m}_k\)</span> be some estimator for <span class="math inline">\(m_k\)</span>. Let’s estimate the
<span class="math inline">\(k\)</span>th component of <span class="math inline">\(E_i=(E_{i,1},\dots,E_{i,p})^\top\)</span> by the residual
<span class="math display">\[\begin{align*}
    \hat{E}_{i,k} = Y_{i,k} - \hat{m}_k(X_i) .
\end{align*}\]</span> Then, we form the residual-based empirical distribution and
copula: <span class="math display">\[\begin{align*}
    \hat{F}_{n,k}(t) = \dfrac{1}{n+1} \sum_{i=1}^n \mathbf{1}\{ \hat{E}_{i,k} \le t \},\quad \hat{C}_n(\mathbf{u}) = \frac{1}{n} \sum_{i=1}^n \prod_{k=1}^p \mathbf{1}\{\hat{F}_{n,k}(\hat{E}_{i,k})\le u_k \}.
\end{align*}\]</span> Finally, to estimate <span class="math inline">\(\mathbf{\theta}^*\)</span>, we settle for the
estimator <span class="math inline">\(\hat{\theta}^{\text{residual}}\)</span> that solves <span class="math display">\[\begin{align}
    \frac{1}{n} \sum_{i=1}^n g_m( \hat{F}_{n,1}(\hat{E}_{i,1}),\dots,\hat{F}_{n,p}(\hat{E}_{i,p}) ;\hat{\theta}^{\text{residual}}) = \int g_m(\mathbf{u};\hat{\theta}^{\text{residual}})\mathrm{d}\hat{C}_n(\mathbf{u}) = 0 .
    \label{eq2}\tag{2}
\end{align}\]</span></p>
<p>Comparing Eq. (<a href="#mjx-eqn-eq2">2</a>) to Eq. (<a href="#mjx-eqn-eq1">1</a>), we
would expect that when the residual-based empirical copula <span class="math inline">\(\hat{C}_n\)</span> is
asymptotically indistinguishable from the oracle empirical copula <span class="math inline">\(C_n\)</span>,
the residual-based copula parameter estimator
<span class="math inline">\(\hat{\theta}^{\text{residual}}\)</span> should be asymptotically
indistinguishable from <span class="math inline">\(\hat{\theta}^{\text{oracle}}\)</span> as well. To formally
reach this conclusion, standard estimating equation theory requires
(among other conditions that we will ignore in this post) that the
estimating equations at the truth should become indistinguishable,
namely <span class="math display">\[\begin{align}
    \int g_m(\mathbf{u};\mathbf{\theta}^*)\mathrm{d}\hat{C}_n(\mathbf{u}) - \int g_m(\mathbf{u};\mathbf{\theta}^*)\mathrm{d}C_n(\mathbf{u}) = o_p(n^{-1/2}) .
    \label{eq3}\tag{3}
\end{align}\]</span></p>
<p>One typical, although ultimately restrictive, approach to establish Eq. (<a href="#mjx-eqn-eq3">3</a>) is to invoke integration by parts
(<span class="citation">Neumeyer, Omelka, and Hudecová (<a href="#ref-NeumeyerOmelkaHudecova2019" role="doc-biblioref">2019</a>)</span>, <span class="citation">Chen, Huang, and Yi (<a href="#ref-ChenHuangYi2021" role="doc-biblioref">2021</a>)</span>): ideally, this would
yield <span class="math display">\[\begin{align*}
    \int g_m(\mathbf{u};\mathbf{\theta}^*)\mathrm{d}\hat{C}_n(\mathbf{u}) \sim &amp;\int \hat{C}_n(\mathbf{u})\mathrm{d}g_m(\mathbf{u};\mathbf{\theta}^*) \\
    \stackrel{\text{up to}~o_p(n^{-1/2})}{\approx} &amp; C_n(\mathbf{u})\mathrm{d}g_m(\mathbf{u};\mathbf{\theta}^*) \sim \int g_m(\mathbf{u};\mathbf{\theta}^*)\mathrm{d}C_n(\mathbf{u}) .
\end{align*}\]</span> In the above “<span class="math inline">\(\sim\)</span>” is only meant to give a drastically
simplified and hence not-quite-correct representation of integration by
parts (we refer the readers to Appendix A in <span class="citation">Radulović, Wegkamp, and Zhao (<a href="#ref-RWZ2017" role="doc-biblioref">2017</a>)</span> for a precise
multivariate integration by parts formula particularly useful for
copulas), but it already conveys the underlying idea: the aim is to
convert <span class="math inline">\(\hat{C}_n\)</span> and <span class="math inline">\(C_n\)</span> in the integrals from measures to integrands
so that proving the closeness between the two integrals is clearly
reduced to proving the closeness between <span class="math inline">\(\hat{C}_n\)</span> and <span class="math inline">\(C_n\)</span>. However,
the integration by parts trick, although popular, often requires
<em>bounded</em> <span class="math inline">\(g_m\)</span> to properly define the measure <span class="math inline">\(\mathrm{d}g_m\)</span>, but this is
often <em>not satisfied for even the most common copulas</em>. For instance, in
Gaussian copula, the score functions are quadratic forms of the
<span class="math inline">\(\Phi^{\leftarrow}(\mathbf{u}_k)\)</span>’s where <span class="math inline">\(\Phi^{\leftarrow}\)</span> is the standard normal quantile function
(see Eq. (2.2) in <span class="citation">Segers, Akker, and Werker (<a href="#ref-SVW2014" role="doc-biblioref">2014</a>)</span>), so are clearly divergent as <span class="math inline">\(u_k\)</span>
approaches 0 or 1.</p>
<p>In <span class="citation">Zhao, Gijbels, and Van Keilegom (<a href="#ref-ZhaoGijbelsKeilegom2022" role="doc-biblioref">2022</a>)</span> we instead adopted a more direct approach.
Let
<span class="math inline">\(g_m^{[k]}(\mathbf{u};\mathbf{\theta}^*)=\frac{\partial}{\partial u_k} g_m(\mathbf{u};\mathbf{\theta}^*)\)</span>.
Then Taylor-expanding Eq. (<a href="#mjx-eqn-eq3">3</a>) we see that we will need
(among other ingredients) an <span class="math inline">\(o_p(n^{-1/2})\)</span> rate for the terms on the
right-hand side of <span class="math display">\[\begin{align}
\int g_m(\mathbf{u};\mathbf{\theta}^*)\mathrm{d}({\hat{C}_n-C_n})(\mathbf{u}) \approx \sum_{k=1}^p \left[ \frac{1}{n} \sum_{i=1}^n g_m^{[k]}(F_k(E_{i,k});\mathbf{\theta}^*) \left\{ \hat{F}_{n,k}(\hat{E}_{i,k})-F_{n,k}(E_{i,k}) \right\}  \right] .
\label{eq4}\tag{4}
\end{align}\]</span> It is <em>not</em> enough that the terms
<span class="math inline">\(\hat{F}_{n,k}(\hat{E}_{i,k})-F_{n,k}(E_{i,k})\)</span> on the right-hand side are <span class="math inline">\(o_p(n^{-1/2})\)</span>
(in fact they are not), due to the divergence of <span class="math inline">\(g_m^{[k]}\)</span>. We need to
take a more careful look at <span class="math inline">\(\hat{F}_{n,k}(\hat{E}_{i,k})-F_{n,k}(E_{i,k})\)</span>, whose
analysis belongs to <em>residual empirical processes</em>. To demonstrate the
benefits of considering the <em>weighted</em> version of such processes, we
first review some basic literature on the weighted (non-residual)
empirical processes in the simplified setting of the real line.</p>
<!-------------------------------------------------------------------------------->
</div>
<div id="weighted-empirical-processes-on-the-real-line" class="section level1">
<h1>Weighted empirical processes on the real line</h1>
<p>In this section we consider estimating a distribution function <span class="math inline">\(F=F_U\)</span>
of a random variable <span class="math inline">\(U\)</span>. We rely on the empirical distribution function
<span class="math inline">\(F_n\)</span> constructed from the i.i.d. observations <span class="math inline">\(U_1, \dots, U_n\)</span> of <span class="math inline">\(U\)</span>:
<span class="math inline">\(F_n(t) = \frac{1}{n+1} \sum_{i=1}^n \mathbf{1}\left\{U_i\le t\right\}\)</span>. The
resulting classical empirical process on the real line
<span class="math inline">\(\sqrt{n} (F_n -F)(t)\)</span>, <span class="math inline">\(t\in \mathbb{R}\)</span> must be one of the most extensively
studied objects in all of probability; for illustration we will just
quote a form of the associated law of the iterated logarithm (LIL):
<span class="math display">\[\begin{align*}
    \limsup_{n\rightarrow\infty} \sup_{t} \left|\dfrac{1}{\sqrt{\log\log(n)}} \sqrt{n} (F_n -F)(t) \right| = \dfrac{1}{\sqrt{2}} .
\end{align*}\]</span> Clearly, the LIL treats all points <span class="math inline">\(t\in \mathbb{R}\)</span> <em>equally</em>.
However, in reality the <span class="math inline">\(F(t)\)</span> at some <span class="math inline">\(t\)</span> is easier to estimate than
others. This is essentially because the variability
<span class="math inline">\(\text{var}\left\{F_n(t)\right\}=F(t)\left\{1-F(t)\right\}/n\)</span> approaches
<span class="math inline">\(0\)</span> when <span class="math inline">\(F(t)\)</span> approaches <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>, for any <span class="math inline">\(n\)</span>. We can clearly
observe this feature in the small simulation study represented by the
following figure, where the “band” enclosing the deviations (based on
100 Monte-Carlo simulations) gets narrower toward the boundaries of the
support of <span class="math inline">\(U\)</span>.</p>
<p><img src="/post/2022-12-07-weighted-residual-empirical-processes-in-semi-parametric-copula-adjusted-for-regression_files/weighted_Fn_02.png" /></p>
<p>Above: Plot of the deviation <span class="math inline">\(F_n-F\)</span>, for sample size <span class="math inline">\(n=50\)</span>,
based on 100 Monte-Carlo simulations. For simplicity we assumed
<span class="math inline">\(U\sim{Unif}(0,1)\)</span>, so <span class="math inline">\(F(t)=t\)</span> for <span class="math inline">\(t\in[0,1]\)</span>. The
deviation from each simulation is represented by a single red dashed
line. The 10% and 90% quantiles of the deviations at each <span class="math inline">\(t\)</span> value
are indicated by the two blue lines. Clearly, the “band” enclosing the
deviations gets narrower toward the boundaries of the support of
<span class="math inline">\(U\)</span>.]</p>
<p>This feature can also be characterized theoretically. For instance, we
can find the LIL for the weighted process <span class="math inline">\(\sqrt{n}(F_n -F)/\sqrt{F}\)</span>,
that is the classical process but now scaled by an additional standard
deviation factor <span class="math inline">\(1/\sqrt{F}\)</span>, from <span class="citation">Csáki (<a href="#ref-Csaki1977" role="doc-biblioref">1977</a>)</span>: for some <span class="math inline">\(0&lt;C&lt;\infty\)</span>,
<span class="math display">\[\begin{align*}
    \limsup_{n\rightarrow\infty} \sup_{F(t)\in(\frac{1}{n},\frac{1}{2}]} \left|\dfrac{\log\log\log(n)}{\log\log(n)} \sqrt{n}\dfrac{(F_n -F)(t)}{\sqrt{F(t)}}\right| = C.
\end{align*}\]</span> Compared to the LIL for the unweighted process earlier, we
can see that the weighted process is just slightly more difficult to
bound, but now <span class="math inline">\(\sqrt{n}(F_n -F)(t)\)</span> clearly enjoys a tighter bound
toward the boundaries of the support of <span class="math inline">\(U\)</span> due to the vanishing
<span class="math inline">\(\sqrt{F(t)}\)</span> there.</p>
<p>Such results on the weighted empirical processes can be generalized to
settings beyond the real line, for instance to sets in <span class="math inline">\(\mathbb{R}^p\)</span>
(<span class="citation">Alexander (<a href="#ref-Alexander1987" role="doc-biblioref">1987</a>)</span>) and sets of functions
(<span class="citation">Giné, Koltchinskii, and Wellner (<a href="#ref-GineKoltchinskiiJonWellner2003" role="doc-biblioref">2003</a>)</span>).</p>
<!-------------------------------------------------------------------------------->
</div>
<div id="results-for-residual-based-estimators" class="section level1">
<h1>Results for residual-based estimators</h1>
<p>For us, the idea of the weighted empirical processes will be applied to
the residual empirical processes, which will further culminate in our
eventual result on the residual-based estimator
<span class="math inline">\(\hat{\theta}^{\text{residual}}\)</span> for the copula parameter. We will first
consider the weighted residual empirical processes.</p>
<div id="results-on-weighted-residual-empirical-process" class="section level2">
<h2>Results on weighted residual empirical process</h2>
<p>Let <span class="math inline">\(f_k\)</span> be the density of <span class="math inline">\(E_k\)</span>, and <span class="math inline">\(\cal{T}_n\)</span> be the <span class="math inline">\(\sigma\)</span>-field
generated by the <span class="math inline">\((\mathbf{X}_i,\mathbf{Y}_i)\)</span>’s, <span class="math inline">\(i\in\{1,\dots,n\}\)</span>. The usual
decomposition of a residual empirical process is <span class="math display">\[\begin{align*}
    \hat{F}_{n,k}(t) - F_{n,k}(t) = f_k(t) \cdot \mathbb{E}\left[ (\hat{m}_k-m_k)(\mathbf{X}) | \cal{T}_n \right] + r_{1,k}(t)
\end{align*}\]</span> where <span class="math inline">\(r_{1,k}\)</span> is a remainder term that could be
controlled as follows:</p>
<div class="alert alert-info" role="alert">
<p>Theorem: Suppose that we can embed <span class="math inline">\(\hat{m}_k-m_k\)</span> into a function class
<span class="math inline">\(\cal{D}\)</span> with bracketing number
<span class="math inline">\(N_{[]}(\tau,\cal{D}) \lesssim (1/\tau)^\beta \exp(K (1/\tau)^{1/\nu})\)</span>
where <span class="math inline">\(\beta\)</span>, <span class="math inline">\(K\)</span> and <span class="math inline">\(\nu\)</span> are constants. Suppose that
<span class="math inline">\(\|\hat{m}_k-m_k\|_\infty=O_p(a_n)\)</span> (where <span class="math inline">\(\|\cdot\|_\infty\)</span> is the
supremum norm over the support of <span class="math inline">\(\mathbf{X}\)</span>). Then under mild regularity
conditions, <span class="math display">\[\begin{gather*}
        \sup_{t\in\ \mathbb{R}} \dfrac{|r_{1,k}(t)|}{ n^{-\frac{1}{2}} \left\{ f_k(t) \cdot a_n + a_n^2\right\}^{\frac{1}{2}(1-1/\nu)} + n^{-\frac{1}{1+1/\nu}} + a_n^2 } = O_p( 1 ) .
    \end{gather*}\]</span></p>
</div>
<p>Clearly, the convergence rate of the remainder term <span class="math inline">\(r_{1,k}\)</span> is
improved by both the rate of <span class="math inline">\(\|\hat{m}_k-m_k\|_\infty\)</span> and the weight
<span class="math inline">\(f_k\)</span>. The latter point is especially beneficial when <span class="math inline">\(f_k\)</span> is a density
“of the usual shape” that decays at its tails, which will allow the rate
of <span class="math inline">\(r_{1,k}\)</span> to be tightened accordingly (exactly similar to how the
rate of <span class="math inline">\(\sqrt{n}(F_n -F)(t)\)</span> is tightened by <span class="math inline">\(\sqrt{F(t)}\)</span> in the
weighted empirical processes on the real line that we reviewed earlier).
These features will tame the divergence of <span class="math inline">\(g_m^{[k]}\)</span> in Eq. (<a href="#mjx-eqn-eq4">4</a>).</p>
<p>Because what eventually form the ingredients of the residual-based
copula <span class="math inline">\(\hat{C}_n\)</span> are the <em>residual ranks</em> <span class="math inline">\(\hat{F}_{n,k}( \hat{E}_{i,k} )\)</span>’s, we need
to go one step further and consider the analogous results for them:</p>
<div class="alert alert-info" role="alert">
<p>Theorem: For all <span class="math inline">\(n\ge 1\)</span>, <span class="math inline">\(k\in\{1,\dots,p\}\)</span> and <span class="math inline">\(i\in\{1,\dots,n\}\)</span>,
<span class="math display">\[\begin{align}
        \label{eq5}\tag{5}
        \hat{F}_{n,k}( \hat{E}_{i,k} ) - F_{n,k}( E_{i,k} ) &amp;= -f_k(E_{i,k}) \left\{ (\hat{m}_k-m_k)(\mathbf{X}_i) - \mathbb{E}\left[ (\hat{m}_k-m_k)(\mathbf{X}) | \cal{T}_n \right] \right\} + r_{1,k}( \hat{E}_{i,k} ) + r_{2,k,i}
    \end{align}\]</span> where <span class="math inline">\(r_{1,k}\)</span> is as in the last theorem, and
<span class="math inline">\(r_{2,k,i}\)</span> is another remainder term that satisfies <span class="math display">\[\begin{align*}
        \max_{i\in\{1,\dots,n\}} \dfrac{ |r_{2,k,i}| }{ \log^{\frac{1}{2}}(n) n^{-\frac{1}{2}} \left\{ f_k(E_{i,k})\cdot a_n \right\}^{\frac{1}{2}} + a_n^2 } = O_p(1) .
    \end{align*}\]</span></p>
</div>
<p>Note that similar to <span class="math inline">\(r_{1,k}\)</span> earlier, the rate of <span class="math inline">\(r_{2,k,i}\)</span> also
enjoys the dependence on <span class="math inline">\(\|\hat{m}_k-m_k\|_\infty\)</span> and the weighing by
<span class="math inline">\(f_k\)</span>.</p>
</div>
<div id="results-on-residual-based-estimator-for-the-copula-parameter" class="section level2">
<h2>Results on residual-based estimator for the copula parameter</h2>
<p>We are now ready to plug in the decomposition of <span class="math inline">\(\hat{F}_{n,k}( \hat{E}_{i,k} ) - F_{n,k}( E_{i,k} )\)</span> in Eq. (<a href="#mjx-eqn-eq5">5</a>) in the last theorem into (<a href="#mjx-eqn-eq4">4</a>). The leading term in Eq. (<a href="#mjx-eqn-eq5">5</a>) (the one proportional to <span class="math inline">\(f_k\)</span>), which is centered, is now summed over <span class="math inline">\(i\in\{1,\dots,n\}\)</span> in (<a href="#mjx-eqn-eq4">4</a>) and so enjoys an additional <span class="math inline">\(n^{-1/2}\)</span>-scaling. the remainder terms are weighted and so tame the divergence of the scores <span class="math inline">\(g_m^{[k]}\)</span>.
Eventually, we arrive at the asymptotic equivalence between the
residual-based PLE and the oracle PLE:</p>
<div class="alert alert-info" role="alert">
<p>Theorem: Under the conditions of the last two theorems, and some
additional regularity conditions which in particular require, for
<span class="math inline">\(m\in\{1,\dots,d\}\)</span> and <span class="math inline">\(k\in\{1,\dots,p\}\)</span>, <span class="math display">\[\begin{align}
        \int \left\{g_m^{[k]}(\mathbf{u};\mathbf{\theta}^*) f_k(F_k^{\leftarrow}(u_k)) \right\}^2\mathrm{d}C(\mathbf{u}) &lt; \infty,
        \label{eq6}\tag{6}
    \end{align}\]</span> the asymptotic equivalence in Eq. (<a href="#mjx-eqn-eq3">3</a>) holds. Furthermore, <span class="math inline">\(\hat{\theta}^{\text{residual}} - \hat{\theta}^{\text{oracle}} = o_p(n^{-1/2})\)</span>.</p>
</div>
<p>Condition (<a href="#mjx-eqn-eq6">6</a>) in fact allows for quite non-trivial divergence of the score functions <span class="math inline">\(g_m\)</span> (it certainly accommodates the Gaussian and the <span class="math inline">\(t\)</span>-copulas). To apply the theorem
above, one still needs to verify the correct upper bound on the
bracketing number for embedding <span class="math inline">\(hat{m}_k-m_k\)</span>, which again turns out to
be non-restrictive. For instance, for partly linear regression
<span class="math inline">\(Y_k = \tilde{m}_k(\mathbf{x}) + \mathbf{\theta}_k^\top \mathbf{w} + E_k\)</span> with <span class="math inline">\(\mathbf{w}\in\mathbb{R}^{q_{\mathbf{L},n}}\)</span>,
we can allow the dimension <span class="math inline">\(q_{\mathbf{L},n}\)</span> of the linear covariate to grow up to
<span class="math inline">\(q_{\mathbf{L},n}=o(n^{1/4})\)</span>.</p>
<!-------------------------------------------------------------------------------->
</div>
</div>
<div id="bibliography" class="section level1 unnumbered">
<h1>Bibliography</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Alexander1987" class="csl-entry">
Alexander, Kenneth S. 1987. <span>“Rates of Growth and Sample Moduli for Weighted Empirical Processes Indexed by Sets.”</span> <em>Probability Theory and Related Fields</em> 75 (3): 379–423.
</div>
<div id="ref-BerghausBucherVolgushev2017" class="csl-entry">
Berghaus, Betina, Axel Bücher, and Stanislav Volgushev. 2017. <span>“Weak Convergence of the Empirical Copula Process with Respect to Weighted Metrics.”</span> <em>Bernoulli</em> 23 (1): 743–72.
</div>
<div id="ref-ChenHuangYi2021" class="csl-entry">
Chen, Xiaohong, Zhuo Huang, and Yanping Yi. 2021. <span>“Efficient Estimation of Multivariate Semi-Nonparametric <span>GARCH</span> Filtered Copula Models.”</span> <em>Journal of Econometrics</em> 222 (1): 484–501.
</div>
<div id="ref-Csaki1977" class="csl-entry">
Csáki, E. 1977. <span>“The Law of the Iterated Logarithm for Normalized Empirical Distribution Function.”</span> <em>Zeitschrift Für Wahrscheinlichkeitstheorie Und Verwandte Gebiete</em> 238 (2): 147–67.
</div>
<div id="ref-FermanianRadulovicWegkamp2004" class="csl-entry">
Fermanian, Jean-David, Dragan Radulović, and Marten Wegkamp. 2004. <span>“Weak Convergence of Empirical Copula Processes.”</span> <em>Bernoulli</em> 10 (5): 847–60.
</div>
<div id="ref-GenestGhoudiRivest1995" class="csl-entry">
Genest, Christian, ‪Kilani Ghoudi‬, and ‪Louis-Paul Rivest. 1995. <span>“A Semiparametric Estimation Procedure of Dependence Parameters in Multivariate Families of Distributions.”</span> <em>Biometrika</em> 82: 543–52.
</div>
<div id="ref-GijbelsOmelkaVeraverbeke2015" class="csl-entry">
Gijbels, Irène, Marek Omelka, and Noël Veraverbeke. 2015. <span>“Estimation of a Copula When a Covariate Affects Only Marginal Distributions.”</span> <em>Scandinavian Journal of Statistics</em> 42 (4): 1109–26.
</div>
<div id="ref-GineKoltchinskiiJonWellner2003" class="csl-entry">
Giné, Evarist, Vladimir Koltchinskii, and Jon. A. Wellner. 2003. <span>“Ratio Limit Theorems for Empirical Processes.”</span> In <em>Stochastic Inequalities and Applications</em>, edited by Evarist Giné, Christian Houdré, and David Nualart, 249–78. Birkhäuser.
</div>
<div id="ref-Nelsen1999" class="csl-entry">
Nelsen, Roger B. 2006. <em>An <span>I</span>ntroduction to <span>C</span>opulas</em>. 2nd ed. New York: Springer.
</div>
<div id="ref-NeumeyerOmelkaHudecova2019" class="csl-entry">
Neumeyer, Natalie, Marek Omelka, and Šárka Hudecová. 2019. <span>“A Copula Approach for Dependence Modeling in Multivariate Nonparametric Time Series.”</span> <em>Journal of Multivariate Analysis</em> 171: 139–62.
</div>
<div id="ref-Oakes1994" class="csl-entry">
Oakes, David. 1994. <span>“Multivariate Survival Distributions.”</span> <em>Journal of Nonparametric Statistics</em> 3 (3–4): 343–54.
</div>
<div id="ref-RWZ2017" class="csl-entry">
Radulović, Dragan, Marten Wegkamp, and Yue Zhao. 2017. <span>“Weak Convergence of Empirical Copula Processes Indexed by Functions.”</span> <em>Bernoulli</em> 23 (4): 3346–84.
</div>
<div id="ref-SVW2014" class="csl-entry">
Segers, Johan, Ramon van den Akker, and Bas J. M. Werker. 2014. <span>“Semiparametric <span>G</span>aussian Copula Models: Geometry and Efficient Rank-Based Estimation.”</span> <em>The Annals of Statistics</em> 42 (5): 1911–40.
</div>
<div id="ref-Sklar1959" class="csl-entry">
Sklar, Abe. 1959. <span>“Fonctions de Répartition à n Dimensions Et Leurs Marges.”</span> <em>Publications de l’Institut de Statistique de L’Université de Paris</em> 8: 229–31.
</div>
<div id="ref-SunFreesRosenberg2008" class="csl-entry">
Sun, Jiafeng, Edward W. Frees, and Marjorie A. Rosenberg. 2008. <span>“Heavy-Tailed Longitudinal Data Modeling Using Copulas.”</span> <em>Insurance: Mathematics and Economics</em> 42 (2): 817–30.
</div>
<div id="ref-ZhaoGijbelsKeilegom2022" class="csl-entry">
Zhao, Yue, Irène Gijbels, and Ingrid Van Keilegom. 2022. <span>“<span class="nocase">Parametric copula adjusted for non- and semiparametric regression</span>.”</span> <em>The Annals of Statistics</em> 50 (2): 754–80.
</div>
</div>
</div>

</main>


















</article>
</div>

<script async src="//yihui.name/js/center-img.js"></script>
 


<script src="//yihui.name/js/math-code.js"></script>
<script async src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>






</body>
</html>

