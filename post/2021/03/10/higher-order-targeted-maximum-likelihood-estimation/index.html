<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>Higher Order Targeted Maximum Likelihood Estimation | YoungStatS</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <link href="//cdn.bootcss.com/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">

  </head>

  <body class="page">
    <nav class="header">
      <div class="banner">
<a href="/" class="text">
<div class="yourfancytitle"> YoungStatS </div> 
</a>
</div>

      <div class="head-meta">
      
        <span><a href="/">&larr; Back to Home</a></span>
       <span>The blog of Young Statisticians Europe (YSE)</span>
        
        
        
          
        
        
        
        
      
      </div>
    </nav>
    
    <footer>
<script src="//yihui.org/js/math-code.js"></script>
<script async
src="cdnjs.cloudflare.com/ajax/libs/mathjax/3.1.0?config=TeX-MML-AM_CHTML">
</script>
<div class="footer">
  <ul class="menu">
    
    <li><a href="/"><span data-hover="Home">Home</span></a></li>
    
    <li><a href="/categories/webinars"><span data-hover="Webinars">Webinars</span></a></li>
    
    <li><a href="/about/"><span data-hover="About">About</span></a></li>
    
    <li><a href="/categories/"><span data-hover="Categories">Categories</span></a></li>
    
  </ul>
  
</div>
</footer>

<div class="container">
<article>
<div class="article-meta">

  <div class="categories">
  
    <a href="/categories/machine-learning">machine-learning</a>
  
  </div>

  <h1><span class="title">Higher Order Targeted Maximum Likelihood Estimation</span></h1>

  
  <h3 class="author">Zeyi Wang and Mark van der Laan
</h3>
  

  
  

</div>



<main>

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p><strong>Summary</strong></p>
<p>We propose a higher order targeted maximum likelihood estimation (TMLE) that only relies on a sequentially and recursively defined set of data-adaptive fluctuations. Without the need to assume the often too stringent higher order pathwise differentiability, the method is practical for implementation and has the potential to be fully computerized.</p>
<div id="background" class="section level1">
<h1>Background</h1>
<div id="targeted-maximum-likelihood-estimation-tmle" class="section level2">
<h2>Targeted Maximum Likelihood Estimation (TMLE)</h2>
<p>It has been particularly of interest for semiparametric theories and real world practices to make efficient and substitution-based estimation for target quantities that are functions of data distribution. TMLE <span class="citation">(<a href="#ref-van2006targeted" role="doc-biblioref">van der Laan and Rubin 2006</a>; <a href="#ref-van2011targeted" role="doc-biblioref">van der Laan and Rose 2011</a>, <a href="#ref-van2018targeted" role="doc-biblioref">2018</a>)</span> provides a framework to construct such estimators and incorporates machine learning into efficient estimation and inference. Here we briefly review the regular first order TMLE.</p>
<p>Suppose that the true distribution <span class="math inline">\(P_0\)</span> lies in a statistical model <span class="math inline">\(\mathcal{M}\)</span>. Start with an initial distribution estimator <span class="math inline">\(P_n^0\)</span>. Given pathwise differentiability of the target <span class="math inline">\(\Psi(P)\)</span> at <span class="math inline">\(P\)</span> with a canonical gradient <span class="math inline">\(D^{(1)}_P\)</span>, consider a least favorable path <span class="math inline">\(\{ \tilde P^{(1)}(P, \epsilon) \}\)</span> through <span class="math inline">\(P\)</span> at <span class="math inline">\(\epsilon=0\)</span>, where scores at <span class="math inline">\(\epsilon=0\)</span> span the efficient influence curve (EIC) <span class="math inline">\(D_{P}^{(1)}\)</span>. Define the TMLE update by maximizing the likelihood along the path, that is, <span class="math inline">\(\epsilon_n^{(1)} = \mathrm{argmin}_\epsilon P_n L(\tilde P^{(1)}(P_n^0, \epsilon) )\)</span>, where <span class="math inline">\(L(P) = - \log p\)</span>. The resulted TMLE update is <span class="math inline">\(P_n^* = \tilde P_n^{(1)} (P_n^0) = \tilde P_n^{(1)} (P_n^0, \epsilon_n^{(1)})\)</span>.</p>
<p>Define <span class="math inline">\(R^{(1)}(P, P_0) = \Psi(P) - \Psi(P_0) + P_0 D_P^{(1)}\)</span> as the exact remainder. Then the TMLE satisfies <span class="math inline">\(P_n D_{P_n^*}^{(1)}\approx 0\)</span> and the following exact expansion
<span class="math display">\[
\Psi(P_n^*) - \Psi(P_0) = R^{(1)}(P_n^*, P_0) - P_0 D_{P_n^*}^{(1)} = (P_n - P_0) D_{P_0}^{(1)} + (P_n - P_0) (D_{P_n^*}^{(1)} - D_{P_0}^{(1)}) - P_n D^{(1)}_{P_n^*} + R^{(1)}(P_n^*, P_0).
\]</span>
Asymptotic efficiency for <span class="math inline">\(P_n^*\)</span> requires:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\{D_{P}^{(1)}: P\in\mathcal{M}\}\)</span> is a <span class="math inline">\(P_0\)</span>-Donsker class (often satisfied, or skipped with sample splitting),</li>
<li>Solving the equation <span class="math inline">\(P_n D^{(1)}_{P_n^*}=0\)</span> exactly or to an <span class="math inline">\(o_P(n^{-1/2})\)</span> term,</li>
<li><span class="math inline">\(R^{(1)}(P_n^*, P_0)\)</span> being exactly zero or up to an <span class="math inline">\(o_P(n^{-1/2})\)</span> term.</li>
</ol>
<p><span class="math inline">\(R^{(1)}(P, P_0)\)</span> is often a second order difference in <span class="math inline">\(p\)</span> and <span class="math inline">\(p_0\)</span>. For example, when it consists of cross products, doubly or multiply robustness may exist.</p>
</div>
<div id="highly-adaptive-lasso-hal" class="section level2">
<h2>Highly Adaptive Lasso (HAL)</h2>
<p>HAL <span class="citation">(<a href="#ref-van2015generally" role="doc-biblioref">van der Laan 2015</a>, <a href="#ref-van2017generally" role="doc-biblioref">2017</a>; <a href="#ref-benkeser2016highly" role="doc-biblioref">Benkeser and van ver Laan 2016</a>)</span> is a nonparametric maximum likelihood estimator that converges in Kullback-Leibler dissimilarity at a minimal rate of <span class="math inline">\(n^{-2/3}(\log~n)^d\)</span>, even when the parameter space only assumes cadlag and finite variation norms.
This generally bounds the exact remainder, and immediately makes the TMLE that uses HAL as an initial asymptotically efficient. However, in finite samples, the second order remainder can still dominate the sampling distribution.</p>
<p>Another important property of HAL is itself being a nonparametric MLE, so it can solve a large class of score equations to best approximates the desired score via increasing the <span class="math inline">\(L_1\)</span>-norm of the HAL-MLE (called undersmoothing) <span class="citation">(<a href="#ref-vanderLaan&amp;Benkeser&amp;Cai19" role="doc-biblioref">van der Laan, Benkeser, and Cai 2019a</a>, <a href="#ref-van2019efficient" role="doc-biblioref">2019b</a>)</span>.</p>
</div>
</div>
<div id="higher-order-fluctuations-with-hal-mle" class="section level1">
<h1>Higher Order Fluctuations with HAL-MLE</h1>
<p>Replace <span class="math inline">\(P_n^0\)</span> in the first order TMLE by a TMLE <span class="math inline">\(\tilde P^{(2)}_n(P_n^0)\)</span> of <span class="math inline">\(\Psi_n^{(1)}(P_0) = \Psi(\tilde P^{(1)}_n(P_0)) = \Psi(\tilde P^{(1)}(P_0, \epsilon_n^{(1)}(P_0)))\)</span>, which is a data-adaptive fluctuation of the original target parameter <span class="math inline">\(\Psi(P_0)\)</span>. Then the final update of a second order TMLE, <span class="math inline">\(\tilde P^{(1)}_n \tilde P^{(2)}_n(P_n^0)\)</span>, is just a first order TMLE that uses as the initial estimator a <span class="math inline">\(\tilde P^{(2)}(P_n^0)\)</span> that is fully tailored for <span class="math inline">\(\Psi_n^{(1)}(P_0)\)</span>.</p>
<p><code>{r 2nd, echo=FALSE, fig.align = 'center', out.width = "85%", fig.cap = "Left panel: regular TMLE. Right panel: second order TMLE. The horizontal axes represent the original target. The vertical axis represents the data-adaptive fluctuation. The second order TMLE searches for a better initial estimator for a regular TMLE. "}</code></p>
<p><img src="/post/2021-03-10-higher-order-targeted-maximum-likelihood-estimation_files/figure_2nd_order.jpg" /></p>
<p>Similarly if we iterate this process, and let <span class="math inline">\(\tilde P^{(k+1)}_n(P_n^0)\)</span> be a regular TMLE tailored for a higher order fluctuation <span class="math inline">\(\Psi_n^{(k)}(P_0) = \Psi^{(k-1)}_n(\tilde P^{(k)}_n(P_0))=\Psi(\tilde P^{(1)}_n \tilde P^{(2)}_n\cdots \tilde P^{(k)}_n(P_n^0))\)</span> for <span class="math inline">\(k=1, \ldots\)</span>, then the final update of a <span class="math inline">\(k+1\)</span>-th order TMLE is <span class="math inline">\(\tilde P^{(1)}_n \tilde P^{(2)}_n\cdots \tilde P^{(k+1)}_n(P_n^0)\)</span>.</p>
<p>The second order TMLE relies on pathwise differentiability of <span class="math inline">\(\Psi_n^{(1)}\)</span>. However, <span class="math inline">\(\Psi_n^{(1)}(P) = \Psi(\tilde P^{(1)}_n(P)) = \Psi(\tilde P^{(1)}(P, \epsilon_n^{(1)}(P)))\)</span>is smooth in <span class="math inline">\(P\)</span> up till the dependence of <span class="math inline">\(\epsilon_n^{(1)}(P) = \mathrm{argmax}_\epsilon P_n \log \tilde p_n^{(1)}(p, \epsilon)\)</span> on <span class="math inline">\(P\)</span>, because <span class="math inline">\(P_n\)</span> is not absolutely continuous w.r.t. <span class="math inline">\(P\)</span> for most <span class="math inline">\(P\)</span> that can occur as an initial or a higher order TMLE-update. This calls for the use of smooth distribution estimators such as HAL-MLE <span class="math inline">\(\tilde{P}_n\)</span> in replacement of the empirical <span class="math inline">\(P_n\)</span>, since <span class="math inline">\(d\tilde P_n/dP\)</span> will exist for all <span class="math inline">\(P\)</span> that can occur as an initial or higher order updates, which ensures pathwise differentiability of <span class="math inline">\(\Psi^{(1)}_n(P_0)\)</span> and the existence of its canonical gradient <span class="math inline">\(D^{(2)}_{n, P}\)</span>.</p>
<p>In general, suppose that <span class="math inline">\(\{\tilde P^{(k)}_n(P, \epsilon)\}\)</span> is a least favorable path through <span class="math inline">\(P\)</span> at <span class="math inline">\(\epsilon=0\)</span>, whose scores at <span class="math inline">\(\epsilon=0\)</span> span <span class="math inline">\(D^{(k)}_{n, P}\)</span>. And the update step is also replaced by optimizing the <span class="math inline">\(\tilde P_n\)</span>-regularized loss, that is, <span class="math inline">\(\epsilon_n^{(k)} = \mathrm{argmin}_\epsilon \tilde P_n L(\tilde P^{(k)}(P_n^0, \epsilon) )\)</span>, which solves <span class="math inline">\(\tilde P_n D^{(k)}_{n, P}=0\)</span> at <span class="math inline">\(P = \tilde P^{(k)}_n(P_n^0)= \tilde P^{(k)}_n(P_n^0, \epsilon_n^{(k)})\)</span>.</p>
<p>A <span class="math inline">\(k+1\)</span>-th order TMLE by its design searches for a better initial estimator given the <span class="math inline">\(k\)</span>-th order TMLE. Specifically, the <span class="math inline">\(k+1\)</span>-th order TMLE moves in the same direction as the steepest descent algorithm for minimizing the <span class="math inline">\(k\)</span>-th exact total remainder that is the discrepancy between <span class="math inline">\(\Psi(\tilde P^{(1)}_n \tilde P^{(2)}_n\cdots \tilde P^{(k)}_n(P)) - \Psi(\tilde P^{(1)}_n \tilde P^{(2)}_n\cdots \tilde P^{(k)}_n(P_0))\)</span> and <span class="math inline">\(\tilde{P}_n D^{(k)}_{n, P_0}\)</span>. Moreover, compared to an oracle steepest descent algorithm, TMLE stops the moment the log-likelihood is not improving anymore, which corresponds exactly to when the TMLE cannot know in what direction a steepest descent algorithm would go. This avoids potential overfitting and ensures a local minimum in close neighborhood of the desired (but unknown) minimum <span class="math inline">\(P_0\)</span>.</p>
</div>
<div id="exact-expansions-of-higher-order-tmle" class="section level1">
<h1>Exact Expansions of Higher Order TMLE</h1>
<p>Denote the <span class="math inline">\(k\)</span>-th exact remainder as the exact remainder of <span class="math inline">\(\tilde P^{(k)}_n(P)\)</span> for the fluctuation <span class="math inline">\(\Psi^{(k-1)}(P_0) = \Psi(\tilde P_n^{(1)}\cdots\tilde P_n^{(k-1)}(P_0))\)</span>:</p>
<p><span class="math display">\[\begin{align*}
R^{(k)}_n(\tilde P^{(k)}_n(P), P_0)
= &amp; \Psi^{(k-1)}(\tilde P^{(k)}_n(P)) - \Psi^{(k-1)}(P_0) + P_0 D^{(k)}_{n, \tilde P^{(k)}_n(P)} \\
= &amp; \Psi(\tilde P^{(1)}\cdots\tilde P^{(k)}_n(P)) - \Psi(\tilde P^{(1)}\cdots\tilde P^{(k-1)}(P_0)) + P_0 D^{(k)}_{n, \tilde P^{(k)}_n(P)}. 
\end{align*}\]</span></p>
<p>Then we have the exact expansion for the <span class="math inline">\(k\)</span>-th order TMLE,</p>
<p><span class="math display">\[\begin{align*}
\Psi(\tilde P^{(1)}_n\cdots\tilde P^{(k)}_n(P)) - \Psi(P_0)
= &amp; \sum_{j=1}^{k-1} (P_n-P_0)D^{(j)}_{n, \tilde P^{(j)}_n(P_0)} + R^{(j)}_n(\tilde P_n^{(j)}(P_0), P_0) \\
&amp; + (P_n-P_0)D^{(k)}_{n, \tilde P^{(k)}_n}(P_n^0) + R^{(k)}_n(\tilde P_n^{(k)}(P_n^0), P_0) \\
&amp; - \sum_{j=1}^{k-1} P_n D^{(j)}_{n, \tilde P^{(j)}_n(P_0)} - P_n D^{(k)}_{n, \tilde P^{(k)}_n(P_n^0)}, 
\end{align*}\]</span></p>
<p>which still holds if we replace <span class="math inline">\(P_n\)</span> with <span class="math inline">\(\tilde P_n\)</span>. This can be further derived as</p>
<p><span class="math display">\[\begin{align*}
\Psi(\tilde P^{(1)}\cdots\tilde P^{(k)}_n(P)) - \Psi(P_0)
=&amp;\sum_{j=1}^{k}\left\{ (\tilde{P}_n-P_0)D^{(j)}_{n,\tilde{P}_n^{(j)}(P_0)}+R_n^{(j)}(\tilde{P}_n^{(j)}(P_0),P_0)\right\} \\
%&amp;&amp;+(P_n-P_0)D^{(k)}_{n,P_n^{(k)}(P_0)}\\
&amp; +R_n^{(k)}(\tilde{P}_n^{(k)}(P_n^0),\tilde{P}_n)-R_n^{(k)}(\tilde{P}_n^{(k)}(P_0),\tilde{P}_n)\\
&amp;-\sum_{j=1}^{k}\tilde{P}_n D^{(j)}_{n,\tilde{P}_n^{(j)}(P_0)}.\end{align*}\]</span></p>
<p>The followings can be shown:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\((\tilde{P}_n-P_0)D^{(j)}_{n,\tilde{P}_n^{(j)}(P_0)}, j=1, \dots, k,\)</span> are generalized <span class="math inline">\(j\)</span>-th order difference in <span class="math inline">\(P_0\)</span> and <span class="math inline">\(\tilde P_n\)</span>, which resemble the performance of higher order <span class="math inline">\(U\)</span>-statistics;</li>
<li><span class="math inline">\(R_n^{(j)}(\tilde{P}_n^{(j)}(P_0),P_0) = O_P(n^{-1})\)</span> given <span class="math inline">\((\tilde P_n - P_n) D_{n, P_0}^{(j)} = O_P(n^{-1/2})\)</span>, which can be achieved by undersmothing HAL;</li>
<li><span class="math inline">\(R_n^{(k)}(\tilde{P}_n^{(k)}(P),\tilde P_n)\)</span> is a generalized <span class="math inline">\(k+1\)</span>-th order difference in <span class="math inline">\(P\)</span> and <span class="math inline">\(\tilde P_n\)</span>, and hence <span class="math inline">\(R_n^{(k)}(\tilde{P}_n^{(k)}(P_n^0),\tilde P_n) - R_n^{(k)}(\tilde{P}_n^{(k)}(P_0),\tilde P_n)=o_P(n^{-1/2})\)</span> so long as <span class="math inline">\(\lVert \tilde p_n - p_0 \rVert = o_P(n^{1/2(k+1)})\)</span> and <span class="math inline">\(\lVert p_n^0 - p_0 \rVert = o_P(n^{1/2(k+1)})\)</span>;<br />
</li>
<li>The last term can be exactly <span class="math inline">\(0\)</span> by defining <span class="math inline">\(\epsilon_n^{(j)}(P)\)</span> as a solution of the corresponding efficient score equation <span class="math inline">\(\tilde P_n D^{(j)}_{n, \tilde P^{(j)}_n(P, \epsilon)}=0\)</span>.</li>
</ol>
</div>
<div id="higher-order-inference" class="section level1">
<h1>Higher Order Inference</h1>
<p>For the sake of statistical inference, we will need that <span class="math inline">\((\tilde P_n - P_n)D^{(1)}_{\tilde P_n^{(1)}(P_0)} = o_P(n^{-1/2})\)</span>, and probably even <span class="math inline">\((\tilde P_n - P_n)D^{(j)}_{\tilde P_n^{(j)}(P_0)} = o_P(n^{-1/2})\)</span> for <span class="math inline">\(j = 2, \dots, k\)</span>. It can be shown that this essentially comes down to controlling <span class="math inline">\((\tilde P_n - P_n) D^{(1)}_{P_0}\)</span>, which again can be achieved by undersmoothing HAL.</p>
<p>Let
<span class="math display">\[
\bar D_n^k = \sum_{j=1}^k D^{(j)}_{n, \tilde P^{(j)}_n\cdots\tilde P^{(k)}_n(P_n^0)}
\]</span>
which is an estimate of the influence curve <span class="math inline">\(\bar D_{n, P_0}^k = \sum_{j=1}^k D^{(j)}_{n, \tilde P^{(j)}_n(P_0)}\)</span>. Note that for <span class="math inline">\(j&gt;1\)</span> the terms are higher order differences, so that <span class="math inline">\(\bar D_n^k\)</span> will converge to the efficient influence curve <span class="math inline">\(D^{(1)}_{P_0}\)</span>.</p>
<p>Let
<span class="math display">\[
\sigma_n^2 = \frac{1}{n}\sum_{i=1}^n \bar D_n^k(O_i)^2
\]</span>
be the sample variance of this estimated influence curve. A corresponding 0.95 confidence interval is given by
<span class="math display">\[
\Psi(P^{(1)}_n\cdots\tilde P^{(k)}_n(P_n^0)) \pm 1.96 \sigma_n/n^{1/2}.
\]</span></p>
</div>
<div id="simulation" class="section level1">
<h1>Simulation</h1>
<p>The first example demonstrates the impact of second order TMLE steps during a process of estimating the average density. The exact total remainder <span class="math inline">\(\bar R^{(1)}(\tilde P_n^{(1)}(P), P_0)\)</span> of first order TMLE is controlled due to the second order updates <span class="math inline">\(P = P_n^0 \mapsto \tilde P_n^{(2)}(P_n^0)\)</span>.</p>
<p><code>{r remainder, echo=FALSE, fig.align = 'center', out.width = "65%"}</code>
<img src="/post/2021-03-10-higher-order-targeted-maximum-likelihood-estimation_files/middle_remainder_ZW-20210114.jpg" /></p>
<p>Below it plots the simulated bias and bias/SD ratio at <span class="math inline">\(n=500\)</span> when we increase the bias in the initial estimator <span class="math inline">\(P_n^0\)</span> by adding a bias mass to each of the support points of the empirical pmf. Second order TMLE provides improved accuracy in both estimation and inference over first order TMLE following likelihood guidance.</p>
<p><code>{r combine, echo=FALSE, fig.align = 'center', out.width = "85%"}</code></p>
<p><img src="/post/2021-03-10-higher-order-targeted-maximum-likelihood-estimation_files/combine_500-20210115.png" /></p>
<p>Lastly, we show an example of estimating average treatment effects (ATEs) while the initial estimator for propensity scores is <span class="math inline">\(n^{-1/4}\)</span>-consistent while that for outcome models is not. The first order TMLE should have <span class="math inline">\(n^{1/2}\)</span>-scaled bias that increases with <span class="math inline">\(n\)</span> while the second order TMLE has a <span class="math inline">\(n^{1/2}\)</span>-bias that should be constant in <span class="math inline">\(n\)</span>. The table below shows that the second order TMLE has a negligible bias and thereby still provides valid inference.</p>
<table>
<thead>
<tr class="header">
<th>n</th>
<th>bias 1-st</th>
<th>bias 2-nd</th>
<th>se 1-st</th>
<th>se 2-nd</th>
<th>mse 1-st</th>
<th>mse 2-nd</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>400</td>
<td>-0.720</td>
<td>0.078</td>
<td>0.815</td>
<td>1.175</td>
<td>1.087</td>
<td>1.178</td>
</tr>
<tr class="even">
<td>750</td>
<td>-0.996</td>
<td>0.029</td>
<td>0.800</td>
<td>1.102</td>
<td>1.278</td>
<td>1.102</td>
</tr>
<tr class="odd">
<td>1000</td>
<td>-1.258</td>
<td>-0.062</td>
<td>0.786</td>
<td>1.066</td>
<td>1.483</td>
<td>1.068</td>
</tr>
<tr class="even">
<td>1200</td>
<td>-1.345</td>
<td>0.022</td>
<td>0.809</td>
<td>1.028</td>
<td>1.570</td>
<td>1.028</td>
</tr>
<tr class="odd">
<td>1600</td>
<td>-1.549</td>
<td>-0.019</td>
<td>0.818</td>
<td>1.055</td>
<td>1.752</td>
<td>1.055</td>
</tr>
<tr class="even">
<td>2500</td>
<td>-2.066</td>
<td>-0.094</td>
<td>0.819</td>
<td>0.999</td>
<td>2.222</td>
<td>1.003</td>
</tr>
</tbody>
</table>
</div>
<div id="discussions" class="section level1">
<h1>Discussions</h1>
<p>Although HAL-MLE-based fluctuations are fundamental to higher order TMLE, the update steps in practice can be based on empirical losses. Note that the <span class="math inline">\(j-1\)</span>-th fluctuation <span class="math inline">\(\Psi(\tilde P_n^{(1)}\cdots\tilde P_n^{(j-1)}(P_0))\)</span>, <span class="math inline">\(j = 0, \dots, k-1\)</span>, is nothing but a pathwise differentiable parameter with a known canonical gradient, <span class="math inline">\(D^{(j)}_{n, P}\)</span>. For jointly targeting this sequence of <span class="math inline">\(k\)</span> parameters, one can solve the empirical <span class="math inline">\(P_n\)</span>-regularized efficient score equations (where the scores still involve HAL-MLEs). As we showed in the technical report, this preserves the exact expansion and even leads to an improved undersmoothing term, and therefore is the recommended implementation. At <span class="math inline">\(k=1\)</span>, this exactly coincides with the regular first order TMLE.</p>
<p><code>{r targets, echo=FALSE, fig.align = 'center', out.width = "85%", fig.cap = "Jointly consider the sequence of data-adaptive fluctuations. "}</code></p>
<p><img src="/post/2021-03-10-higher-order-targeted-maximum-likelihood-estimation_files/figure_targets.jpg" /></p>
<p>An important next step is the (automated) computation of the first and higher order canonical gradients with least squares regression or symmetric matrix inversion <span class="citation">(<a href="#ref-van2021higher" role="doc-biblioref">van der Laan, Wang, and van der Laan 2021</a>)</span>, thereby opening up the computation of higher order TMLEs with standard machinery, avoiding delicate analytics needed to determine closed forms.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-benkeser2016highly" class="csl-entry">
Benkeser, David, and Mark J van ver Laan. 2016. <span>“The Highly Adaptive Lasso Estimator.”</span> In <em>2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA)</em>, 689–96. IEEE.
</div>
<div id="ref-van2015generally" class="csl-entry">
van der Laan, Mark J. 2015. <span>“A Generally Efficient Targeted Minimum Loss Based Estimator.”</span>
</div>
<div id="ref-van2017generally" class="csl-entry">
———. 2017. <span>“A Generally Efficient Targeted Minimum Loss Based Estimator Based on the Highly Adaptive Lasso.”</span> <em>The International Journal of Biostatistics</em> 13 (2).
</div>
<div id="ref-vanderLaan&amp;Benkeser&amp;Cai19" class="csl-entry">
van der Laan, Mark J, David Benkeser, and Weixin Cai. 2019a. <span>“Causal Inference Based on Undersmoothing the Highly Adaptive Lasso.”</span>
</div>
<div id="ref-van2019efficient" class="csl-entry">
———. 2019b. <span>“Efficient Estimation of Pathwise Differentiable Target Parameters with the Undersmoothed Highly Adaptive Lasso.”</span> <em>arXiv Preprint arXiv:1908.05607</em>.
</div>
<div id="ref-van2011targeted" class="csl-entry">
van der Laan, Mark J, and Sherri Rose. 2011. <em>Targeted Learning: Causal Inference for Observational and Experimental Data</em>. Springer Science &amp; Business Media.
</div>
<div id="ref-van2018targeted" class="csl-entry">
———. 2018. <em>Targeted Learning in Data Science</em>. Springer.
</div>
<div id="ref-van2006targeted" class="csl-entry">
van der Laan, Mark J, and Daniel Rubin. 2006. <span>“Targeted Maximum Likelihood Learning.”</span> <em>The International Journal of Biostatistics</em> 2 (1).
</div>
<div id="ref-van2021higher" class="csl-entry">
van der Laan, Mark J, Zeyi Wang, and Lars van der Laan. 2021. <span>“Higher Order Targeted Maximum Likelihood Estimation.”</span> <em>arXiv Preprint arXiv:2101.06290</em>.
</div>
</div>
</div>

</main>


















</article>
</div>

<script async src="//yihui.name/js/center-img.js"></script>
 


<script src="//yihui.name/js/math-code.js"></script>
<script async src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>






</body>
</html>

