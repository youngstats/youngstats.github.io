<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=4321&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>Extrapolation to unseen domains: from theory to applications | YoungStatS</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <link href="//cdn.bootcss.com/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">

  </head>

  <body class="page">
    <nav class="header">
      <div class="banner">
<a href="/" class="text">
<div class="yourfancytitle"> YoungStatS </div> 
</a>
</div>

      <div class="head-meta">
      
        <span><a href="/">&larr; Back to Home</a></span>
       <span>The blog of Young Statisticians Europe (YSE)</span>
        
        
        
          
        
        
        
        
      
      </div>
    </nav>
    
    <footer>
<script src="//yihui.org/js/math-code.js"></script>
<script async
src="cdnjs.cloudflare.com/ajax/libs/mathjax/3.1.0?config=TeX-MML-AM_CHTML">
</script>
<div class="footer">
  <ul class="menu">
    
    <li><a href="/"><span data-hover="Home">Home</span></a></li>
    
    <li><a href="/categories/webinars"><span data-hover="Webinars">Webinars</span></a></li>
    
    <li><a href="/about/"><span data-hover="About">About</span></a></li>
    
    <li><a href="/categories/"><span data-hover="Categories">Categories</span></a></li>
    
  </ul>
  
</div>
</footer>

<div class="container">
<article>
<div class="article-meta">

  <div class="categories">
  
    <a href="/categories/webinars">webinars</a>
  
     &hercon; <a href="/categories/data-science">data-science</a>
  
     &hercon; <a href="/categories/machine-learning">machine-learning</a>
  
  </div>

  <h1><span class="title">Extrapolation to unseen domains: from theory to applications</span></h1>

  

  
  

</div>



<main>



<p><strong>Extrapolation to unseen domains: from theory to applications</strong></p>
<p>Monday, April 22nd, 2024, 8:00 PT / 11:00 ET / 17:00 CET</p>
<p><img src="/post/2024-04-05-extrapolation-generalization-to-novel-domains-in-data-science_files/noname-crop.png" /></p>
<p>3rd joint webinar of the <a href="https://imstat.org/ims-groups/ims-new-researchers-group/">IMS New Researchers Group</a>, <a href="https://math.ethz.ch/sfs/news-and-events/young-data-science.html">Young Data Science Researcher Seminar Zürich</a> and the YoungStatS Project.</p>
<p>When &amp; Where:</p>
<ul>
<li>Monday, April 22nd, 2024, 8:00 PT / 11:00 ET / 17:00 CET</li>
<li>Online, via <a href="https://washington.zoom.us/j/92385046970">Zoom</a>. The registration form is available
<a href="https://docs.google.com/forms/d/e/1FAIpQLSdJ_BYx3B4mkeduSutoS5fsLjjm9bGGxzUWwOdcZeHoOUWWaA/viewform">here</a>.</li>
</ul>
<p>Speakers:</p>
<ul>
<li><a href="https://msimchowitz.github.io/">Max Simchowitz</a>, Robot Locomotion Group, MIT: “<em>Statistical Learning under Heterogeneous Distribution Shift</em>”</li>
</ul>
<p>Abstract: What makes a trained predictor, e.g. neural network, more or less susceptible to
performance degradation under distribution shift? Spurious correlation, lack of diversity in
the training data, and brittleness of the trained model are all possible culprits. In this talk, we will investigate a less well-studied factor: that of the statistical complexity of the individual features themselves. We will show that, for a very general class of predictors with a certain additive structure, empirical risk minimization is less sensitive to distribution shifts in “simple features” than “complex” ones, where simplicity/complexity are measured in terms of natural statistical quantities. We demonstrate that this arises because standard ERM learns the dependence on the “simpler” feature more quickly, whilst avoiding the risk of overfitting to more “complex” features. We will conclude by drawing connections to the orthogonal machine learning literature, and validating our theory on various experimental domains (even those in which the additivity assumption fails to hold).</p>
<ul>
<li><a href="https://lotfollahi.com/">Mohammad Lotfollahi</a>, Wellcome Sanger Institute, University of Cambridge: “<em>Generative machine learning to model cellular perturbations</em>”</li>
</ul>
<p>Abstract: The field of cellular biology has long sought to understand the intricate mechanisms that govern cellular responses to various perturbations, be they chemical, physical, or biological. Traditional experimental approaches, while invaluable, often face limitations in scalability and throughput, especially when exploring the vast combinatorial space of potential cellular states. Enter generative machine learning that has shown exceptional promise in modeling complex biological systems. This talk will highlight recent successes, address the challenges and limitations of current models, and discuss the future direction of this exciting interdisciplinary field. Through examples of practical applications, we will illustrate the transformative potential of generative ML in advancing our understanding of cellular perturbations and in shaping the future of biomedical research.</p>
<ul>
<li><a href="https://zhijing-jin.com/fantasy/about/">Zhijing Jin</a>, Max Planck Institute and ETH Zürich: “<em>A Paradigm Shift in Addressing Distribution Shifts: Insights from Large Language Models</em>”</li>
</ul>
<p>Abstract: Traditionally, the challenge of distribution shifts - where the training data distribution differs from the test data distribution - has been a central concern in statistical learning and model generalization. Traditional methods have primarily focused on techniques such as domain adaptation, and transfer learning. However, the rise of large language models (LLMs) such as ChatGPT has ushered in a novel empirical success, triggering a significant “shift” in problem formulation and approach for traditional distribution shift problems. In this talk, I will start with two formulations for LLMs: (1) the engineering heuristics aimed at transforming “out-of-distribution” (OOD) problems into “in-distribution” scenarios, which is further accompanied by (2) the hypothesized “emergence of intelligence” through massive scaling of data and model parameters, which challenges our traditional views on distribution shifts. I will sequentially examine these aspects, first by presenting behavioral tests of these models’ generalization capabilities across unseen data, and then by conducting intrinsic checks to uncover the mechanisms LLMs learned. This talk seeks to provoke thoughts on several questions: Do the strategies of “making OOD problem IID” and facilitating the “emergence of intelligence” by scaling, truly stand up to scientific scrutiny? Furthermore, what do these developments imply for the field of statistical learning and the broader evolution of AI.</p>
<p>Discussant: <a href="https://people.math.ethz.ch/~nicolai/">Nicolai Meinshausen</a>, ETH Zürich</p>
<p>YoungStatS project of the Young Statisticians Europe initiative (FENStatS) is supported by the Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics (IMS).</p>

</main>





</article>
</div>

<script async src="//yihui.name/js/center-img.js"></script>
 







</body>
</html>

