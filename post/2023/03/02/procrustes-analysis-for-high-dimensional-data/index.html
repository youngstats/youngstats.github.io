<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>Procrustes Analysis for High-Dimensional Data | YoungStatS</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <link href="//cdn.bootcss.com/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">

  </head>

  <body class="page">
    <nav class="header">
      <div class="banner">
<a href="/" class="text">
<div class="yourfancytitle"> YoungStatS </div> 
</a>
</div>

      <div class="head-meta">
      
        <span><a href="/">&larr; Back to Home</a></span>
       <span>The blog of Young Statisticians Europe (YSE)</span>
        
        
        
          
        
        
        
        
      
      </div>
    </nav>
    
    <footer>
<script src="//yihui.org/js/math-code.js"></script>
<script async
src="cdnjs.cloudflare.com/ajax/libs/mathjax/3.1.0?config=TeX-MML-AM_CHTML">
</script>
<div class="footer">
  <ul class="menu">
    
    <li><a href="/"><span data-hover="Home">Home</span></a></li>
    
    <li><a href="/categories/webinars"><span data-hover="Webinars">Webinars</span></a></li>
    
    <li><a href="/about/"><span data-hover="About">About</span></a></li>
    
    <li><a href="/categories/"><span data-hover="Categories">Categories</span></a></li>
    
  </ul>
  
</div>
</footer>

<div class="container">
<article>
<div class="article-meta">

  <div class="categories">
  
    <a href="/categories/high-dimensional-data">high-dimensional-data</a>
  
     &hercon; <a href="/categories/spatial-statistics">spatial statistics</a>
  
  </div>

  <h1><span class="title">Procrustes Analysis for High-Dimensional Data</span></h1>

  
  <h3 class="author">Angela Andreella and Livio Finos
</h3>
  

  
  

</div>



<main>



<div id="overview" class="section level2">
<h2>Overview</h2>
<p>The <strong>Procrustes problem</strong> aims to match matrices using similarity transformations (i.e., rotation, reflection, translation, and scaling transformations) to minimize their Frobenius distance. It allows the comparison of matrices with dimensions defined in an arbitrary coordinate system. This method raised the interest of applied researchers, highlighting its potentiality through many applications in several fields, such as ecology, biology, analytical chemometrics, and psychometrics.</p>
<p>The interest of a large audience from applied fields stimulates, in parallel, the growth of a vast body of literature. Despite this, essentially all applications comprise spatial coordinates (i.e., two- or three-dimensional). <a href="https://www.sciencedirect.com/science/article/pii/S0896627311007811">Haxby et al. (2011)</a> first utilized this approach in a different context: align functional Magnetic Resonance Images (fMRI). The coordinates are hence substituted by voxels (i.e., three-dimensional pixels), and the problem becomes inherently <strong>high-dimensional</strong>. The approach rapidly grew in popularity in the neuroimaging community because of its effectiveness. However, the proposed solution needs to be revised: the extension from the spatial context to a more general and high-dimensional one is a theoretical challenge that needs adequate attention.</p>
<p>We must deal essentially with two problems moving in the high-dimensional context like the fMRI one:</p>
<ol style="list-style-type: lower-roman">
<li><p><strong>Ill-posed solution</strong>: It is a barely noticeable problem with spatial coordinates because the solution of the orthogonal transformation is unique up to rotations; hence, the user can choose the point of view that provides the nicest picture. When the dimensions do not have a spatial meaning (e.g., fMRI data), any rotation completely changes the interpretation of the results.</p></li>
<li><p><strong>Computational burden</strong>: The estimation algorithm of Procrustes-based methods involves a series of singular value decomposition (SVD) of <span class="math inline">\(m \times m\)</span> matrices (<span class="math inline">\(m\)</span> dimensions). In a typical fMRI data set, the subjects (i.e., the matrices to be aligned) have a few hundred (observations/rows) <span class="math inline">\(n\)</span> and hundreds of thousands of voxels (dimensions/columns) <span class="math inline">\(m\)</span>.</p></li>
</ol>
</div>
<div id="proposal" class="section level2">
<h2>Proposal</h2>
<p>To tackle these problems, in <a href="https://link.springer.com/article/10.1007/s11336-022-09859-5">Andreella and Finos, 2022</a>, we revised the <strong>perturbation model</strong> proposed by <a href="https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1991.tb01825.x">Goodall, 1991</a>, which rephrases the Procrustes problem as a statistical model.</p>
<div id="background" class="section level3">
<h3>Background</h3>
<p>Let <span class="math inline">\(\{\mathbf{X}_{i} \in \mathrm{I\!R}^{n \times m} \}_{i = 1,\dots ,N}\)</span> be a set of matrices to be aligned and <span class="math inline">\({\mathcal {O}}(m)\)</span> the orthogonal group in dimension <span class="math inline">\(m\)</span>. The perturbation model is defined as</p>
<p><span class="math display">\[\begin{equation}
{\mathbf{X}}_i= \alpha _i ({\mathbf{M}} + {\mathbf{E}}_i){\mathbf{R}}_i^\top + {\mathbf {1}}_n^\top {\mathbf{t}}_i \quad \quad \text {subject to }\quad {\mathbf{R}}_i \in {\mathcal {O}}(m)
\end{equation}\]</span></p>
<p>where <span class="math inline">\({\mathbf{E}}_i \sim \mathcal{MN}\mathcal{}_{n,m}(0,\mathbf{\Sigma }_n,\mathbf{\Sigma }_m)\)</span> - i.e., the matrix normal distribution with <span class="math inline">\(\mathbf{\Sigma }_n \in \mathrm{I\!R}^{n \times n}\)</span> and <span class="math inline">\(\mathbf{\Sigma }_m \in \mathrm{I\!R}^{m \times m}\)</span> scale parameters - <span class="math inline">\({\mathbf{M}} \in \mathrm{I\!R}^{n \times m}\)</span> is the shared reference matrix, <span class="math inline">\(\alpha_i \in \mathrm{I\!R}^{+}\)</span> is the isotropic scaling, <span class="math inline">\(\mathbf{t}_i \in \mathrm{I\!R}^{1 \times m}\)</span> defines the translation vector, and <span class="math inline">\({\mathbf{1}}_n \in \mathrm{I\!R}^{1 \times n}\)</span> is a vector of ones.</p>
<p>This work mainly aims to compare the shapes <span class="math inline">\(\mathbf{X}_{i}\)</span> instead of the form’s analysis of the matrices. For that, the parameters of interest are <span class="math inline">\(\mathbf{R}_i\)</span>, <span class="math inline">\(\alpha_i\)</span> and <span class="math inline">\(\mathbf{t}_i\)</span>, whereas <span class="math inline">\(\mathbf{M}\)</span>, <span class="math inline">\(\mathbf{\Sigma}_n\)</span>, and <span class="math inline">\(\mathbf{\Sigma}_m\)</span> are considered as nuisance parameters for each <span class="math inline">\(i=1,\dots,N\)</span>. Clearly, the estimation of the unknown parameters changes if these nuisance parameters are</p>
<ul>
<li><p><strong>Known</strong> (Theorem 1 in <a href="https://link.springer.com/article/10.1007/s11336-022-09859-5">Andreella and Finos, 2022</a>): Let the SVD of <span class="math inline">\({\mathbf{X}}_i^\top \mathbf{\Sigma }_n^{-1} {\mathbf{M}} \mathbf{\Sigma }_m^{-1}\)</span> be <span class="math inline">\({\mathbf{U}}_i {\mathbf{D}}_i {\mathbf{V}}_i^\top\)</span>. Then, the maximum likelihood estimators equal
<span class="math display" id="eq:mle">\[\begin{equation}
\hat{\mathbf{R}}_i^\top = {\mathbf{U}}_i {\mathbf{V}}_i^\top \quad \hat{\alpha _i}_{\hat{\mathbf{R}}_i^\top}^{\top}=||\mathbf{\Sigma }_m^{-1/2}\hat{\mathbf{R}}_i^\top {\mathbf{X}}_i^\top \mathbf{\Sigma }_n^{-1/2}||^2/tr({\mathbf{D}}_i). \tag{1}
\end{equation}\]</span></p></li>
<li><p><strong>Unknown</strong> (Algorithm 1 in <a href="https://link.springer.com/article/10.1007/s11336-022-09859-5">Andreella and Finos, 2022</a>): no closed-form solution for <span class="math inline">\(\mathbf{R}_i\)</span> and <span class="math inline">\(\alpha_i\)</span>, and an iterative algorithm proposed by <a href="https://link.springer.com/article/10.1007/BF02291478">Gower, 1975</a> must be used where at each step the nuisance parameters are updated.</p></li>
</ul>
<blockquote>
<p>When <span class="math inline">\(n&lt;&lt;m\)</span> in both cases (unknown and known nuisance parameters) the Procrustes solutions are infinite (Lemma 1 in <a href="https://link.springer.com/article/10.1007/s11336-022-09859-5">Andreella and Finos, 2022</a>) and the SVD leads to polynomial time complexity in <span class="math inline">\(m\)</span>, i.e., we must deal with problems (i) and (ii) described in the first section.</p>
</blockquote>
</div>
<div id="promises-model-and-efficient-promises-model" class="section level3">
<h3>ProMises model and Efficient ProMises model</h3>
<p>In <a href="https://link.springer.com/article/10.1007/s11336-022-09859-5">Andreella and Finos, 2022</a>, we dealt with problem (i) proposing the <strong>ProMises (Procrustes von Mises–Fisher) model</strong>, while with problem (ii) proposing the <strong>Efficient ProMises model</strong>. Therefore:</p>
<ol style="list-style-type: lower-roman">
<li><p>The <strong>ill-posed problem</strong> is solved by assuming that the orthogonal matrix parameter <span class="math inline">\(\mathbf{R}_i\)</span> follows the von Mises–Fisher distribution (<a href="https://academic.oup.com/biomet/article-abstract/59/3/665/484888?login=true">Downs, 1972</a>), i.e.,<span class="math display">\[f({\mathbf{R}}_i) = C({\mathbf{F}}, k ) \exp \big \{tr(k {\mathbf{F}}^\top {\mathbf{R}}_i)\big \},\]</span> and choosing a proper location matrix parameter <span class="math inline">\(\mathbf{F} \in \mathbb{R}^{m \times m}\)</span>. We call this extension the <strong>ProMises model</strong>.
In particular, <span class="math inline">\(\mathbf{F} \in \mathbb{R}^{m \times m}\)</span> must be specified to be a <strong>full rank</strong> matrix to have a unique maximum a posteriori solution for <span class="math inline">\(\mathbf{R}_i\)</span> with the orientation structure of <span class="math inline">\(\mathbf{F}\)</span> (Lemma 3 in <a href="https://link.springer.com/article/10.1007/s11336-022-09859-5">Andreella and Finos, 2022</a>). The prior information about <span class="math inline">\({\mathbf{R}}_i\)</span>’s structure is directly entered in the SVD step; the maximum a posteriori estimators turn out to be a slight modification of the estimators defined in Equation <a href="#eq:mle">(1)</a>. We decompose <span class="math inline">\({\mathbf{X}}_i^\top \mathbf{\Sigma }_n^{-1} {\mathbf{M}} \mathbf{\Sigma }_m^{-1} + k {\mathbf{F}}\)</span> instead of <span class="math inline">\({\mathbf{X}}_i^\top \mathbf{\Sigma }_n^{-1} {\mathbf{M}} \mathbf{\Sigma }_m^{-1}\)</span> (Theorem 2 in <a href="https://link.springer.com/article/10.1007/s11336-022-09859-5">Andreella and Finos, 2022</a>).</p></li>
<li><p>The <strong>computational burden</strong> is reduced, proving that the minimization problem is solved by projecting the matrices <span class="math inline">\(\mathbf{X}_i\)</span> into an <span class="math inline">\(n\)</span>-lower dimensional space using a specific <strong>semi-orthogonal transformation</strong>. Considering the thin SVD of <span class="math inline">\({\mathbf{X}}_i = {\mathbf{L}}_i {\mathbf{S}}_i {\mathbf{Q}}_i^\top\)</span> for each <span class="math inline">\(i=1\dots,N\)</span>, where <span class="math inline">\(\mathbf{Q}_i\)</span> has dimensions <span class="math inline">\(m \times n\)</span>, the following holds <span class="math display">\[\max _{{\mathbf{R}}_i \in {\mathcal {O}}(m)} tr({\mathbf{R}}_i^\top {\mathbf{X}}_i^\top \mathbf{\Sigma }_n^{-1} {\mathbf{X}}_j \mathbf{\Sigma }_m^{-1} + k \mathbf{F}) = \max _{{\mathbf{R}}_i^{\star } \in {\mathcal {O}}(n)} tr({\mathbf{R}}_i^{ \star \top } {\mathbf{Q}}_i^\top {\mathbf{X}}_i^\top \mathbf{\Sigma }_n^{-1} {\mathbf{X}}_j \mathbf{\Sigma }_m^{-1} {\mathbf{Q}}_j^\top + k \mathbf{Q}_i^\top \mathbf{F} \mathbf{Q}_j)\]</span> (Lemma 5 in <a href="https://link.springer.com/article/10.1007/s11336-022-09859-5">Andreella and Finos, 2022</a>). We reach the same maximum while working in the reduced space of the first <span class="math inline">\(n\)</span> eigenvectors, which contains all the information, instead of the full data set.
The ProMises model applies to matrices with virtually any number of columns. We denote this approach as the <strong>Efficient ProMises model</strong>.
In practice, it projects the matrices <span class="math inline">\(\mathbf{X}_i\)</span> into an <span class="math inline">\(n\)</span>-lower-dimensional space using a specific semi-orthogonal transformation <span class="math inline">\(\mathbf{Q}_i\)</span> which preserves all the data’s information. It aligns, then, the reduced <span class="math inline">\(n \times n\)</span> matrices <span class="math inline">\(\{{\mathbf{X}}_i {\mathbf{Q}}_i \in \mathrm{I\!R}^{n \times n }\}_{i = 1, \dots , N}\)</span> by the <strong>ProMises model</strong>. Finally, it projects the aligned matrices back to the original <span class="math inline">\(n \times m\)</span>-size matrices <span class="math inline">\(\{{\mathbf{X}}_i \in \mathrm{I\!R}^{n \times m }\}_{i = 1, \dots , N}\)</span> using the transpose of <span class="math inline">\(\{{\mathbf{Q}}_i\}_{i = 1, \dots , N}\)</span>.</p></li>
</ol>
<p>Finally, we proved that the proposed prior distribution is conjugate, making the estimation process quite fast (Algorithm 2 in <a href="https://link.springer.com/article/10.1007/s11336-022-09859-5">Andreella and Finos, 2022</a>). The location posterior parameter equaling the following:</p>
<p><span class="math display">\[{\mathbf{F}}^\star =\mathbf{X_i}^\top \mathbf{\Sigma }_n^{-1} {\mathbf{M}} \mathbf{\Sigma }_m^{-1} + k {\mathbf{F}}.\]</span></p>
<p>which can be rephrased as a combination of the maximum likelihood estimate <span class="math inline">\(\mathbf{{\hat{R}}}_i\)</span> and the prior mode <span class="math inline">\(\mathbf{P}\)</span> (i.e., the orthogonal part of <span class="math inline">\(\mathbf{F}\)</span>), multiplied by corresponding measures of variation.</p>
<p>The method proposed is implemented in <a href="https://github.com/angeella/ProMisesModel" class="uri">https://github.com/angeella/ProMisesModel</a> using the programming language Python and in <a href="https://github.com/angeella/alignProMises" class="uri">https://github.com/angeella/alignProMises</a> in the R package <code>alignProMises</code>.</p>
</div>
</div>
<div id="fmri-application" class="section level2">
<h2>fMRI application</h2>
<div id="motivation" class="section level3">
<h3>Motivation</h3>
<p>The alignment problem is recognized in fMRI multi-subject studies because the brain’s anatomical and functional structures vary across subjects. The most used anatomical alignment is the Montréal Neurological Institute (MNI) space normalization, i.e., the brain images are aligned to an anatomical template by affine transformations using a set of major anatomical landmarks. However, this alignment does not explore the between-subjects variability in anatomical positions of the functional loci since the functional brain regions are not consistently placed on the anatomical landmarks defined by the MNI template.</p>
<p>Specifically, each subject’s brain activation can be represented by a matrix, where the rows represent the stimuli/time points, and the columns represent the voxels. The stimuli are time-synchronized among subjects, so we have correspondence among the matrices’ rows. However, the columns are not assumed to be in correspondence among subjects. Each time series of brain activation (i.e., each of the matrices’ columns) represents the voxels’ functional characteristics that the anatomical normalization fails to align. We aim to represent the neural responses to stimuli in a common high-dimensional space rather than in a canonical anatomical space that does not consider the variability of functional topographies loci. For further details about the functional alignment problem in the fMRI data analysis framework, please see <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/hbm.26170">Andreella et al. (2022)</a>.</p>
</div>
<div id="data-description" class="section level3">
<h3>Data description</h3>
<p>We applied the proposed method to data from Pernet et al. (2015), available at <a href="https://openneuro.org/datasets/ds000158/versions/1.0.0">https://openneuro.org/datasets/ds000158/versions/1.0.0</a>. The study consists of neural activations of <span class="math inline">\(218\)</span> subjects passively listening to vocal (i.e., speech) and nonvocal sounds. Using a standard processing procedure, we preprocessed the data using the Functional MRI of the Brain Software Library (FSL). Anatomical and functional alignments (based on the Efficient ProMises model) were compared, having images preprocessed in the same way, but in one case, the functional alignment was applied, while in the other case not. For details about the experimental design and data acquisition, please see <a href="https://www.sciencedirect.com/science/article/pii/S1053811915005558">Pernet et al. (2015)</a>.</p>
<p>In the fMRI context, we defined the hyperparameter <span class="math inline">\(\mathbf{F}\)</span> as a Euclidean similarity matrix using each voxel’s <span class="math inline">\(3\)</span>D anatomical coordinates. In this way, <span class="math inline">\(\mathbf{F}\)</span> is a symmetric matrix with ones in the diagonal, which means that voxels with the same spatial location are combined with weights equaling <span class="math inline">\(1\)</span>, and the weights decrease as the voxels to be combined are more spatially distant. This definition also satisfies the requirement to have unique solutions of <span class="math inline">\(\mathbf{R}_i\)</span>, i.e., full-rank matrix (Lemma 3 in <a href="https://link.springer.com/article/10.1007/s11336-022-09859-5">Andreella and Finos, 2022</a>).</p>
</div>
<div id="functional-connectivity" class="section level3">
<h3>Functional connectivity</h3>
<p>We performed seed-based correlation analysis, which shows the level of functional connectivity between a seed and every voxel in the brain. The analysis process was defined as follows: First, the subject images were aligned using the Efficient ProMises model, then the element-wise arithmetic mean across subjects was calculated, and finally, the functional connectivity analysis was developed on this average matrix.</p>
<p>We considered the frontal pole as seed, being a region with functional diversity. Figure 1 shows the correlation values between the seed and each voxel in the brain using data without functional alignment (top of Figure 1 and with functional alignment using the Efficient ProMises model (bottom of Figure 1). The first evidence is that the functional alignment produces more interpretable maps, where the various regions, such as the superior temporal gyrus, are delineated by marked spatial edges, while the non-aligned map produces more spread regions, hence being less interpretable. It is interesting to evaluate the regions more correlated with the frontal pole, for example, the superior temporal gyrus. This region is associated with the processing of auditory stimuli. The correlation of the superior temporal gyrus with the seed is clear in the bottom part of Figure 1, where functionally aligned images are used.</p>
<p><img src="/post/2023-03-02-procrustes-analysis-for-high-dimensional-data_files/cover.png" /></p>
<p>Figure: Seed-based correlation map using data only aligned anatomically (top figure), and data also functionally aligned by the Efficient ProMises model (bottom figure). The black point refers to the seed used (i.e., frontal pole with MNI coordinates (0, 64, 18)).</p>
<p>The preprocessed data are available on the GitHub repository: <a href="http://github.com/angeella/fMRIdata" class="uri">http://github.com/angeella/fMRIdata</a>, as well as the code used to perform functional connectivity: <a href="https://github.com/angeella/ProMisesModel/" class="uri">https://github.com/angeella/ProMisesModel/</a>.</p>
</div>
</div>
<div id="take-home-messages" class="section level2">
<h2>Take home messages</h2>
<p>We proposed the Efficient ProMises model in <a href="https://link.springer.com/article/10.1007/s11336-022-09859-5">Andreella and Finos, 2022</a>, which provides a methodologically grounded approach to the Procrustes problem allowing functional alignment on high-dimensional data in a computationally efficient way.</p>
<p>The issues of the many Procrustes-based functional alignment approaches:</p>
<ul>
<li>non-uniqueness, i.e., critical interpretation of the aligned matrices and related results, and</li>
<li>inapplicability</li>
</ul>
<p>when <span class="math inline">\(n≪m\)</span> are completely surpassed. Indeed, the ProMises method returns unique and interpretable orthogonal transformations, and its efficient approach extends the applicability to high-dimensional data.</p>
<p>The presented method is particularly useful in fMRI data analysis because it allows the functional alignment of images having roughly <span class="math inline">\(200 \times 200,000\)</span> dimensions, obtaining a unique representation of the aligned images in the brain space and a unique interpretation of the related results.</p>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<p>Andreella, A., &amp; Finos, L. (2022). Procrustes analysis for high-dimensional data. Psychometrika, 87(4), 1422-1438.</p>
<p>Andreella, A., Finos, L., &amp; Lindquist, M. A. (2023). Enhanced hyperalignment via spatial prior information. Human Brain Mapping, 44(4), 1725-1740.</p>
<p>Downs, T. D. (1972). Orientation statistics. Biometrika, 59(3), 665-676.</p>
<p>Goodall, C. (1991). Procrustes methods in the statistical analysis of shape. Journal of the Royal Statistical Society: Series B (Methodological), 53(2), 285-321.</p>
<p>Gower, J. C. (1975). Generalized procrustes analysis. Psychometrika, 40, 33-51.</p>
<p>Haxby, J. V., Guntupalli, J. S., Connolly, A. C., Halchenko, Y. O., Conroy, B. R., Gobbini, M. I., … &amp; Ramadge, P. J. (2011). A common, high-dimensional model of the representational space in human ventral temporal cortex. Neuron, 72(2), 404-416.</p>
<p>Pernet, C. R., McAleer, P., Latinus, M., Gorgolewski, K. J., Charest, I., Bestelmeyer, P. E., … &amp; Belin, P. (2015). The human voice areas: Spatial organization and inter-individual variability in temporal and extra-temporal cortices. Neuroimage, 119, 164-174.</p>
</div>
<div id="authors" class="section level2">
<h2>Authors</h2>
<ul>
<li><a href="https://angeella.github.io/">Angela Andreella</a> is an assistant professor in social statistics in the Department of Economics at the University of Venice.</li>
<li><a href="http://livioivil.github.io/">Livio Finos</a> is a full professor in statistics in the Department of Statistics at the University of Padova.</li>
</ul>
</div>

</main>


















</article>
</div>

<script async src="//yihui.name/js/center-img.js"></script>
 


<script src="//yihui.name/js/math-code.js"></script>
<script async src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>






</body>
</html>

