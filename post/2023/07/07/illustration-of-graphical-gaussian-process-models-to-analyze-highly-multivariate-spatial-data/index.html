<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>Illustration of Graphical Gaussian Process models to analyze highly multivariate spatial data | YoungStatS</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <link href="//cdn.bootcss.com/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">

  </head>

  <body class="page">
    <nav class="header">
      <div class="banner">
<a href="/" class="text">
<div class="yourfancytitle"> YoungStatS </div> 
</a>
</div>

      <div class="head-meta">
      
        <span><a href="/">&larr; Back to Home</a></span>
       <span>The blog of Young Statisticians Europe (YSE)</span>
        
        
        
          
        
        
        
        
      
      </div>
    </nav>
    
    <footer>
<script src="//yihui.org/js/math-code.js"></script>
<script async
src="cdnjs.cloudflare.com/ajax/libs/mathjax/3.1.0?config=TeX-MML-AM_CHTML">
</script>
<div class="footer">
  <ul class="menu">
    
    <li><a href="/"><span data-hover="Home">Home</span></a></li>
    
    <li><a href="/categories/webinars"><span data-hover="Webinars">Webinars</span></a></li>
    
    <li><a href="/about/"><span data-hover="About">About</span></a></li>
    
    <li><a href="/categories/"><span data-hover="Categories">Categories</span></a></li>
    
  </ul>
  
</div>
</footer>

<div class="container">
<article>
<div class="article-meta">

  <div class="categories">
  
    <a href="/categories/graphical-models">graphical-models</a>
  
     &hercon; <a href="/categories/spatial-statistics">spatial statistics</a>
  
  </div>

  <h1><span class="title">Illustration of Graphical Gaussian Process models to analyze highly multivariate spatial data</span></h1>

  
  <h3 class="author">Debangan Dey,  Abhirup Datta,  Sudipto Banerjee
</h3>
  

  
  

</div>



<main>




<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Abundant multivariate spatial data from the natural and environmental sciences demands research on the joint distribution of multiple spatially dependent variables (<span class="citation">Wackernagel (<a href="#ref-wackernagel2013multivariate">2013</a>)</span>, <span class="citation">Cressie and Wikle (<a href="#ref-creswikle11">2011</a>)</span>, <span class="citation">Banerjee and Gelfand (<a href="#ref-ban14">2014</a>)</span>). Here, our goal is to estimate associations over spatial locations for each variable and those among the variables.</p>
<p>In this document, we will present an example of a simulated multivariate spatial data and how we can use Graphical Gaussian processes (GGP) (<span class="citation">Dey and Banerjee (<a href="#ref-dey2021ggp">2021</a>)</span>) to model the data. First, we’ll introduce the general multivariate spatial model, and then we will introduce a variable graph and how to simulate a Graphical Gaussian process (Matérn) for that variable graph. Next, we will lay out the estimation steps of GGP parameters and how the estimated parameters compare against the truth.</p>
</div>
<div id="model" class="section level1">
<h1>Model</h1>
<p>Let <span class="math inline">\(y(s)\)</span> denote a <span class="math inline">\(q\times 1\)</span> vector of spatially-indexed dependent outcomes for any location <span class="math inline">\(s \in \mathcal D \subset \mathbb{R}^d\)</span> with typically <span class="math inline">\(d=2\)</span> or <span class="math inline">\(3\)</span>. On our spatial domain <span class="math inline">\(\mathcal D\)</span>, a multivariate spatial regression model provides a marginal spatial regression model for each outcome as follows:
<span class="math display" id="eq:mgp">\[\begin{equation}
y_i(s) = x_i(s)^{T}\beta_i + w_i(s) + \epsilon_i(s)\;,\quad i=1,2,\ldots,q,\; s \in \mathcal D
\tag{1}
\end{equation}\]</span>
where <span class="math inline">\(y_i(s)\)</span> is the <span class="math inline">\(i\)</span>-th element of <span class="math inline">\(y(s)\)</span>, <span class="math inline">\(x_i(s)\)</span> is a <span class="math inline">\(p_i\times 1\)</span> vector of predictors, <span class="math inline">\(\beta_i\)</span> is the <span class="math inline">\(p_i\times 1\)</span> vector of slopes, <span class="math inline">\(w_i(s)\)</span> is a spatially correlated process and <span class="math inline">\(\epsilon_i(s) \stackrel{ind}{\sim} N(0,\tau^2_i)\)</span> is the random error in outcome <span class="math inline">\(i\)</span>. We usually assume that <span class="math inline">\(\epsilon_i(s)\)</span> are independent across <span class="math inline">\(i\)</span>, whereas <span class="math inline">\(w(s)=(w_1(s), w_2(s),\ldots, w_q(s))^T\)</span> is a zero-centred multivariate Gaussian process (GP) with a zero mean and a cross-covariance function that introduces dependency across space and among the <span class="math inline">\(q\)</span> variables. Let <span class="math inline">\(\mathcal S_i\)</span> represent the collection of places where the <span class="math inline">\(i-\)</span>th variable was observed. The reference set of locations for our approach is <span class="math inline">\(\mathcal L = \cup_i\mathcal S_i\)</span>.</p>
<p>The definition of <span class="math inline">\(w(s)\)</span> as the <span class="math inline">\(q \times 1\)</span> multivariate graphical Matérn GP (<span class="citation">Dey and Banerjee (<a href="#ref-dey2021ggp">2021</a>)</span>) with respect to a decomposable variable graph <span class="math inline">\({\mathcal G}_{\mathcal V}\)</span> yields the distribution of each <span class="math inline">\(w_i\)</span>. The marginal parameters for each component Matérn process <span class="math inline">\(w_i(\cdot)\)</span> are denoted by <span class="math inline">\(\{\phi_{ii}, \sigma_{ii} | i = 1.\ldots,q\}\)</span>.</p>
<p>It is sufficient to limit the intra-site covariance matrix <span class="math inline">\(\Sigma=(\sigma_{ij})\)</span> to be of the following form to assure a valid multivariate Matérn cross-covariance function (Theorem 1 of <span class="citation">Apanasovich and Sun (<a href="#ref-apanasovich2012valid">2012</a>)</span>):</p>
<p><span class="math display" id="eq:constraints">\[\begin{equation}
\begin{array}{cc}
   \nu_{ij} =&amp; \frac 12 (\nu_{ii} + \nu_{jj}) + \Delta_A (1 - A_{ij}) \mbox{ where } \Delta_A \geq 0, A=(A_{ij}) \mbox{ for all } i \geq 0, A_{ii}=1 \\
    \sum_{i,j} c_ic_j\phi_{ij} \leq 0  &amp;\mbox{ is a conditionally non-negative definite matrix } \\
    \sigma_{ij} =&amp; b_{ij} \frac{\Gamma(\frac 12 (\nu_{ii}+\nu_{jj} + d))\Gamma(\nu_{ij})}{\phi_{ij}^{2\Delta_A+\nu_{ii}+\nu_{jj}}\Gamma(\nu_{ij} + \frac d2)} \mbox{ where } \Delta_A \geq 0, \mbox{ and } B=(b_{ij}) &gt; 0, \mbox{ i.e., is p.d.}
\end{array}
\tag{2}
\end{equation}\]</span>
This is equal to <span class="math inline">\(\Sigma = (B \odot (\gamma_{ij}))\)</span>, where <span class="math inline">\(\gamma_{ij}\)</span>’s are constants collecting the components in <a href="#eq:constraints">(2)</a> involving just <span class="math inline">\(\nu_{ij}\)</span>’s and <span class="math inline">\(\phi_{ij}\)</span>’s, and <span class="math inline">\(\odot\)</span> indicates the Hadamard (element-wise) product.</p>
<p>In the following example, we’ll assume <span class="math inline">\(\Delta_A=0, \nu_{ij} = \frac{\nu_{ii}+\nu_{jj}}{2}\)</span>, <span class="math inline">\(\phi_{ij}^2 = \frac{\phi_{ii}^2 + \phi_{jj}^2}{2}\)</span>. The constraints in <a href="#eq:constraints">(2)</a> simplifies to -</p>
<p><span class="math display" id="eq:constraints-simp">\[\begin{equation}
\begin{array}{cc}
    \sigma_{ij} =&amp; (\sigma_{ii} \sigma_{jj})^{\frac{1}{2}} * \frac{\phi_{ii}^{\nu_{ii}}\phi_{jj}^{\nu_{jj}}}{\phi_{ij}^{\nu_{ij}}}  \frac{\Gamma(\nu_{ij})}{\Gamma(\nu_{ii})^{\frac 12} \Gamma(\nu_{ij})^{\frac 12}} r_{ij} \mbox{ where }, \mbox{ and } R=(r_{ij}) &gt; 0, \mbox{ i.e., is p.d.}
\end{array}
\tag{3}
\end{equation}\]</span></p>
<p>We also take <span class="math inline">\(\nu_{ii} = \nu_{jj} = 0.5\)</span> in our case and hence, we only need to estimate the marginal scale (<span class="math inline">\(\phi_{ii}\)</span>) and variance parameters (<span class="math inline">\(\sigma_{ii}\)</span>) and cross-correlation parameters <span class="math inline">\(r_{ij}\)</span>. We also assume variable graph (<span class="math inline">\({\mathcal G}_{\mathcal V}\)</span>) to be decomposable.</p>
</div>
<div id="simulation" class="section level1">
<h1>Simulation</h1>
<p>As a first step of simulation, we need to fix a decomposable variable graph (<span class="math inline">\({\mathcal G}_{\mathcal V}\)</span>). Depending on the perfect ordering of cliques in the graph, the density function of the full process can be factorized. Using this property, we can calculate the variance-covarinace matrix of the GGP and use it to simulate observation from a multivariate GGP.</p>
<div id="decomposable-variable-graph" class="section level2">
<h2>Decomposable variable graph</h2>
<p>If <span class="math inline">\(\mathcal G_\mathcal V\)</span> is decomposable, it has a perfect clique sequence <span class="math inline">\(\{K_1,K_2,\cdots,K_p\}\)</span> with separators <span class="math inline">\(\{S_2,\ldots,S_m\}\)</span>, and <span class="math inline">\(w(\mathcal L)\)</span>’s joint density may be factorized as follows.
<span class="math display" id="eq:ggp-fact-2">\[\begin{equation}
    f_M(w(\mathcal L)) = \frac{\Pi_{m=1}^{p} f_C(w_{K_m}( \mathcal L))}{\Pi_{m=2}^{p} f_C(w_{S_m}( \mathcal L))}\;,
\tag{4}
\end{equation}\]</span>
where <span class="math inline">\(S_m=F_{m-1} \cap K_m\)</span> and <span class="math inline">\(F_{m-1}= K_1 \cup \cdots \cup K_{m-1}\)</span>, and
<span class="math inline">\(f_A\)</span> denotes the the density of a GP over <span class="math inline">\(\mathcal L\)</span> with covariance function <span class="math inline">\(A\)</span> for <span class="math inline">\(A \in \{M,C\}\)</span>.</p>
<p>Here, we assume we have <span class="math inline">\(10\)</span> variables with each being observed at <span class="math inline">\(250\)</span> locations uniformly chosen from the <span class="math inline">\((0,1) \times (0,1)\)</span> square. We assume the variable graph to be as follows.</p>
<p><img src="/post/2023-07-07-illustration-of-graphical-gaussian-process-models-to-analyze-highly-multivariate-spatial-data_files/unnamed-chunk-3-1.png" /></p>
<p>We calculate the perfect ordering of the cliques of the graph above and list the cliques and separators below. To visualize the decomposition, we also plot the junction tree (definition below) of the graph -</p>
<p>The <strong>junction graph</strong> <span class="math inline">\(G\)</span> of a decomposable graph <span class="math inline">\(\mathcal G_\mathcal V\)</span> is a complete graph with the cliques of <span class="math inline">\(\mathcal G_\mathcal V\)</span> as its nodes. Every edge in the junction graph is denoted as a link, which is also the intersection of the two cliques, and can be empty. A <strong>spanning tree</strong> of a graph is defined as a subgraph comprising all the vertices of the original graph and is a tree (acyclic graph). If a spanning tree <span class="math inline">\(J\)</span> of the junction graph of <span class="math inline">\(G\)</span> satisfies the following property: for any two cliques <span class="math inline">\(C\)</span> and <span class="math inline">\(D\)</span> of the graph, every node in the unique path between <span class="math inline">\(C\)</span> and <span class="math inline">\(D\)</span> in the tree contains <span class="math inline">\(C \cap D\)</span>. Then <span class="math inline">\(J\)</span> is called the <strong>junction tree</strong> for the graph <span class="math inline">\(\mathcal G_\mathcal V\)</span>. Here, the junction tree of the graph has the perfectly ordered cliques as its nodes and the separators denoted as edges.</p>
<p><img src="/post/2023-07-07-illustration-of-graphical-gaussian-process-models-to-analyze-highly-multivariate-spatial-data_files/unnamed-chunk-4-1.png" /></p>
<p><img src="/post/2023-07-07-illustration-of-graphical-gaussian-process-models-to-analyze-highly-multivariate-spatial-data_files/unnamed-chunk-5-1.png" />
Hence, in this case, the likelihood of the decomposable GGP would be as follows -
<span class="math display" id="eq:ggp-fact-ex">\[\begin{equation}
    f_M(w(\mathcal L)) = \frac{f_C(w_{(1,2,3)}(\mathcal L)) f_C(w_{(2,3,4)}(\mathcal L)) f_C(w_{(4,5,6)}(\mathcal L)) f_C(w_{(6,7,8)}(\mathcal L)) f_C(w_{(8,9,10)}(\mathcal L))}{f_C(w_{(2,3)}(\mathcal L)) f_C(w_{(4)}(\mathcal L)) f_C(w_{(6)}(\mathcal L)) f_C(w_{(8)}(\mathcal L))}\;,
\tag{5}
\end{equation}\]</span></p>
</div>
<div id="simulating-latent-ggp" class="section level2">
<h2>Simulating latent GGP</h2>
<p>First we fix the marginal and cross-covariance parameters of the process.</p>
<p>Now, the precision matrix of the GGP <span class="math inline">\(w(\mathcal L)\)</span> satisfies (Lemma 5.5 of <span class="citation">Lauritzen (<a href="#ref-lauritzen1996graphical">1996</a>)</span>)
<span class="math display" id="eq:m-decomp">\[\begin{equation}
M(\mathcal L,\mathcal L)^{-1} = \sum_{m=1}^{p} [{C}_{[K_m \boxtimes \mathcal{G_L}]}^{-1}]^{\mathcal V \times \mathcal L} - \sum_{m=2}^{p} [{C}_{[S_m \boxtimes \mathcal{G_L}]}^{-1}] ^{\mathcal V \times \mathcal L}\;, %= \sum_{m=1}^{p} [{M}_{[K_m \boxtimes \mathcal{G_L}]}^{-1}] ^\mathcal V - \sum_{m=2}^{p} [{M}_{[S_m \boxtimes \mathcal{G_L}]}^{-1}] ^\mathcal V\;
\tag{6}
\end{equation}\]</span></p>
<p>The <a href="#eq:m-decomp">(6)</a> and <a href="#eq:ggp-fact-2">(4)</a> shows that inverting the full cross-covariance matrix only requires inverting the clique and separator specific covariance matrices. Hence, the computational complexity for calculating likelihood of a multivariate GGP boils down to <span class="math inline">\(O(n^3 c^3)\)</span> (here, <span class="math inline">\(O(250^3 * 27)\)</span>) where <span class="math inline">\(c= 3\)</span> is the maximum size of a clique in the perfect clique ordering of the graph. On the contrary, the likelihood of a full GGP would need <span class="math inline">\(O(n^3 q^3)\)</span> complexity, i.e. <span class="math inline">\(O(250^3 * 1000)\)</span> in our case.</p>
<p>We use the <a href="#eq:m-decomp">(6)</a> to calculate the covariance of the latent GGP and we use it to simulate the latent process.</p>
</div>
<div id="simulating-multivariate-spatial-outcome" class="section level2">
<h2>Simulating multivariate spatial outcome</h2>
<p>Now we use <a href="#eq:mgp">(1)</a> to simulate our multivariate outcome <span class="math inline">\(y_i(.), i= 1, \cdots, 10\)</span>. In order to do that, we fix some covariates <span class="math inline">\(x_i(\cdot)\)</span> and simulate error process <span class="math inline">\(\epsilon_{i}(\cdot)\)</span> independently from <span class="math inline">\(N(0,\tau^2_{ii})\)</span>. We take <span class="math inline">\(\tau_{ii}^2=\tau^2=0.25\)</span> for all <span class="math inline">\(i\)</span>.</p>
<p>For analyzing the prediction accuracy of our method, we randomly pick <span class="math inline">\(20\%\)</span> of the locations for each outcome variable and consider them missing. We will only be working with the training set to fit the model and judge our prediction accuracy on the test set.</p>
<p>Now we visualize the surface of the training data by variables below.</p>
</div>
</div>
<div id="data-analysis" class="section level1">
<h1>Data analysis</h1>
<p>The analysis of our simulated data can be broken down in the following steps.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Marginal parameter estimation</strong>: We estimate the marginal scale (<span class="math inline">\(\phi_{ii}\)</span>), variance (<span class="math inline">\(\sigma^2_{ii}\)</span>) and smoothness parameters (<span class="math inline">\(\nu_{ii}\)</span>) from the component Gaussian processes. We also estimate the error variance (<span class="math inline">\(\tau^2_{ii}\)</span>) for each marginal processes.</p></li>
<li><p><strong>Initialize Gibbs sampling</strong>: For this step, we need the following components:</p>
<ol style="list-style-type: decimal">
<li><strong>Processing the variable graph</strong>: We fix our variable graph for the estimation process. First we calculate cliques, separators of the graph. Then we color the nodes and edges of the graph for parallel computation purposes.</li>
<li><strong>Starting cross-correlation</strong>: We use the estimated marginal parameters and an initial correlation parameter to start off.</li>
</ol></li>
<li><p><strong>Gibbs sampling</strong>: We run our Gibbs sampler in two steps -</p>
<ol style="list-style-type: decimal">
<li><strong>Sampling latent spatial processes</strong></li>
<li><strong>Sampling latent correlations</strong></li>
</ol></li>
</ol>
<div id="marginal-parameter-estimation" class="section level2">
<h2>Marginal parameter estimation</h2>
<ol style="list-style-type: decimal">
<li>We first estimate the marginal process parameters using <strong>BRISC</strong> package in R. We estimate scale (<span class="math inline">\(\phi_{ii}\)</span>) and variance (<span class="math inline">\(\sigma_{ii}\)</span>) parameters. We fix the marginal smoothness (<span class="math inline">\(\nu_ii\)</span>) and cross-smoothness (<span class="math inline">\(\nu_{ij}\)</span>) parameters at <span class="math inline">\(0.5\)</span>.</li>
<li>Same as the true cross-covariance, we calculate the cross-scale parameters as: <span class="math inline">\(\phi_{ij}= \sqrt{\frac{\phi_{ii}^2 + \phi_{jj}^2}{2}}\)</span>.</li>
<li>We estimate the marginal regression coefficients (<span class="math inline">\(\beta_i\)</span>).</li>
<li>We estimate the marginal error variance (<span class="math inline">\(\tau_{ii}^2\)</span>).</li>
</ol>
</div>
<div id="initialize-gibbs-sampling" class="section level2">
<h2>Initialize Gibbs sampling</h2>
<ol style="list-style-type: decimal">
<li><strong>Process the variable graph</strong>:</li>
</ol>
<p>First, we color the nodes of the variable graph. This will allow us to simulate the latent processes belonging to the same color in parallel.</p>
<p>We also construct a new graph <span class="math inline">\(\mathcal G_E(\mathcal G_V)=(E_\mathcal V,E^*)\)</span> which denotes this graph on the set of edges <span class="math inline">\(E_\mathcal V\)</span>, i.e., there is an edge <span class="math inline">\(((i,j),(i&#39;,j&#39;))\)</span> in this new graph <span class="math inline">\(\mathcal G_E(\mathcal G_V)\)</span> if <span class="math inline">\(\{i,i&#39;,j,j&#39;\}\)</span> are in some clique <span class="math inline">\(K\)</span> of <span class="math inline">\(\mathcal G_V\)</span>. This allows us to facilitate parallel update of cross-correlation parameters corresponding to edges in the same color.</p>
<p>These two procedures are examples of chromatic Gibbs sampling.</p>
<p><img src="/post/2023-07-07-illustration-of-graphical-gaussian-process-models-to-analyze-highly-multivariate-spatial-data_files/unnamed-chunk-13-1.png" /></p>
<p>2.<strong>Calculate initial cross-covariance matrix</strong>: Start with an initial cross-correlation matrix. We Take a convex combination: <span class="math inline">\(0.5*diag(q) + 0.5*cor(Y.train)\)</span>. Then we use this initial cross-correlation parameters and estimated marginal parameters to store the cross covariance matrices for cliques and separators only.</p>
<p>The largest matrix we need to store is of size <span class="math inline">\(nq_c \times nq_c\)</span> matrix where <span class="math inline">\(q_c\)</span> is the size of maximum clique or separator).</p>
</div>
<div id="gibbs-sampling" class="section level2">
<h2>Gibbs sampling</h2>
<p>We iteratively perform the following steps until we reach a desired number of samples (<span class="math inline">\(N\)</span>) -
1. <strong>Sampling latent processes</strong> : Using random draws from multivariate normal distribution
2. <strong>Sampling correlations</strong>: Metropolis-hastings
3. <strong>Jumping between graphs</strong>: Reversible jump MCMC.</p>
<div id="sampling-latent-processes" class="section level3">
<h3>Sampling latent processes</h3>
<p>To update the latent random effects <span class="math inline">\(w\)</span>, let <span class="math inline">\(\mathcal L=\{s_1,\ldots,s_n\}\)</span> and <span class="math inline">\(o_i=\mbox{diag}(I(s_1 \in \mathcal S_i), \ldots, I(s_n \in \mathcal S_i))\)</span> denote the vector of missing observations for the <span class="math inline">\(i\)</span>-th outcome. With <span class="math inline">\(X_{i}(\mathcal L) = (x_i(s_1),\ldots,x_i(s_n))^T\)</span>, <span class="math inline">\(y_i(\mathcal L)\)</span> and <span class="math inline">\(w_i(\mathcal L)\)</span> defined similarly, we obtain
<span class="math display">\[\begin{equation*}
\begin{split}
p(w_i(\mathcal L) | \cdot) &amp; \sim N\left(\mathcal M_i^{-1}\mu_i, \mathcal M_i^{-1}\right)\;, \mbox{ where } \\
\mathcal M_i &amp; =\frac{1}{\tau_i^2}\mbox{diag}(o_i) + \sum_{K \ni i}M_{\{i\} \times \mathcal L|(K\setminus \{i\}) \times \mathcal L}^{-1} - \sum_{S \ni i}M^{-1}_{\{i\} \times \mathcal L|(S\setminus \{i\}) \times \mathcal L}\;, \\
\mu_i &amp;= \frac{(y_i(\mathcal L) - x_i(\mathcal L)^{T}\beta_i)\odot o_i}{\tau_i^2} + \\
&amp; \qquad \sum_{K \ni i}  T_{i}(K) w({(K\setminus \{i\}) \times \mathcal L}) - \sum_{S \ni i} T_i(S) w({(S\setminus \{i\}) \times \mathcal L})\;, \\
T_{i}(A) &amp;= M_{\{i\} \times \mathcal L|(A\setminus \{i\}) \times \mathcal L}^{-1}  M_{\{i\} \times \mathcal L,(A\setminus \{i\}) \times \mathcal L}M_{(A\setminus \{i\}) \times \mathcal L}^{-1}, \mbox{ for } A \in \{K,S\}.\\
% U_i(S) &amp;= M_{\{i\} \times \mathcal L|(S\setminus \{i\}) \times \mathcal L}^{-1}  M_{(S\setminus \{i\}) \times \mathcal L,\{i\} \times \mathcal L}M_{(S\setminus \{i\}) \times \mathcal L}^{-1}.
\end{split}\label{eqn: gibbs-sam-latent}
\end{equation*}\]</span></p>
<p>%We denote <span class="math inline">\(\mathscr T_i= \mathcal L \setminus \mathscr S_i\)</span>.
% for a clique <span class="math inline">\(K\)</span> in variable graph <span class="math inline">\(\mathcal{G_V}\)</span>, the set - <span class="math inline">\(K \times \mathcal L = K \times \mathscr S\)</span>, <span class="math inline">\(S \times \mathcal L= S \times \mathscr S\)</span>,
%<span class="math inline">\(i_{K}=\{K \setminus i\} \times \mathscr S\)</span>, <span class="math inline">\(i \times \mathscr S_i = i_{\mathscr S}\)</span> and <span class="math inline">\(i \times \mathscr T_i= i_{\mathscr T}\)</span>, <span class="math inline">\(i_{K} \cup i_{\mathscr T}= i_{K\mathscr T}\)</span> and <span class="math inline">\(i_{K} \cup i_{\mathscr S}= i_{K \times \mathcal L}\)</span> . Also, for a clique <span class="math inline">\(K\)</span> in containing a variable <span class="math inline">\(i\)</span>, let’s denote <span class="math inline">\(M_{i_{{\mathscr S}.K}} = M_{i_{\mathscr S}} - M_{i_{\mathscr S}, i_{K\mathscr T}}M_{i_{K\mathscr T}}^{-1}M_{i_{K\mathscr T},i_{\mathscr S}}\)</span> and <span class="math inline">\(M_{i_{{\mathscr T}.K}} = M_{i_{\mathscr T}} - M_{i_{\mathscr T}, i_{K \times \mathcal L}}M_{i_{K \times \mathcal L}}^{-1}M_{i_{K \times \mathcal L},i_{\mathscr T}}\)</span>.</p>
</div>
<div id="sampling-cross-correlation-parameters" class="section level3">
<h3>Sampling cross-correlation parameters</h3>
<p>Requires only checking positive-definiteness of the clique-specific cross-covariance matrix and inverting it. The largest matrix inversion across all these updates is of the order <span class="math inline">\(nc \times nc\)</span>, corresponding to the largest clique. The largest matrix that needs storing is also of dimension <span class="math inline">\(nc \times nc\)</span>. These result in appreciable reduction of computations from any multivariate Matérn model that involves <span class="math inline">\(nq \times nq\)</span> matrices and positive-definiteness checks for <span class="math inline">\(q \times q\)</span> matrices at every iteration.</p>
<ol style="list-style-type: decimal">
<li><p>For every correlation parameter corresponding to edges in the current graph, we draw a new correlation value from the proposal distribution.</p></li>
<li><p>We accept or reject the proposal based on the Metropolis-Hastings acceptance probability.</p></li>
</ol>
</div>
</div>
</div>
<div id="results" class="section level1">
<h1>Results</h1>
<p>We create three plots below among true and estimated parameters for - (1) product of marginal scale and variance parameter (<span class="math inline">\(\sigma_{ii}\)</span>), (2) cross-correlation parameter (<span class="math inline">\(r_{ij}\)</span>) and (3) product of cross-covariance and cross-scale parameter (<span class="math inline">\(\sigma_{ij}*\phi_{ij}\)</span>). The points on all the plots align on the <span class="math inline">\(y=x\)</span> line thus showing the accuracy of our estimation. We also create a plot across <span class="math inline">\(10\)</span> variables showing true test data values and predicted values from our model. The points align on <span class="math inline">\(y=x\)</span> line and mean-square error varied from <span class="math inline">\(0.403\)</span> to <span class="math inline">\(1.064\)</span>.</p>
<p><img src="/post/2023-07-07-illustration-of-graphical-gaussian-process-models-to-analyze-highly-multivariate-spatial-data_files/unnamed-chunk-15-1.png" /></p>
<p><img src="/post/2023-07-07-illustration-of-graphical-gaussian-process-models-to-analyze-highly-multivariate-spatial-data_files/unnamed-chunk-15-2.png" /></p>
<p><img src="/post/2023-07-07-illustration-of-graphical-gaussian-process-models-to-analyze-highly-multivariate-spatial-data_files/unnamed-chunk-15-3.png" /></p>
<p><img src="/post/2023-07-07-illustration-of-graphical-gaussian-process-models-to-analyze-highly-multivariate-spatial-data_files/unnamed-chunk-15-4.png" /></p>
</div>
<div id="about-the-authors" class="section level1">
<h1>About the authors</h1>
<ul>
<li><a href="https://debangandey.rbind.io">Debangan Dey</a>, National Institute of Mental Health.</li>
<li><a href="https://abhidatta.com/">Abhirup Datta</a>, Dept. of Biostatistics, Johns Hopkins Bloomberg School of Public Health.</li>
<li><a href="http://sudipto.bol.ucla.edu/">Sudipto Banerjee</a>, Dept. of Biostatistics, UCLA Fielding School of Public Health.</li>
</ul>
<div id="bibliography" class="section level2 unnumbered">
<h2>Bibliography</h2>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-apanasovich2012valid" class="csl-entry">
Apanasovich, Marc G Genton, Tatiyana V, and Ying Sun. 2012. <span>“A Valid Matern Class of Cross-Covariance Functions for Multivariate Random Fields with Any Number of Components.”</span> <em>Journal of the American Statistical Association</em> 107 (497): 180–93.
</div>
<div id="ref-ban14" class="csl-entry">
Banerjee, B. P. Carlin, S., and A. E. Gelfand. 2014. <em>Hierarchical Modeling and Analysis for Spatial Data</em>. Chapman &amp; Hall/CRC, Boca Raton, FL.
</div>
<div id="ref-creswikle11" class="csl-entry">
Cressie, Noel A. C., and Christopher K. Wikle. 2011. <em>Statistics for Spatio-Temporal Data</em>. Wiley Series in Probability and Statistics. Wiley, Hoboken, NJ.
</div>
<div id="ref-dey2021ggp" class="csl-entry">
Dey, Abhirup Datta, Debangan, and Sudipto Banerjee. 2021. <span>“Graphical Gaussian Process Models for Highly Multivariate Spatial Data.”</span> <em>Biometrika</em> December.
</div>
<div id="ref-lauritzen1996graphical" class="csl-entry">
Lauritzen, Steffen L. 1996. <em>Graphical Models</em>. Vol. 17. Clarendon Press.
</div>
<div id="ref-wackernagel2013multivariate" class="csl-entry">
Wackernagel, Hans. 2013. <em>Multivariate Geostatistics: An Introduction with Applications</em>. Springer Science &amp; Business Media.
</div>
</div>
</div>
</div>

</main>


















</article>
</div>

<script async src="//yihui.name/js/center-img.js"></script>
 


<script src="//yihui.name/js/math-code.js"></script>
<script async src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>






</body>
</html>

