<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>Distribution generalization and causal inference | YoungStatS</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <link href="//cdn.bootcss.com/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">

  </head>

  <body class="page">
    <nav class="header">
      <div class="banner">
<a href="/" class="text">
<div class="yourfancytitle"> YoungStatS </div> 
</a>
</div>

      <div class="head-meta">
      
        <span><a href="/">&larr; Back to Home</a></span>
       <span>The blog of Young Statisticians Europe (YSE)</span>
        
        
        
          
        
        
        
        
      
      </div>
    </nav>
    
    <footer>
<script src="//yihui.org/js/math-code.js"></script>
<script async
src="cdnjs.cloudflare.com/ajax/libs/mathjax/3.1.0?config=TeX-MML-AM_CHTML">
</script>
<div class="footer">
  <ul class="menu">
    
    <li><a href="/"><span data-hover="Home">Home</span></a></li>
    
    <li><a href="/categories/webinars"><span data-hover="Webinars">Webinars</span></a></li>
    
    <li><a href="/about/"><span data-hover="About">About</span></a></li>
    
    <li><a href="/categories/"><span data-hover="Categories">Categories</span></a></li>
    
  </ul>
  
</div>
</footer>

<div class="container">
<article>
<div class="article-meta">

  <div class="categories">
  
    <a href="/categories/webinars">webinars</a>
  
     &hercon; <a href="/categories/causal-inference">causal-inference</a>
  
     &hercon; <a href="/categories/data-science">data-science</a>
  
     &hercon; <a href="/categories/machine-learning">machine-learning</a>
  
  </div>

  <h1><span class="title">Distribution generalization and causal inference</span></h1>

  

  
  

</div>



<main>



<p><strong>Distribution generalization and causal inference</strong></p>
<p>Monday, March 20th, 2023, 7:00 PT / 10:00 ET / 15:00 CET</p>
<p><img src="/post/2023-02-28-distribution-generalization-in-causal-inference_files/cover_jointimszurichyoungstats_causal.png" /></p>
<p>1st joint webinar of the <a href="https://imstat.org/ims-groups/ims-new-researchers-group/">IMS New Researchers Group</a>, <a href="https://math.ethz.ch/sfs/news-and-events/young-data-science.html">Young Data Science Researcher Seminar Zürich</a> and the YoungStatS Project.</p>
<p>When &amp; Where:</p>
<ul>
<li>Monday, March 20th, 2023, 7:00 PT / 10:00 ET / 15:00 CET</li>
<li>Online, via Zoom. The registration form is available
<a href="https://docs.google.com/forms/d/e/1FAIpQLSdWLS-kZZUhaNHxmEmVUWLiUzKQ6JARRCHrrj-B3tOgWWrkgQ/viewform?usp=mail_form_link">here</a>.</li>
</ul>
<p>Speakers:</p>
<ul>
<li><a href="https://statweb.rutgers.edu/zijguo/">Zijian Guo</a>, Rutgers University: “<em>Statistical Inference for Maximin Effects: Identifying Stable Associations across Multiple Studies</em>”</li>
</ul>
<p>Abstract: Integrative analysis of data from multiple sources is critical to making generalizable discoveries. Associations that are consistently observed across multiple source populations are more likely to be generalized to target populations with possible distributional shifts. In this paper, we model the heterogeneous multi-source data with multiple high-dimensional regressions and make inferences for the maximin effect (Meinshausen and Bühlmann, AoS, 43(4), 1801–1830). The maximin effect provides a measure of stable associations across multi-source data. A significant maximin effect indicates that a variable has commonly shared effects across multiple source populations, and these shared effects may be generalized to a broader set of target populations. There are challenges associated with inferring maximin effects because its point estimator can have a non-standard limiting distribution. We devise a novel sampling method to construct valid confidence intervals for maximin effects. The proposed confidence interval attains a parametric length. This sampling procedure and the related theoretical analysis are of independent interest for solving other non-standard inference problems. Using genetic data on yeast growth in multiple environments, we demonstrate that the genetic variants with significant maximin effects have generalizable effects under new environments.</p>
<ul>
<li><a href="https://saramagliacane.github.io/">Sara Magliacane</a>, University of Amsterdam and MIT-IBM Watson AI LAB: “<em>Causality-inspired ML: what can causality do for ML?</em>”</li>
</ul>
<p>Abstract: Applying machine learning to real-world cases often requires methods that are robust w.r.t. heterogeneity, missing not at random or corrupt data, selection bias, non i.i.d. data etc. and that can generalize across different domains. Moreover, many tasks are inherently trying to answer causal questions and gather actionable insights, a task for which correlations are usually not enough. Several of these issues are addressed in the rich causal inference literature. On the other hand, often classical causal inference methods require either a complete knowledge of a causal graph or enough experimental data (interventions) to estimate it accurately. Recently, a new line of research has focused on causality-inspired machine learning, i.e. on the application ideas from causal inference to machine learning methods without necessarily knowing or even trying to estimate the complete causal graph. In this talk, I will present an example of this line of research in the unsupervised domain adaptation case, in which we have labelled data in a set of source domains and unlabelled data in a target domain (“zero-shot”), for which we want to predict the labels. In particular, given certain assumptions, our approach is able to select a set of provably “stable” features (a separating set), for which the generalization error can be bound, even in case of arbitrarily large distribution shifts. As opposed to other works, it also exploits the information in the unlabelled target data, allowing for some unseen shifts w.r.t. to the source domains. While using ideas from causal inference, our method never aims at reconstructing the causal graph or even the Markov equivalence class, showing that causal inference ideas can help machine learning even in this more relaxed setting.</p>
<p>Discussant: <a href="https://niklaspfister.github.io/">Niklas Pfister</a>, University of Copenhagen</p>
<p>YoungStatS project of the Young Statisticians Europe initiative (FENStatS) is supported by the Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics (IMS).</p>

</main>





</article>
</div>

<script async src="//yihui.name/js/center-img.js"></script>
 







</body>
</html>

