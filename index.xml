<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>YoungStatS</title>
    <link>https://youngstats.github.io/</link>
    <description>Recent content on YoungStatS</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 06 Apr 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://youngstats.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Developments in Bayesian Nonparametrics</title>
      <link>https://youngstats.github.io/post/2021/04/06/bnp-webinar/</link>
      <pubDate>Tue, 06 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/04/06/bnp-webinar/</guid>
      <description>&lt;p&gt;The second &lt;em&gt;&amp;ldquo;One World webinar&amp;rdquo;&lt;/em&gt; organized by YoungStatS will take place on April 21st.
The focus of this webinar will be on illustrating modern advances in Bayesian Nonparametrics data analysis, discussing challenging theoretical problems and stimulating case-studies within this active area of research.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Wednesday, April 21st, 16:30 CEST&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Online&lt;/em&gt;, via Zoom. The registration form is available &lt;a href=&#34;https://forms.gle/vfinGjQJMeqhq6HQ7&#34;&gt;here&lt;/a&gt;. Further details and the Zoom link will be sent to the registered addresses only.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://csml.stats.ox.ac.uk/people/panero/&#34;&gt;Francesca Panero&lt;/a&gt; (University of Oxford, UK): &lt;em&gt;&amp;ldquo;Sparse Spatial Random Graphs&amp;rdquo;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://martacatalano.github.io/&#34;&gt;Marta Catalano&lt;/a&gt; (University of Torino, Italy). &lt;em&gt;&amp;ldquo;Measuring dependence in the Wasserstein distance for Bayesian nonparametric models&amp;rdquo;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://salleuska.github.io/&#34;&gt;Sally Paganin&lt;/a&gt; (Harvard School of Public Health, USA) &lt;em&gt;&amp;ldquo;Informative model-based clustering via Centered Partition Processes&amp;rdquo;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.google.com/unimib.it/camerlenghi-federico/&#34;&gt;Federico Camerlenghi&lt;/a&gt; (University of Milano Bicocca, Italy)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;The webinar is part of YoungStatS project of the Young Statisticians Europe initiative (FENStatS) supported by the Bernoulli Society for Mathematical Statistics and Probability.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For more information, please visit our &lt;a href=&#34;https://youngstats.github.io/post/2020/09/10/webinars/&#34;&gt;website&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Analysis of a Two-Layer Neural Network via Displacement Convexity</title>
      <link>https://youngstats.github.io/post/2021/03/14/analysis-of-a-two-layer-neural-network-via-displacement-convexity/</link>
      <pubDate>Sun, 14 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/03/14/analysis-of-a-two-layer-neural-network-via-displacement-convexity/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;abstract&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;We consider the problem of learning a function defined on a compact domain, using linear combinations
of a large number of “bump-like” components (neurons). This idea lies at the core of a variety of methods
from two-layer neural networks to kernel regression, to boosting. In general, the resulting risk minimization
problem is non-convex and is solved by gradient descent or its variants. Nonetheless, little is known about
global convergence properties of these approaches. In this work, we show that, as the number of neurons
diverges and the bump width tends to zero, the gradient flow has a limit which is a viscous porous medium
equation. By virtue of a property named “displacement convexity,” we show an exponential dimension-free
convergence rate for gradient descent.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-14-analysis-of-a-two-layer-neural-network-via-displacement-convexity_files/screenshot.82.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This post is based on the paper &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-javanmard2020analysis&#34; role=&#34;doc-biblioref&#34;&gt;Javanmard et al. 2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fitting a function with a linear combination of components.&lt;/strong&gt; In
supervised learning, we are given data points
&lt;span class=&#34;math inline&#34;&gt;\(\{(y_j,{\boldsymbol{x}}_j)\}_{j\le n}\)&lt;/span&gt;, which are often assumed to be
independent and identically distributed. Here
&lt;span class=&#34;math inline&#34;&gt;\({\boldsymbol{x}}_j\in {\mathbb R}^d\)&lt;/span&gt; is a feature vector, and
&lt;span class=&#34;math inline&#34;&gt;\(y_j\in{\mathbb R}\)&lt;/span&gt; is a label or response variable. We would like to
fit a model &lt;span class=&#34;math inline&#34;&gt;\(\widehat{f}:{\mathbb R}^d\to{\mathbb R}\)&lt;/span&gt; to predict the
labels at new points &lt;span class=&#34;math inline&#34;&gt;\({\boldsymbol{x}}\in{\mathbb R}^d\)&lt;/span&gt;. One of the most
fruitful ideas in this context is to use functions that are linear
combinations of simple components:
&lt;span class=&#34;math display&#34;&gt;\[\widehat{f}({\boldsymbol{x}};{\boldsymbol{w}}) = \frac{1}{N}\sum_{i=1}^N \sigma({\boldsymbol{x}};{\boldsymbol{w}}_i)\, ,\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\sigma:{\mathbb R}^d\times{\mathbb R}^D\to{\mathbb R}\)&lt;/span&gt; is a
component function (a ‘neuron’ or ‘unit’ in the neural network
parlance), and
&lt;span class=&#34;math inline&#34;&gt;\({\boldsymbol{w}}=({\boldsymbol{w}}_1,\dots,{\boldsymbol{w}}_N)\in{\mathbb R}^{D\times N}\)&lt;/span&gt;
are the parameters to be learnt from data. Specific instantiations of
this idea include, e.g., two-layer neural networks with radial
activations, sparse deconvolution, kernel ridge regression, random
feature methods, and boosting.&lt;/p&gt;
&lt;p&gt;A common approach towards fitting parametric models is by risk
minimization:
&lt;span class=&#34;math display&#34;&gt;\[R_N({\boldsymbol{w}}) = {\mathbb E}\Big\{\Big[y-\frac{1}{N}\sum_{i=1}^N\sigma({\boldsymbol{x}};{\boldsymbol{w}}_i)\Big]^2\Big\}\, .\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Despite the impressive practical success of these methods, the risk
function &lt;span class=&#34;math inline&#34;&gt;\(R_N({\boldsymbol{w}})\)&lt;/span&gt; is highly non-convex and little is
known about the global convergence of algorithms that try to minimize
it. The main objective of this work is to introduce a nonparametric
underlying regression model for which a global convergence result can be
proved for stochastic gradient descent (SGD), in the limit of a large
number of neurons.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Setting and main result.&lt;/strong&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(\Omega\subset{\mathbb R}^d\)&lt;/span&gt; be a
compact convex set with smooth boundary, and let
&lt;span class=&#34;math inline&#34;&gt;\(\{(y_j,{\boldsymbol{x}}_j)\}_{j\ge 1}\)&lt;/span&gt; be i.i.d. with
&lt;span class=&#34;math inline&#34;&gt;\({\boldsymbol{x}}_j\sim {\sf Unif}(\Omega)\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\({\mathbb E}(y_j|{\boldsymbol{x}}_j)=f({\boldsymbol{x}}_j)\)&lt;/span&gt;, where the
function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is smooth. We try to fit these data using a combination of
bumps, namely
&lt;span class=&#34;math display&#34;&gt;\[\widehat{f}({\boldsymbol{x}};{\boldsymbol{w}})= \frac{1}{N}\sum_{i=1}^NK^\delta({\boldsymbol{x}}-{\boldsymbol{w}}_i)\, ,\]&lt;/span&gt;
where
&lt;span class=&#34;math inline&#34;&gt;\(K^\delta({\boldsymbol{x}}) = \delta^{-d}K({\boldsymbol{x}}/\delta)\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(K:{\mathbb R}^d\to{\mathbb R}_{\ge 0}\)&lt;/span&gt; is a first order kernel with
compact support. The weights &lt;span class=&#34;math inline&#34;&gt;\({\boldsymbol{w}}_i\in{\mathbb R}^d\)&lt;/span&gt;
represent the centers of the bumps. Our model is general enough to
include a broad class of radial-basis function (RBF) networks which are
known to be universal function approximators. To the best of our
knowledge, there is no result on the global convergence of stochastic
gradient descent for learning RBF networks, and we establish the first
result of this type.&lt;/p&gt;
&lt;p&gt;We prove that, for sufficiently large &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; and small &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;, gradient
descent algorithms converge to weights &lt;span class=&#34;math inline&#34;&gt;\({\boldsymbol{w}}\)&lt;/span&gt; with nearly
optimum prediction error, provided &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is strongly concave. Let us
emphasize that the resulting population risk &lt;span class=&#34;math inline&#34;&gt;\(R_N({\boldsymbol{w}})\)&lt;/span&gt; is
non-convex regardless of the concavity properties of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;. Our proof
unveils a novel mechanism by which global convergence takes place.
Convergence results for non-convex empirical risk minimization are
generally proved by carefully ruling out local minima in the cost
function. Instead we prove that, as &lt;span class=&#34;math inline&#34;&gt;\(N\to\infty\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\delta\to 0\)&lt;/span&gt;, the
gradient descent dynamics converges to a gradient flow in Wasserstein
space, and that the corresponding cost function is ‘displacement
convex.’ Breakthrough results in optimal transport theory guarantee
dimension-free convergence rates for this limiting dynamics
&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-carrillo2003kinetic&#34; role=&#34;doc-biblioref&#34;&gt;Carrillo, McCann, and Villani 2003&lt;/a&gt;)&lt;/span&gt;. In particular, we expect the cost function
&lt;span class=&#34;math inline&#34;&gt;\(R_N({\boldsymbol{w}})\)&lt;/span&gt; to have many local minima, which are however
completely neglected by gradient descent.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof idea.&lt;/strong&gt; Let us start by providing some high-level insights about
our approach. Think about each model parameter as a particle moving
under the effect of other particles according to the SGD updates. Now,
instead of studying the microscopic dynamics of this system of
particles, we analyze the macroscopic dynamics of the medium when the
number of particles (i.e., the size of the hidden layer of the neural
network) goes to infinity. These dynamics are formulated through a
partial differential equation (more specifically, a viscous porous
medium equation) that describes the evolution of the mass density over
space and time. The nice feature of this approach is that, while the SGD
trajectory is a random object, it shows that in the large particle size
limit, it concentrates around the deterministic solution of this partial
differential equation (PDE).&lt;/p&gt;
&lt;p&gt;For a rigorous analysis and implementation of this idea, we use
propagation-of-chaos techniques. Specifically, we show that, in the
large &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; limit, the evolution of the weights
&lt;span class=&#34;math inline&#34;&gt;\(\{{\boldsymbol{w}}_i\}_{i=1}^N\)&lt;/span&gt; under gradient descent can be replaced
by the evolution of a probability distribution &lt;span class=&#34;math inline&#34;&gt;\(\rho^{\delta}_t\)&lt;/span&gt; which
satisfies the viscous porous medium PDE (with Neumann boundary
conditions). This PDE can also be described as the Wasserstein &lt;span class=&#34;math inline&#34;&gt;\(W_2\)&lt;/span&gt;
gradient flow for the following effective risk
&lt;span class=&#34;math display&#34;&gt;\[R^{\delta}(\rho) = \frac{1}{|\Omega|} \, \int_{\Omega} \big[f({\boldsymbol{x}}) - K^\delta\ast \rho({\boldsymbol{x}})\big]^2{\rm d}{\boldsymbol{x}}\, ,\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(|\Omega|\)&lt;/span&gt; is the volume of the set &lt;span class=&#34;math inline&#34;&gt;\(\Omega\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\ast\)&lt;/span&gt; is the
usual convolution. The use of &lt;span class=&#34;math inline&#34;&gt;\(W_2\)&lt;/span&gt; gradient flows to analyze two-layer
neural networks was recently developed in several papers
&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mei2018mean&#34; role=&#34;doc-biblioref&#34;&gt;Mei, Montanari, and Nguyen 2018&lt;/a&gt;; &lt;a href=&#34;#ref-rotskoff2018neural&#34; role=&#34;doc-biblioref&#34;&gt;Rotskoff and Vanden-Eijnden 2019&lt;/a&gt;; &lt;a href=&#34;#ref-chizat2018global&#34; role=&#34;doc-biblioref&#34;&gt;Chizat and Bach 2018&lt;/a&gt;; &lt;a href=&#34;#ref-sirignano2018mean&#34; role=&#34;doc-biblioref&#34;&gt;Sirignano and Spiliopoulos 2020&lt;/a&gt;)&lt;/span&gt;.
However, we cannot rely on earlier results because of the specific
boundary conditions in our problem.&lt;/p&gt;
&lt;p&gt;Note that even though the cost &lt;span class=&#34;math inline&#34;&gt;\(R^{\delta}(\rho)\)&lt;/span&gt; is quadratic and
convex in &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;, its &lt;span class=&#34;math inline&#34;&gt;\(W_2\)&lt;/span&gt; gradient flow can have multiple fixed
points, and hence global convergence cannot be guaranteed. Indeed, the
mathematical property that controls global convergence of &lt;span class=&#34;math inline&#34;&gt;\(W_2\)&lt;/span&gt; gradient
flows is not ordinary convexity but &lt;em&gt;displacement convexity&lt;/em&gt;. Roughly
speaking, displacement convexity is convexity along geodesics of the
&lt;span class=&#34;math inline&#34;&gt;\(W_2\)&lt;/span&gt; metric. Note that the risk function &lt;span class=&#34;math inline&#34;&gt;\(R^{\delta}(\rho)\)&lt;/span&gt; is &lt;em&gt;not&lt;/em&gt;
even displacement convex. However, for small &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;, we can formally
approximate &lt;span class=&#34;math inline&#34;&gt;\(K^\delta\ast \rho\approx \rho\)&lt;/span&gt;, and hence hope to replace
the risk function &lt;span class=&#34;math inline&#34;&gt;\(R^{\delta}(\rho)\)&lt;/span&gt; with the simpler one
&lt;span class=&#34;math display&#34;&gt;\[R(\rho) = \frac{1}{|\Omega|}\int_{\Omega} \big[f({\boldsymbol{x}}) - \rho({\boldsymbol{x}})\big]^2{\rm d}{\boldsymbol{x}}\, .\]&lt;/span&gt;
Most of our technical work is devoted to making rigorous this
&lt;span class=&#34;math inline&#34;&gt;\(\delta\to 0\)&lt;/span&gt; approximation.&lt;/p&gt;
&lt;p&gt;Remarkably, the risk function &lt;span class=&#34;math inline&#34;&gt;\(R(\rho)\)&lt;/span&gt; is strongly displacement convex
(provided &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is strongly concave). A long line of work in PDE and
optimal transport theory establishes dimension-free convergence rates
for its &lt;span class=&#34;math inline&#34;&gt;\(W_2\)&lt;/span&gt; gradient flow &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-carrillo2003kinetic&#34; role=&#34;doc-biblioref&#34;&gt;Carrillo, McCann, and Villani 2003&lt;/a&gt;)&lt;/span&gt;. By putting
everything together, we are able to show that SGD converges
exponentially fast to a near-global optimum with a rate that is
controlled by the convexity parameter of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A numerical illustration.&lt;/strong&gt; We demonstrate in a simple numerical
example that the convergence rate predicted by our asymptotic theory is
in excellent agreement with simulations. In the left column of the
figure below, we plot the true function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; together with the neural
network estimate obtained by running stochastic gradient descent (SGD)
with &lt;span class=&#34;math inline&#34;&gt;\(N=200\)&lt;/span&gt; neurons at several points in time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. Different plots
correspond to different values of &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; with
&lt;span class=&#34;math inline&#34;&gt;\(\delta\in\{1/5, 1/10, 1/20\}\)&lt;/span&gt;. We observe that the network estimates
converge to a limit curve which is an approximation of the true function
&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;. As expected, the quality of the approximation improves as &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;
gets smaller. In the right column, we report the evolution of the
population risk and we compare it to the risk predicted by the limit PDE
(corresponding to &lt;span class=&#34;math inline&#34;&gt;\(\delta=0\)&lt;/span&gt;). The PDE curve appears to capture well the
evolution of SGD towards optimality.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-14-analysis-of-a-two-layer-neural-network-via-displacement-convexity_files/screenshot.81.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;authors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Authors&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-14-analysis-of-a-two-layer-neural-network-via-displacement-convexity_files/javanmard.jpg&#34; width=&#34;75&#34; /&gt;
Adel Javanmard, Data Science and Operations Department, Marshall School of Business, University of Southern California&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-14-analysis-of-a-two-layer-neural-network-via-displacement-convexity_files/mondelli.jpg&#34; width=&#34;75&#34; /&gt;
Marco Mondelli, Institute of Science and Technology (IST) Austria&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-14-analysis-of-a-two-layer-neural-network-via-displacement-convexity_files/montanari.jpg&#34; width=&#34;75&#34; /&gt;
Andrea Montanari, Department of Electrical Engineering and Department of Statistics, Stanford University&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-carrillo2003kinetic&#34; class=&#34;csl-entry&#34;&gt;
Carrillo, José A., Robert J. McCann, and Cédric Villani. 2003. &lt;span&gt;“Kinetic Equilibration Rates for Granular Media and Related Equations: Entropy Dissipation and Mass Transportation Estimates.”&lt;/span&gt; &lt;em&gt;Revista Matematica Iberoamericana&lt;/em&gt; 19 (3): 971–1018.
&lt;/div&gt;
&lt;div id=&#34;ref-chizat2018global&#34; class=&#34;csl-entry&#34;&gt;
Chizat, Lenaic, and Francis Bach. 2018. &lt;span&gt;“On the Global Convergence of Gradient Descent for over-Parameterized Models Using Optimal Transport.”&lt;/span&gt; In &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt;, 3040–50.
&lt;/div&gt;
&lt;div id=&#34;ref-javanmard2020analysis&#34; class=&#34;csl-entry&#34;&gt;
Javanmard, Adel, Marco Mondelli, Andrea Montanari, and others. 2020. &lt;span&gt;“Analysis of a Two-Layer Neural Network via Displacement Convexity.”&lt;/span&gt; &lt;em&gt;Annals of Statistics&lt;/em&gt; 48 (6): 3619–42.
&lt;/div&gt;
&lt;div id=&#34;ref-mei2018mean&#34; class=&#34;csl-entry&#34;&gt;
Mei, Song, Andrea Montanari, and Phan-Minh Nguyen. 2018. &lt;span&gt;“A Mean Field View of the Landscape of Two-Layer Neural Networks.”&lt;/span&gt; &lt;em&gt;Proceedings of the National Academy of Sciences&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.1073/pnas.1806579115&#34;&gt;https://doi.org/10.1073/pnas.1806579115&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-rotskoff2018neural&#34; class=&#34;csl-entry&#34;&gt;
Rotskoff, Grant M., and Eric Vanden-Eijnden. 2019. &lt;span&gt;“Trainability and Accuracy of Neural Networks: An Interacting Particle System Approach.”&lt;/span&gt; &lt;em&gt;Communications on Pure and Applied Mathematics&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-sirignano2018mean&#34; class=&#34;csl-entry&#34;&gt;
Sirignano, Justin, and Konstantinos Spiliopoulos. 2020. &lt;span&gt;“Mean Field Analysis of Neural Networks: A Law of Large Numbers.”&lt;/span&gt; &lt;em&gt;SIAM Journal on Applied Mathematics&lt;/em&gt; 80 (2): 725–52.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Scalable Empirical Bayes Approach to Variable Selection in Generalized Linear Models</title>
      <link>https://youngstats.github.io/post/2021/03/13/a-scalable-empirical-bayes-approach-to-variable-selection-in-generalized-linear-models/</link>
      <pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/03/13/a-scalable-empirical-bayes-approach-to-variable-selection-in-generalized-linear-models/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In the toolbox of most scientists over the past century, there have been few methods as powerful and as versatile as linear regression. The introduction of the generalized linear model (GLM) framework in the 1970’s extended the inferential and predictive capabilities to binary or count data. While the effect of this ‘Swiss Army knife’ of scientific research cannot be overstated, rapid (and amazing) technological advances in other areas have pushed it beyond its theoretical capacity.&lt;/p&gt;
&lt;p&gt;Linear regression is obtained by applying Gauss’ ordinary least squares (OLS) method, which relies on having a sample size which is sufficiently larger than the number of predictors. However, in this age of ‘high throughput’ data, such as RNA sequencing, the number of predictors (say, the expression level of thousands of genes) greatly exceeds the number of observations. This is often called the ‘large P, small n’ problem. In such cases, the OLS procedure does not work, and the natural next step is to perform variable selection and choose a small number of predictors which are associated with the response variable.&lt;/p&gt;
&lt;p&gt;The question is, how to choose the right predictors? A good selection method should detect most (ideally, all) the true predictors, but not at the expense of getting too many false ones (in the ideal case, it would yield none.) This has been one of the greatest challenges in statistics in the past 25 years. The most widely used approach is based on adding a ‘penalty term’ to the optimization problem, so as to practically put a constraint on the number of predictors which can have a non-zero effect on the outcome. Among the methods which rely on this approach, the most well-known is the lasso. The approach we take in our paper is different. We assume that the effect predictors have on the outcome can be either positive, negative, or have no effect at all (which we call null predictors). Mathematically, we model it as a three-component mixture in which the effects of the non-null predictors are assumed to follow normal distributions. This approach is related to the Bayesian ‘spike-and-slab’ approach. The difference is that in the spike-and-slab model, the non-null predictors are assumed to follow a single normal distribution with mean 0. The difference between the two approaches is illustrated in Figure 1.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-14-a-scalable-empirical-bayes-approach-to-variable-selection-in-generalized-linear-models_files/fig1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 1 A graphical representation of the spike and slab model vs. our mixture model&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Real data, however, is noisy, which means that the measured effects of the null predictors is not a spike at zero. Rather, it is a Gaussian distribution which has a fair amount of overlap with the distributions of the true predictors. This is what makes the variable selection task so hard, regardless of which approach is used.&lt;/p&gt;
&lt;p&gt;The three-component mixture of normal distributions which we use in our model allows us, with a simple adaptation, to perform variable selection in the GLM framework. For example, we apply it to binary-type response in order to identify which gut bacteria is associated with obesity status. It can also be used to perform variable selection in survival analysis by modeling the number of new events at time t as a Poisson process. We identify five genes which are associated with survival probability of breast cancer patients.&lt;/p&gt;
&lt;p&gt;Our method (which is implemented in an R package called SEMMS) includes three important features. First, it allows the user to ‘lock in’ predictors, which means that it is possible to choose certain substantively important variables to always be included in the model, and not be subject to the selection procedure. Second, for the initialization of the algorithm we can use the variables selected by other methods (e.g., lasso, MCP) and because the algorithm will never result in a decrease in the log-likelihood from one iteration to the next, our final model will always be at least as good as the one obtained by the other variable selection method which was used in the initialization step. Third, the model accounts for correlations between pairs of predictors. This greatly alleviates the problem of identifying true predictors in the presence of multicollinearity. Methods based on penalization often struggle in the presence of multicollinearity. This is demonstrated in the following example, where the objective is to identify which of 4,088 genes is associated with the production rate of riboflavin. Out of 8,353,828 possible pairs of genes, 70,349 have correlation coefficient greater than 0.8 (in absolute value). This high degree of multicollinearity, combined with the relatively small sample size (N=71) makes this dataset especially challenging for variable selection methods. The model selected by SEMMS results in much smaller residuals than ones obtained from other variable selection approaches. It also provides much better prediction for low values of riboflavin.&lt;/p&gt;
&lt;p&gt;We conclude with a few words of caution. No matter which method is used, beware of declaring the selected model as the “best” because (i) the number of putative variables is so large that there is no way to evaluate all possible models, and (ii) some selected predictors can be part of a network of highly correlated variables. The riboflavin data provides an excellent example, which is illustrated in network diagram in Figure 2.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-14-a-scalable-empirical-bayes-approach-to-variable-selection-in-generalized-linear-models_files/fig2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 2 Riboflavin data – a graphical representation of the model found by SEMMS&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;SEMMS detects six strong predictors for riboflavin production rate (shown as red diamonds), but it also detects that four of them are highly correlated with other genes (orange dots). One gene (YURQ_at) is co-expressed with a large group of genes, each of which could be considered as a relevant predictor for the response. A network diagram like this helps to illustrate not only that there may be no single “best” model, but also that there may be complicated interactions between the predictors, and thus, the relationship between the predictors and the response may not be linear.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-14-a-scalable-empirical-bayes-approach-to-variable-selection-in-generalized-linear-models_files/HaimBar-280x400.jpg&#34; width=&#34;75&#34; /&gt;
Haim Bar is an Associate Professor in Statistics at the University of Connecticut. His professional interests include statistical modeling, shrinkage estimation, high throughput applications in genomics, Bayesian statistics, variable selection, and machine learning.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-14-a-scalable-empirical-bayes-approach-to-variable-selection-in-generalized-linear-models_files/Jim%20Booth_crop.jpg&#34; width=&#34;75&#34; /&gt;
James Booth is a Professor in the Department of Statistics and Data Science at Cornell University. He completed his PhD at the University of Kentucky in 1987 working with advisers Joseph Gani and Richard Kryscio, after which he was hired as an Assistant Professor at the University of Florida. He was hired at Cornell in 2004.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-14-a-scalable-empirical-bayes-approach-to-variable-selection-in-generalized-linear-models_files/marty-wells_crop.jpg&#34; width=&#34;75&#34; /&gt;
Martin T. Wells is the Charles A. Alexander Professor of Statistical Science and Chair of the Department of Statistics and Science at Cornell University. He also has joint appointments in the Cornell Law and Weill Medical Schools.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Compositional scalar-on-function regression as a tool (not only) for geological data</title>
      <link>https://youngstats.github.io/post/2021/03/10/compositional-scalar-on-function-regression-as-a-tool-not-only-for-geological-data/</link>
      <pubDate>Wed, 10 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/03/10/compositional-scalar-on-function-regression-as-a-tool-not-only-for-geological-data/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Compositional data are characterized by the fact that the relevant information is contained not necessarily in the absolute values but rather in the relative proportions between particular components. As an example, take household expenditures for different purposes (housing, groceries, travel etc.) or geochemical composition of a certain soil sample. In the latter case, the resulting composition of chemical elements is determined strongly by the particle size distribution (PSD, i.e., distribution of the size of soil grains). These distributions - although sampled in their discrete form as histogram data - show both relative and functional character and therefore can be described through probability density functions. A valid question to ask is how to modify the common multiple and/or functional regression model for the introduced case of relative (compositional) data.&lt;/p&gt;
&lt;div id=&#34;functional-regression-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Functional regression model&lt;/h2&gt;
&lt;p&gt;First, take a look at the standard functional data analysis (FDA) approach which was developed for functions from &lt;span class=&#34;math inline&#34;&gt;\(L^2\)&lt;/span&gt; space. A functional linear regression model with functional predictor is built as
&lt;span class=&#34;math display&#34;&gt;\[
y_{i} = \beta_{0} + \int_{I} \beta_{1}(t)f_{i}(t)dt + \epsilon_{i},\quad i=1,\dots,N,\quad t \in I
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\beta_{0}\)&lt;/span&gt; is the scalar intercept and &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt; represents the functional regression parameter. This model can be seen as an extension of the multiple regression - therefore, the estimators &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta_{0}}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta_{1}}\)&lt;/span&gt; minimize the following sum of squared errors (SSE)
&lt;span class=&#34;math display&#34;&gt;\[
\text{SSE} (\beta_{0},\beta_{1}) = \sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\int_{I}\beta_{1}(t)f_{i}(t)dt\right)^2.
\]&lt;/span&gt;
Unfortunately, it is not common for functional data to be available in its continuous form - we are usually left with dicrete observations. To represent the sparsely measured data as functions, a proper basis expansion for both the predictors and &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt; is necessary. This way, a reduction to a multivariate problem can be achieved. Furthermore, it is useful to apply the results of functional principal component analysis (FPCA) to project the data into a lower-dimensional space.&lt;/p&gt;
&lt;p&gt;But how can we use these ideas and adapt them for the situation where the covariate consists of density functions? As each PSD forms a probability density function on the considered support, specific properties of densities (scale invariance, relative scale, unit integral) prevent from using standard FDA methods directly to PSDs. Instead, we acknowledge the possibility to represent density functions in the Bayes space &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B^2}\)&lt;/span&gt; with square-integrable log-densities as they can be then adequately represented in the &lt;span class=&#34;math inline&#34;&gt;\(L^2\)&lt;/span&gt; space due to the isomorphism between &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B^2}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(L^2\)&lt;/span&gt;. Frequently, the &lt;em&gt;centered log-ratio&lt;/em&gt; (clr) transformation
&lt;span class=&#34;math display&#34;&gt;\[
\text{clr}(f)(t):=f_{c}(t)= \text{ln} f(t) - \frac{1}{\eta}\int_{I}\text{ln}f(t) dt
\]&lt;/span&gt;
is used to the original densities with &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; representing the length of their common (bounded) support &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt;. It can be shown that the clr transformation of densities enforces the resulting functions to integrate on &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; to 0. To represent the original data in continuous form while fulfilling the zero integral constraint, the so-called &lt;em&gt;compositional splines&lt;/em&gt; were developed (more on this in Machalová et al. (2020)) and used throughout the regression modeling.&lt;/p&gt;
&lt;p&gt;In our geological example, 96 soil samples from loesses are examined and the task is to analyze how the geochemistry of the samples is influenced by their PSDs. The cubic polynomials were chosen for the spline basis of the PSDs together with 16 knots represented in the graphs by the grey dashed lines. The resulting clr densities are now ready to serve as predictor in our regression model. For the response, the clr-transformed geochemical compositions of the observed soil samples are taken into consideration. In this case, each composition is characterized by a real vector consisting of concentrations of 9 elements (Al, Si, K, Ca, Fe, As, Rb, Sr, Zr).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-10-compositional-scalar-on-function-regression-as-a-tool-not-only-for-geological-data_files/fig1.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;compositional-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Compositional regression&lt;/h2&gt;
&lt;p&gt;As mentioned above, the FPCA is a useful technique here to filter out noise which could distort the regression estimates - the FPCA allows us to represent the predictor using only a few functional principal components while explaining a substantial percentage of the variability of the original data. In this case, 3 principal components were used as they explained over 90% of the variability. The regression modeling is then performed on these functional principal components. The resulting functional parameters &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt; are shown in the plot below (in their clr form, of course).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-10-compositional-scalar-on-function-regression-as-a-tool-not-only-for-geological-data_files/fig2.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quality-of-the-model-interpretation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Quality of the model, interpretation&lt;/h2&gt;
&lt;p&gt;To assess the goodness-of-fit of the regression model, standard coefficient of determination can be computed with values close to one indicating a good fit of the model. Another possibility is to use a nonparametrical method of bootstrap confidence bands. The idea of bootstrap bands is based on resampling the residuals - for each part of the composition, the residuals can be estimated as
&lt;span class=&#34;math display&#34;&gt;\[
\hat{\epsilon}_{i}=y_i-\hat{y}_i.
\]&lt;/span&gt;
By resampling we are able to compute an arbitrary number of bootstrap samples
&lt;span class=&#34;math display&#34;&gt;\[
y_{i}^{boot}=\beta_{0}+\int_{I}\beta_{1}(t)\cdot f_{i}(t)dt + \epsilon_{i}^{boot},\quad i=1,\dots,N,
\]&lt;/span&gt;
and the resulting bootstrap estimates of the functional regression parameter then form a band “around” &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt;. Here, 100 bootstrap functions were plotted together with the estimate of &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt;. The bootstrap bands appear to be very useful for interpretation of the functional parameters &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt; as shown for Al and Ca bellow.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-10-compositional-scalar-on-function-regression-as-a-tool-not-only-for-geological-data_files/fig3.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While sticking with the clr form of &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt; (further as clr(&lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt;)) and their zero integral constraint, the functions have to cross the &lt;em&gt;x&lt;/em&gt;-axis meaning that we are able to split the original support &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; on subdomains where clr(&lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt;) is positive or negative, respectively. The same can be said about the clr transformation of the particle size distributions. For interpretation, we look at the positive and negative subdomains individually. For subdomain where the clr transformed PSDs are positive (&lt;span class=&#34;math inline&#34;&gt;\(I^{+}\)&lt;/span&gt;), three situations may occur:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the estimated parameter clr(&lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt;) is positive - in that case, we can expect an increasing relative presence of the given element within the geochemical composition (by considering intepretation of the clr representation of this element).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;clr(&lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt;) is negative, resulting in decreasing relative presence.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;clr(&lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt;) &lt;span class=&#34;math inline&#34;&gt;\(\approx 0\)&lt;/span&gt;, meaning that the relative presence of the given element within the geochemical composition is not influenced by the respective particle sizes of the PSDs. The bootstrap confidence bands can be used to define these subdomains.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Clearly, the opposite would apply for the subdomain with negative clr transformed PSDs (&lt;span class=&#34;math inline&#34;&gt;\(I^{-}\)&lt;/span&gt;). In case of Al it means that its relative presence in the composition is strongly (positively) influenced by the finest fractions and there is also a stronger negative effect of the fraction around 10 &lt;span class=&#34;math inline&#34;&gt;\(\mu m\)&lt;/span&gt;. For Ca completely opposite effects can be observed.&lt;/p&gt;
&lt;p&gt;To sum up, the specific properties of compositional data (as multivariate data) and probability density functions (as functional data) need a proper adaptation of standard statistical methods. Here, the linear regression was addressed by presenting a compositional scalar-on-function regression model with functional predictor and real response. Hopefully the presented example demonstrated that the compositional approach with the clr transformation not only provides an adequate platform for working with probability densities, but also leads to an easier and more straight-forward interpretation of the resulting parameters.&lt;/p&gt;
&lt;div id=&#34;based-on&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Based on:&lt;/h3&gt;
&lt;p&gt;Talská, R., Hron, K., Matys Grygar, T.: &lt;em&gt;Compositional scalar-on-function regression with application to sediment particle size distributions.&lt;/em&gt; Mathematical Geosciences, accepted for publication.&lt;/p&gt;
&lt;p&gt;Machalová, J., Talská, R., Hron, K., Gába, A.: &lt;em&gt;Compositional splines for representation of density functions.&lt;/em&gt; Computational Statistics (2020). &lt;a href=&#34;https://doi.org/10.1007/s00180-020-01042-7&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1007/s00180-020-01042-7&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;about-authors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;About authors&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-10-compositional-scalar-on-function-regression-as-a-tool-not-only-for-geological-data_files/ivana_pavlu_photo.png&#34; width=&#34;75&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ivana Pavlů&lt;/strong&gt; is a PhD student at Department of Mathematical Analysis and Applications of Mathematics, Palacký University in Olomouc, Czech Republic, &lt;a href=&#34;mailto:ivana.pavlu@upol.cz&#34; class=&#34;email&#34;&gt;ivana.pavlu@upol.cz&lt;/a&gt;. In her research she primarily focuses on functional data analysis of probability density functions using the Bayes spaces methodology.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-10-compositional-scalar-on-function-regression-as-a-tool-not-only-for-geological-data_files/Hron2017-small3.png&#34; width=&#34;75&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Karel Hron&lt;/strong&gt; is a Professor at Department of Mathematical Analysis and Applications of Mathematics, Palacký University in Olomouc, Czech Republic, &lt;a href=&#34;mailto:karel.hron@upol.cz&#34; class=&#34;email&#34;&gt;karel.hron@upol.cz&lt;/a&gt;. His research chiefly focuses on the statistical analysis of compositional data and its applications in a wide range of fields (geology, analytical chemistry, metabolomics, time-use epidemiology and others). He co-authored the book Applied Compositional Data Analysis, published in Springer Series in Statistics.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Higher Order Targeted Maximum Likelihood Estimation</title>
      <link>https://youngstats.github.io/post/2021/03/10/higher-order-targeted-maximum-likelihood-estimation/</link>
      <pubDate>Wed, 10 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/03/10/higher-order-targeted-maximum-likelihood-estimation/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We propose a higher order targeted maximum likelihood estimation (TMLE) that only relies on a sequentially and recursively defined set of data-adaptive fluctuations. Without the need to assume the often too stringent higher order pathwise differentiability, the method is practical for implementation and has the potential to be fully computerized.&lt;/p&gt;
&lt;div id=&#34;background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Background&lt;/h1&gt;
&lt;div id=&#34;targeted-maximum-likelihood-estimation-tmle&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Targeted Maximum Likelihood Estimation (TMLE)&lt;/h2&gt;
&lt;p&gt;It has been particularly of interest for semiparametric theories and real world practices to make efficient and substitution-based estimation for target quantities that are functions of data distribution. TMLE &lt;span class=&#34;citation&#34;&gt;(van der Laan and Rubin &lt;a href=&#34;#ref-van2006targeted&#34; role=&#34;doc-biblioref&#34;&gt;2006&lt;/a&gt;; van der Laan and Rose &lt;a href=&#34;#ref-van2011targeted&#34; role=&#34;doc-biblioref&#34;&gt;2011&lt;/a&gt;, &lt;a href=&#34;#ref-van2018targeted&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; provides a framework to construct such estimators and incorporates machine learning into efficient estimation and inference. Here we briefly review the regular first order TMLE.&lt;/p&gt;
&lt;p&gt;Suppose that the true distribution &lt;span class=&#34;math inline&#34;&gt;\(P_0\)&lt;/span&gt; lies in a statistical model &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{M}\)&lt;/span&gt;. Start with an initial distribution estimator &lt;span class=&#34;math inline&#34;&gt;\(P_n^0\)&lt;/span&gt;. Given pathwise differentiability of the target &lt;span class=&#34;math inline&#34;&gt;\(\Psi(P)\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; with a canonical gradient &lt;span class=&#34;math inline&#34;&gt;\(D^{(1)}_P\)&lt;/span&gt;, consider a least favorable path &lt;span class=&#34;math inline&#34;&gt;\(\{ \tilde P^{(1)}(P, \epsilon) \}\)&lt;/span&gt; through &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(\epsilon=0\)&lt;/span&gt;, where scores at &lt;span class=&#34;math inline&#34;&gt;\(\epsilon=0\)&lt;/span&gt; span the efficient influence curve (EIC) &lt;span class=&#34;math inline&#34;&gt;\(D_{P}^{(1)}\)&lt;/span&gt;. Define the TMLE update by maximizing the likelihood along the path, that is, &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_n^{(1)} = \mathrm{argmin}_\epsilon P_n L(\tilde P^{(1)}(P_n^0, \epsilon) )\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(L(P) = - \log p\)&lt;/span&gt;. The resulted TMLE update is &lt;span class=&#34;math inline&#34;&gt;\(P_n^* = \tilde P_n^{(1)} (P_n^0) = \tilde P_n^{(1)} (P_n^0, \epsilon_n^{(1)})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Define &lt;span class=&#34;math inline&#34;&gt;\(R^{(1)}(P, P_0) = \Psi(P) - \Psi(P_0) + P_0 D_P^{(1)}\)&lt;/span&gt; as the exact remainder. Then the TMLE satisfies &lt;span class=&#34;math inline&#34;&gt;\(P_n D_{P_n^*}^{(1)}\approx 0\)&lt;/span&gt; and the following exact expansion
&lt;span class=&#34;math display&#34;&gt;\[
\Psi(P_n^*) - \Psi(P_0) = R^{(1)}(P_n^*, P_0) - P_0 D_{P_n^*}^{(1)} = (P_n - P_0) D_{P_0}^{(1)} + (P_n - P_0) (D_{P_n^*}^{(1)} - D_{P_0}^{(1)}) - P_n D^{(1)}_{P_n^*} + R^{(1)}(P_n^*, P_0).
\]&lt;/span&gt;
Asymptotic efficiency for &lt;span class=&#34;math inline&#34;&gt;\(P_n^*\)&lt;/span&gt; requires:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\{D_{P}^{(1)}: P\in\mathcal{M}\}\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(P_0\)&lt;/span&gt;-Donsker class (often satisfied, or skipped with sample splitting),&lt;/li&gt;
&lt;li&gt;Solving the equation &lt;span class=&#34;math inline&#34;&gt;\(P_n D^{(1)}_{P_n^*}=0\)&lt;/span&gt; exactly or to an &lt;span class=&#34;math inline&#34;&gt;\(o_P(n^{-1/2})\)&lt;/span&gt; term,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(R^{(1)}(P_n^*, P_0)\)&lt;/span&gt; being exactly zero or up to an &lt;span class=&#34;math inline&#34;&gt;\(o_P(n^{-1/2})\)&lt;/span&gt; term.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(R^{(1)}(P, P_0)\)&lt;/span&gt; is often a second order difference in &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p_0\)&lt;/span&gt;. For example, when it consists of cross products, doubly or multiply robustness may exist.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;highly-adaptive-lasso-hal&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Highly Adaptive Lasso (HAL)&lt;/h2&gt;
&lt;p&gt;HAL &lt;span class=&#34;citation&#34;&gt;(van der Laan &lt;a href=&#34;#ref-van2015generally&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;, &lt;a href=&#34;#ref-van2017generally&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;; Benkeser and van ver Laan &lt;a href=&#34;#ref-benkeser2016highly&#34; role=&#34;doc-biblioref&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt; is a nonparametric maximum likelihood estimator that converges in Kullback-Leibler dissimilarity at a minimal rate of &lt;span class=&#34;math inline&#34;&gt;\(n^{-2/3}(\log~n)^d\)&lt;/span&gt;, even when the parameter space only assumes cadlag and finite variation norms.
This generally bounds the exact remainder, and immediately makes the TMLE that uses HAL as an initial asymptotically efficient. However, in finite samples, the second order remainder can still dominate the sampling distribution.&lt;/p&gt;
&lt;p&gt;Another important property of HAL is itself being a nonparametric MLE, so it can solve a large class of score equations to best approximates the desired score via increasing the &lt;span class=&#34;math inline&#34;&gt;\(L_1\)&lt;/span&gt;-norm of the HAL-MLE (called undersmoothing) &lt;span class=&#34;citation&#34;&gt;(M. J. van der Laan, Benkeser, and Cai &lt;a href=&#34;#ref-vanderLaan&amp;amp;Benkeser&amp;amp;Cai19&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;&lt;a href=&#34;#ref-vanderLaan&amp;amp;Benkeser&amp;amp;Cai19&#34; role=&#34;doc-biblioref&#34;&gt;a&lt;/a&gt;, &lt;a href=&#34;#ref-van2019efficient&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;&lt;a href=&#34;#ref-van2019efficient&#34; role=&#34;doc-biblioref&#34;&gt;b&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;higher-order-fluctuations-with-hal-mle&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Higher Order Fluctuations with HAL-MLE&lt;/h1&gt;
&lt;p&gt;Replace &lt;span class=&#34;math inline&#34;&gt;\(P_n^0\)&lt;/span&gt; in the first order TMLE by a TMLE &lt;span class=&#34;math inline&#34;&gt;\(\tilde P^{(2)}_n(P_n^0)\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\Psi_n^{(1)}(P_0) = \Psi(\tilde P^{(1)}_n(P_0)) = \Psi(\tilde P^{(1)}(P_0, \epsilon_n^{(1)}(P_0)))\)&lt;/span&gt;, which is a data-adaptive fluctuation of the original target parameter &lt;span class=&#34;math inline&#34;&gt;\(\Psi(P_0)\)&lt;/span&gt;. Then the final update of a second order TMLE, &lt;span class=&#34;math inline&#34;&gt;\(\tilde P^{(1)}_n \tilde P^{(2)}_n(P_n^0)\)&lt;/span&gt;, is just a first order TMLE that uses as the initial estimator a &lt;span class=&#34;math inline&#34;&gt;\(\tilde P^{(2)}(P_n^0)\)&lt;/span&gt; that is fully tailored for &lt;span class=&#34;math inline&#34;&gt;\(\Psi_n^{(1)}(P_0)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-10-higher-order-targeted-maximum-likelihood-estimation_files/figure_2nd_order.jpg&#34; /&gt;
&lt;em&gt;Figure 1: Left panel: regular TMLE. Right panel: second order TMLE. The horizontal axes represent the original target. The vertical axis represents the data-adaptive fluctuation. The second order TMLE searches for a better initial estimator for a regular TMLE.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Similarly if we iterate this process, and let &lt;span class=&#34;math inline&#34;&gt;\(\tilde P^{(k+1)}_n(P_n^0)\)&lt;/span&gt; be a regular TMLE tailored for a higher order fluctuation &lt;span class=&#34;math inline&#34;&gt;\(\Psi_n^{(k)}(P_0) = \Psi^{(k-1)}_n(\tilde P^{(k)}_n(P_0))=\Psi(\tilde P^{(1)}_n \tilde P^{(2)}_n\cdots \tilde P^{(k)}_n(P_n^0))\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(k=1, \ldots\)&lt;/span&gt;, then the final update of a &lt;span class=&#34;math inline&#34;&gt;\(k+1\)&lt;/span&gt;-th order TMLE is &lt;span class=&#34;math inline&#34;&gt;\(\tilde P^{(1)}_n \tilde P^{(2)}_n\cdots \tilde P^{(k+1)}_n(P_n^0)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The second order TMLE relies on pathwise differentiability of &lt;span class=&#34;math inline&#34;&gt;\(\Psi_n^{(1)}\)&lt;/span&gt;. However, &lt;span class=&#34;math inline&#34;&gt;\(\Psi_n^{(1)}(P) = \Psi(\tilde P^{(1)}_n(P)) = \Psi(\tilde P^{(1)}(P, \epsilon_n^{(1)}(P)))\)&lt;/span&gt;is smooth in &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; up till the dependence of &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_n^{(1)}(P) = \mathrm{argmax}_\epsilon P_n \log \tilde p_n^{(1)}(p, \epsilon)\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;, because &lt;span class=&#34;math inline&#34;&gt;\(P_n\)&lt;/span&gt; is not absolutely continuous w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; for most &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; that can occur as an initial or a higher order TMLE-update. This calls for the use of smooth distribution estimators such as HAL-MLE &lt;span class=&#34;math inline&#34;&gt;\(\tilde{P}_n\)&lt;/span&gt; in replacement of the empirical &lt;span class=&#34;math inline&#34;&gt;\(P_n\)&lt;/span&gt;, since &lt;span class=&#34;math inline&#34;&gt;\(d\tilde P_n/dP\)&lt;/span&gt; will exist for all &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; that can occur as an initial or higher order updates, which ensures pathwise differentiability of &lt;span class=&#34;math inline&#34;&gt;\(\Psi^{(1)}_n(P_0)\)&lt;/span&gt; and the existence of its canonical gradient &lt;span class=&#34;math inline&#34;&gt;\(D^{(2)}_{n, P}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In general, suppose that &lt;span class=&#34;math inline&#34;&gt;\(\{\tilde P^{(k)}_n(P, \epsilon)\}\)&lt;/span&gt; is a least favorable path through &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(\epsilon=0\)&lt;/span&gt;, whose scores at &lt;span class=&#34;math inline&#34;&gt;\(\epsilon=0\)&lt;/span&gt; span &lt;span class=&#34;math inline&#34;&gt;\(D^{(k)}_{n, P}\)&lt;/span&gt;. And the update step is also replaced by optimizing the &lt;span class=&#34;math inline&#34;&gt;\(\tilde P_n\)&lt;/span&gt;-regularized loss, that is, &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_n^{(k)} = \mathrm{argmin}_\epsilon \tilde P_n L(\tilde P^{(k)}(P_n^0, \epsilon) )\)&lt;/span&gt;, which solves &lt;span class=&#34;math inline&#34;&gt;\(\tilde P_n D^{(k)}_{n, P}=0\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(P = \tilde P^{(k)}_n(P_n^0)= \tilde P^{(k)}_n(P_n^0, \epsilon_n^{(k)})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A &lt;span class=&#34;math inline&#34;&gt;\(k+1\)&lt;/span&gt;-th order TMLE by its design searches for a better initial estimator given the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th order TMLE. Specifically, the &lt;span class=&#34;math inline&#34;&gt;\(k+1\)&lt;/span&gt;-th order TMLE moves in the same direction as the steepest descent algorithm for minimizing the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th exact total remainder that is the discrepancy between &lt;span class=&#34;math inline&#34;&gt;\(\Psi(\tilde P^{(1)}_n \tilde P^{(2)}_n\cdots \tilde P^{(k)}_n(P)) - \Psi(\tilde P^{(1)}_n \tilde P^{(2)}_n\cdots \tilde P^{(k)}_n(P_0))\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tilde{P}_n D^{(k)}_{n, P_0}\)&lt;/span&gt;. Moreover, compared to an oracle steepest descent algorithm, TMLE stops the moment the log-likelihood is not improving anymore, which corresponds exactly to when the TMLE cannot know in what direction a steepest descent algorithm would go. This avoids potential overfitting and ensures a local minimum in close neighborhood of the desired (but unknown) minimum &lt;span class=&#34;math inline&#34;&gt;\(P_0\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exact-expansions-of-higher-order-tmle&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exact Expansions of Higher Order TMLE&lt;/h1&gt;
&lt;p&gt;Denote the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th exact remainder as the exact remainder of &lt;span class=&#34;math inline&#34;&gt;\(\tilde P^{(k)}_n(P)\)&lt;/span&gt; for the fluctuation &lt;span class=&#34;math inline&#34;&gt;\(\Psi^{(k-1)}(P_0) = \Psi(\tilde P_n^{(1)}\cdots\tilde P_n^{(k-1)}(P_0))\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
R^{(k)}_n(\tilde P^{(k)}_n(P), P_0)
= &amp;amp; \Psi^{(k-1)}(\tilde P^{(k)}_n(P)) - \Psi^{(k-1)}(P_0) + P_0 D^{(k)}_{n, \tilde P^{(k)}_n(P)} \\
= &amp;amp; \Psi(\tilde P^{(1)}\cdots\tilde P^{(k)}_n(P)) - \Psi(\tilde P^{(1)}\cdots\tilde P^{(k-1)}(P_0)) + P_0 D^{(k)}_{n, \tilde P^{(k)}_n(P)}. 
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then we have the exact expansion for the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th order TMLE,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\Psi(\tilde P^{(1)}_n\cdots\tilde P^{(k)}_n(P)) - \Psi(P_0)
= &amp;amp; \sum_{j=1}^{k-1} (P_n-P_0)D^{(j)}_{n, \tilde P^{(j)}_n(P_0)} + R^{(j)}_n(\tilde P_n^{(j)}(P_0), P_0) \\
&amp;amp; + (P_n-P_0)D^{(k)}_{n, \tilde P^{(k)}_n}(P_n^0) + R^{(k)}_n(\tilde P_n^{(k)}(P_n^0), P_0) \\
&amp;amp; - \sum_{j=1}^{k-1} P_n D^{(j)}_{n, \tilde P^{(j)}_n(P_0)} - P_n D^{(k)}_{n, \tilde P^{(k)}_n(P_n^0)}, 
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which still holds if we replace &lt;span class=&#34;math inline&#34;&gt;\(P_n\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\tilde P_n\)&lt;/span&gt;. This can be further derived as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\Psi(\tilde P^{(1)}\cdots\tilde P^{(k)}_n(P)) - \Psi(P_0)
=&amp;amp;\sum_{j=1}^{k}\left\{ (\tilde{P}_n-P_0)D^{(j)}_{n,\tilde{P}_n^{(j)}(P_0)}+R_n^{(j)}(\tilde{P}_n^{(j)}(P_0),P_0)\right\} \\
%&amp;amp;&amp;amp;+(P_n-P_0)D^{(k)}_{n,P_n^{(k)}(P_0)}\\
&amp;amp; +R_n^{(k)}(\tilde{P}_n^{(k)}(P_n^0),\tilde{P}_n)-R_n^{(k)}(\tilde{P}_n^{(k)}(P_0),\tilde{P}_n)\\
&amp;amp;-\sum_{j=1}^{k}\tilde{P}_n D^{(j)}_{n,\tilde{P}_n^{(j)}(P_0)}.\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The followings can be shown:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((\tilde{P}_n-P_0)D^{(j)}_{n,\tilde{P}_n^{(j)}(P_0)}, j=1, \dots, k,\)&lt;/span&gt; are generalized &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th order difference in &lt;span class=&#34;math inline&#34;&gt;\(P_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tilde P_n\)&lt;/span&gt;, which resemble the performance of higher order &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;-statistics;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(R_n^{(j)}(\tilde{P}_n^{(j)}(P_0),P_0) = O_P(n^{-1})\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\((\tilde P_n - P_n) D_{n, P_0}^{(j)} = O_P(n^{-1/2})\)&lt;/span&gt;, which can be achieved by undersmothing HAL;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(R_n^{(k)}(\tilde{P}_n^{(k)}(P),\tilde P_n)\)&lt;/span&gt; is a generalized &lt;span class=&#34;math inline&#34;&gt;\(k+1\)&lt;/span&gt;-th order difference in &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tilde P_n\)&lt;/span&gt;, and hence &lt;span class=&#34;math inline&#34;&gt;\(R_n^{(k)}(\tilde{P}_n^{(k)}(P_n^0),\tilde P_n) - R_n^{(k)}(\tilde{P}_n^{(k)}(P_0),\tilde P_n)=o_P(n^{-1/2})\)&lt;/span&gt; so long as &lt;span class=&#34;math inline&#34;&gt;\(\lVert \tilde p_n - p_0 \rVert = o_P(n^{1/2(k+1)})\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\lVert p_n^0 - p_0 \rVert = o_P(n^{1/2(k+1)})\)&lt;/span&gt;;&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;The last term can be exactly &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; by defining &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_n^{(j)}(P)\)&lt;/span&gt; as a solution of the corresponding efficient score equation &lt;span class=&#34;math inline&#34;&gt;\(\tilde P_n D^{(j)}_{n, \tilde P^{(j)}_n(P, \epsilon)}=0\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;higher-order-inference&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Higher Order Inference&lt;/h1&gt;
&lt;p&gt;For the sake of statistical inference, we will need that &lt;span class=&#34;math inline&#34;&gt;\((\tilde P_n - P_n)D^{(1)}_{\tilde P_n^{(1)}(P_0)} = o_P(n^{-1/2})\)&lt;/span&gt;, and probably even &lt;span class=&#34;math inline&#34;&gt;\((\tilde P_n - P_n)D^{(j)}_{\tilde P_n^{(j)}(P_0)} = o_P(n^{-1/2})\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(j = 2, \dots, k\)&lt;/span&gt;. It can be shown that this essentially comes down to controlling &lt;span class=&#34;math inline&#34;&gt;\((\tilde P_n - P_n) D^{(1)}_{P_0}\)&lt;/span&gt;, which again can be achieved by undersmoothing HAL.&lt;/p&gt;
&lt;p&gt;Let
&lt;span class=&#34;math display&#34;&gt;\[
\bar D_n^k = \sum_{j=1}^k D^{(j)}_{n, \tilde P^{(j)}_n\cdots\tilde P^{(k)}_n(P_n^0)}
\]&lt;/span&gt;
which is an estimate of the influence curve &lt;span class=&#34;math inline&#34;&gt;\(\bar D_{n, P_0}^k = \sum_{j=1}^k D^{(j)}_{n, \tilde P^{(j)}_n(P_0)}\)&lt;/span&gt;. Note that for &lt;span class=&#34;math inline&#34;&gt;\(j&amp;gt;1\)&lt;/span&gt; the terms are higher order differences, so that &lt;span class=&#34;math inline&#34;&gt;\(\bar D_n^k\)&lt;/span&gt; will converge to the efficient influence curve &lt;span class=&#34;math inline&#34;&gt;\(D^{(1)}_{P_0}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let
&lt;span class=&#34;math display&#34;&gt;\[
\sigma_n^2 = \frac{1}{n}\sum_{i=1}^n \bar D_n^k(O_i)^2
\]&lt;/span&gt;
be the sample variance of this estimated influence curve. A corresponding 0.95 confidence interval is given by
&lt;span class=&#34;math display&#34;&gt;\[
\Psi(P^{(1)}_n\cdots\tilde P^{(k)}_n(P_n^0)) \pm 1.96 \sigma_n/n^{1/2}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simulation&lt;/h1&gt;
&lt;p&gt;The first example demonstrates the impact of second order TMLE steps during a process of estimating the average density. The exact total remainder &lt;span class=&#34;math inline&#34;&gt;\(\bar R^{(1)}(\tilde P_n^{(1)}(P), P_0)\)&lt;/span&gt; of first order TMLE is controlled due to the second order updates &lt;span class=&#34;math inline&#34;&gt;\(P = P_n^0 \mapsto \tilde P_n^{(2)}(P_n^0)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-10-higher-order-targeted-maximum-likelihood-estimation_files/middle_remainder_ZW-20210114.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Below it plots the simulated bias and bias/SD ratio at &lt;span class=&#34;math inline&#34;&gt;\(n=500\)&lt;/span&gt; when we increase the bias in the initial estimator &lt;span class=&#34;math inline&#34;&gt;\(P_n^0\)&lt;/span&gt; by adding a bias mass to each of the support points of the empirical pmf. Second order TMLE provides improved accuracy in both estimation and inference over first order TMLE following likelihood guidance.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-10-higher-order-targeted-maximum-likelihood-estimation_files/combine_500-20210115.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Lastly, we show an example of estimating average treatment effects (ATEs) while the initial estimator for propensity scores is &lt;span class=&#34;math inline&#34;&gt;\(n^{-1/4}\)&lt;/span&gt;-consistent while that for outcome models is not. The first order TMLE should have &lt;span class=&#34;math inline&#34;&gt;\(n^{1/2}\)&lt;/span&gt;-scaled bias that increases with &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; while the second order TMLE has a &lt;span class=&#34;math inline&#34;&gt;\(n^{1/2}\)&lt;/span&gt;-bias that should be constant in &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. The table below shows that the second order TMLE has a negligible bias and thereby still provides valid inference.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;n&lt;/th&gt;
&lt;th&gt;bias 1-st&lt;/th&gt;
&lt;th&gt;bias 2-nd&lt;/th&gt;
&lt;th&gt;se 1-st&lt;/th&gt;
&lt;th&gt;se 2-nd&lt;/th&gt;
&lt;th&gt;mse 1-st&lt;/th&gt;
&lt;th&gt;mse 2-nd&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;400&lt;/td&gt;
&lt;td&gt;-0.720&lt;/td&gt;
&lt;td&gt;0.078&lt;/td&gt;
&lt;td&gt;0.815&lt;/td&gt;
&lt;td&gt;1.175&lt;/td&gt;
&lt;td&gt;1.087&lt;/td&gt;
&lt;td&gt;1.178&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;750&lt;/td&gt;
&lt;td&gt;-0.996&lt;/td&gt;
&lt;td&gt;0.029&lt;/td&gt;
&lt;td&gt;0.800&lt;/td&gt;
&lt;td&gt;1.102&lt;/td&gt;
&lt;td&gt;1.278&lt;/td&gt;
&lt;td&gt;1.102&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;1000&lt;/td&gt;
&lt;td&gt;-1.258&lt;/td&gt;
&lt;td&gt;-0.062&lt;/td&gt;
&lt;td&gt;0.786&lt;/td&gt;
&lt;td&gt;1.066&lt;/td&gt;
&lt;td&gt;1.483&lt;/td&gt;
&lt;td&gt;1.068&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;1200&lt;/td&gt;
&lt;td&gt;-1.345&lt;/td&gt;
&lt;td&gt;0.022&lt;/td&gt;
&lt;td&gt;0.809&lt;/td&gt;
&lt;td&gt;1.028&lt;/td&gt;
&lt;td&gt;1.570&lt;/td&gt;
&lt;td&gt;1.028&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;1600&lt;/td&gt;
&lt;td&gt;-1.549&lt;/td&gt;
&lt;td&gt;-0.019&lt;/td&gt;
&lt;td&gt;0.818&lt;/td&gt;
&lt;td&gt;1.055&lt;/td&gt;
&lt;td&gt;1.752&lt;/td&gt;
&lt;td&gt;1.055&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;2500&lt;/td&gt;
&lt;td&gt;-2.066&lt;/td&gt;
&lt;td&gt;-0.094&lt;/td&gt;
&lt;td&gt;0.819&lt;/td&gt;
&lt;td&gt;0.999&lt;/td&gt;
&lt;td&gt;2.222&lt;/td&gt;
&lt;td&gt;1.003&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;discussions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Discussions&lt;/h1&gt;
&lt;p&gt;Although HAL-MLE-based fluctuations are fundamental to higher order TMLE, the update steps in practice can be based on empirical losses. Note that the &lt;span class=&#34;math inline&#34;&gt;\(j-1\)&lt;/span&gt;-th fluctuation &lt;span class=&#34;math inline&#34;&gt;\(\Psi(\tilde P_n^{(1)}\cdots\tilde P_n^{(j-1)}(P_0))\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(j = 0, \dots, k-1\)&lt;/span&gt;, is nothing but a pathwise differentiable parameter with a known canonical gradient, &lt;span class=&#34;math inline&#34;&gt;\(D^{(j)}_{n, P}\)&lt;/span&gt;. For jointly targeting this sequence of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; parameters, one can solve the empirical &lt;span class=&#34;math inline&#34;&gt;\(P_n\)&lt;/span&gt;-regularized efficient score equations (where the scores still involve HAL-MLEs). As we showed in the technical report, this preserves the exact expansion and even leads to an improved undersmoothing term, and therefore is the recommended implementation. At &lt;span class=&#34;math inline&#34;&gt;\(k=1\)&lt;/span&gt;, this exactly coincides with the regular first order TMLE.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-10-higher-order-targeted-maximum-likelihood-estimation_files/figure_targets.jpg&#34; /&gt;
&lt;em&gt;Figure 2: Jointly consider the sequence of data-adaptive fluctuations.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;An important next step is the (automated) computation of the first and higher order canonical gradients with least squares regression or symmetric matrix inversion &lt;span class=&#34;citation&#34;&gt;(van der Laan, Wang, and van der Laan &lt;a href=&#34;#ref-van2021higher&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt;, thereby opening up the computation of higher order TMLEs with standard machinery, avoiding delicate analytics needed to determine closed forms.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references hanging-indent&#34;&gt;
&lt;div id=&#34;ref-benkeser2016highly&#34;&gt;
&lt;p&gt;Benkeser, David, and Mark J van ver Laan. 2016. “The Highly Adaptive Lasso Estimator.” In &lt;em&gt;2016 Ieee International Conference on Data Science and Advanced Analytics (Dsaa)&lt;/em&gt;, 689–96. IEEE.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-van2015generally&#34;&gt;
&lt;p&gt;van der Laan, Mark J. 2015. “A Generally Efficient Targeted Minimum Loss Based Estimator.”&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-van2017generally&#34;&gt;
&lt;p&gt;———. 2017. “A Generally Efficient Targeted Minimum Loss Based Estimator Based on the Highly Adaptive Lasso.” &lt;em&gt;The International Journal of Biostatistics&lt;/em&gt; 13 (2).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-vanderLaan&amp;amp;Benkeser&amp;amp;Cai19&#34;&gt;
&lt;p&gt;van der Laan, Mark J, David Benkeser, and Weixin Cai. 2019a. “Causal Inference Based on Undersmoothing the Highly Adaptive Lasso.”&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-van2019efficient&#34;&gt;
&lt;p&gt;———. 2019b. “Efficient Estimation of Pathwise Differentiable Target Parameters with the Undersmoothed Highly Adaptive Lasso.” &lt;em&gt;arXiv Preprint arXiv:1908.05607&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-van2011targeted&#34;&gt;
&lt;p&gt;van der Laan, Mark J, and Sherri Rose. 2011. &lt;em&gt;Targeted Learning: Causal Inference for Observational and Experimental Data&lt;/em&gt;. Springer Science &amp;amp; Business Media.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-van2018targeted&#34;&gt;
&lt;p&gt;———. 2018. &lt;em&gt;Targeted Learning in Data Science&lt;/em&gt;. Springer.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-van2006targeted&#34;&gt;
&lt;p&gt;van der Laan, Mark J, and Daniel Rubin. 2006. “Targeted Maximum Likelihood Learning.” &lt;em&gt;The International Journal of Biostatistics&lt;/em&gt; 2 (1).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-van2021higher&#34;&gt;
&lt;p&gt;van der Laan, Mark J, Zeyi Wang, and Lars van der Laan. 2021. “Higher Order Targeted Maximum Likelihood Estimation.” &lt;em&gt;arXiv Preprint arXiv:2101.06290&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Give me an adequate correlation: assessing relationships in percentage (or proportional) data</title>
      <link>https://youngstats.github.io/post/2021/02/04/give-me-an-adequate-correlation-assessing-relationships-in-percentage-or-proportional-data/</link>
      <pubDate>Thu, 04 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/02/04/give-me-an-adequate-correlation-assessing-relationships-in-percentage-or-proportional-data/</guid>
      <description>


&lt;div id=&#34;correlations-and-negative-bias&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Correlations and negative bias&lt;/h2&gt;
&lt;p&gt;We assume that you are quite familiar with the following problem.
Consider a data set where the information is expressed in percentages or proportions.
An example are household expenditures, given as average amounts (in Euros) the households
are spending on food, housing, transportation, etc.
Since the expenditures would not be comparable among countries with very different economic
level, it makes sense to express the data as proportions (or percentages) of the single
categories on the total expenditures.
Now one can be interested in relationships between the different variables (expenditure
categories), and the common tools for this task would be to use the
Pearson or Spearman correlation coefficient to determine the strength of the associations.
However, is this appropriate to compute correlations from
percentages (or proportions, or any other constrained data), or more generally, from data
carrying relative information? Denote the underlying variables, expressed in proportions
or percentages, by &lt;span class=&#34;math inline&#34;&gt;\(x_1,\ldots,x_D\)&lt;/span&gt;.
When computing correlations, we
must count with a negative bias of the covariance, which leads to relations like
&lt;span class=&#34;math display&#34;&gt;\[\mathrm{cov}(x_1,x_2)+\mathrm{cov}(x_1,x_3)+\ldots+\mathrm{cov}(x_1,x_D)=-\mathrm{var}(x_1).\]&lt;/span&gt;
Consequently, the correlation coefficients cannot vary freely between &lt;span class=&#34;math inline&#34;&gt;\(-1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;;
they are forced towards negative values and thus do not produce reliable and interpretable results.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;initial-choice-variation-matrix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Initial choice: variation matrix&lt;/h2&gt;
&lt;p&gt;The way to obtain reasonable correlations with percentage data is to consider ratios,
or even mathematically easier, log-ratios as the source information. Log-ratios do not
change when the data are rescaled, thus the sum of components (100 for percentages) is
even irrelevant.
In our household expenditure data set we could thus even work with the raw data, expressed
in Euros, and the results of the analysis would be the same as for the
normalized data.
The scale invariant data are called &lt;em&gt;compositional&lt;/em&gt; in this context.
A natural way to assess strength of relationship between components of percentage
(compositional) data is thus to think in terms of their &lt;em&gt;proportionality&lt;/em&gt;. This leads
to the so called &lt;em&gt;variation matrix&lt;/em&gt;, which is defined as
&lt;span class=&#34;math display&#34;&gt;\[\mathbf{T}=\left(\mathrm{var}\left(\ln\frac{x_i}{x_j}\right)\right)_{i,j=1}^D.\]&lt;/span&gt;
If the components are proportional, the respective element of the variation matrix is
zero, and vice versa. Thus, bigger values of the variation matrix refer to deviations
from proportionality. Here, &lt;em&gt;var&lt;/em&gt; is denotes the variance, and practically one can
use classical or robust variance estimation, where the latter is preferable in presence
of outliers.
The variation matrix is thus definitely a reasonable choice to express relationship
between the variables, however, it cannot be interpreted in terms of correlations,
as possible negative relationships between the components are not captured.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;correlations-with-weighted-symmetric-pivot-coordinates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Correlations with (weighted) symmetric pivot coordinates&lt;/h2&gt;
&lt;p&gt;We can see that relating the original components has its clear limitations. A possible
alternative could be to consider &lt;em&gt;relative information&lt;/em&gt; carried by the original components
of a given composition, information contained in log-ratios to other components. The first
choice is to aggregate these logratios which leads, say for components &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;,
to the following variables (we refer to &lt;em&gt;symmetric pivot coordinates&lt;/em&gt;):
&lt;span class=&#34;math display&#34;&gt;\[\begin{eqnarray}
z_1^s&amp;amp;=&amp;amp;C\left(\frac{\sqrt{D-2}}{\sqrt{D-2}+\sqrt{D}}\ln\frac{x_1}{x_2}+\ln\frac{x_1}{x_3}+\ldots+\ln\frac{x_1}{x_D}\right),\\
z_2^s&amp;amp;=&amp;amp;C\left(\frac{\sqrt{D-2}}{\sqrt{D-2}+\sqrt{D}}\ln\frac{x_2}{x_1}+\ln\frac{x_2}{x_3}+\ldots+\ln\frac{x_2}{x_D}\right),
\end{eqnarray}\]&lt;/span&gt;
with a normalizing positive constant &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;. The first logratios in both variables are downscaled in order to remove a possible negative bias (from a geometrical perspective, we are talking about &lt;em&gt;orthonormality&lt;/em&gt; of the resulting variables/coordinates); interestingly, &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{D-2}/(\sqrt{D-2}+\sqrt{D})\rightarrow 1/2\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(D\rightarrow\infty\)&lt;/span&gt;. Even more, the effect of pairwise logratios which are aggregated into &lt;span class=&#34;math inline&#34;&gt;\(z_1^s\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z_2^s\)&lt;/span&gt; can be &lt;em&gt;weighted&lt;/em&gt;, e.g., according to their proportionality to &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;, respectively, by using the inverse values of the respective elements of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{T}\)&lt;/span&gt;. This guarantees that components not related to &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; have a limited impact to
the construction of both symmetric pivot coordinates.&lt;/p&gt;
&lt;p&gt;Now, standard (classical or robust) correlation coefficients between &lt;span class=&#34;math inline&#34;&gt;\(z_1^s\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z_2^s\)&lt;/span&gt; can be computed to estimate the relationship between &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; in the composition.
Its interpretation follows directly the construction of symmetric pivot coordinates. This means that the original components are replaced by their &lt;em&gt;dominance&lt;/em&gt; over
“averaged” contributions of the other components (ev. appropriately weighted) – a quite natural interpretation for data carrying relative information. Moreover, the correlation can be considered in both the positive and negative sense without any danger of the negative bias. We should be only aware that each correlation coefficient is coming from a set of coordinates which are constructed specifically for the given couple of components, so the resulting “correlation matrix” should not be simply treated in the multivariate sense.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;relative-structure-of-household-expenditures&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Relative structure of household expenditures&lt;/h2&gt;
&lt;p&gt;Let us come back to the initial example where the interest was in the relative structure of household expenditures, reported for several countries of the European Union.
Such a data set is contained as &lt;code&gt;expendituresEU&lt;/code&gt; in the library &lt;code&gt;robCompositions&lt;/code&gt;. With a
proportional representation of the expenditures the wealth status of countries is
suppressed and the focus is on the &lt;em&gt;relative&lt;/em&gt; correlation structure. In order to suppress the influence of outlying observations, the Spearman correlation coefficients between
the components are computed and the result is plotted in the heatmap below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-02-04-correlation/Raw_data_correlation.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are some strongly related groups of components visible, like &lt;em&gt;Foodstuff&lt;/em&gt;, &lt;em&gt;Alcohol&lt;/em&gt; and &lt;em&gt;Communications&lt;/em&gt;, which could be considered as those belonging to “basic” expenditures, and there is also another related group of components which could be connected rather to wealth of households (&lt;em&gt;Recreation&lt;/em&gt;, &lt;em&gt;Furnishing&lt;/em&gt;, &lt;em&gt;Transport&lt;/em&gt; and &lt;em&gt;Other&lt;/em&gt;). On the other hand, there are also some strongly negatively correlated couples of components, like &lt;em&gt;Food&lt;/em&gt; with &lt;em&gt;Recreation&lt;/em&gt;, &lt;em&gt;Other&lt;/em&gt; and &lt;em&gt;Furnishings&lt;/em&gt;, respectively, or &lt;em&gt;Recreation&lt;/em&gt; and &lt;em&gt;Health&lt;/em&gt;. The question is which of these negative correlation coefficients is a consequence of the negative bias, and which indeed reflects strong negative relationship between the &lt;em&gt;relative&lt;/em&gt; household expenditures.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-02-04-correlation/Variation_matrix.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As the next step we want to see whether these relationships can be observed also in the (robust) variation matrix, working with log-ratios instead of the original components. Indeed, the proportionality holds definitely for the first group of components (&lt;em&gt;Foodstuff&lt;/em&gt;, &lt;em&gt;Alcohol&lt;/em&gt; and &lt;em&gt;Communications&lt;/em&gt;), however, proportionality of supplementary expenditures is now structured differently (see, e.g., a strong relationship between the components &lt;em&gt;Recreation&lt;/em&gt; and &lt;em&gt;Other&lt;/em&gt;). In any case, any “negative proportionality” cannot be derived from the variation matrix.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-02-04-correlation/Sym_coordinates.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Therefore, we turn to symmetric pivot coordinates; they can be computed, similarly as the variation matrix, using the &lt;code&gt;robCompositions&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(robCompositions)
data(expendituresEU)
D &amp;lt;- ncol(expendituresEU)
Rs &amp;lt;- matrix(NA,nrow=D,ncol=D)
rownames(Rs) &amp;lt;- colnames(expendituresEU)
colnames(Rs) &amp;lt;- colnames(expendituresEU)
for(i in 1:D){
  for(j in 1:D){
    Z &amp;lt;- pivotCoord(expendituresEU[,c(i,j, (1:D)[-c(i,j)])],method=&amp;quot;symm&amp;quot;)
  Rs[i,j] &amp;lt;- cor(Z[,1:2],method=&amp;quot;spearman&amp;quot;)[1,2]
  }
}
round(Rs,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                 Food Alcohol Clothing Housing Furnishings Health Transport
## Food            1.00    0.58     0.31   -0.06       -0.55   0.44     -0.50
## Alcohol         0.58    1.00    -0.02   -0.09       -0.22   0.19     -0.35
## Clothing        0.31   -0.02     1.00   -0.16        0.24  -0.07      0.08
## Housing        -0.06   -0.09    -0.16    1.00        0.24   0.15      0.29
## Furnishings    -0.55   -0.22     0.24    0.24        1.00  -0.54      0.68
## Health          0.44    0.19    -0.07    0.15       -0.54   1.00     -0.29
## Transport      -0.50   -0.35     0.08    0.29        0.68  -0.29      1.00
## Communications  0.72    0.66     0.03   -0.08       -0.51   0.32     -0.47
## Recreation     -0.44   -0.03    -0.17    0.36        0.73  -0.51      0.78
## Education       0.16    0.09     0.16   -0.32       -0.24   0.13     -0.44
## Restaurants    -0.44   -0.42     0.18    0.13        0.39  -0.08      0.28
## Other          -0.64   -0.28    -0.16    0.32        0.68  -0.27      0.67
##                Communications Recreation Education Restaurants Other
## Food                     0.72      -0.44      0.16       -0.44 -0.64
## Alcohol                  0.66      -0.03      0.09       -0.42 -0.28
## Clothing                 0.03      -0.17      0.16        0.18 -0.16
## Housing                 -0.08       0.36     -0.32        0.13  0.32
## Furnishings             -0.51       0.73     -0.24        0.39  0.68
## Health                   0.32      -0.51      0.13       -0.08 -0.27
## Transport               -0.47       0.78     -0.44        0.28  0.67
## Communications           1.00      -0.24      0.15       -0.61 -0.35
## Recreation              -0.24       1.00     -0.44        0.11  0.75
## Education                0.15      -0.44      1.00        0.33 -0.42
## Restaurants             -0.61       0.11      0.33        1.00  0.21
## Other                   -0.35       0.75     -0.42        0.21  1.00&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the correlations are not computed from the original components and we have to refer to their dominance over averaged contributions of the other components instead, but the negative bias of correlations is eliminated now. Indeed, when looking at the heatmap, the main clusters of strongly positively correlated components remain unchanged. However, more substantial changes can be observed for negative correlations. From those mentioned previously only that one between &lt;em&gt;Food&lt;/em&gt; and &lt;em&gt;Other&lt;/em&gt; (in the symmetric pivot coordinates sense) remained. Nevertheless, we should be aware that for the computation of correlations we simply aggregated all log-ratios of the couple with the remaining components, and there are clearly those which could strongly influence the resulting symmetric pivot coordinates, although they are not related to any of components of interest. This is definitely the case of &lt;em&gt;Education&lt;/em&gt;, which is not proportional to the vast majority of components (see heatmap of the variation matrix), and whose influence should thus be rather suppressed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-02-04-correlation/Weighted_symm_coordinates.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This can be done by using weighted symmetric pivot coordinates, the grouping of positively correlated components is now suprisingly even more similar to the case of the initial proportional data. Of course, by considering a different interpretation of the “components” in both cases! However, now we are free again from any possible negative bias of correlations. The “tuned” symmetric pivot coordinates reveal three negative relationships, those between &lt;em&gt;Recreation&lt;/em&gt; and &lt;em&gt;Health&lt;/em&gt;, &lt;em&gt;Food&lt;/em&gt; and &lt;em&gt;Other&lt;/em&gt;, and also &lt;em&gt;Restaurants&lt;/em&gt; and &lt;em&gt;Communications&lt;/em&gt;. All of them have a quite intuitive interpretation and support the &lt;strong&gt;take-home message&lt;/strong&gt; that (weighted) symmetric pivot coordinates are a reasonable alternative to correlations of proportional or percentage data.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;This article is based on&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Filzmoser, P. Hron, K. and Templ, M. (2018) &lt;em&gt;Applied Compositional Data Analysis. With Worked Examples in R&lt;/em&gt;. Springer Series in Statistics. Springer, Cham, Switzerland, 2018,
ISBN: 978-3-319-96422-5.&lt;/p&gt;
&lt;p&gt;Kynčlová, P., Hron, K., Filzmoser, P. (2017) &lt;em&gt;Correlation between compositional parts based on symmetric balances&lt;/em&gt;. Mathematical Geosciences, 49 (6), 777-796.&lt;/p&gt;
&lt;p&gt;Hron, K., Engle, M., Filzmoser, P., Fišerová, E. (2021) &lt;em&gt;Weighted symmetric pivot coordinates for compositional data with geochemical applications&lt;/em&gt;. Mathematical Geosciences,
&lt;a href=&#34;https://doi.org/10.1007/s11004-020-09862-5&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1007/s11004-020-09862-5&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;authors-biography&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Authors’ biography&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-02-04-correlation/Hron2017-small3.png&#34; alt=&#34;drawing&#34; width=&#34;75&#34;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Karel Hron&lt;/strong&gt; is a Professor at Department of Mathematical Analysis and Applications of Mathematics, Palacký University in Olomouc, Czech Republic, &lt;a href=&#34;mailto:karel.hron@upol.cz&#34; class=&#34;email&#34;&gt;karel.hron@upol.cz&lt;/a&gt;. His research chiefly focuses on the statistical analysis of compositional data and its applications in a wide range of fields (geology, analytical chemistry, metabolomics, time-use epidemiology and others). He co-authored the book &lt;em&gt;Applied Compositional Data Analysis&lt;/em&gt;, published in Springer Series in Statistics.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-02-04-correlation/pf2018small.jpg&#34; alt=&#34;drawing&#34; width=&#34;100&#34;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Peter Filzmoser&lt;/strong&gt; is a Professor of Statistics at the Vienna University of Technology
(TU Wien), Austria, &lt;a href=&#34;mailto:Peter.Filzmoser@tuwien.ac.at&#34; class=&#34;email&#34;&gt;Peter.Filzmoser@tuwien.ac.at&lt;/a&gt;. He has authored more than 200 research
articles and several R packages and has co-authored books on compositional data analysis
(Springer, 2018), on multivariate methods in chemometrics (CRC Press, 2009) and on analyzing
environmental data (Wiley, 2008).&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Recent Advances in COVID-19 modelling</title>
      <link>https://youngstats.github.io/post/2021/02/04/covid-webinar/</link>
      <pubDate>Thu, 04 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/02/04/covid-webinar/</guid>
      <description>&lt;p&gt;YoungStatS project of Young Statisticians Europe, FENStatS, proudly announces our first One World YoungStatS webinar.
With four young scholars, we will discuss &lt;strong&gt;Recent Advances in the Modelling of COVID-19&lt;/strong&gt;, presenting novel statistical models, interesting advancements and applications of mechanistic models, as well as their combinations.&lt;/p&gt;
&lt;p&gt;When: &lt;strong&gt;Wednesday, February 10th, 16:00 (Central European Time)&lt;/strong&gt;.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cécile Tran Kiem (Institute Pasteur, Paris, France)&lt;/li&gt;
&lt;li&gt;Pierfrancesco Alaimo Di Loro and Marco Mingione (StatGroup-19, Italy)&lt;/li&gt;
&lt;li&gt;Kevin Kunzmann (University of Cambridge, United Kingdom)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prof. Jean-Stéphane Dhersin (Université Sorbonne Paris Nord, France)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Moderators:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Christian Capezza (University of Naples Federico II, Italy)&lt;/li&gt;
&lt;li&gt;Geneviève Robin (CNRS, Université d&amp;rsquo;Évry Val d&amp;rsquo;Essonne, France)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Introduction:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Andrej Srakar (University of Ljubljana, Slovenia, coordinator of YoungStatS)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One World YoungStatS webinar is addressed towards presenting recent interesting work of leading young researchers in statistics, probability and econometrics. The webinar is part of YoungStatS project of the Young Statisticians Europe initiative (FENStatS) supported by the Bernoulli Society for Mathematical Statistics and Probability, and will take place on monthly level. For more information, please visit our &lt;a href=&#34;https://youngstats.github.io/post/2020/09/10/webinars/&#34;&gt;website&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you missed our first One World YoungStatS seminar, you can &lt;a href=&#34;https://youtu.be/CdjrQs2pESo?t=243&#34;&gt;watch the recording on our Youtube channel&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Locally adapative k-nearest neighbour classification</title>
      <link>https://youngstats.github.io/post/2021/01/31/local-knn/</link>
      <pubDate>Sun, 31 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/01/31/local-knn/</guid>
      <description>


&lt;div id=&#34;abstract&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Binary classification is one of the cornerstones of modern data science, but, until recently, our understanding of classical methods such as the &lt;em&gt;k&lt;/em&gt;-nn algorithm was limited to settings where feature vectors were compactly supported. Based on a new analysis of this classifier, we propose a variant with significantly lower risk for heavy-tailed distributions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-k-nearest-neighbour-classifier&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-nearest neighbour classifier&lt;/h2&gt;
&lt;p&gt;The basic classifier that we consider here was introduced by Fix and Hodges (1951), and is arguably the simplest and most intuitive nonparametric classifier. For some fixed value of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, we classify a test point &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; to the class that is most prevalent among the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; points in our test data which lie closest to &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;center&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-01-29-local-knn/BasicIllustration.png&#34; class=&#34;class&#34; label=&#34;fig:Basic&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
Figure 1. Basic example of classification approach.
&lt;/center&gt;
&lt;p&gt;In the simple example in Figure 1 where the black point is to be classified, with &lt;span class=&#34;math inline&#34;&gt;\(k=1\)&lt;/span&gt; we assign to the green class, with &lt;span class=&#34;math inline&#34;&gt;\(k=2\)&lt;/span&gt; we assign to the red class (ties being broken in favour of the red class), and with &lt;span class=&#34;math inline&#34;&gt;\(k=3\)&lt;/span&gt; we assign to the green class.&lt;/p&gt;
&lt;p&gt;The choice of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; will clearly have a huge impact on the performance of the classifier. It is often chosen using cross-validation so that, over many splits of the data set into test set and training set, we choose the value of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; that gives the most accurate predictions over all points.&lt;/p&gt;
&lt;p&gt;Our main finding is that it is often possible to achieve better performance when the value of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is allowed to depend on the location of the test point. Indeed, for some constant &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; (which may be chosen by cross-validation), when &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the sample size, &lt;span class=&#34;math inline&#34;&gt;\(\bar{f}\)&lt;/span&gt; is the marginal density of the data points, and &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the test point, we find that a choice of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; roughly equal to
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\tag{1}
  B \{ n\bar{f}(x) \}^{4/(d+4)}
\end{equation}\]&lt;/span&gt;
results in minimax rate-optimal performance over suitable classes of data-generating mechanisms. In contrast, for heavy-tailed data we see that the standard, fixed-&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; classifier is suboptimal. Although &lt;span class=&#34;math inline&#34;&gt;\(\bar{f}\)&lt;/span&gt; is usually unknown, it can typically be estimated well enough from data. In fact, in many modern applications we have access to large amount of unlabelled &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; data that can be used for this purpose.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;theoretical-results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Theoretical results&lt;/h2&gt;
&lt;p&gt;Given full knowledge using knowledge of the underlying data-generating mechanism, the optimal decision rule is the Bayes classifier, which assigns points to the class with largest posterior probability. As even this optimal classifier makes mistakes, we typically evaluate the performance of a data-driven classification rule by comparing it to the Bayes classifier. Given a classification rule &lt;span class=&#34;math inline&#34;&gt;\(C:\mathbb{R}^d \rightarrow \{0,1\}\)&lt;/span&gt;, define its excess risk to be
&lt;span class=&#34;math display&#34;&gt;\[
  \mathcal{E}_P(C)= \mathbb{P}_P\{C(X) \neq Y\} - \mathbb{P}_P\{C^\mathrm{B}(X) \neq Y\},
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(C^\mathrm{B}\)&lt;/span&gt; is the Bayes classifier, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is the test point and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is its true class label such that &lt;span class=&#34;math inline&#34;&gt;\((X,Y)\)&lt;/span&gt; has distribution &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;. This quantity is non-negative and equal to zero if and only if &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; is optimal. Classifiers that make similar predictions to the Bayes classifier perform well.&lt;/p&gt;
&lt;p&gt;Our results hold over classes &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P}_{d,\rho}\)&lt;/span&gt; of distributions of &lt;span class=&#34;math inline&#34;&gt;\((X,Y)\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^d \times \{0,1\}\)&lt;/span&gt; satisfying certain regularity conditions, including that they have twice-differentiable densities and a bounded &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;th moment. Ignoring sub-polynomial factors, we find that the standard fixed-&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; nearest neighbour classifier, trained on a data set of size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, satisfies
&lt;span class=&#34;math display&#34;&gt;\[
  \sup_{P \in \mathcal{P}_{d,\rho}} \mathcal{E}_P(C_n^{k\text{nn}}) = O\biggl( \frac{1}{k} + \Bigl( \frac{k}{n} \Bigr)^{\min(4/d,\,\rho/(\rho+d))} \biggr).
\]&lt;/span&gt;
When &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is chosen to minimise this right-hand side, we obtain a rate of convergence of &lt;span class=&#34;math inline&#34;&gt;\(n^{-\min(\frac{\rho}{2\rho+d},\frac{4}{4+d})}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;On the other hand, we find that when &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is chosen according to &lt;span class=&#34;math inline&#34;&gt;\((1)\)&lt;/span&gt; above, we have
&lt;span class=&#34;math display&#34;&gt;\[
  \sup_{P \in \mathcal{P}_{d,\rho}} \mathcal{E}_P(C_n^{k_\mathrm{O}\text{nn}}) =O( n^{-\min(\rho/(\rho+d),4/(4+d))}).
\]&lt;/span&gt;
For small values of &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;, i.e. for heavy-tailed distributions, there is a gap between these rates.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulation&lt;/h2&gt;
&lt;p&gt;To illustrate the potential benefits of the local procedure, consider the following simulation. Following Cannings, Berrett and Samworth (2020), we take &lt;span class=&#34;math inline&#34;&gt;\(n_1=n_0=100\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[
  P_1 = t_5 \times t_5 \quad \text{ and } P_0 = N(1,1) \times t_5,
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(t_5\)&lt;/span&gt; is the &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-distribution with &lt;span class=&#34;math inline&#34;&gt;\(5\)&lt;/span&gt; degrees of freedom. This represents a setting in which there is a gap between the rates of convergence given above. Our results show that the optimal rate here is approximately &lt;span class=&#34;math inline&#34;&gt;\(n^{-2/3}\)&lt;/span&gt;, which is achieved by the local classifier, while with an optimal choice of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; the standard classifier is only guaranteed to achieve a rate of &lt;span class=&#34;math inline&#34;&gt;\(n^{-5/12}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The data is displayed in the left-most plot of Figure 2, together with vertical lines indicating the action of Bayes classifier, which selects class 0 when the data points lie between the two lines and selects class 1 otherwise. First, we run the standard &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-nearest neighbour classifier on the data, with the value of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; chosen by leave-one-out cross-validation from &lt;span class=&#34;math inline&#34;&gt;\(\{1,\ldots,20\}\)&lt;/span&gt;. The middle plot of Figure  shows those points of the data set for which the standard classifier classifies differently to the Bayes classifier. Finally, we run our local classifier, where we assume that &lt;span class=&#34;math inline&#34;&gt;\(\bar{f}\)&lt;/span&gt; is known, and where the value of &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; is chosen by leave-one-out cross-validation on a grid of size &lt;span class=&#34;math inline&#34;&gt;\(20\)&lt;/span&gt;.&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;https://youngstats.github.io/post/2021-01-29-local-knn/Simulation.png&#34; class=&#34;class&#34; label=&#34;fig:Simulation&#34; /&gt;
Figure 2. Simulated data and different classifiers.
&lt;/center&gt;
&lt;p&gt;Perhaps the most striking aspect of the results is that the local classifier agrees with the Bayes classifier much more often than the standard classifier, with only &lt;span class=&#34;math inline&#34;&gt;\(9\)&lt;/span&gt; disagreements compared to the &lt;span class=&#34;math inline&#34;&gt;\(43\)&lt;/span&gt; disagreements of the standard classifier. Looking a little closer, we can see that the remaining mistakes that the local classifier makes are concentrated around the Bayes decision boundaries. Standard theoretical analysis of classification problems reveals that such points typically represent the hardest point to classify. We finally see that many, though by no means all, of the points at which the standard classifier makes a mistake appear in low-density regions, for example towards the bottom of the plots.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;about-the-authors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;About the authors&lt;/h2&gt;
&lt;p&gt;This post was written by Thomas B. Berrett, and was based on&lt;/p&gt;
&lt;p&gt;Cannings, T. I., Berrett, T. B. and Samworth, R. J. (2020) Local nearest neighbour classification with applications to semi-supervised learning. &lt;em&gt;Annals of Statistics&lt;/em&gt;, &lt;strong&gt;48&lt;/strong&gt;, 1789–1814. &lt;a href=&#34;https://projecteuclid.org/euclid.aos/1594972839&#34;&gt;.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-01-29-local-knn/CanningsTI.jpg&#34; class=&#34;class&#34; style=&#34;width:10.0%&#34; /&gt; &lt;a href=&#34;https://www.maths.ed.ac.uk/~tcannings/About_Me.html&#34;&gt;Timothy I. Cannings&lt;/a&gt; is a lecturer in statistics and data science at the School of Mathematics, University of Edinburgh. He completed his PhD with Prof Richard Samworth in the Statistical Laboratory at the University of Cambridge in 2015. He then worked with Prof Yingying Fan as a Postdoc at the University of Southern California.&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-01-29-local-knn/BerrettTB.jpg&#34; class=&#34;class&#34; style=&#34;width:10.0%&#34; /&gt; &lt;a href=&#34;https://thomasberrett.github.io&#34;&gt;Thomas B. Berrett&lt;/a&gt; is an Assistant Professor in the Department of Statistics at the University of Warwick. He completed his PhD with Prof Richard Samworth in the Statistical Laboratory at the University of Cambridge in 2018, and later worked as a Postdoc with Prof Cristina Butucea at CREST, ENSAE, Institut Polytechnique de Paris.&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-01-29-local-knn/SamworthRJ.jpg&#34; class=&#34;class&#34; style=&#34;width:10.0%&#34; /&gt; &lt;a href=&#34;http://www.statslab.cam.ac.uk/~rjs57/&#34;&gt;Richard J. Samworth&lt;/a&gt; is Professor of Statistical Science and Director of the Statistical Laboratory at the University of Cambridge. He is also a Teaching Fellow at St John’s College.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>PLS for Big Data: A unified parallel algorithm for regularised group PLS</title>
      <link>https://youngstats.github.io/post/2021/01/28/pls-for-big-data/</link>
      <pubDate>Thu, 28 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/01/28/pls-for-big-data/</guid>
      <description>


&lt;p&gt;&lt;em&gt;We look at the problem of learning latent structure between two blocks of data through the partial least squares (PLS) approach. These methods include approaches for supervised and unsupervised statistical learning. We review these methods and present approaches to decrease the computation time and scale the method to big data&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Given two blocks of data, the PLS approach seeks latent variables which are constructed as linear combinations of the original datasets. These latent variables are constructed according to specific covariance or correlation requirements. As such the latent variables can be used as a data reduction tool that sheds light on the relationship between the datasets. For two blocks of data there are four established PLS methods that can be used to construct these latent variables:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;PLS-SVD&lt;/li&gt;
&lt;li&gt;PLS-W2A&lt;/li&gt;
&lt;li&gt;Canonical correlation analysis (CCA)&lt;/li&gt;
&lt;li&gt;PLS-R&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These methods provide a range of useful methods for data exploration and prediction for datasets with high dimensional structure. However, as shown in the sparse PCA literature (Johnstone 2004) in high dimensional settings (&lt;span class=&#34;math inline&#34;&gt;\(n&amp;lt;&amp;lt;p\)&lt;/span&gt;) the standard latent variable estimates may be biased and sparsity inducing methods are a natural choice. After reviewing the two-block PLS methods and placing them in a common framework we provide details for the application of sparse PLS methods that use regularisation via either the lasso or sparse group lasso penalisation.&lt;/p&gt;
&lt;div id=&#34;finding-the-latent-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Finding the Latent variables&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X: n\times p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y: n\times q\)&lt;/span&gt; be the datasets comprised of n observations on p and q variables respectively, the latent variables &lt;span class=&#34;math inline&#34;&gt;\(\xi = X u\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\omega = Y v\)&lt;/span&gt; bare a striking similarity to the principal components from principal component analysis (PCA) where &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; is a p-vector and &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; is a q-vector. The PLS-SVD, PLS-W2A and PLS-R methods optimise the sample covariance between the latent variables
&lt;span class=&#34;math display&#34;&gt;\[
\text{max}_{\xi,\omega} \widehat{Cov}(\xi, \omega) \rightarrow \text{max}_{u,v} \widehat{Cov}(Xu, Yv) = \text{max}_{u,v} u^TX^TYv
\]&lt;/span&gt;
subject to &lt;span class=&#34;math inline&#34;&gt;\(\|u\|=\|v\| = 1\)&lt;/span&gt; and orthogonality constraints for the different methods. Whereas CCA optimises
&lt;span class=&#34;math display&#34;&gt;\[
\text{max}_{\xi,\omega} \widehat{Corr}(\xi, \omega) \rightarrow\text{max}_{u,v} \widehat{Corr}(Xu, Yv) = \text{max}_{u,v} u^T(X^TX)^{-1/2}X^TY(Y^TY)^{-1/2}v
\]&lt;/span&gt;
subject to orthogonality constraints similar to the PLS-SVD. These orthogonality constraints are enforced by removing the effect of the constructed latent variables using projection matrices. Once the effect has been removed the optimisation is repeated on the projected data. This process is the same for all PLS methods and can be simply adjusted to allow for sparsity in the weights (&lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;) using sparse regularising penalties (e.g. lasso, group lasso, sparse group lasso).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;computational-speedups&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Computational speedups&lt;/h3&gt;
&lt;p&gt;Due to the algorithmic similarity of the different methods some additional computational approaches can be used to speed up the required computation for the PLS approach. In our paper, we consider reducing memory requirements and speeding up computation by making use of the “bigmemory” R package to allocate shared memory and make use of memory-mapped files. Rather than loading the full matrices when computing the matrix cross-product (&lt;span class=&#34;math inline&#34;&gt;\(X^TY\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(Y^TY\)&lt;/span&gt;) we instead read chucks of the matrices, compute the cross-product on these chucks in parallel, and add these cross-products together, ie.
&lt;span class=&#34;math display&#34;&gt;\[
X^TY = \sum_{c=1}^CX_c^TY_c
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(X_c\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_c\)&lt;/span&gt; are matrix chucks formed as the subsets of the rows of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. Additional computational approaches are used for when either p or q are large or when n is very large and data is streaming in while q is small.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-on-emnist&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Example on EMNIST&lt;/h3&gt;
&lt;p&gt;We show an example using PLS regression for a discrimination task, namely the extended MNIST dataset. This data set consists of n = 280,000 handwritten digit images. It contains an equal number of samples for each digit class (0 to 9) where the dimension of the predictors is &lt;span class=&#34;math inline&#34;&gt;\(p=784\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(q=10\)&lt;/span&gt; classes. The images are already split into a training set of 240,000 cases and a test set of 40,000 cases. Since we have a large sample size &lt;span class=&#34;math inline&#34;&gt;\(n &amp;gt; p, q\)&lt;/span&gt; we opt not to consider regularisation for this example. The PLS-DA method is able to recover an accuracy of 86% in around 3 minutes using 20 latent variables and 2 cores.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-01-28-pls-for-big-data/EMNISTw.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We investigated the relationship between the number of chunks and the number of cores used in the algorithm. The plot below shows the elapsed computation time for fitting a single component of the PLS discriminant analysis algorithm using 2, 4 or 6 cores (on a laptop equipped with 8 cores). On the vertical axis, &lt;span class=&#34;math inline&#34;&gt;\(ngx\)&lt;/span&gt; indicates that &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; chunks were used in our algorithm.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references-and-related-work&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References and related work&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Johnstone, I. M. and Lu, A. Y. (2004) Sparse principal component analysis. Technical Report. Department of Statistics, Stanford University, Stanford.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sutton, M., Thiébaut, R., &amp;amp; Liquet, B. (2018). Sparse partial least squares with group and subgroup structure. Statistics in Medicine, 37(23), 3338–3356.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Lafaye de Micheaux, P., Liquet, B., &amp;amp; Sutton, M. (2019). PLS for Big Data: A unified parallel algorithm for regularised group PLS. Statistics Surveys, 13, 119–149.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Liquet, B., Lafaye de Micheaux, P., Hejblum, B. P., &amp;amp; Thiébaut, R. (2016). Group and sparse group partial least square approaches applied in genomics context. Bioinformatics , 32(1), 35–42.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Machine learning for causal inference that works</title>
      <link>https://youngstats.github.io/post/2021/01/26/machine-learning-for-causal-inference-that-works/</link>
      <pubDate>Tue, 26 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/01/26/machine-learning-for-causal-inference-that-works/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://youngstats.github.io/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I’ve kindly been invited to share a few words about a recent &lt;a href=&#34;https://projecteuclid.org/euclid.ba/1580461461&#34;&gt;paper&lt;/a&gt; my colleagues and I published in &lt;em&gt;Bayesian Analysis&lt;/em&gt;: “Bayesian Regression Tree Models for Causal Inference: Regularization, Confounding, and Heterogeneous Effects”. In that paper, we motivate and describe a method that we call Bayesian causal forests (BCF), which is now implemented in an R package called &lt;a href=&#34;https://github.com/jaredsmurray/bcf&#34;&gt;bcf&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The goal of this post is to work through a simple toy example to illustrate the strengths of BCF. Through this example I hope to explain what I mean when I say that BCF is “machine learning for causal inference that works”.&lt;/p&gt;
&lt;div id=&#34;problem-setting&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem setting&lt;/h3&gt;
&lt;p&gt;Suppose we want to estimate a possibly heterogeneous treatment effect of a binary treatment variable. This means, for example, that we want to know if a new drug reduces the duration of a headache and we think maybe the drug works better for some people and worse for other people. In addition to the question “how well (if at all) does the drug work?” we also want to know if the people for whom it works better can be characterized in terms of observable attributes, perhaps age, gender, or ethnicity. People either get the drug or not (we do not consider differing doses). Unfortunately, who gets the drug is not randomized, which complicates things. For example, if people who take the drug happen to be the ones with longer duration headaches (on average), that could skew our impression of how effective the drug is.&lt;/p&gt;
&lt;p&gt;Although we do not have a randomized sample, let’s assume we are lucky enough to have the next best thing, which is that we observe all the attributes of each patient that affect how likely they are to have taken the drug. It is well known that access to these factors allows us to correctly estimate the treatment effect by turning the causal inference problem into a regression problem (aka supervised learning). Specifically, in that case the treatment effect can be expressed as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\tau(x_i) = \mbox{E}(Y \mid Z = 1, X = x_i) - \mbox{E}(Y \mid Z = 0, X = x_i)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is the response or outcome variable (the duration of headache), &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; is the treatment assignment (did the patient take the drug or not), and &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; is a vector of attributes of patient &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. This difference is called the conditional average treatment effect, or CATE; “conditional” refers to the fixed &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; vector, “average” refers to the expectation over &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, and “treatment effect” refers to the difference between to treated (&lt;span class=&#34;math inline&#34;&gt;\(Z = 1\)&lt;/span&gt;) and untreated (&lt;span class=&#34;math inline&#34;&gt;\(Z = 0\)&lt;/span&gt;), or control, groups. If this quantity differs for distinct &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;, we say that there are “heterogeneous” effects.&lt;/p&gt;
&lt;p&gt;The good news is that we have many methods that efficiently estimate conditional expectations in the difference above. The bad news, which wasn’t widely appreciated even just a few years ago, is that those methods don’t work as well as they should in terms of estimating the CATEs. Let’s take a look at why that is.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simple-machine-learning-cate-estimators-are-high-variance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Simple machine learning CATE estimators are high-variance&lt;/h3&gt;
&lt;p&gt;A natural thing to do when faced with estimating two conditional expectations is simply to estimate them separately, training two separate machine learning models using the control group data and the treated group data individually. With enough data, this approach works just fine, but if the conditional expectation functions underlying the data are complicated relative to the available sample size, this approach can be highly unstable. This instability arises because fitting the two functions completely separately provides no control, or &lt;em&gt;regularization&lt;/em&gt;, over the implied fluctuations in the CATE (the difference between the two conditional mean functions).&lt;/p&gt;
&lt;p&gt;It is well-known that for good nonparametric function estimation, effective regularization is necessary to prevent overfitting; this is the main insight from decades of supervised machine learning. But in causal inference, the goal is not estimating the conditional expectations themselves, but rather their difference. Without penalizing complexity of &lt;span class=&#34;math inline&#34;&gt;\(\tau(x)\)&lt;/span&gt; itself, one runs the risk of overfitting the treatment effects! And that’s exactly what happens in our example below.&lt;/p&gt;
&lt;p&gt;This excessive variability has a fairly simple fix, which is to regularize the difference the same way you would penalize the complexity of an unknown function:
&lt;span class=&#34;math display&#34;&gt;\[
\mbox{min}_{f_0, f_1}\;\;\;\; \frac{1}{n_0} \sum_{i: z_i = 0}||y_i - f_0(x_i)||^2_2 + \frac{1}{n_1} \sum_{i: z_i = 1} ||y_i - f_1(x_i)||^2_2 + \lambda_0||f_0|| + \lambda_1||f_1|| + \lambda_{\tau}||f_1 - f_0||
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\lambda_0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{\tau}\)&lt;/span&gt; are regularization tuning parameters and &lt;span class=&#34;math inline&#34;&gt;\(||\cdot||\)&lt;/span&gt; denotes a measure of the complexity of a function.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-new-problem-regularization-induced-confounding-ric&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A new problem: regularization induced confounding (RIC)&lt;/h3&gt;
&lt;p&gt;Incorporating the &lt;span class=&#34;math inline&#34;&gt;\(||f_1 - f_0||\)&lt;/span&gt; penalty solves one problem but introduces a new, subtler, one. Adding a constant to &lt;span class=&#34;math inline&#34;&gt;\(f_1\)&lt;/span&gt; does not increase the complexity of &lt;span class=&#34;math inline&#34;&gt;\(f_1\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(f_1 - f_0\)&lt;/span&gt;, but doing so may allow the complexity of &lt;span class=&#34;math inline&#34;&gt;\(f_0\)&lt;/span&gt; to be &lt;em&gt;decreased&lt;/em&gt; without worsening the fit to the data (the first two terms of the objective function above). In practical terms, this means that the new regularization term we just introduced might have the unintended effect of inflating our treatment effect estimates!&lt;/p&gt;
&lt;p&gt;When specifically might this happen? It can happen when the true &lt;span class=&#34;math inline&#34;&gt;\(f_0\)&lt;/span&gt; is quite complex and the probability of being treated is a monotone function of &lt;span class=&#34;math inline&#34;&gt;\(f_0\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}(Z = 1 \mid x) = \pi(x) = \pi(f_0(x))
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and &lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial \pi}{\partial f_0}\)&lt;/span&gt; never changes sign. Under this assumption, the treated observations in our data would tend to have higher outcome values, which our model could chalk up to a treatment effect without needing to learn the complicated pattern of &lt;span class=&#34;math inline&#34;&gt;\(f_0\)&lt;/span&gt; because it is implicitly encoded in the treatment assignment variable, &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Is this situation plausible? Well, in our headache drug example, it would mean that people are more likely to take a drug if they are likely to have a very long lasting headache if they &lt;em&gt;didn’t&lt;/em&gt; take it. If people (or their doctors) expect the drug to help, this assumption makes total sense! We call these sorts of situations &lt;em&gt;targeted selection&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Solving the RIC issue turns out to be pretty easy, too: simply add an estimate of &lt;span class=&#34;math inline&#34;&gt;\(\pi(x)\)&lt;/span&gt; as a control variable. This allows the model to learn the true &lt;span class=&#34;math inline&#34;&gt;\(f_0\)&lt;/span&gt; with a &lt;em&gt;simple&lt;/em&gt; representation based on the extra feature, in the event that targeted selection is occurring.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Example&lt;/h3&gt;
&lt;p&gt;Now let’s work through a simple example illustrating these ideas. We will consider a nonlinear but elementary conditional expectation function and simulate our treatment assignment variable according to targeted selection.
&lt;span class=&#34;math display&#34;&gt;\[
\begin{split}
\tau(x) &amp;amp;= -1 + x\\
f_0(x) &amp;amp;= 2 \{\sin(v x) + 1\}\\
f_1(x) &amp;amp;= f_0(x) + \tau(x) = -1 + x + 2 \{\sin(v x) + 1\}\\
\pi(x) &amp;amp;= f_0(x)/5\\
y_i &amp;amp;= f_0(x_i) + \tau(x_i) + \sigma\epsilon_i
\end{split}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i\)&lt;/span&gt; are independent and identically distributed standard normal random variables. Our sample consists of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; evenly spaced observations &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; on the unit interval. This data generating process (DGP) guarantees that the probability of treatment ranges between 0.1 to 0.9. The parameter &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; governs the “complexity” of &lt;span class=&#34;math inline&#34;&gt;\(f_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(f_1\)&lt;/span&gt;, while the parameter &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; governs the statistical difficulty of the learning problem.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# set sample size and set control variable values
n = 1000
x = seq(0,1,length.out = n)

# set the problem difficulty
v = 30
kappa = 2

# define functions
mu = function(x){2*(sin(v*x)+1)}
tau = function(x){-1 + x}
pi = function(x){mu(x)/5 + 0.1}

# draw treatment assignment
z = rbinom(n,1,pi(x))

# draw outcome
f_xz = mu(x) + tau(x)*z
sigma = kappa*sd(f_xz)
y = f_xz + sigma*rnorm(n)

# calculate the true average treatment effect (ATE)
print(mean(tau(x)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# calculate the naive estimate of the ATE
print(mean(y[z==1]) - mean(y[z==0]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9889149&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Observe that the naive estimate is way off from the truth due to strong confounding.&lt;/p&gt;
&lt;p&gt;Next, let’s use the separate regressions approach to estimating the treatment effect. To do this we will use the R package &lt;a href=&#34;https://github.com/jingyuhe/xbart&#34;&gt;XBART&lt;/a&gt;, based on another &lt;a href=&#34;https://arxiv.org/abs/2002.03375&#34;&gt;paper&lt;/a&gt; of mine. It can be downloaded and installed from here (but must be compiled from source).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit.f1 = XBART(y[z==1],x[z==1],x,
               num_sweeps = sweeps, burnin = b, num_trees = 20,
               tau = var(y[z==1])/20)

yhat1 = rowMeans(fit.f1$yhats_test[,(b+1):sweeps])

fit.f0 = XBART(y[z==0],x[z==0],x,
               num_sweeps = sweeps, burnin = b, num_trees = 20, 
               tau = var(y[z==0])/20)

yhat0 = rowMeans(fit.f0$yhats_test[,(b+1):sweeps])

tau.est1 &amp;lt;- yhat1 - yhat0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, let’s explicitly regularize the treatment effect. We will do this using the R package XBCF, which can be downloaded &lt;a href=&#34;https://github.com/socket778/XBCF&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;xbcf_fit = XBCF(scale(y), x, x, z, 
                 num_sweeps = sweeps, burnin = b, Nmin = 1, verbose = FALSE,
                 num_cutpoints = 20, max_depth = 250,
                 num_trees_pr = 20,  tau_pr = tau1, 
                 num_trees_trt = 20, alpha_trt = 0.7, beta_trt = 2, tau_trt = tau2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in if (class(y) != &amp;quot;matrix&amp;quot;) {: the condition has length &amp;gt; 1 and only
## the first element will be used&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tau.est2 = getTaus(xbcf_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, let’s do it the right way and also incorporate the estimated propensity scores. First, we estimate them. Here, we again use XBART, but your favorite classification algorithm would be okay, too.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitz = XBART.multinomial(y = z, num_class = 2, X = x, Xtest = x, 
                         num_trees = 20, num_sweeps = sweeps, max_depth=250, 
                         Nmin=6, num_cutpoints=50, tau_a = 2, tau_b = 1, 
                         burnin = b, verbose = FALSE, parallel = TRUE, 
                         sample_weights_flag = TRUE, weight = 5,update_tau = TRUE) 

pihat = apply(fitz$yhats_test[(b+1):sweeps,,], c(2, 3), mean)[,2]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With those estimates in hand, we then run XBCF again, this time including the propensity score as an extra feature.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;xbcf_fit.ps = XBCF(scale(y), cbind(pihat,x), x, z, 
                 num_sweeps = sweeps, burnin = b, Nmin = 1, verbose = FALSE,
                 num_cutpoints = 20, max_depth = 250,
                 num_trees_pr = 20,  tau_pr = tau1, 
                 num_trees_trt = 20, alpha_trt = 0.7, beta_trt = 2, tau_trt = tau2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in if (class(X) != &amp;quot;matrix&amp;quot;) {: the condition has length &amp;gt; 1 and only
## the first element will be used&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in if (class(y) != &amp;quot;matrix&amp;quot;) {: the condition has length &amp;gt; 1 and only
## the first element will be used&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tau.est3 = getTaus(xbcf_fit.ps)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we can plot the results against the truth and compute the root mean squared estimation error of the CATEs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(x,tau(x),type=&amp;#39;l&amp;#39;,ylim=c(-1.5,1.5),col=&amp;#39;red&amp;#39;,lty=2,lwd=2,bty=&amp;#39;n&amp;#39;,ylab=expression(tau))
lines(x,tau.est1,col=&amp;#39;lightgray&amp;#39;,lwd=3)
lines(x,tau.est2,col=&amp;#39;blue&amp;#39;,lwd=3)
lines(x,tau.est3,col=&amp;#39;green&amp;#39;,lwd=3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-01-26-machine-learning-for-causal-inference-that-works_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;40%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rmse1 = sqrt(mean((tau(x)-tau.est1)^2))
rmse2 = sqrt(mean((tau(x)-tau.est2)^2))
rmse3 = sqrt(mean((tau(x)-tau.est3)^2))

print(round(c(rmse1,rmse2,rmse3),2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.15 0.45 0.18&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The pattern here is bad (gray), better (blue), best (green) from left to right. The true heterogeneous treatment effect function is depicted by the dashed red line. The third approach (the approach we advocate for in the BCF paper) is not &lt;em&gt;always&lt;/em&gt; better, but it is most of the time, sometimes by a very large margin. Try it out yourself, varying the sample size (&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;) and the two difficulty parameters (&lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusions&lt;/h3&gt;
&lt;p&gt;There are now many researchers working at the intersection of machine learning and causal inference. What distinguishes our work is a focus on building tools that work in practice, which requires understanding the role of regularization in causal inference and engineering methods that impose effective regularization schemes that have been calibrated to the kind of data we expect to encounter in common applications.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
