<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>YoungStatS</title>
    <link>https://youngstats.github.io/</link>
    <description>Recent content on YoungStatS</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 19 Sep 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://youngstats.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Macroeconomic Determinants of Migration. A Comparative Analysis For Old vs New European Member States</title>
      <link>https://youngstats.github.io/post/2022/09/18/macroeconomic-determinants-of-migration-a-comparative-analysis-for-old-vs-new-european-member-states/</link>
      <pubDate>Sun, 18 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/09/18/macroeconomic-determinants-of-migration-a-comparative-analysis-for-old-vs-new-european-member-states/</guid>
      <description>


&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Migration is a multifaceted phenomenon including macro-, meso-, and
micro-triggers that, when combined, decide an individual’s final
decision to migrate. Apart from being an essential component of
population change, migration is also an important component of
population estimates and labor market force estimations. Jennissen
(2004) argues that the presence of migrant populations has a favorable
impact on natural population growth since migrants’ age characteristics
and fertility rates are often greater than those of the indigenous
population. Furthermore, the migration problem is far more significant
for population growth in European nations, which are generally seeing a
drop in natural population increase. In this environment, it’s critical
to identify and quantify the factors involved in the decision to
migrate. Income disparities or income inequality, economic development,
the tax system, the economic cycle, the availability of new job
opportunities, unemployment, and other variables have been highlighted
as factors affecting migration by several migration theories (Kumpikaite
and Zickute, 2012). People migrate for a variety of reasons, including
improved living conditions or an escape from adverse circumstances in
their native country. This is the foundation of Lee’s (1966) push and
pull hypothesis, which is one of the primary neo-classical migration
hypotheses. Individuals are influenced by supply-push forces to leave
their home country, while demand-pull variables draw migrants to the
destination country.&lt;/p&gt;
&lt;p&gt;Panel data analysis is the study of datasets in which entities are
observed through time and allows for the management of missing variables
without having to examine them. By examining changes in the dependent
variable across time, one may rule out the impact of neglected factors
that fluctuate between entities but remain constant over time. The basic
premise is that if unobserved factors (such as those peculiar to
nations) influence the dependent variable yet stay constant over time,
the changes in the dependent variable must come from other sources.&lt;/p&gt;
&lt;p&gt;The notation for panel data will be the following:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} (x_{1,it}, x_{2,it}, ..., x_{k,it}, y_{it}), i=1,2, ..., n; t=1,2, ..., T \end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;em&gt;i&lt;/em&gt; is the subscript for the entity being observed (in our case
study the country) and &lt;em&gt;t&lt;/em&gt; is the subscript for the date at which the
entity is observed (in our case study the year). Using these notations
we would have data for the variables
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} x_1, x_2, ..., x_k, y \end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The fixed effects approach&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The fixed effects model would be written as:&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} y_{it}=\beta_1 x_{1,it} + \beta_2 x_{2,it} + \dots + \beta_k x_{k,it} + \alpha_i + u_{it} \end{equation}\]&lt;/span&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_{j,it}\)&lt;/span&gt; represents the value of regressor &lt;em&gt;j&lt;/em&gt;, for entity &lt;em&gt;i&lt;/em&gt; and
time period &lt;em&gt;t&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_j\)&lt;/span&gt; represents the coefficients of the independent variables,
that do not vary across individuals.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha_i\)&lt;/span&gt; represents the entity specific intercepts.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;The random effects approach&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;the unobserved variable specific to the individual entity is encompassed
in the error term. The entities will have a common mean value for the
intercept (let’s denote this with &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;) and the specific differences
in the intercept values of each country would be reflected in an error
term (denoted with &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i\)&lt;/span&gt;).&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} y_{it}=\alpha + \sum_{j=1}^{k} \beta_j x_{j,it} + \epsilon_i + u_{it} \end{equation}\]&lt;/span&gt;
&lt;p&gt;We will obtain a composite error term, which is composed of
&lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i\)&lt;/span&gt;, the individual (cross section) specific error &lt;span class=&#34;math inline&#34;&gt;\(u_{it}\)&lt;/span&gt; and
a combined cross section and time series error:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} w_{it}=\epsilon_i + u_{it} \end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thus substituting both equations we obtain:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} y_{it}=\alpha + \sum_{j=1}^{k} \beta_j x_{j,it} + w_{it} \end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The main benefit of using panel data related techniques is that
individual heterogeneity of individual entities (countries) can be
explicitly taken into account in panel data estimation; additionally,
panel data offers “more informative data, more variability, less
collinearity, more degrees of freedom, and more efficiency” by combining
time series and cross-sectional observations (Baltagi, 2001).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The database involved&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Crude Rate of Net Migration plus adjustment is the dependent
variable in both estimated models. It is calculated as the annual ratio
of net migration to the average population. It is expressed in terms of
1000 people (of the average population). The difference between the
total number of immigrants and the total number of emigrants is referred
to as net migration; statistical adjustments refers to adjusting net
migration by taking the difference between total population change and
natural change; the indicator roughly covers the difference between
inward and outward migration. In the model, the variable is called “Net
Migration.”&lt;/p&gt;
&lt;p&gt;The independent variables in the models are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unemployment: The long-term unemployment rate is the percentage of
people who have been jobless for more than a year out of the total
number of people who are working.&lt;/li&gt;
&lt;li&gt;Earnings (adjusted): The adjusted gross disposable income of
households divided by the PPP (purchasing power parities) of the actual
individual consumption of households and by the entire resident
population (in purchasing power standard (PPS) per inhabitant) yields
gross disposable income of households per capita.&lt;/li&gt;
&lt;li&gt;Gini Coefficient (of equivalised disposable income): A relationship
between the cumulative shares of the population disposed based on
equivalised disposable income and the cumulative share of the
equivalised total disposable income received by the population is
defined as the Gini Coefficient (of equivalised disposable income).&lt;/li&gt;
&lt;li&gt;Poverty: The at-risk-of-poverty rate is the proportion of people whose
equivalised disposable income is less than the risk-of-poverty
threshold, which is set at 60% of the national median equivalised
disposable income.&lt;/li&gt;
&lt;li&gt;Economic Freedom: A 0–10 scale that assesses the degree of economic
liberty in five key areas: government size, legal system, sound money,
international trade freedom, and regulation.&lt;/li&gt;
&lt;li&gt;Hospital beds: the number of beds available in hospitals; the variable
is calculated per 100,000 people.&lt;/li&gt;
&lt;li&gt;Health spending: the overall amount spent on health as a proportion of
GDP ( percent ).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Eurostat is the data source for all variables except Economic Freedom.
The Fraser Institute is the source of data for the Economic Freedom
Index. Data was collected for 25 EU nations over a period of 18 years,
from 2000 to 2017.&lt;/p&gt;
&lt;p&gt;Figure 1 depicts the dispersion of average Net Migration rates for EU
nations for the period 2000–2017. Lithuania (-8.84), Latvia (-7.19),
and Romania (-7.19) are the nations with the lowest average Net
Migration between 2000 and 2017. (-5.48). Bulgaria, Estonia, Poland, and
Slovakia all had negative Net Migration averages over the time period
studied. This was to be expected, given that these economies in Central
and Eastern Europe are generally migration-sending countries (Jennissen,
2004). Luxembourg, Spain, and Sweden, on the other hand, have the
greatest average Net Migration rates, indicating that they are typically
migrant-receiving nations. The Net Migration Rate’s behavior proposes
that the group of nations be divided into two: typically sending and
traditionally receiving countries.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-09-18-macroeconomic-determinants-of-migration-a-comparative-analysis-for-old-vs-new-european-member-states_files/Figure%201.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 1 – Average Net Migration for the period 2000 – 2017 for the EU
countries; source: author’s processing in Tableau, based on Eurostat data.&lt;/p&gt;
&lt;p&gt;Figure 2 depicts the significant relationship between real GDP/capita,
chain linked volume (2010) in Euro/capita, and net migration rate
(averages from 2000 to 2017 were used). Nations with lower GDP per
capita also have negative or low Net Migration rates, indicating that
“poorer” countries are more likely to be the source of migrant flows
(sending countries). Nations with greater GDP/capita, on the other hand,
have higher migration rates, implying that they are primarily receiving
countries for migrants.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-09-18-macroeconomic-determinants-of-migration-a-comparative-analysis-for-old-vs-new-european-member-states_files/Figure%202.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 2 – Net Migration rate vs. GDP/capita – comparison between old
and new EU member states; source: author’s processing in Tableau, based
on Eurostat data.&lt;/p&gt;
&lt;p&gt;The tendency of Net Migration for old and new member states warrants the
dataset being separated into two categories. It is clear from this
research that the group of New Member States acts as Migrant Sending
Countries for the whole time under consideration, while the Old Member
States operate as Migrant Receiving Countries. As a result, the
empirical application will be split into two parts: one model for the
Old member nations and then another model (Table 1) for the New member
states (Table 2).&lt;/p&gt;
&lt;p&gt;Table 1. Results of estimation for fixed and random effects models for
Old Member States; source: author’s own results&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;23%&#34; /&gt;
&lt;col width=&#34;38%&#34; /&gt;
&lt;col width=&#34;38%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Fixed Effects&lt;/th&gt;
&lt;th&gt;Random Effects&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Unemployment&lt;/td&gt;
&lt;td&gt;-1.0468*** (0.0903)&lt;/td&gt;
&lt;td&gt;-1.0295*** (0.0881)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Ln_Income&lt;/td&gt;
&lt;td&gt;4.9042** (2.4037)&lt;/td&gt;
&lt;td&gt;8.1458*** (1.5631)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Gini&lt;/td&gt;
&lt;td&gt;-0.1777 (0.1691)&lt;/td&gt;
&lt;td&gt;-0.3282** (0.1391)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Poverty&lt;/td&gt;
&lt;td&gt;0.6876*** (0.1657)&lt;/td&gt;
&lt;td&gt;0.7331*** (0.1417)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Economic Freedom&lt;/td&gt;
&lt;td&gt;-4.5181*** (1.5230)&lt;/td&gt;
&lt;td&gt;-2.9796*** (1.0923)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Health Expenditure&lt;/td&gt;
&lt;td&gt;-1.4840*** (0.4175)&lt;/td&gt;
&lt;td&gt;-1.7844*** (0.2965)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Intercept&lt;/td&gt;
&lt;td&gt;-1.6247 (25.7912)&lt;/td&gt;
&lt;td&gt;-40.0461** (17.8813)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;R2&lt;/td&gt;
&lt;td&gt;Within 0.4184 Between 0.6589 Overall 0.5056&lt;/td&gt;
&lt;td&gt;Within 0.4109 Between 0.8405 Overall 0.5828&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;F&lt;/td&gt;
&lt;td&gt;29.85***&lt;/td&gt;
&lt;td&gt;229.47*** (Wald chi2)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Corr (u_i, xb)&lt;/td&gt;
&lt;td&gt;0.2190&lt;/td&gt;
&lt;td&gt;0 (assumed)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Sigma_u&lt;/td&gt;
&lt;td&gt;2.0795&lt;/td&gt;
&lt;td&gt;1.3274&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Sigma_e&lt;/td&gt;
&lt;td&gt;3.0081&lt;/td&gt;
&lt;td&gt;3.0081&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Rho&lt;/td&gt;
&lt;td&gt;0.3233&lt;/td&gt;
&lt;td&gt;0.1629&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;*** Significant at 0.01; ** Significant at 0.05&lt;/p&gt;
&lt;p&gt;(standard errors of the coefficients are reported in parenthesis)&lt;/p&gt;
&lt;p&gt;Table 2. Results of estimation for fixed and random effects models for
New Member States; source: author’s own results&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;23%&#34; /&gt;
&lt;col width=&#34;38%&#34; /&gt;
&lt;col width=&#34;38%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Fixed Effects&lt;/th&gt;
&lt;th&gt;Random Effects&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Unemployment&lt;/td&gt;
&lt;td&gt;-0.3900** (0.1547)&lt;/td&gt;
&lt;td&gt;-0.3893*** (0.1461)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Gini&lt;/td&gt;
&lt;td&gt;-0.3527** (0.1377)&lt;/td&gt;
&lt;td&gt;-0.5432*** (0.1226)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Economic Freedom&lt;/td&gt;
&lt;td&gt;4.3493*** (1.1280)&lt;/td&gt;
&lt;td&gt;4.3283*** (1.0755)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Health Expenditure&lt;/td&gt;
&lt;td&gt;-1.1240* (0.5841)&lt;/td&gt;
&lt;td&gt;-0.4513 (0.4765)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Intercept&lt;/td&gt;
&lt;td&gt;-15.6762 (8.0909)&lt;/td&gt;
&lt;td&gt;-13.3742** (7.5791)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;R2&lt;/td&gt;
&lt;td&gt;Within 0.1663 Between 0.0205 Overall 0.0855&lt;/td&gt;
&lt;td&gt;Within 0.1483 Between 0.5198 Overall 0.3147&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;F&lt;/td&gt;
&lt;td&gt;8.28***&lt;/td&gt;
&lt;td&gt;38.90*** (Wald chi2)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Corr (u_i, xb)&lt;/td&gt;
&lt;td&gt;-0.1069&lt;/td&gt;
&lt;td&gt;0 (assumed)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Sigma_u&lt;/td&gt;
&lt;td&gt;3.9181&lt;/td&gt;
&lt;td&gt;2.2254&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Sigma_e&lt;/td&gt;
&lt;td&gt;3.5999&lt;/td&gt;
&lt;td&gt;3.5999&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Rho&lt;/td&gt;
&lt;td&gt;0.5422&lt;/td&gt;
&lt;td&gt;0.2765&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;*** Significant at 0.01; ** Significant at 0.05; * Significant at
0.1&lt;/p&gt;
&lt;p&gt;(standard errors of the coefficients are reported in parenthesis)&lt;/p&gt;
&lt;p&gt;The influence of macroeconomic factors on the Crude Rate Net Migration
of European economies was quantified using panel data regression models.
A preliminary study of the dependent variable’s distribution reveals
that the net migration rate differs across Old Member States
(migrant-receiving nations) and New Member States (sending countries for
migrants). As a result, we opt to estimate two models for the two sets
of nations, following Mihi-Ramirez et al. (2017) method’s. Unemployment
rate, per capita income, Gini coefficient, poverty rate, Economic
Freedom Index, and two other characteristics relating to the health
system were included as independent variables (number of beds in
hospitals and health expenditure as percentage in GDP). The analysed
period was 2000 – 2017.&lt;/p&gt;
&lt;p&gt;The findings corroborated migratory economic theory. In terms of the
labor market, unemployment is a large and powerful supply push factor
for migration. Only for the Old Member states does income appear as a
key influence, validating the neo-classical economic theory of
migration, which claims that variations in earnings across nations are
one of the main factors driving labor movement (Massey et al, 1993).
Moving on to the social component, the Gini coefficient has been
established as a strong driving force behind migration in both Old and
New Member States. Poverty appears to be a factor with reduced
explanatory power, with a positive coefficient for the Old member states
and no significance for the New member states.&lt;/p&gt;
&lt;p&gt;In terms of Economic Freedom, the factor has a considerable beneficial
impact on net migration rates only in the New Member States.&lt;/p&gt;
&lt;p&gt;Furthermore, health-related macroeconomic factors were included in the
model, as well as the circular cumulative causation hypothesis, which
states that variations in standard of living across nations are the
primary cause of migration (Massey et al. 1993). The health system, on
the other hand, could not be proven to be a factor of migration.&lt;/p&gt;
&lt;p&gt;Because international migration has such a substantial impact on
European population dynamics, understanding and analyzing the factors
that influence international migration is critical. The findings of this
study might be utilized not just to produce migration forecasts, but
also to establish migration policies that would improve migrants’ labor
and social integration.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Smaranda Cimpoeru, PhD&lt;/p&gt;
&lt;p&gt;Department of Statistics and Econometrics, The Bucharest University of
Economic Studies&lt;/p&gt;
&lt;p&gt;E-mail:
&lt;a href=&#34;mailto:smaranda.cimpoeru@csie.ase.ro&#34; class=&#34;email&#34;&gt;smaranda.cimpoeru@csie.ase.ro&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Minimax Estimation and Identity Testing of Markov Chains</title>
      <link>https://youngstats.github.io/post/2022/09/18/minimax-estimation-and-identity-testing-of-markov-chains/</link>
      <pubDate>Sun, 18 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/09/18/minimax-estimation-and-identity-testing-of-markov-chains/</guid>
      <description>


&lt;p&gt;We briefly review the two classical problems of distribution estimation
and identity testing (in the context of property testing), then propose
to extend them to a Markovian setting. We will see that the sample
complexity depends not only on the number of states, but also on the
stationary and mixing properties of the chains.&lt;/p&gt;
&lt;div id=&#34;the-distribution-setting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The distribution setting&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Estimation/Learning.&lt;/strong&gt; A fundamental problem in statistics is to
estimate a probability distribution from independent samples. Consider
an alphabet &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}\)&lt;/span&gt; of size &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, and draw &lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots, X_n\)&lt;/span&gt;
from a distribution &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; over &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}\)&lt;/span&gt;. How large must &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; –the
sample size– be in order to obtain a good estimator
&lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}(X_1, \dots, X_n)\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;? In order to make the question
precise, we choose a notion of distance &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; between distributions,
pick two small numbers &lt;span class=&#34;math inline&#34;&gt;\(\delta, \varepsilon &amp;gt; 0\)&lt;/span&gt; and can for instance
say that an estimator is good, when with high probability &lt;span class=&#34;math inline&#34;&gt;\(1 - \delta\)&lt;/span&gt;
over the random choice of the sample, the estimator is &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;
close to the true distribution. Framed as a (probably approximately
correct) minimax problem, we define the sample complexity of the problem
to be &lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
    n_0(\varepsilon, \delta) = {\mathrm{\arg\min}} \{n \in \mathbb{N} \colon \min_{\hat{\mu}} \max_{\mu}  \mathbb{P}(\rho(\hat{\mu}(X_1, \dots, X_n), \mu) &amp;gt; \varepsilon) &amp;lt; \delta\},
\end{equation*}\]&lt;/span&gt; where the maximum is taken over all distributions over
&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; bins, and the minimum over all estimators. The problem of
determining &lt;span class=&#34;math inline&#34;&gt;\(n_0\)&lt;/span&gt; is then typically addressed by providing two distinct
answers. On one hand, we construct an estimator that would be good for
any distribution given that &lt;span class=&#34;math inline&#34;&gt;\(n &amp;gt; n_0^{UB}\)&lt;/span&gt;. Conversely, we set up a hard
problem such that no estimator can be good when &lt;span class=&#34;math inline&#34;&gt;\(n &amp;lt; n_0^{LB}\)&lt;/span&gt;. This
essentially leads to &lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
    n_0^{LB} \leq n_0 \leq n_0^{UB},
\end{equation*}\]&lt;/span&gt; and the smaller the gap between the upper and lower
bounds, the better we understand the sample complexity of the
statistical problem. With respect to the total variation distance
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
    \rho_{TV}(\mu, \mu&amp;#39;) = \frac{1}{2} \sum_{x \in \mathcal{X}} |\mu(x) - \mu&amp;#39;(x)|,
\end{equation*}\]&lt;/span&gt; it is folklore (see e.g. &lt;span class=&#34;citation&#34;&gt;Waggoner (&lt;a href=&#34;#ref-waggoner2015lp&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;) that
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
    n_0(\varepsilon, \delta) \asymp \frac{d + \log 1 /\delta}{\varepsilon^2},
\end{equation*}\]&lt;/span&gt;and that the empirical distribution achieves the minimax
rate ( &lt;span class=&#34;math inline&#34;&gt;\(f \asymp g\)&lt;/span&gt; stands for &lt;span class=&#34;math inline&#34;&gt;\(c g \leq f \leq C g\)&lt;/span&gt; for two universal
constants &lt;span class=&#34;math inline&#34;&gt;\(c,C \in \mathbb{R}\)&lt;/span&gt;). The take-away is that if we want to
estimate a distribution over &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; symbols w.r.t total variation, the
statistical hardness of the problem grows roughly linearly with the
support size.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Identity testing.&lt;/strong&gt; A different problem is to imagine that the
practitioner has access to independent data sampled from some unknown
distribution &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and the full description of a reference distribution
&lt;span class=&#34;math inline&#34;&gt;\(\bar{\mu}\)&lt;/span&gt;. They then have to make a determination as to whether the
data was sampled from &lt;span class=&#34;math inline&#34;&gt;\(\bar{\mu}\)&lt;/span&gt;, or from a distribution which is at
least &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; far from &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; (composite). To better compare with
the previous problem, we keep the total variation metric. We briefly
note that the unknown distribution being closer than &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;, but
not equal to &lt;span class=&#34;math inline&#34;&gt;\(\bar{\mu}\)&lt;/span&gt;, is not among the choices –we require
separation of the hypotheses.&lt;/p&gt;
&lt;p&gt;One can come up with a simple ``testing-by-learning” approach to solve
the problem. First estimate the unknown distribution down to precision
&lt;span class=&#34;math inline&#34;&gt;\(\varepsilon/2\)&lt;/span&gt; using the sample, and then verify whether the estimator
is close to &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}\)&lt;/span&gt;. However, identity testing corresponds to a
binary, seemingly easier question. As such, we would expect the sample
complexity to be less than that of the estimation problem. Among the
first to investigate the problem in the case where &lt;span class=&#34;math inline&#34;&gt;\(\bar{\mu}\)&lt;/span&gt; is the
uniform distribution was &lt;span class=&#34;citation&#34;&gt;Paninski (&lt;a href=&#34;#ref-paninski2008coincidence&#34; role=&#34;doc-biblioref&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt;, who showed
that the complexity of the problem grows roughly as the square root of
the alphabet size, i.e. much more economical than estimation. For this
uniformity testing problem, several procedures are known to achieve the
minimax rate (although not all work in all regimes of parameters). One
can for instance count the number of collisions in the sample, count the
number of bins appearing exactly once in the sample, or even compute a
chi-square statistic. In fact, the complexity for the worst-case
&lt;span class=&#34;math inline&#34;&gt;\(\bar{\mu}\)&lt;/span&gt; has been pinned-down to (see &lt;span class=&#34;citation&#34;&gt;Diakonikolas et al. (&lt;a href=&#34;#ref-diakonikolas2017optimal&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;)
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
    n_0(\varepsilon, \delta) \asymp \frac{\sqrt{d \log 1 /\delta} + \log 1 /\delta}{\varepsilon^2}
\end{equation*}\]&lt;/span&gt;
In recent years, the sub-field of distribution testing
has expanded beyond this basic question to investigate a vast collection
of properties of distributions –see the excellent survey of
&lt;span class=&#34;citation&#34;&gt;Canonne (&lt;a href=&#34;#ref-canonne2020survey&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; for an overview.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-markovian-setting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Markovian setting&lt;/h2&gt;
&lt;p&gt;We depart from independence and increase the richness of the process by
considering the Markovian setting. More specifically, we wish to perform
statistical inference from sampling a ``single trajectory” of a Markov
chain, i.e. a sequence of observations &lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots, X_m\)&lt;/span&gt; where
&lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; is drawn from an unknown, arbitrary initial distribution, and the
transition probabilities governing &lt;span class=&#34;math inline&#34;&gt;\(X_t \to X_{t+1}\)&lt;/span&gt; are collected in a
row-stochastic matrix &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;. It is a challenging but fair representation
of a process outside of the control of the scientist, that cannot for
example be restarted, or set to any particular state. For simplicity of
the exposition below, we will assume that &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is ergodic and time
reversible, but the approach and results can be extended to the periodic
or non-reversible cases (see the concluding remarks). To measure
distance between Markov chains, we here consider the infinity matrix
norm between their respective transition matrices, &lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
    \rho(P, P&amp;#39;) = \|P - P&amp;#39;\|_{\infty} = \max_{x \in \mathcal{X}} \sum_{x&amp;#39; \in \mathcal{X}} |P(x,x&amp;#39;) - P&amp;#39;(x,x&amp;#39;)|,
\end{equation*}\]&lt;/span&gt; and recognize that it corresponds to a uniform control
over the conditional distributions defined by each state w.r.t total
variation. The notion of sample size (we have only one sample) is
replaced with the trajectory length. Definitions of the minimax
estimation and identity testing problems follow from this sampling model
and choice of metric. Since the number of parameters in the model jumps
approximately from &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(d^2\)&lt;/span&gt;, a first guess would be to replace &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;
by &lt;span class=&#34;math inline&#34;&gt;\(d^2\)&lt;/span&gt; in the sample complexities obtained in the iid setting.
However, our ability to test or estimate the conditional distribution
defined by a state is restricted by the number of times we visit it. For
instance, if the stationary probability –the long run probability of
visit– of some states is small, or if the chain exhibits a sticky
behavior, we would not be able to make decisions unless the trajectory
length increases accordingly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estimation/Learning.&lt;/strong&gt; Finite sample analyses for the problem are
relatively recent, with the first treatments of &lt;span class=&#34;citation&#34;&gt;Hao, Orlitsky, and Pichapati (&lt;a href=&#34;#ref-hao2018learning&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;
in expectation and &lt;span class=&#34;citation&#34;&gt;Wolfer and Kontorovich (&lt;a href=&#34;#ref-pmlr-v98-wolfer19a&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; in the PAC framework.&lt;/p&gt;
&lt;p&gt;We estimate the unknown &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; with the empirical tally matrix, where
&lt;span class=&#34;math inline&#34;&gt;\(\widehat{P}(x,x&amp;#39;)\)&lt;/span&gt; is computed by counting transitions from &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(x&amp;#39;\)&lt;/span&gt;
and dividing by the number of visits to &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; (modulo some mild
smoothing). Tighter upper and lower bounds on the sample complexity were
later obtained (see &lt;span class=&#34;citation&#34;&gt;Wolfer and Kontorovich (&lt;a href=&#34;#ref-wolfer2021&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt;)
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
\begin{split}
    c \left( \frac{d}{\pi_{\star} \varepsilon^2} + \frac{d \log d}{{\gamma_\star}} \right) \leq m_0(\varepsilon, \delta) \leq C \left( \frac{d + \log 1/(\varepsilon \delta)}{\pi_{\star} \varepsilon^2} +  \frac{\log 1/(\pi_\star \delta)}{\pi_{\star} {\gamma_\star}} \right) \quad \\
\end{split}
\end{equation*}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(c, C &amp;gt; 0\)&lt;/span&gt; are universal constants,
&lt;span class=&#34;math inline&#34;&gt;\(\pi_\star = \min_{x \in \mathcal{X}} \pi(x)\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; the stationary
distribution of &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\gamma_\star\)&lt;/span&gt; is the absolute spectral gap
–the difference between the two largest eigenvalues of &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; in
magnitude– that largely controls the mixing time of the (reversible)
Markov chain. The bounds are independent of the starting state, but
require a priori knowledge of bounds on &lt;span class=&#34;math inline&#34;&gt;\(\pi_\star\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\gamma_\star\)&lt;/span&gt;
to be informative. Notably, procedures exist to efficiently estimate
&lt;span class=&#34;math inline&#34;&gt;\(\pi_\star\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\gamma_\star\)&lt;/span&gt; from the data
&lt;span class=&#34;citation&#34;&gt;Hsu et al. (&lt;a href=&#34;#ref-hsu2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Wolfer (&lt;a href=&#34;#ref-wolfer2022&#34; role=&#34;doc-biblioref&#34;&gt;2022&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We witness a stark difference between the iid and Markovian settings: a
phase transition appears in the sample complexity. Roughly speaking and
taking &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; as a constant, when the mixing time of the chain is
larger than the number of states, the second term becomes the dominant
one, i.e. the difficulty of estimation is chiefly controlled by how well
we can navigate among the states. When the chain mixes more quickly,
moving from state to state is not bottleneck, and the harder task is the
estimation of the respective conditional distributions. An additional
caveat is that the chain will generally not visit all states equally in
the long run. As a result, the sample complexity necessarily depends on
the proportion &lt;span class=&#34;math inline&#34;&gt;\(\pi_\star\)&lt;/span&gt; of the trajectory spent in the least visited
state. The astute reader may notice that when the mixing time is small
and the stationary distribution is uniform (&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is doubly stochastic),
we recover up to logarithmic factors the previously intuited complexity
of &lt;span class=&#34;math inline&#34;&gt;\(d^2/\varepsilon^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Identity testing.&lt;/strong&gt; For the Markov chain identity testing problem
(still w.r.t the matrix uniform norm), we rely on a two-stage testing
procedure. First, we verify that we visited all states about
&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{d}/\varepsilon^2\)&lt;/span&gt; times. As a second step, we perform identity
testing of all conditional distributions. The Markov property ensures
that our transition observations are drawn independently from the
conditional distribution, thus that we can rely on one of the known iid
testing procedures as a black-box. We obtain the below-listed upper and
lower bounds on the sample complexity (see &lt;span class=&#34;citation&#34;&gt;Wolfer and Kontorovich (&lt;a href=&#34;#ref-pmlr-v108-wolfer20a&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;)
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
\begin{split}
    c \left( \frac{\sqrt{d}}{\bar{\pi}_\star \varepsilon^2} + \frac{d}{\bar{\gamma}_\star} \right) \leq m_0(\varepsilon, \delta) \leq C \left( \frac{\sqrt{d}}{\bar{\pi}_\star \varepsilon^2} \log \frac{d}{\varepsilon \delta} + \frac{\log 1/(\bar{\pi}_\star \delta)}{\bar{\pi}_\star \bar{\gamma}_\star} \right) \qquad \\
\end{split}
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-09-18-minimax-estimation-and-identity-testing-of-markov-chains_files/Figure1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 1: Topology of class of Markov chains achieving the lower bounds
in &lt;span class=&#34;math inline&#34;&gt;\(d /\gamma_\star\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-09-18-minimax-estimation-and-identity-testing-of-markov-chains_files/Figure2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 2: Example of the class in Figure 1 with &lt;span class=&#34;math inline&#34;&gt;\(d = 9\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(\tau \in \{-1, 1\}^3\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; \eta \ll 1\)&lt;/span&gt;. The mixing time of the
chain is of the order of &lt;span class=&#34;math inline&#34;&gt;\(1/\eta\)&lt;/span&gt;, and is decoupled from the proximity
parameter &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Perhaps surprisingly, the bounds only depends on properties
&lt;span class=&#34;math inline&#34;&gt;\(\bar{\pi}_\star\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\bar{\gamma}_\star\)&lt;/span&gt; of the reference stochastic
matrix. As a matter of fact, the unknown chain need not even be
irreducible for the procedure to work. A quadratic speed-up also appears
in the bounds, which will affect the dominant term when the chain mixes
faster than &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{d}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;It is instructive to inspect the set of reversible Markov chains that
achieves the lower bounds in &lt;span class=&#34;math inline&#34;&gt;\(d / \gamma_\star\)&lt;/span&gt; (see Figure 1 and Figure
2). Every element consists of an “inner clique” and an “outer rim”. The
inner clique can be made arbitrarily sticky in the sense that the chain
will only move from one state to another within the clique with
underwhelming probability &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt;, which has an overall slowing effect on
the mixing time of the chain. On the other hand, being on one state of
the clique, the chain has at least constant –but parametrizable–
probability of reaching one of the two connected states in the outer
rim. In order to distinguish between two chains where one of the inner
nodes has &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;-different transition probabilities towards the
rim than the other, the trajectory will necessarily have to go through a
large fraction of the states.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related work and closing comments.&lt;/strong&gt; As mentioned earlier in this
note, the results are valid for a substantially larger class of chains.
In the non-reversible setting, the absolute spectral gap can readily be
replaced with the pseudo-spectral gap &lt;span class=&#34;citation&#34;&gt;Paulin (&lt;a href=&#34;#ref-paulin2015concentration&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;,
and if the chain is irreducible (but possibly periodic), one can instead
perform inference on the lazy process governed by &lt;span class=&#34;math inline&#34;&gt;\((P+I)/2\)&lt;/span&gt;, which can
be simulated with an additional fair coin. More recently,
&lt;span class=&#34;citation&#34;&gt;Chan, Ding, and Li (&lt;a href=&#34;#ref-chan2021learning&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; investigated the Markov chain minimax learning
and testing problems by considering cover times instead of mixing times.&lt;/p&gt;
&lt;p&gt;We end this post by stressing that the minimax rates pertain to the
choice of metric. We refer the interested reader to
&lt;span class=&#34;citation&#34;&gt;Daskalakis, Dikkala, and Gravin (&lt;a href=&#34;#ref-daskalakis2018testing&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Cherapanamjeri and Bartlett (&lt;a href=&#34;#ref-cherapanamjeri2019testing&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Fried and Wolfer (&lt;a href=&#34;#ref-pmlr-v151-fried22a&#34; role=&#34;doc-biblioref&#34;&gt;2022&lt;/a&gt;)&lt;/span&gt;,
who analyze the identity testing problem with respect to a different
notion of discrepancy between Markov chains. Although the last-mentioned
framework can so far only handle reversible chains, it advantageously
leads to minimax rates that are independent of mixing or hitting.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-canonne2020survey&#34; class=&#34;csl-entry&#34;&gt;
Canonne, Clément L. 2020. &lt;span&gt;“A Survey on Distribution Testing: Your Data Is Big. But Is It Blue?”&lt;/span&gt; &lt;em&gt;Theory of Computing&lt;/em&gt;, 1–100.
&lt;/div&gt;
&lt;div id=&#34;ref-chan2021learning&#34; class=&#34;csl-entry&#34;&gt;
Chan, Siu On, Qinghua Ding, and Sing Hei Li. 2021. &lt;span&gt;“Learning and Testing Irreducible &lt;span&gt;M&lt;/span&gt;arkov Chains via the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-Cover Time.”&lt;/span&gt; In &lt;em&gt;Algorithmic Learning Theory&lt;/em&gt;, 458–80. PMLR.
&lt;/div&gt;
&lt;div id=&#34;ref-cherapanamjeri2019testing&#34; class=&#34;csl-entry&#34;&gt;
Cherapanamjeri, Yeshwanth, and Peter L Bartlett. 2019. &lt;span&gt;“Testing Symmetric &lt;span&gt;M&lt;/span&gt;arkov Chains Without Hitting.”&lt;/span&gt; In &lt;em&gt;Conference on Learning Theory&lt;/em&gt;, 758–85. PMLR.
&lt;/div&gt;
&lt;div id=&#34;ref-daskalakis2018testing&#34; class=&#34;csl-entry&#34;&gt;
Daskalakis, Constantinos, Nishanth Dikkala, and Nick Gravin. 2018. &lt;span&gt;“Testing Symmetric &lt;span&gt;M&lt;/span&gt;arkov Chains from a Single Trajectory.”&lt;/span&gt; In &lt;em&gt;Conference on Learning Theory&lt;/em&gt;, 385–409. PMLR.
&lt;/div&gt;
&lt;div id=&#34;ref-diakonikolas2017optimal&#34; class=&#34;csl-entry&#34;&gt;
Diakonikolas, Ilias, Themis Gouleakis, John Peebles, and Eric Price. 2017. &lt;span&gt;“Optimal Identity Testing with High Probability.”&lt;/span&gt; &lt;em&gt;arXiv Preprint arXiv:1708.02728&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-pmlr-v151-fried22a&#34; class=&#34;csl-entry&#34;&gt;
Fried, Sela, and Geoffrey Wolfer. 2022. &lt;span&gt;“Identity Testing of Reversible &lt;span&gt;M&lt;/span&gt;arkov Chains.”&lt;/span&gt; In &lt;em&gt;Proceedings of the 25th International Conference on Artificial Intelligence and Statistics&lt;/em&gt;, 151:798–817. Proceedings of Machine Learning Research. PMLR.
&lt;/div&gt;
&lt;div id=&#34;ref-hao2018learning&#34; class=&#34;csl-entry&#34;&gt;
Hao, Yi, Alon Orlitsky, and Venkatadheeraj Pichapati. 2018. &lt;span&gt;“On Learning &lt;span&gt;M&lt;/span&gt;arkov Chains.”&lt;/span&gt; &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 31.
&lt;/div&gt;
&lt;div id=&#34;ref-hsu2019&#34; class=&#34;csl-entry&#34;&gt;
Hsu, Daniel, Aryeh Kontorovich, David A. Levin, Yuval Peres, Csaba Szepesvári, and Geoffrey Wolfer. 2019. &lt;span&gt;“Mixing Time Estimation in Reversible &lt;span&gt;M&lt;/span&gt;arkov Chains from a Single Sample Path.”&lt;/span&gt; &lt;em&gt;Ann. Appl. Probab.&lt;/em&gt; 29 (4): 2439–80. &lt;a href=&#34;https://doi.org/10.1214/18-AAP1457&#34;&gt;https://doi.org/10.1214/18-AAP1457&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-paninski2008coincidence&#34; class=&#34;csl-entry&#34;&gt;
Paninski, Liam. 2008. &lt;span&gt;“A Coincidence-Based Test for Uniformity Given Very Sparsely Sampled Discrete Data.”&lt;/span&gt; &lt;em&gt;IEEE Transactions on Information Theory&lt;/em&gt; 54 (10): 4750–55.
&lt;/div&gt;
&lt;div id=&#34;ref-paulin2015concentration&#34; class=&#34;csl-entry&#34;&gt;
Paulin, Daniel. 2015. &lt;span&gt;“Concentration Inequalities for &lt;span&gt;M&lt;/span&gt;arkov Chains by &lt;span&gt;M&lt;/span&gt;arton Couplings and Spectral Methods.”&lt;/span&gt; &lt;em&gt;Electronic Journal of Probability&lt;/em&gt; 20: 1–32.
&lt;/div&gt;
&lt;div id=&#34;ref-waggoner2015lp&#34; class=&#34;csl-entry&#34;&gt;
Waggoner, Bo. 2015. &lt;span&gt;“&lt;span class=&#34;math inline&#34;&gt;\(L_p\)&lt;/span&gt; Testing and Learning of Discrete Distributions.”&lt;/span&gt; In &lt;em&gt;Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science&lt;/em&gt;, 347–56.
&lt;/div&gt;
&lt;div id=&#34;ref-wolfer2022&#34; class=&#34;csl-entry&#34;&gt;
Wolfer, Geoffrey. 2022. &lt;span&gt;“Empirical and Instance-Dependent Estimation of &lt;span&gt;M&lt;/span&gt;arkov Chain and Mixing Time.”&lt;/span&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-pmlr-v98-wolfer19a&#34; class=&#34;csl-entry&#34;&gt;
Wolfer, Geoffrey, and Aryeh Kontorovich. 2019. &lt;span&gt;“Minimax Learning of Ergodic &lt;span&gt;M&lt;/span&gt;arkov Chains.”&lt;/span&gt; In &lt;em&gt;Proceedings of the 30th International Conference on Algorithmic Learning Theory&lt;/em&gt;, 98:904–30. Proceedings of Machine Learning Research. PMLR.
&lt;/div&gt;
&lt;div id=&#34;ref-pmlr-v108-wolfer20a&#34; class=&#34;csl-entry&#34;&gt;
———. 2020. &lt;span&gt;“Minimax Testing of Identity to a Reference Ergodic &lt;span&gt;M&lt;/span&gt;arkov Chain.”&lt;/span&gt; In &lt;em&gt;Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics&lt;/em&gt;, 108:191–201. Proceedings of Machine Learning Research. PMLR.
&lt;/div&gt;
&lt;div id=&#34;ref-wolfer2021&#34; class=&#34;csl-entry&#34;&gt;
———. 2021. &lt;span&gt;“Statistical Estimation of Ergodic &lt;span&gt;M&lt;/span&gt;arkov Chain Kernel over Discrete State Space.”&lt;/span&gt; &lt;em&gt;Bernoulli&lt;/em&gt; 27 (1): 532–53. &lt;a href=&#34;https://doi.org/10.3150/20-BEJ1248&#34;&gt;https://doi.org/10.3150/20-BEJ1248&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Regularization by Noise for Stochastic Differential and Stochastic Partial Differential Equations</title>
      <link>https://youngstats.github.io/post/2022/06/03/regularization-by-noise-for-stochastic-differential-and-stochastic-partial-differential-equations/</link>
      <pubDate>Fri, 03 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/06/03/regularization-by-noise-for-stochastic-differential-and-stochastic-partial-differential-equations/</guid>
      <description>


&lt;p&gt;&lt;strong&gt;Regularization by Noise for Stochastic Differential and Stochastic Partial Differential Equations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-06-03-regularization-by-noise-for-stochastic-differential-and-stochastic-partial-differential-equations_files/cover_image_reg_by_noise_spde.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The regularizing effects of noisy perturbations of differential equations is a central subject of stochastic analysis. Recent breakthroughs initiated a new wave of interest, particularly concerning non-Markovian, infinite dimensional, and rough-stochastic / Young-stochastic hybrid systems.&lt;/p&gt;
&lt;p&gt;On the webinar, selected younger scholars will present their recent works on the topic.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wednesday, September 21st, 7:00 PT / 10:00 EST / 16:00 CET.&lt;/li&gt;
&lt;li&gt;Online, via Zoom. The registration form is available &lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSdA_UmGknYO3Oo_yqVM80SqgnIZUgEyvINzdOMTI0BzhCMdPw/viewform?vc=0&amp;amp;c=0&amp;amp;w=1&amp;amp;flr=0&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.mi.fu-berlin.de/math/groups/stoch/members/Doktoranden/kremp.html&#34;&gt;Helena Kremp&lt;/a&gt;, Freie Universität Berlin, Germany: &lt;em&gt;Weak rough-path-type solutions for singular Lévy SDEs&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Abstract: Since the works by Delarue, Diel and Cannizzaro, Chouk (in the Brownian noise setting), and our previous work, the existence and uniqueness of solutions to the martingale problem associated to multidimensional SDEs with additive &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;-stable Lévy noise for &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; in (1, 2] and rough Besov drift of regularity &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; &amp;gt; (2-2&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;)/3 is known. Motivated by the equivalence of probabilistic weak solutions to SDEs with bounded, measurable drift and solutions to the martingale problem, we define a (non-canonical) weak solution concept for singular Lévy diffusions, proving moreover equivalence to the martingale solution in both the Young (&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; &amp;gt; (1-&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;)/2), as well as in the rough regime (&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; &amp;gt; (2-2&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;))/3). This turns out to be highly non-trivial in the rough case and forces us to define certain rough stochastic sewing integrals involved. In particular, we show that the canonical weak solution concept (introduced also by Athreya, Butkovsky, Mytnik), which is well-posed in the Young case, yields non-uniqueness of solutions in the rough case. If time permits, we apply our theory to the Brox diffusion with Lévy noise and prove a homogenization result for singular diffusions in the periodic Besov drift setting. The talk is based on joint work with Nicolas Perkowski.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.iam.uni-bonn.de/users/galeati/home&#34;&gt;Lucio Galeati&lt;/a&gt;, University of Bonn, Germany: &lt;em&gt;Advances on singular SDEs with fractional noise in subcritical regimes&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Abstract: In recent years, there has been a lot of interest in regulariaation by noise for SDEs driven by fractional Brownian motion of parameter &lt;span class=&#34;math inline&#34;&gt;\(H\in (0,1)\)&lt;/span&gt;, with first results going back to Nualart, Ouknine (2002) and Catellier, Gubinelli (2016). The main challenges when dealing with these equations are the presence of a non-Lipschitz (possibly singular) drift and the lack of any martingale or Markovian structure; in particular, classical stochastic analysis and PDE tools break down for &lt;span class=&#34;math inline&#34;&gt;\(H\neq 1/2\)&lt;/span&gt; and new techniques must be developed. Here I will present some advances on the topic, including generalizations to the regime &lt;span class=&#34;math inline&#34;&gt;\(H\in (1,\infty)\)&lt;/span&gt;, construction of the associated stochastic flow of solutions and stability estimates for SDEs driven by different drifts. The key tools in our analysis come from rough path theory and are given by Young integration and suitable versions of the stochastic sewing lemma. Based on a joint work (arXiv:2207.03475) with M. Gerencser (TU Wien).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.mics.centralesupelec.fr/en/teammics&#34;&gt;Lukas Anzeletti&lt;/a&gt;, Université Paris-Saclay, France: &lt;em&gt;Regularisation by noise for SDEs with (fractional) Brownian noise and a comparison of different notions of solutions&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Abstract: We study existence and uniqueness of solutions to the equation &lt;span class=&#34;math inline&#34;&gt;\(X_t=b(X_t)dt + dB_t\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; may be distributional and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; is a fractional Brownian motion with Hurst parameter &lt;span class=&#34;math inline&#34;&gt;\(H\leq 1/2\)&lt;/span&gt;. We follow two approaches, namely using the stochastic sewing lemma and nonlinear Young integrals in &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-variation. Furthermore, in the Brownian case &lt;span class=&#34;math inline&#34;&gt;\(H=1/2\)&lt;/span&gt; we present examples of drifts in which the classical notion of a solution and solutions in a path-by-path sense coincide, respectively not coincide. Partly based on joint work with Alexandre Richard and Etienne Tanré.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://math.univ-lyon1.fr/~honore/index.html&#34;&gt;Igor Honoré&lt;/a&gt;, Université Claude Bernard Lyon 1, France: &lt;em&gt;Selection by vanishing viscosity for the transport equation&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Abstract: We consider a transport equation whose the coefficients can be in a negative Besov space, we provide controls in Hölder spaces of some kind of vanishing viscosity solutions from Kolmogorov equation associated with a stochastic process. We perform a new analysis based on a proxy around the flow associated with regularize coefficients, and thanks to a cut locus technique we obtain a Hölder control which does not depend on the regularity of the coefficients. The regularity only matters for the considered type of solution. As a consequence, we can give a meaning of some product of distributions. Finally, by a fixed point theorem, we can match the coefficient with the solution itself. This is exactly the inviscid Burgers’ equation whose the selection principle allows to avoid the well-known time singularity.&lt;/p&gt;
&lt;p&gt;Discussant: &lt;a href=&#34;https://www.wias-berlin.de/people/butkovsky/&#34;&gt;Oleg Butkovsky&lt;/a&gt;, Weierstrass Institute for Applied Analysis and Stochastics, Germany&lt;/p&gt;
&lt;p&gt;The webinar is part of YoungStatS project of the Young Statisticians Europe initiative (FENStatS) supported by the Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics (IMS).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Theory and Methods for Inference in Multi-armed Bandit Problems</title>
      <link>https://youngstats.github.io/post/2022/04/19/theory-and-methods-for-inference-in-multi-armed-bandit-problems/</link>
      <pubDate>Tue, 19 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/04/19/theory-and-methods-for-inference-in-multi-armed-bandit-problems/</guid>
      <description>


&lt;p&gt;&lt;strong&gt;Theory and Methods for Inference in Multi-armed Bandit Problems&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-04-19-theory-and-methods-for-inference-in-multi-armed-bandit-problems_files/Slika1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Multi-armed bandit (MAB) algorithms have been argued for decades as useful to conduct adaptively-randomized experiments. By skewing the allocation of the arms towards the more efficient or informative ones, they have the potential to enhance participants’ welfare, while resulting in a more flexible, efficient, and ethical alternative compared to traditional fixed studies. However, such allocation strategies complicate the problem of statistical inference. It is now recognized that traditional inference methods are typically not valid when used in MAB-collected data, leading to considerable biases in classical estimators and other relevant issues in hypothesis testing problems.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wednesday, May 11th, 7:00 PT / 10:00 EST / 16:00 CET.&lt;/li&gt;
&lt;li&gt;Online, via Zoom. The registration form is available &lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSexdudNXI46npEnpQZ0IwmcUNejJ8wSeZBjw1lNU4CftaFwUA/viewform?vc=0&amp;amp;c=0&amp;amp;w=1&amp;amp;flr=0&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.google.com/view/anandkalvit&#34;&gt;Anand Kalvit&lt;/a&gt;, Columbia University: “A Closer Look at the Worst-case Behavior of Multi-armed Bandit Algorithms”&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.stat.cmu.edu/~aramdas/&#34;&gt;Aaditya Ramdas&lt;/a&gt;, Carnegie Mellon University: “Safe, Anytime-Valid Inference in the face of 3 sources of bias in bandit data analysis”&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ruohanzhan.github.io/&#34;&gt;Ruohan Zhan&lt;/a&gt;, Stanford University: “Inference on Adaptively Collected Data”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prof. &lt;a href=&#34;https://www0.gsb.columbia.edu/faculty/azeevi/&#34;&gt;Assaf Zeevi&lt;/a&gt;, Columbia University&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The webinar is part of YoungStatS project of the Young Statisticians Europe initiative (FENStatS) supported by the Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics (IMS).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you missed this webinar, you can &lt;a href=&#34;https://www.youtube.com/watch?v=syoPMcLt3g4&#34;&gt;watch the recording on our YouTube channel&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Selection of Priors in Bayesian Structural Equation Modeling</title>
      <link>https://youngstats.github.io/post/2022/02/14/selection-of-priors-in-bayesian-structural-equation-modeling/</link>
      <pubDate>Mon, 14 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/02/14/selection-of-priors-in-bayesian-structural-equation-modeling/</guid>
      <description>


&lt;p&gt;&lt;strong&gt;Selection of Priors in Bayesian Structural Equation Modelling&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-02-14-selection-of-priors-in-bayesian-structural-equation-modeling_files/Cover2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Structural equation modeling (SEM) is an important framework within the social sciences that encompasses a wide variety of statistical models. Traditionally, estimation of SEMs has relied on maximum likelihood. Unfortunately, there also exist a variety of situations in which maximum likelihood performs subpar. This led researchers to turn to alternative estimation methods, in particular, Bayesian estimation of SEMs or BSEM. However, it is currently unclear how to specify the prior distribution in order to attain the advantages of Bayesian approaches.&lt;/p&gt;
&lt;p&gt;On the webinar, selected statisticians will present their recent works on the topic.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wednesday, April 20th, 7:00 PT / 10:00 ET / 16:00 Berlin/Amsterdam&lt;/li&gt;
&lt;li&gt;Online, via Zoom. The registration form is available
&lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSe-BAMdH2dhuON-kxKpVW3OgtnO2Qd6MWKlyUoaF_udbp5X2Q/viewform?vc=0&amp;amp;c=0&amp;amp;w=1&amp;amp;flr=0&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.mcgill.ca/psychology/milica-miocevic&#34;&gt;Milica Miočević&lt;/a&gt;, McGill University, Canada&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://winterstat.github.io/&#34;&gt;Sonja D. Winter&lt;/a&gt;, University of Missouri, USA&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://research.vu.nl/en/persons/mauricio-garnier-villarreal&#34;&gt;Mauricio Garnier-Villarreal&lt;/a&gt;, Vrije Universiteit Amsterdam, The Netherlands&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://saravanerp.com/&#34;&gt;Sara van Erp&lt;/a&gt;, Utrecht University, The Netherlands&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The webinar is part of YoungStatS project of the Young Statisticians Europe initiative (FENStatS) supported by the Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics (IMS).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you missed this webinar, you can &lt;a href=&#34;https://www.youtube.com/watch?v=Cuwhzd-8z4k&#34;&gt;watch the recording on our YouTube channel&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Recent Advances in Approximate Bayesian Inference</title>
      <link>https://youngstats.github.io/post/2022/02/08/recent-advances-in-approximate-bayesian-inference/</link>
      <pubDate>Tue, 08 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/02/08/recent-advances-in-approximate-bayesian-inference/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;Recent Advances in Approximate Bayesian Inference&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-02-09-recent-advances-in-approximate-bayesian-inference_files/featured.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In approximate Bayesian computation, likelihood function is intractable and needs to be itself estimated using forward simulations of the statistical model (Beaumont et al., 2002; Marin et al., 2012; Sisson et al., 2019; Martin et al., 2020). Recent years have seen numerous advances in approximate inference methods, which have enabled Bayesian inference in increasingly challenging scenarios involving complex probabilistic models and large datasets.&lt;/p&gt;
&lt;p&gt;On the webinar, selected young statisticians will present their recent works on the topic.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wednesday, June 15th, 7:00 PT / 10:00 EST / 16:00 CET&lt;/li&gt;
&lt;li&gt;Online, via Zoom. The registration form is available &lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSdd1MztoBmjD8BXFzyURnSPoGolVnojTCoaZMgbQ4ooN5Fybw/viewform&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.lorenzopacchiardi.me/&#34;&gt;Lorenzo Pacchiardi&lt;/a&gt;, University of Oxford, United Kingdom, &lt;embed&gt; link to &lt;a href=&#34;https://drive.google.com/file/d/1h3kbElZP1XuWNi0zYdFXg5Sve_KqVc5N/view&#34;&gt;Presentation&lt;/a&gt;. &lt;/embed&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.stats.ox.ac.uk/~pompe/&#34;&gt;Emilia Pompe&lt;/a&gt;, University of Oxford, United Kingdom, &lt;embed&gt; link to &lt;a href=&#34;https://drive.google.com/file/d/1FtzRCxPScHyuX6541aFO44d6OOKN7Ibg/view&#34;&gt;Presentation&lt;/a&gt;. &lt;/embed&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.mimuw.edu.pl/~lr306321/&#34;&gt;Łukasz Rajkowski&lt;/a&gt;, University of Warsaw, Poland, &lt;embed&gt; link to &lt;a href=&#34;https://drive.google.com/file/d/1-jYFEphT9QP9hpmXUKHPaXhmjgQ-67_S/view&#34;&gt;Presentation&lt;/a&gt;. &lt;/embed&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://theomoins.github.io/aboutme/&#34;&gt;Théo Moins&lt;/a&gt;, Inria Grenoble Rhône-Alpes, France, &lt;embed&gt; link to &lt;a href=&#34;https://drive.google.com/file/d/1d-eT9CjudhQkanRBXJZOI6lxPZb5dFfX/view&#34;&gt;Presentation&lt;/a&gt;. &lt;/embed&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.julyanarbel.com/&#34;&gt;Julyan Arbel&lt;/a&gt;, Inria Grenoble Rhône-Alpes, France, &lt;embed&gt; link to &lt;a href=&#34;https://drive.google.com/file/d/17ZxJykILqJAjNICSWcG-TDtjTzyEj5jn/view&#34;&gt;Presentation&lt;/a&gt;. &lt;/embed&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The webinar is part of YoungStatS project of the Young Statisticians Europe initiative (FENStatS) supported by the Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics (IMS).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you missed this webinar, you can &lt;a href=&#34;https://www.youtube.com/watch?v=Ee-qmAIwrxs&#34;&gt;watch the recording on our YouTube channel&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Recent Advancements in Applied Instrumental Variable Methods</title>
      <link>https://youngstats.github.io/post/2022/02/07/recent-advancements-in-applied-instrumental-variable-methods/</link>
      <pubDate>Mon, 07 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/02/07/recent-advancements-in-applied-instrumental-variable-methods/</guid>
      <description>


&lt;p&gt;&lt;strong&gt;Recent Advancements in Applied Instrumental Variable Methods&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-02-09-recent-advancements-in-applied-instrumental-variable-methods_files/featured.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Instrumental variables (IV) is one of most important and widespread research designs in economics and statistics, as it can identify causal effects in the presence of unobserved confounding. Over the past 30 years the science of IV has advanced considerably, in part through the contributions of Nobel Laureates Joshua Angrist, Guido Imbens, and James Heckman. Recent years have brought significant advances in how IV is applied, in shift-share designs, with judge or examiner instruments, and in settings with rich or complex controls.&lt;/p&gt;
&lt;p&gt;In this webinar, selected econometricians will present their recent work on applied IV methods.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wednesday, June 29th, 8:00 PT / 11:00 EST / 17:00 CET&lt;/li&gt;
&lt;li&gt;Online, via Zoom. The registration form is available
&lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSe3w6IjY-oHDeRGT5HqE5_RgcZ0YGJNpTq5KDeyAdY77GqmKQ/viewform&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.google.com/view/borusyak/home&#34;&gt;Kirill Borusyak&lt;/a&gt;, University College London, United Kingdom: on shift-share IV and other composite instruments&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://som.yale.edu/faculty/paul-goldsmith-pinkham&#34;&gt;Paul Goldsmith-Pinkham&lt;/a&gt;, Yale University, USA: on judge/examiner instruments&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.publichealth.columbia.edu/people/our-faculty/sc4501&#34;&gt;Stephen C. Coussens&lt;/a&gt;, Columbia University, USA: on efficient estimation from compliance weighting&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://economics.uchicago.edu/directory/alexander-torgovitsky&#34;&gt;Alexander Torgovitsky&lt;/a&gt;, University of Chicago, USA: on when two-stage least squares estimates local average treatment effects, &lt;embed&gt; link to &lt;a href=&#34;https://drive.google.com/file/d/1_MRCSLy07GY-D9vgG54mCorNAa6HjVGr/view&#34;&gt;Presentation&lt;/a&gt;. &lt;/embed&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://about.peterhull.net/&#34;&gt;Peter Hull&lt;/a&gt;, Brown University, USA&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The webinar is part of YoungStatS project of the Young Statisticians
Europe initiative (FENStatS) supported by the Bernoulli Society for
Mathematical Statistics and Probability and the Institute of
Mathematical Statistics (IMS).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you missed this webinar, you can &lt;a href=&#34;https://www.youtube.com/watch?v=7D-JKtHgSQ8&#34;&gt;watch the recording on our YouTube channel&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Measuring dependence in the Wasserstein distance for Bayesian nonparametric models</title>
      <link>https://youngstats.github.io/post/2022/01/17/measuring-dependence-in-the-wasserstein-distance-for-bayesian-nonparametric-models/</link>
      <pubDate>Mon, 17 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/01/17/measuring-dependence-in-the-wasserstein-distance-for-bayesian-nonparametric-models/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;Bayesian nonparametric (BNP) models are a prominent tool for performing flexible inference with a natural quantification of uncertainty. Traditionallly, flexible inference within a &lt;strong&gt;homogeneous&lt;/strong&gt; sample is performed with &lt;strong&gt;exchangeable&lt;/strong&gt; models of the type &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots, X_n|\tilde \mu \sim T(\tilde \mu)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\tilde \mu\)&lt;/span&gt; is a random measure and &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; is a suitable transformation. Notable examples for &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; include normalization for random probabilities (Regazzini et al., 2003), kernel mixtures for densities (Lo, 1984) and for hazards (Dykstra and Laud, 1981; James, 2005), exponential transformations for survival functions (Doksum, 1974) and cumulative transformations for cumulative hazards (Hjort, 1990).&lt;/p&gt;
&lt;p&gt;Very often, though, the data presents some structural &lt;strong&gt;heterogeneity&lt;/strong&gt; one should carefully take into account, especially when analyzing data from different sources that are related in some way. For instance this happens in the study of clinical trials of a COVID-19 vaccine in different countries or when understanding the effects of a certain policy adopted by multiple regions. In these cases, besides modeling heterogeneity, one further aims at introducing some probabilistic mechanism that allows for &lt;strong&gt;borrowing information&lt;/strong&gt; across different studies. To achieve this goal, in the last 20 years a wealth of BNP models that rely on &lt;strong&gt;partial exchangeability&lt;/strong&gt; have been proposed (see Cifarelli &amp;amp; Regazzini (1978) for prioneering ideas, MacEachern (1999, 2000) for seminal results and Quintana et al. (2021) for a recent review). The general recipe consists in assigning a different random measure &lt;span class=&#34;math inline&#34;&gt;\(\tilde \mu_i\)&lt;/span&gt; to every data source or group of observations and then introduce flexible forms of dependence between the random measures. For simplicity we stick to the case of two groups, so that the partially exchangeable models are of the general form
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\tag{1}
X_1,\dots,X_{n_1}| (\tilde \mu_1, \tilde \mu_2) \stackrel{\text{iid}}{\sim} T(\tilde \mu_1); \qquad \qquad
Y_1, \dots, Y_{n_2} | (\tilde \mu_1, \tilde \mu_2) \stackrel{\text{iid}}{\sim} T(\tilde \mu_2);
\end{equation}\]&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\((\tilde \mu_1, \tilde \mu_2) \sim Q\)&lt;/span&gt; is a vector of dependent random measures. The borrowing of information across different groups is regulated by the amount of dependence between random measures. One can picture &lt;strong&gt;two extreme situations&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When the random measures are completely dependent, that is, &lt;span class=&#34;math inline&#34;&gt;\(\tilde \mu_1 = \tilde \mu_2\)&lt;/span&gt; almost surely, the observations are &lt;strong&gt;fully exchangeable&lt;/strong&gt; and there is no distinction between the different groups. In such case we denote the random measures as &lt;span class=&#34;math inline&#34;&gt;\((\tilde \mu_1^\text{ex}, \tilde \mu_2^\text{ex})\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;When the random measures are independent, the two groups of (exchangeable) observations are &lt;strong&gt;independent&lt;/strong&gt; and thus do not interact.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thus, it is crucial to translate one’s prior beliefs on the dependence structure into the specification of the model. Interestingly, though there have been many proposals on how to model dependence, few results on how to measure it are available. The current state-of-the-art is given by the pairwise linear correlation between &lt;span class=&#34;math inline&#34;&gt;\(T(\tilde \mu_1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(T(\tilde \mu_2)\)&lt;/span&gt;, which provides a useful proxy based on the first two moments of the random measures, but it does not take into account their whole infinite-dimensional structure. We thus look for a measure of dependence with the following properties:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;It is &lt;strong&gt;model non-specific&lt;/strong&gt; (i.e., it does not depend on the choice of transformation &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;);&lt;/li&gt;
&lt;li&gt;It is based on the &lt;strong&gt;whole infinite-dimensional distribution&lt;/strong&gt; of BNP priors;&lt;/li&gt;
&lt;li&gt;It can be naturally extended to &lt;strong&gt;more than 2 groups&lt;/strong&gt; of observations;&lt;/li&gt;
&lt;li&gt;It can be &lt;strong&gt;strictly bounded&lt;/strong&gt; in terms of the &lt;strong&gt;hyperparameters&lt;/strong&gt; of BNP models.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We point out that this last property is fundamental because it allows to fix the hyperparameters of BNP models so to achieve the desired level of dependence.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;proposal&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Proposal&lt;/h2&gt;
&lt;p&gt;Our proposal is based on two main ideas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In partially exchangeable models (1) all the dependence between different groups of observations is introduced at the level of the random measures. Thus, we can measure the dependence directly &lt;strong&gt;in terms of the underlying vector of random measures&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\((\tilde \mu_1, \tilde \mu_2)\)&lt;/span&gt;. This ensures Property 1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Complete dependence (&lt;span class=&#34;math inline&#34;&gt;\(\tilde \mu_1^{\text{ex}} = \mu_2^{\text{ex}}\)&lt;/span&gt; almost surely) coincides with the full exchangeability of the observations, which can thus be pictured as a degenerate distribution in the space of joint distributions &lt;span class=&#34;math inline&#34;&gt;\(\{ \mathcal{L}(\tilde \mu_1, \tilde \mu_2) | \mathcal{L}(\tilde \mu_1) = \mathcal{L}(\tilde \mu_2) \}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L}\)&lt;/span&gt; denotes the probability law of a random object (Figure 1). A natural way to measure dependence of any other joint distribution &lt;span class=&#34;math inline&#34;&gt;\((\tilde \mu_1, \tilde \mu_2)\)&lt;/span&gt; is then to measure the distance from the extreme case &lt;span class=&#34;math inline&#34;&gt;\((\tilde \mu_1^{\text{ex}}, \tilde \mu_2^{\text{ex}})\)&lt;/span&gt;. This ensures Property 2 and Property 3. Informally, we refer to the &lt;strong&gt;distance from exchangeability&lt;/strong&gt;, with the underlying idea that the observations in (1) are partially exchangeable and, under complete dependence of the random measures, one retrieves full exchangeability of the observations.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-01-17-measuring-dependence-in-the-wasserstein-distance-for-bayesian-nonparametric-models_files/1.png&#34; /&gt;&lt;/p&gt;
&lt;center&gt;
&lt;em&gt;Figure 1.&lt;/em&gt;
&lt;/center&gt;
&lt;p&gt;The choice of the distance on the laws of vectors of random measures is very delicate. On one hand we look for an intuitive notion of distance with a natural geometrical interpretation, on the other hand we also want to show Property 4, and these are clearly two competing aspects. We define
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\tag{2}
d_{\mathcal{W}}\bigg( \mathcal{L}\begin{pmatrix} \tilde \mu_1 \\ \tilde \mu_2 \end{pmatrix}, \mathcal{L} \begin{pmatrix} \tilde \mu_1^{\text{ex}} \\ \tilde \mu_2^{\text{ex}} \end{pmatrix} \bigg) = \sup_A W\bigg( \mathcal{L}\begin{pmatrix} \tilde \mu_1(A) \\ \tilde \mu_2(A) \end{pmatrix},  \mathcal{L} \begin{pmatrix} \tilde \mu_1^{\text{ex}}(A) \\ \tilde \mu_2^{\text{ex}} (A)\end{pmatrix} \bigg),
\end{equation}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; is the &lt;strong&gt;Wasserstein distance&lt;/strong&gt; (of order 2) between probability distributions in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^2\)&lt;/span&gt;. We briefly recall that if &lt;span class=&#34;math inline&#34;&gt;\(C(P,Q)\)&lt;/span&gt; = &lt;span class=&#34;math inline&#34;&gt;\(\{(X,Y): \mathcal{L}(X) = P; \mathcal{L}(Y) = Q\}\)&lt;/span&gt; denotes the set of &lt;em&gt;couplings&lt;/em&gt; between two probabilities &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;, then
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\tag{3}
W(P,Q)^2 = \inf_{(X,Y) \in C(P,Q)} \mathbb{E} \|X-Y\|^2.
\end{equation}\]&lt;/span&gt;
Because of its strong geometric properties, the Wasserstein distance is ideal to compare distributions with different support as in our case (see Figure 1). Indeed, other common distances or divergences would not provide an informative notion of discrepancy in this context (e.g. total variation distance, Hellinger distance, KL-divergence). As it is often the case, there is a trade-off between the naturalness of a distance and our ability to find closed forms expressions. In particular, we have to deal with the following challenges:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In general one can prove that the infimum in (3) is a minimum, that is, that there exists an &lt;em&gt;optimal coupling&lt;/em&gt;. Nonetheless, since we are dealing with probabilities in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^d\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(d&amp;gt;1\)&lt;/span&gt;, a general expression for the &lt;strong&gt;optimal coupling is not available&lt;/strong&gt; with very few exceptions (e.g., between multivariate normal distributions).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Even if one finds the optimal coupling, the expression of the Wasserstein distance amounts to evaluating a (difficult) &lt;strong&gt;multivariate integral&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Many noteworthy models in the BNP literature are based on dependent random measures with jointly independent vectors which, in analogy with the 1-dimensional case, we call &lt;em&gt;completely random vectors&lt;/em&gt;. The law of such vectors is specified in an indirect way through a &lt;strong&gt;multivariate Lévy measure&lt;/strong&gt;. Thus, we need to develop strict bounds in terms of the Lévy measures to find closed form expressions that depend on the hyperparameters of the models (Property 4).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;In our work (Catalano et al., 2021) we face the challenges above and obtain the following results:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;When dealing with completely random vectors, &lt;span class=&#34;math inline&#34;&gt;\((\tilde \mu_1(A), \tilde \mu_2(A))\)&lt;/span&gt; can always be approximated with compound Poisson (CP) distributions. We find upper bounds of the Wasserstein distance between completely random vectors in terms of the Wasserstein distance between the jumps of the corresponding CP approximations, whose law is expressed &lt;strong&gt;directly in terms of the Lévy measures&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We find a &lt;strong&gt;general expression for the optimal coupling&lt;/strong&gt; between the jumps of the CP approximation of &lt;span class=&#34;math inline&#34;&gt;\((\tilde \mu_1, \tilde \mu_2)\)&lt;/span&gt; and the ones of the CP approximation of &lt;span class=&#34;math inline&#34;&gt;\((\tilde \mu_1^\text{ex}, \tilde \mu_2^{\text{ex}})\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Once we have found the optimal coupling, the Wasserstein distance between the jumps of the CP approximations amounts to the evaluation of a multivariate integral. We develop &lt;strong&gt;case-by-case&lt;/strong&gt; analyses to evaluate these &lt;strong&gt;multivariate integrals&lt;/strong&gt; numerically.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Putting these three elements together, we are able to derive strict upper bounds of the distance in (2) for &lt;strong&gt;noteworthy models in the BNP literature&lt;/strong&gt;, which include compound random measures (Griffin &amp;amp; Leisen, 2017; Riva-Palacio &amp;amp; Leisen, 2019), Clayton Lévy copula (Tankov, 2003; Epifani &amp;amp; Lijoi, 2010) and additive models (Griffiths &amp;amp; Milne, 1978; Lijoi et al., 2014). These upper bounds are expressed in terms of the hyperparameters of the corresponding models (see Figure 2) and thus enable a principled prior specification of their dependence structure.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-01-17-measuring-dependence-in-the-wasserstein-distance-for-bayesian-nonparametric-models_files/2.png&#34; /&gt;&lt;/p&gt;
&lt;center&gt;
&lt;em&gt;Figure 2. On the left: distance from exchangeability for additive random measures. On the right: distance from exchangeability for compound random measures.&lt;/em&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;div id=&#34;discussion-and-further-work&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Discussion and further work&lt;/h2&gt;
&lt;p&gt;We highlight two take-home messages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We can use the Wasserstein distance to build a natural and tractable distance on a wide class of (vectors of) random measures. This opens the way to many possible uses of a distance between infinite dimensional random structures, going beyond the measurement of dependence.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A natural way to measure the dependence between infinite-dimensional random quantities is to evaluate the distance from an extreme situation, as the one of complete dependence.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In Catalano et al (2021) we have moved the first steps. Yet, there are a number of questions that still remain open. In particular,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We measure dependence in terms of distance from the extreme situation of complete dependence. How can we simultaneously quantify the &lt;strong&gt;discrepancy from&lt;/strong&gt; the other extreme, that is, &lt;strong&gt;independence&lt;/strong&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Using the distance from exchangeability to measure dependence is useful for relative comparisons between dependence structures (“&lt;span class=&#34;math inline&#34;&gt;\(\tilde \mu_1\)&lt;/span&gt; is more dependent than &lt;span class=&#34;math inline&#34;&gt;\(\tilde \mu_2\)&lt;/span&gt;”) but prevents absolute quantifications of dependence (“&lt;span class=&#34;math inline&#34;&gt;\(\tilde \mu\)&lt;/span&gt; has an intermediate dependence structure”). In order to extend the measure in this direction we need to find the &lt;strong&gt;maximum&lt;/strong&gt; possible value of the &lt;strong&gt;distance from exchangeability&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In Catalano et al (2022+) we deal with these questions. Their answer sheds light into the deep geometric properties of the Wasserstein distance, which are key to the definition of a &lt;strong&gt;Wasserstein Index of Dependence&lt;/strong&gt; in [0,1]: our goal since the beginning. This opens interesting research directions on how to use the index to assess heterogeneity in the data, going beyond the restrictive assumption of independence.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Catalano, M., Lavenant, H., Lijoi, A., Prünster, I. (2022+). A Wasserstein Index of Dependence for Random Measures. arXiv:2109.06646.&lt;/p&gt;
&lt;p&gt;Catalano, M., A. Lijoi, and I. Prünster (2021). Measuring dependence in the Wasserstein distance for Bayesian nonparametric models. Ann. Statist. 49(5), 2916–2947.&lt;/p&gt;
&lt;p&gt;Cifarelli, D.M. &amp;amp; Regazzini, E. (1978). Nonparametric statistical problems under partial exchangeability: The role of associative means. Technical Report.&lt;/p&gt;
&lt;p&gt;Doksum, K. (1974). Tailfree and neutral random probabilities and their posterior distributions. The Annals of Probability,
2(2):183–201.&lt;/p&gt;
&lt;p&gt;Dykstra, R. L. and Laud, P. (1981). A Bayesian nonparametric approach to reliability. The Annals of Statistics, 9(2):356– 367.&lt;/p&gt;
&lt;p&gt;Epifani, I. and Lijoi, A. (2010). Nonparametric priors for vectors of survival functions. Statist. Sinica 20 1455–1484.&lt;/p&gt;
&lt;p&gt;Hjort, N. L. (1990). Nonparametric Bayes estimators based on beta processes in models for life history data. The Annals of Statistics, 18(3):1259–1294.&lt;/p&gt;
&lt;p&gt;Griffin, J. E. &amp;amp; Leisen, F. (2017). Compound random measures and their use in bayesian non-parametrics. J. Royal Stat. Soc. Ser. B 79, 525–545.&lt;/p&gt;
&lt;p&gt;Griffiths, R. C. and Milne, R. K. (1978). A class of bivariate Poisson processes. J. Multivariate Anal. 8 380–395.&lt;/p&gt;
&lt;p&gt;James, L. F. (2005). Bayesian Poisson process partition calculus with an application to Bayesian Lévy moving averages. The Annals of Statistics, 33(4):1771–1799.&lt;/p&gt;
&lt;p&gt;Lijoi,A., Nipoti,B. and Prünster, I. (2014). Bayesian inference with dependent normalized completely random measures. Bernoulli 20 1260–1291.&lt;/p&gt;
&lt;p&gt;MacEachern, S. N. (1999). Dependent nonparametric processes. in ASA Proceedings of the Section on Bayesian Statistical Science, Alexandria, VA: American Statistical Association.&lt;/p&gt;
&lt;p&gt;MacEachern, S. N. (2000). Dependent Dirichlet processes. Tech. Report, Ohio State University.&lt;/p&gt;
&lt;p&gt;Riva Palacio, A. and Leisen, F. (2021). Compound vectors of subordinators and their associated positive Lévy copulas. J. Multivariate Anal. 183.&lt;/p&gt;
&lt;p&gt;Quintana, F. A., Müller, P., Jara, A., and MacEachern, S. N. (2021). The dependent Dirichlet process and related models. Statistical Science, to appear.&lt;/p&gt;
&lt;p&gt;Tankov, P. (2003). Dependence structure of spectrally positive multidimensional Lévy processes. Unpub- lished Manuscript.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;authors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Authors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://martacatalano.github.io/&#34;&gt;Marta Catalano&lt;/a&gt; is Assistant Professor in the Department of Statistics at the University of Warwick.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mypage.unibocconi.eu/antoniolijoi/&#34;&gt;Antonio Lijoi&lt;/a&gt; is Professor in the Department of Decision Sciences at Bocconi University.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mypage.unibocconi.eu/igorpruenster/&#34;&gt;Igor Prünster&lt;/a&gt; is Professor in the Department of Decision Sciences at Bocconi University.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Universal estimation with Maximum Mean Discrepancy (MMD)</title>
      <link>https://youngstats.github.io/post/2022/01/13/universal-estimation-with-maximum-mean-discrepancy-mmd/</link>
      <pubDate>Thu, 13 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/01/13/universal-estimation-with-maximum-mean-discrepancy-mmd/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;em&gt;This is an updated version of a blog post on RIKEN AIP Approximate Bayesian Inference team webpage:&lt;/em&gt; &lt;a href=&#34;https://team-approx-bayes.github.io/blog/mmd/&#34; class=&#34;uri&#34;&gt;https://team-approx-bayes.github.io/blog/mmd/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-01-13-universal-estimation-with-maximum-mean-discrepancy-mmd_files/cover2.png&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;INTRODUCTION&lt;/h2&gt;
&lt;p&gt;A very old and yet very exciting problem in statistics is the definition of a universal estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt;. An estimation procedure that would work all the time. Close your eyes, push the button, it works, for any model, in any context.&lt;/p&gt;
&lt;p&gt;Formally speaking, we want that for some metric &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; on probability distributions, for any statistical model &lt;span class=&#34;math inline&#34;&gt;\((P_\theta,\theta\in\Theta)\)&lt;/span&gt;, given &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt; drawn i.i.d from some &lt;span class=&#34;math inline&#34;&gt;\(P^0\)&lt;/span&gt; &lt;em&gt;not necessarily in the model&lt;/em&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d\left(P_{\hat{\theta}},P^0 \right) \leq \inf_{\theta\in\Theta} d\left(P_{\theta},P^0 \right) + r_n(\Theta),
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(r_n(\Theta) \rightarrow 0\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow \infty\)&lt;/span&gt; holds, either in expectation or with large probability.&lt;/p&gt;
&lt;p&gt;Why would this be nice? Well, first, if the model is well specified, that is, &lt;span class=&#34;math inline&#34;&gt;\(P^0 = P_{\theta^0}\)&lt;/span&gt; for some &lt;span class=&#34;math inline&#34;&gt;\(\theta^0\in\Theta\)&lt;/span&gt;, we would have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d\left(P_{\hat{\theta}},P^0 \right) \leq  r_n(\Theta) \rightarrow 0,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;so the estimator is consistent. But the ill-specified case is also relevant. Remember that “&lt;em&gt;all models are wrong, some models are useful&lt;/em&gt;”. A very interesting case is Huber’s contamination model: assume that the data is drawn from the model, but might be corrupted with small probability. That is, &lt;span class=&#34;math inline&#34;&gt;\(P^0 = (1-\varepsilon) P_{\theta^0} + \varepsilon Q\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\in[0,1]\)&lt;/span&gt; is small and &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;, the contamination distribution, can be absolutely whatever. Then we would have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d\left(P_{\hat{\theta}},P^0 \right) \leq d(P_{\theta^0},P^0) + r_n(\Theta) = d(P_{\theta^0},(1-\varepsilon) P_{\theta^0} + \varepsilon Q) + r_n(\Theta).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If the metric &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is such that &lt;span class=&#34;math inline&#34;&gt;\(d(P_{\theta^0},(1-\varepsilon) P_{\theta^0} + \varepsilon Q) \leq C \varepsilon\)&lt;/span&gt; for some constant &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;, we end up with&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d\left(P_{\hat{\theta}},P^0 \right) \leq C \varepsilon +  r_n(\Theta),
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which means that, as long as &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; remains quite small, the estimator is still not too bad. That is, the estimator is robust to contamination.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-mle-does-not-work&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;THE MLE DOES NOT WORK&lt;/h2&gt;
&lt;p&gt;In case you believe that popular estimators such as Maximum Likelihood Estimator (MLE) is universal, surprise: it’s not.&lt;/p&gt;
&lt;p&gt;First, if the model does not satisfy regularity assumptions, the MLE is not even defined. For example, consider a location model, that is, &lt;span class=&#34;math inline&#34;&gt;\(P_\theta\)&lt;/span&gt; has the density
&lt;span class=&#34;math display&#34;&gt;\[ p_\theta(x) = g(x-\theta) \]&lt;/span&gt;
and consider
&lt;span class=&#34;math display&#34;&gt;\[ g(x) = \frac{\exp(-|x|)}{2\sqrt{\pi|x|}} . \]&lt;/span&gt;
Obviously, the likelihood is infinite at each data point, that is, as soon as &lt;span class=&#34;math inline&#34;&gt;\(\theta\in\{X_1,\dots,X_n\}\)&lt;/span&gt;, we have
&lt;span class=&#34;math display&#34;&gt;\[ \prod_{i=1}^n p_\theta(X_i) = +\infty. \]&lt;/span&gt;
Even when the MLE is well defined, there are examples where it is known to be inconsistent [8]. It is also well known to be non robust to contamination in the data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-examples-of-universal-estimators&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;SOME EXAMPLES OF UNIVERSAL ESTIMATORS&lt;/h2&gt;
&lt;p&gt;The first example of universal estimator we are aware of: Yatracos’ estimator [7], with &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; being the total variation distance. It works with the rate &lt;span class=&#34;math inline&#34;&gt;\(r_n(\Theta) = [\mathrm{dim}(\Theta)/n]^{\frac{1}{2}}\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt; is in some finite dimensional space. And it doesn’t work for nonparametric estimation. Still, it’s nice, and the paper is beautiful (and short). Equally beautiful is the book by Devroye and Lugosi [9] which studies many estimations methods for the total variation distance, including variants of Yatracos’ estimator.&lt;/p&gt;
&lt;p&gt;Another example is Birgé, Barraud and Sart’s &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;-estimator [8], which satisfies a similar result for the Hellinger distance. If you want to read the paper, be aware: this is extremely difficult to prove! It is also very nice, because the Hellinger distance looks locally very similar to the KL in many models. In some sense, the &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;-estimator actually does what the MLE should do.&lt;/p&gt;
&lt;p&gt;By the way. We live in the big data era, high dimensional data, big networks, more layers, you know. So it must be said that Yatracos’ estimator, and the &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;-estimators, cannot be used in practice. They require an exhaustive search in a fine discretization of the parameter space, don’t expect to do that with a deep NN. Don’t expect to do it either for a very shallow NN, not even for a linear regression in dimension 50 (as discussed later, a variant of Yatracos’ estimator by Devroye and Lugosi might be feasible, though).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mmd-estimation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;MMD-ESTIMATION&lt;/h2&gt;
&lt;p&gt;Let us now describe yet another metric, and another estimator. This metric is based on kernels. So, let &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; be a kernel on the observations space, &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}\)&lt;/span&gt;. This means that there is an Hilbert space &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}\)&lt;/span&gt;, equipped with a norm &lt;span class=&#34;math inline&#34;&gt;\(\left\lVert\cdot\right\rVert _{\mathcal{H}}\)&lt;/span&gt;, and a continuous map &lt;span class=&#34;math inline&#34;&gt;\(\Phi:\mathcal{X}\rightarrow \mathcal{H}\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(K(x,x&amp;#39;) = \left&amp;lt;\Phi(x),\Phi(x&amp;#39;)\right&amp;gt;\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Given a probability distribution &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}\)&lt;/span&gt;, let us define the kernel mean embedding&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
 \mu(P) = \int \Phi(x) P(\mathrm{d} x) = \mathbb{E} _{X \sim P }[\Phi(X)].
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Wait. Of course, this is not always defined! It is, say, if &lt;span class=&#34;math inline&#34;&gt;\(\int \left\lVert\Phi(x)\right\rVert_{\mathcal{H}} P(\mathrm{d} x) &amp;lt;+\infty\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now, it appears that some kernels &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; are known such that:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-roman&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(k(x,x) = \left\| \Phi(x) \right\|_{\mathcal{H}}^2 \leq 1\)&lt;/span&gt; for any &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, which in turn ensures that &lt;span class=&#34;math inline&#34;&gt;\(\mu(P)\)&lt;/span&gt; is well defined for any &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P\mapsto \mu(P)\)&lt;/span&gt; is one to one.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For example, when &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}=\mathbb{R}^d\)&lt;/span&gt;, the Gaussian kernel&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
k(x,x&amp;#39;) = \exp\left(- \frac{\left\lVert x-x&amp;#39;\right\rVert^2}{\gamma^2} \right),
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;for some &lt;span class=&#34;math inline&#34;&gt;\(\gamma&amp;gt;0\)&lt;/span&gt;, satisfies (i) and (ii).&lt;/p&gt;
&lt;p&gt;For any such kernel,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d_{k}(P,Q)=\left\lVert \mu(P)-\mu(Q)\right\rVert_{\mathcal{H}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;is a distance between probability distributions. We can now define the MMD-estimator. Let &lt;span class=&#34;math inline&#34;&gt;\(\hat{P}_n\)&lt;/span&gt; denote the empirical probability distribution, that is,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{P} _n
= \frac{1}{n}\sum_{i=1}^{n} \delta_{X_i}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; is defined by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\theta} = \arg\min_{\theta\in\Theta}d_k(P_\theta,\hat{P}_n).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note: [12] below provide some conditions ensuring that the minimizer indeed exists. But actually, if if does not exist, just take any &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;-minimizer for &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; small enough, and all the good properties discussed below still hold.&lt;/p&gt;
&lt;p&gt;Note: if you believed the statement “&lt;em&gt;Close your eyes, push the button, it works&lt;/em&gt;” above, you will of course be disappointed. Life is not that simple. The choice of the kernel &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is far from easy, and is of course context dependent.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-shortest-consistency-proof-ever&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;THE SHORTEST CONSISTENCY PROOF EVER?&lt;/h2&gt;
&lt;p&gt;We now prove that, as long as the kernel satisfies (i) and (ii) above, for any statistical model &lt;span class=&#34;math inline&#34;&gt;\((P_\theta,\theta\in\Theta)\)&lt;/span&gt; (parametric, or not!), given &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt; drawn i.i.d from some &lt;span class=&#34;math inline&#34;&gt;\(P^0\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E} \left[ d_k\left(P_{\hat{\theta}},P^0 \right) \right] \leq \inf_{\theta\in\Theta} d_k\left(P_{\theta},P^0 \right) + \frac{2}{\sqrt{n}}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is taken from our paper [1]. (Note that the expectation is with respect to the sample: &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}=\hat{\theta}(X_1,\dots,X_n)\)&lt;/span&gt;, the dependence with respect to the sample is always dropped from the notation in statistics and in machine learning).&lt;/p&gt;
&lt;p&gt;First, for any &lt;span class=&#34;math inline&#34;&gt;\(\theta\in\Theta\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d_k\left(P_{\hat{\theta}_n},P^0 \right) \leq d_k\left(P_{\hat{\theta}_n},\hat{P}_n \right) +  d_k\left(\hat{P}_n,P^0 \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;by the triangle inequality. Using the defining property of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt;, that is, that it mimizes the first term in the right-hand side,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d_k\left(P_{\hat{\theta}_n},P^0 \right) \leq d_k\left(P_{\theta},\hat{P}_n \right) +  d_k\left(\hat{P}_n,P^0 \right),
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and using again the triangle inequality,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d_k\left(P_{\hat{\theta}_n},P^0 \right) \leq d_k \left(P_{\theta},P^0 \right) + 2 d_k\left(\hat{P}_n,P^0 \right).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Take the expectation of both sides, and keeping in mind that this holds for any &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, this gives:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E} \left[ d_k\left(P_{\hat{\theta}},P^0 \right) \right] \leq \inf_{\theta\in\Theta} d_k\left(P_{\theta},P^0 \right) + 2 \mathbb{E} \left[d_k\left(\hat{P}_n,P^0 \right) \right].
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So, it all boils down to a control of the expectation of &lt;span class=&#34;math inline&#34;&gt;\(d_k(\hat{P}_n,P^0 )\)&lt;/span&gt; in the right-hand side. Using Jensen’s inequality,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E} \left[ d_k\left(\hat{P}_n,P^0 \right)\right] \leq \sqrt{\mathbb{E} \left[d_k^2\left(\hat{P}_n,P^0 \right)\right]}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;so let us just focus on bounding the expected square distance. Using the definition of the MMD distance,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E} \left[d_k^2\left(\hat{P}_n,P^0 \right)\right] = \mathbb{E} \left[ \left\lVert \frac{1}{n} \sum_{i=1}^n \Phi(X_i)-\mathbb{E}_{X\sim P^0}[\Phi(X)]  \right\rVert_{\mathcal{H}}^2 \right] = \mathrm{Var}\left(\frac{1}{n} \sum_{i=1}^n \Phi(X_i) \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and so, as &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt; are i.i.d,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E} \left[d_k^2\left(\hat{P}_n,P^0 \right)\right] = \frac{1}{n} \mathrm{Var}[\Phi(X_1)] \leq \frac{1}{n} \mathbb{E}\left[ \left\lVert\Phi(X_1) \right\rVert_\mathcal{H}^2\right] \leq \frac{1}{n}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-compute-the-mmd-estimator&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;HOW TO COMPUTE THE MMD-ESTIMATOR?&lt;/h2&gt;
&lt;p&gt;Now, of course, one question remains: is it easier to compute &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; than Yatracos’ estimator?&lt;/p&gt;
&lt;p&gt;This question is discussed in depth in [1] and [12]. The main message is that the minimization of &lt;span class=&#34;math inline&#34;&gt;\(d_k^2(P_\theta,\hat{P}_{n})\)&lt;/span&gt; is usually a smooth, but non-convex problem (in [1] we exhibit one model for which the problem is convex, though). An unbiased estimate of the gradient of this quantity is easy to build. So, it is possible to use a stochastic gradient algorithm (SGA), but because of the non-convexity of the problem, it is not possible to show that this will lead to a global minimum. Still, in practice, the performances of the estimator obtained by using the SGA are excellent, see [1,12].&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;historical-references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;HISTORICAL REFERENCES&lt;/h2&gt;
&lt;p&gt;The idea to use an estimator of the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\theta} = \arg\min_{\theta\in\Theta} d(P_\theta,\hat{P}_n)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;goes back to the 50s, see [5], under the name “minimum distance estimation” (MDE). The paper [6] is followed by a discussion by Sture Holm who argues that this leads to robust estimators when the distance &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is bounded. The reader can try for example the Kolmogorov-Smirnov distance defined for &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}=\mathbb{R}\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d(P,Q) = \sup_{a\in\mathbb{R}} |P(X\leq a) - Q(X\leq a)|.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Another example is the total variation distance. Note that the initial procedure proposed by Yatracos [7] is &lt;em&gt;not&lt;/em&gt; the MDE with the TV distance (but it is an MDE with respect to another, model dependent semi-metric).&lt;/p&gt;
&lt;p&gt;Also, we mention that the procedure used by Barraud, Birgé and Sart in [8] &lt;em&gt;cannot&lt;/em&gt; be interpreted as minimum distance estimation.&lt;/p&gt;
&lt;p&gt;The MMD distance has been used in kernel methods for years, we refer the reader to the excellent tutorial [10]. However, up to your knowledge, the first time it was used as described in this blog post was in [11] where the authors used this technique to estimate &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; in a special type of model &lt;span class=&#34;math inline&#34;&gt;\((P_\theta,\theta\in\Theta)\)&lt;/span&gt; called Generative Adversarial Network (GAN, I guess you already heard about it). The first general study of MMD-estimation is [12], where the authors study the consistency and asymptotic normality of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; (among others!).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;advertisement-our-recent-works-on-mmd&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ADVERTISEMENT: OUR RECENT WORKS ON MMD&lt;/h2&gt;
&lt;p&gt;Our preprint on MMD-estimation and robustness study specifically the robustness properties of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt;. Namely, in Huber’s contamination model, we study in detail the dependence of &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{E}[d(P_{\hat{\theta}},P^0)]\)&lt;/span&gt; with respect to &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, the sample size, and &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;, the level of contamination. Moreover, in this paper, we also extend the consistency proof to the case where the observations are not independent.&lt;/p&gt;
&lt;p&gt;[1] &lt;a href=&#34;https://doi.org/10.3150/21-BEJ1338&#34;&gt;B.-E. Chérief-Abdellatif and P. Alquier (2019). Finite Sample Properties of Parametric MMD Estimation: Robustness to Misspecification and Dependence. Bernoulli, 2022, vol. 28(1), no. 1, pp. 181-213.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In the short paper on MMD-Bayes, we study a generalized Bayes procedure using the MMD distance:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\pi(\theta|X_1,\dots,X_n) \propto \exp\left(- \eta d_k^2(P_\theta,\hat{P}_n) \right) \pi(\theta)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; is some prior distribution and &lt;span class=&#34;math inline&#34;&gt;\(\eta&amp;gt;0\)&lt;/span&gt; some tuning parameter. This leads to robust Bayesian estimators.&lt;/p&gt;
&lt;p&gt;[2] &lt;a href=&#34;http://proceedings.mlr.press/v118/cherief-abdellatif20a.html&#34;&gt;B.-E. Chérief-Abdellatif and P. Alquier (2020). MMD-Bayes: Robust Bayesian Estimation via Maximum Mean Discrepancy. Proceedings of The 2nd Symposium on Advances in Approximate Bayesian Inference (AABI), Proceedings of Machine Learning Research, vol. 118, pp. 1-21.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We finally have two recent preprints on MMD-estimation in regression models and in copulas models, respectively. These models have in common that they are semi-parametric: we want to estimate a parameter that does not completely define the distribution of the data. For example, in the linear regression model &lt;span class=&#34;math inline&#34;&gt;\(Y_i = \left&amp;lt;\theta,X_i\right&amp;gt; + \varepsilon_i\)&lt;/span&gt;, we usually only specify the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;, say &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{N}(0,\sigma^2)\)&lt;/span&gt;. However, in order to use the MMD-estimator as defined above, one should also specify the distribution of the &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt;’s. In [4], we propose a trick to avoid that, but the analysis becomes immediately much more complex.&lt;/p&gt;
&lt;p&gt;[3] &lt;a href=&#34;https://doi.org/10.1080/01621459.2021.2024836&#34;&gt;P. Alquier, B.-E. Chérief-Abdellatif, A. Derumigny and J.-D. Fermanian. Estimation of Copulas via Maximum Mean Discrepancy, to appear in Journal of the American Statistical Association.&lt;/a&gt; The paper comes with the R package: &lt;a href=&#34;https://cran.r-project.org/web/packages/MMDCopula/&#34;&gt;MMDCopula&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;[4] &lt;a href=&#34;http://arxiv.org/abs/2006.00840&#34;&gt;P. Alquier and M. Gerber (2020). Universal Robust Regression via Maximum Mean Discrepancy. Preprint arxiv:2006.00840.&lt;/a&gt; We are currently working on the corresponding R package.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;REFERENCES&lt;/h2&gt;
&lt;p&gt;[5] &lt;a href=&#34;https://projecteuclid.org/download/pdf_1/euclid.aoms/1177707038&#34;&gt;J. Wolfowitz (1957). The minimum distance method. The Annals of Mathematical Statistics, 28(1), 75-88.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[6] P. J. Bickel (1976). Another Look at Robustness: A Review of Reviews and Some New Developments. Scandinavian Journal of Statistics 3, 145-168.&lt;/p&gt;
&lt;p&gt;[7] &lt;a href=&#34;https://projecteuclid.org/download/pdf1/euclid.aos/1176349553&#34;&gt;Y. G. Yatracos (1985). Rates of convergence of minimum distance estimators and Kolmogorov’s entropy. The Annals of Statistics, 13(2), 768-774.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[8] &lt;a href=&#34;https://link.springer.com/article/10.1007/s00222-016-0673-5&#34;&gt;Y. Baraud, L. Birgé and M. Sart (2017). A new method for estimation and model selection: &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;-estimation. Inventiones mathematicae, 207, 425-517.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[9] &lt;a href=&#34;https://www.springer.com/gp/book/9780387951171&#34;&gt;L. Devroye and G. Lugosi (2001). Combinatorial methods in density estimation. Springer.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[10] &lt;a href=&#34;https://www.nowpublishers.com/article/Details/MAL-060&#34;&gt;K. Muandet, K. Fukumizu, B. Sriperumbudur and B. Schölkopf, (2017). Kernel mean embedding of distributions: A review and beyond. Foundations and Trends in Machine Learning, 10(1-2), 1-141.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[11] &lt;a href=&#34;http://auai.org/uai2015/proceedings/papers/230.pdf&#34;&gt;G. K. Dziugaite, D. M. Roy and Z. Ghahramani (2015). Training generative neural networks via maximum mean discrepancy optimization. UAI’15: Proceedings of the Thirty-First Conference on Uncertainty in Artificial IntelligenceJuly 2015, 258-267.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[12] &lt;a href=&#34;http://arxiv.org/abs/1906.05944&#34;&gt;F.-X. Briol, A. Barp, A. B. Duncan and M. Girolami (2019). Statistical Inference for Generative Models via Maximum Mean Discrepancy. Preprint arXiv:1906.05944.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;about-the-author&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ABOUT THE AUTHOR&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://pierrealquier.github.io/&#34;&gt;Pierre Alquier&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Please visit the webpage of my co-authors on this topic:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://badreddinecheriefabdellatif.github.io/&#34;&gt;Badr-Eddine Chérief-Abdellatif&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://alexisderumigny.wordpress.com/&#34;&gt;Alexis Derumigny&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.crest.fr/pagesperso.php?user=2975&#34;&gt;Jean-David Fermanian&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://research-information.bris.ac.uk/en/persons/mathieu-gerber&#34;&gt;Mathieu Gerber&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Reconciling the Gaussian and Whittle Likelihood with an application to estimation in the frequency domain</title>
      <link>https://youngstats.github.io/post/2022/01/06/reconciling-the-gaussian-and-whittle-likelihood/</link>
      <pubDate>Thu, 06 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/01/06/reconciling-the-gaussian-and-whittle-likelihood/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(\{X_t: t\in \mathbb{Z}\}\)&lt;/span&gt; is a second order stationary time series where &lt;span class=&#34;math inline&#34;&gt;\(c(r) = \text{cov}(X_{t+r},X_t)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(f(\omega) = \sum_{r\in\mathbb{Z}}c(r)e^{ir\omega}\)&lt;/span&gt; are the corresponding autocovariance and spectral density function, respectively. For notational convenience, we assume the time series is centered, that is &lt;span class=&#34;math inline&#34;&gt;\(\textrm{E}(X_t)=0\)&lt;/span&gt;.
Our aim is to fit a parametric second-order stationary model (specified by &lt;span class=&#34;math inline&#34;&gt;\(\{c_{f_\theta}(r)\}\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(f_\theta(\omega)\)&lt;/span&gt;) to the observed time series &lt;span class=&#34;math inline&#34;&gt;\(\underline{X}_n = (X_1, ..., X_n)^\top\)&lt;/span&gt;.
There are two classical estimation methods based on the quasi-likelihood criteria. The first is a time-domain method which minimizes the (negative log) quasi Gaussian likelihood
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\mathcal{L}_n(\theta) = \frac{1}{n}\big(\underline{X}_n^{\top} \Gamma_n(f_\theta)\underline{X}_n + \log \det\Gamma_n(f_\theta) \big),
\end{equation}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\([\Gamma_n(f_\theta)]_{s,t} = c_{f_\theta}(s-t)\)&lt;/span&gt; is a covariance matrix of &lt;span class=&#34;math inline&#34;&gt;\(\underline{X}_n\)&lt;/span&gt;.
The second is a frequency-domain method which minimizes the Whittle likelihood
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
K_n(\theta) = \frac{1}{n}\sum_{k=1}^{n}\bigg(\frac{|J_n(\omega_k)|^2}{f_\theta(\omega_k)} + \log f_\theta(\omega_k)\bigg) \qquad \omega_k = \frac{2\pi k}{n},
\end{equation}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(J_n(\omega_k) =n^{-1/2}\sum_{t=1}^{n} X_t e^{it\omega_k}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(1 \leq k \leq n\)&lt;/span&gt;, is the discrete Fourier transform (DFT) of the (observed) time series.&lt;/p&gt;
&lt;p&gt;The Whittle likelihood is computationally a very attractive method. Thus, it has become a popular method for parameter estimation of both long and short memory stationary time series. Moreover, the Whittle likelihood has gained traction as a quasi-likelihood between the periodogram and conjectured spectral density.&lt;/p&gt;
&lt;p&gt;Despite its advantages, it is well-known that for small samples, the Whittle likelihood can give rise to estimators with a substantial bias. &lt;a href=&#34;https://doi.org/10.1214/aos/1176350838&#34;&gt;Dahlhaus (1988)&lt;/a&gt; showed that the finite sample bias in the periodogram impacts the performance of the
Whittle likelihood. To remedy this, he proposed the &lt;em&gt;tapered Whittle&lt;/em&gt; likelihood based on the tapered periodogram. He proved that the tapered periodogram led to a significant reduction in bias.&lt;/p&gt;
&lt;p&gt;However, as far as we are aware, there are no results that explain the “precise difference” between the Gaussian likelihood and the Whittle likelihood. The objective of our paper to bridge the gap between the time- and frequency-domain approaches by deriving an exact expression of the difference between the Gaussian and Whittle likelihood.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Contribution&lt;/strong&gt; of this paper is three-fold:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We obtain a linear transformation (which we named the &lt;em&gt;complete DFT&lt;/em&gt;) that is “biorthogonal” to the DFT of an observed time series.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We use the complete DFT to rewrite the Gaussian likelihood in the frequency domain.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Using an approximation for the difference, we propose a new frequency domain quasi-likelihood criteria — the &lt;em&gt;boundary corrected Whittle&lt;/em&gt; and the
&lt;em&gt;hybrid Whittle&lt;/em&gt; likelihood.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the following sections, we summarize each contribution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-complete-dft&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The complete DFT&lt;/h2&gt;
&lt;p&gt;We introduce our main theorem which obtains a transform that is biorthogonal to the DFT.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem 1 (The biorthogonal transform)&lt;/strong&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(\{X_t\}\)&lt;/span&gt; be a centered second order stationary time series with spectral density &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; which is bounded and strictly positive. For &lt;span class=&#34;math inline&#34;&gt;\(\tau \in \mathbb{Z}\)&lt;/span&gt;, let &lt;span class=&#34;math inline&#34;&gt;\(\hat{X}_{\tau,n}\)&lt;/span&gt; denote the best linear predictors of &lt;span class=&#34;math inline&#34;&gt;\(X_\tau\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(\underline{X}_n\)&lt;/span&gt;.
Let &lt;span class=&#34;math inline&#34;&gt;\(\widetilde{J_n}(\omega;f) = J_n(\omega) + \widehat{J_n}(\omega;f)\)&lt;/span&gt; be the &lt;em&gt;complete DFT&lt;/em&gt; where
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
\widehat{J_n}(\omega;f) = n^{-1/2} (\sum_{\tau \leq 0} \hat{X}_{\tau,n}e^{i\tau \omega} + 
\sum_{\tau &amp;gt; n} \hat{X}_{\tau,n}e^{i\tau \omega} ).
\end{equation*}\]&lt;/span&gt;
Then,
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
\text{cov}(\widetilde{J_n}(\omega_{k_1};f), J_n(\omega_{k_2}))
= f(\omega_{k_1}) \delta_{k_1 = k_2} \quad 1\leq k_1, k_2 \leq n,
\end{equation*}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\delta_{k_1 = k_2} = 1\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(k_1=k_2\)&lt;/span&gt; and zero otherwise.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;From the above theorem, the biorthogonal transform corresponding to the regular DFT—henceforth called the &lt;strong&gt;complete DFT&lt;/strong&gt;—contains the regular DFT &lt;em&gt;plus&lt;/em&gt; the Fourier transform of the best linear predictors outside the domain of observation. A visualization of the observations and the predictors that are involved in the construction of the complete DFT is given below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-01-06-reconciling-the-gaussian-and-whittle-likelihood_files/proj2.png&#34; style=&#34;width:70.0%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;contrasting-the-gaussian-and-whittle-likelihood&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Contrasting the Gaussian and Whittle likelihood&lt;/h2&gt;
&lt;p&gt;Using the complete DFT, we can represent the Gaussian likelihood in the frequency domain.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem 2 (A frequency domain representation of the Gaussian likelihood)&lt;/strong&gt; The quadratic term in the Gaussian likelihood can be rewritten as
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
\mathcal{L}_n(\theta) = \frac{1}{n}\underline{X}_n^{\top} \Gamma_n(f_\theta)\underline{X}_n 
= \frac{1}{n} \sum_{k=1}^{n} \frac{\widetilde{J_n}(\omega_{k};f_\theta) }{f_\theta(\omega_k)}.
\end{equation*}\]&lt;/span&gt; This yields the difference between the Gaussian and Whittle likelihood
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
\mathcal{L}_n(\theta) - K_n(\theta)
= \frac{1}{n} \sum_{k=1}^{n} \frac{\widehat{J_n}(\omega_{k};f_\theta) \overline{J_n(\omega_k)}}{f_\theta(\omega_k)}.
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;From the above theorem, we observe that the difference between the Gaussian and Whittle likelihood is due to the linear predictions outside the domain of observation. We interpret the Gaussian likelihood in terms of the information criterion. The Whittle likelihood estimator selects the spectral density &lt;span class=&#34;math inline&#34;&gt;\(f_\theta\)&lt;/span&gt; which best fits the periodogram. On the other hand, since the complete DFT is constructed based on &lt;span class=&#34;math inline&#34;&gt;\(f_\theta\)&lt;/span&gt;, the Gaussian likelihood estimator selects the spectral density which &lt;em&gt;simultaneously&lt;/em&gt; predicts and fits the time series.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;new-frequency-domain-quasi-likelihood-criteria&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;New frequency domain quasi-likelihood criteria&lt;/h2&gt;
&lt;p&gt;For both the specified and misspecified case, the Gaussian and Whittle likelihood estimate the &lt;em&gt;spectral divergence&lt;/em&gt; between the true spectral density &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; and the conjectured spectral density &lt;span class=&#34;math inline&#34;&gt;\(f_\theta\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
\mathbb{E}_f [\mathcal{L}_n(\theta)] 
~~\text{or} ~~ \mathbb{E}_f [K_n(\theta)]
= I(f;f_\theta) + O(n^{-1}),
\end{equation*}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(I(f;f_\theta) = n^{-1}\sum_{k=1}^{n}\big(\frac{f(\omega_k)}{f_\theta(\omega_k)} + \log f_\theta(\omega_k)\big)\)&lt;/span&gt;. Therefore, even for the misspecified case, asymptotically, both estimators have a meaningful interpretation. However, there is still a finite sample bias in both likelihoods which is of order &lt;span class=&#34;math inline&#34;&gt;\(O(n^{-1})\)&lt;/span&gt;. To remedy this, we introduce a new frequency domain quasi-likelihood criterion:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;(The boundary corrected Whittle likelihood)&lt;/strong&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
W_n(\theta;f) = \frac{1}{n}\sum_{k=1}^{n}\bigg(\frac{\widetilde{J_n}(\omega_{k};f)\overline{J_n(\omega_k)} }{f_\theta(\omega_k)}+
\log f_\theta(\omega_k)\bigg).
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Then, due to the Theorem 1, the (infeasible) boundary corrected Whittle likelihood is an &lt;em&gt;unbiased estimator&lt;/em&gt; of the spectral divergence.
To obtain the feasible boundary corrected Whittle criterion, we use the following approximations:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Estimate the true spectral density &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; by fitting AR&lt;span class=&#34;math inline&#34;&gt;\((p)\)&lt;/span&gt; model to the data (it is often called the &lt;em&gt;plug-in&lt;/em&gt; method).&lt;/li&gt;
&lt;li&gt;Calculate the complete DFT and the boundary corrected Whittle likelihood based on the best fitting AR&lt;span class=&#34;math inline&#34;&gt;\((p)\)&lt;/span&gt; spectral density,
&lt;span class=&#34;math inline&#34;&gt;\(\widehat{f}_p\)&lt;/span&gt;, described in 1.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The resulting feasible boundary corrected Whittle criterion, denotes &lt;span class=&#34;math inline&#34;&gt;\(W_{p,n}(\theta;\widehat{f_p})\)&lt;/span&gt;, gives a higher-order approximation of
the spectral divergence than the Gaussian and Whittle likelihoods. We use &lt;span class=&#34;math inline&#34;&gt;\(\widehat{\theta} = \arg\min_{\theta}W_{p,n}(\theta;\widehat{f}_p)\)&lt;/span&gt;
for a frequency-domain parameter estimation.&lt;/p&gt;
&lt;p&gt;We mention that by a similar argument to that given above, one can also
taper the regular DFT (but not the complete DFT) when defining the boundary corrected Whittle (the result we
called it the &lt;strong&gt;hybrid Whittle&lt;/strong&gt; likelihood). In the simulations, we have observed that this can often out perform the other likelihoods.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulation&lt;/h2&gt;
&lt;p&gt;To end this post, we present some estimation results under misspecification. For sample size &lt;span class=&#34;math inline&#34;&gt;\(n=20\)&lt;/span&gt;, the true data generating process is a Gaussian ARMA&lt;span class=&#34;math inline&#34;&gt;\((3,2)\)&lt;/span&gt; process, but we fit the following two models&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ARMA(1,1)&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(X_t = \phi X_{t-1} + \varepsilon_t + \psi \varepsilon_{t-1}\)&lt;/span&gt; and&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AR(2)&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(X_t = \phi_1 X_{t-1} + \phi_2 X_{t-2} + \varepsilon_t\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;to the time series.
Below shows (the logarithm of) the true spectral density and the best fitting ARMA&lt;span class=&#34;math inline&#34;&gt;\((1,1)\)&lt;/span&gt; and AR&lt;span class=&#34;math inline&#34;&gt;\((2)\)&lt;/span&gt; spectral densities.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-01-06-reconciling-the-gaussian-and-whittle-likelihood_files/armaspec.png&#34; style=&#34;width:70.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;All simulations are conducted over 1,000 replications and for each simulation, we calculate the parameter estimators of five quasi-likelihood criteria: (1) the Gaussian; (2) the Whittle; (3) the tapered Whittle; (4) the &lt;span style=&#34;color:blue&#34;&gt;boundary corrected Whittle&lt;/span&gt;; and (5) the &lt;span style=&#34;color:blue&#34;&gt;hybrid Whittle&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The table below shows the bias and the standard deviation (in the parentheses) of the estimated coefficients and spectral divergence. &lt;span style=&#34;color:red&#34;&gt;&lt;strong&gt;Red text&lt;/strong&gt;&lt;/span&gt; denotes the smallest root-mean-square error (RMSE) and &lt;span style=&#34;color:blue&#34;&gt;&lt;strong&gt;Blue text&lt;/strong&gt;&lt;/span&gt; denotes the second smallest RMSE.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-01-06-reconciling-the-gaussian-and-whittle-likelihood_files/result-table-no-caption.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Based on the above table, we conclude that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The new likelihoods tend to out perform the Whittle likelihood.&lt;/li&gt;
&lt;li&gt;The new likelihoods can significantly reduce the bias and have the smallest or second smallest RMSE.&lt;/li&gt;
&lt;li&gt;There is no clear winner, but the Gaussian, Tapered Whittle, and hybrid Whittle are close contenders.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;R. Dahlhaus. Small sample effects in time series analysis: a new asymptotic theory and a new estimate. &lt;em&gt;Ann. Statist.&lt;/em&gt;, 16(2):808-841, 1988.&lt;/p&gt;
&lt;p&gt;S. Subba Rao and J. Yang. Reconciling the Gaussian and Whittle Likelihood with an application to estimation in the frequency domain. &lt;em&gt;Ann. Statist.&lt;/em&gt;, 49(5):2774-2802, 2021.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;authors-biography&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Authors’ biography&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://sites.google.com/view/junhoyang&#34;&gt;Junho Yang&lt;/a&gt; is an Assistant Research Fellow in the Institute of Statistical Science at Academia Sinica, Taiwan. His research focuses on time series analysis, Toeplitz matrices, and spatial statistics and its applications.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.stat.tamu.edu/~suhasini/&#34;&gt;Suhasini Subba Rao&lt;/a&gt; is a Professor of Statistics at Texas A&amp;amp;M University, USA. Her research focuses on time series analysis, nonstationary processes, nonlinear processes, and spatio-temporal models.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
