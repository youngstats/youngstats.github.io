<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>YoungStatS</title>
    <link>https://youngstats.github.io/</link>
    <description>Recent content on YoungStatS</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 Jul 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://youngstats.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Depth Quantile Functions</title>
      <link>https://youngstats.github.io/post/2021/07/01/depth-quantile-functions/</link>
      <pubDate>Thu, 01 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/07/01/depth-quantile-functions/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-07-01-depth-quantile-functions_files/f1.png&#34; /&gt;
&lt;em&gt;Figure 1: Depth quantile functions for the wine data (d=13), class 2 vs class 3. Blue curves correspond to between class comparisons, red/pink correspond to within class comparisons.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A common technique in modern statistics is the so-called kernel trick, where data is mapped into a (usually) infinite-dimensional feature space, where various statistical tasks can be carried out. Relatedly, we introduce the &lt;strong&gt;depth quantile function&lt;/strong&gt; (DQF), &lt;span class=&#34;math inline&#34;&gt;\(q_{ij}(\alpha)\)&lt;/span&gt; which similarly maps observations into an infinite dimensional space (the double index will become clear below), though in this case, these new representations of the data are functions of a one-dimensional variable &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; which allows plotting. By construction, described below, these functions encode geometric information about the underlying data set. Consequently, we obtain a tool that permits an interpretable visualization of point cloud geometry regardless of dimension of the data. Additionally, tools from functional data analysis can now be used to solve problems (classification, anomaly detection, etc).&lt;/p&gt;
&lt;p&gt;The primary tool used is that of Tukey’s half space depth (HSD), which provides a higher dimensional analog to the order statistics as a measure of centrality of an observation in a data set (where, for instance, the median is the most central or “deepest” point). In fact, the one dimensional version of HSD (&lt;span class=&#34;math inline&#34;&gt;\(D(x) = \min\{F_n(x), 1-F_n(x)\}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(F_n\)&lt;/span&gt; the EDF) is all we need, as we consider projections of our data onto lines before computing centrality, see below.&lt;/p&gt;
&lt;p&gt;It is known that for HSD in any dimension, the level sets &lt;span class=&#34;math inline&#34;&gt;\(\{x:D(x)\geq\lambda\}\)&lt;/span&gt; are necessarily convex, thus not conforming to the shape of the underlying density. Additionally, in high dimensions, it’s likely that most points live near the boundary of the point cloud (the convex hull), i.e. we expect almost all points to be “non-central”. To get around this second problem, we instead consider, for every pair of points (&lt;span class=&#34;math inline&#34;&gt;\(x_i, x_j\)&lt;/span&gt;), the midpoint &lt;span class=&#34;math inline&#34;&gt;\(m_{ij} = \frac{x_i + x_j}{2}\)&lt;/span&gt; as the base point in the construction of our feature functions &lt;span class=&#34;math inline&#34;&gt;\(q_{ij}(\alpha)\)&lt;/span&gt;. Thus, we construct a &lt;em&gt;matrix&lt;/em&gt; of feature functions, with each observation corresponding to &lt;span class=&#34;math inline&#34;&gt;\(n-1\)&lt;/span&gt; feature functions, one for every other observation in the data set (though current work uses only the average of these functions or appropriate subsets of them). The choice of the midpoint as base point is motivated as follows. Consider a 2-class modal classification problem, where each class is represented by a component of a mixture of two normal distributions for which the corresponding cluster centers (the means) are sufficiently separated. When considering a pair of observations from different classes, their midpoint is likely to live in a region between the two point clouds with few nearby observations, in other words, a low density region with a high measure of centrality. The opposite can be expected for within class comparisons, i.e. two observations from the same class.&lt;/p&gt;
&lt;p&gt;To addresses the convexity issue alluded to above, we use “local” versions of the HSD. This is done by taking random subsets of the data space containing &lt;span class=&#34;math inline&#34;&gt;\(m_{ij}\)&lt;/span&gt; and computing the HSD for this point after projection of the subset onto the line &lt;span class=&#34;math inline&#34;&gt;\(\ell_{ij}\)&lt;/span&gt; that is determined by the two points &lt;span class=&#34;math inline&#34;&gt;\(x_i,x_j\)&lt;/span&gt;. Specifically, the subsets are given by the data residing in randomly selected spherical cones of a fixed angle (which is a tuning parameter) with axis of symmetry &lt;span class=&#34;math inline&#34;&gt;\(\ell_{ij}\)&lt;/span&gt; (see figure 2.) We define a distribution of cone tips along &lt;span class=&#34;math inline&#34;&gt;\(\ell_{ij}\)&lt;/span&gt;, which induces a distribution of “local” depths (HSD), and define the DQF as the corresponding quantile function of this distribution. Using directions determined by pairs of points (i.e. the lines &lt;span class=&#34;math inline&#34;&gt;\(\ell_{ij}\)&lt;/span&gt;) addresses a challenge with high dimensional data: which direction should one look to capture interesting feature of the data? It also results in this method being automatically adaptive to sparseness (data living in a lower dimensional subspace of our data space).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-07-01-depth-quantile-functions_files/f2.png&#34; /&gt;
&lt;em&gt;Figure 2: A local depth for midpoint in red. Depth value will be 2 for this cone tip.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As a result of this construction, we end up with a multi-scale method, a function defined on [0,1], that is non-degenerate at both boundaries (in contrast to most multi-scale methods). One can show that the derivative of &lt;span class=&#34;math inline&#34;&gt;\(q_{ij}(\alpha)\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(\alpha\to 0\)&lt;/span&gt; yields information about the density at &lt;span class=&#34;math inline&#34;&gt;\(m_{ij}\)&lt;/span&gt;, while &lt;span class=&#34;math inline&#34;&gt;\(q_{ij}(1)\)&lt;/span&gt; is related to its centrality in the entire point cloud. The manner in which &lt;span class=&#34;math inline&#34;&gt;\(q_{ij}\)&lt;/span&gt; grows with increasing &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, while less interpretable, yields valuable information about the observations it corresponds to.&lt;/p&gt;
&lt;p&gt;As an example of how this information might be used, we again consider the 2-class classification problem. Each observation is described by two functions, the average function &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{|C_1|}\sum_{j\in C_1}q_{ij}(\alpha)\)&lt;/span&gt; for comparisons with class 1 (&lt;span class=&#34;math inline&#34;&gt;\(C_1\)&lt;/span&gt;), and similarly the average function for comparisons with class 2. In line with the heuristic laid out above, it can be seen in figure 1 that for an observation from class 1, comparisons with class two tends to yield functions that have low density (so are slow to grow for small quantile levels) and large value for &lt;span class=&#34;math inline&#34;&gt;\(\alpha=1\)&lt;/span&gt; corresponding to high centrality. In-class comparisons have the opposite properties. A simple heuristic for solving this classification problem might be to use the first few loadings from an fPCA and your favorite classifier for Euclidean data. Results on several data sets using an untuned SVM were competitive with existing methods with extensive tuning.&lt;/p&gt;
&lt;p&gt;Finally, the construction only depends on the data via inner products, meaning that DQFs can be constructing on any data type for which a kernel is defined, for instance persistence diagrams in topological data analysis, allowing for a visualization of non-Euclidean data in addition to high (including infinite) dimensional Euclidean data.&lt;/p&gt;
&lt;p&gt;Reference: Chandler, G. and Polonik, W. “Multiscale geometric feature extraction for high-dimensional and non-Euclidean data with applications.” Ann. Statist. 49 (2) 988 - 1010, April 2021. (&lt;a href=&#34;https://arxiv.org/abs/1811.10178&#34; class=&#34;uri&#34;&gt;https://arxiv.org/abs/1811.10178&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Concentration Inequality in Machine Learning</title>
      <link>https://youngstats.github.io/post/2021/06/30/concentration-inequality-in-machine-learning/</link>
      <pubDate>Wed, 30 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/06/30/concentration-inequality-in-machine-learning/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;The fifth “One World webinar” organized by YoungStatS will take place on September 15th, 2021. Selected young European researchers active in the areas of probability and machine learning will present their recent contributions. The webinar is joint cooperation between the Young Researchers Committee of the Bernoulli Society and the YoungStatS project.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:
- Wednesday, September 15th, 17:00 CEST
- Online, via Zoom. The registration form will be provided on the YoungStatS website. Further details and the Zoom link will be sent to the registered addresses only.&lt;/p&gt;
&lt;p&gt;Speakers:
- Antoine Marchina (Université de Paris)
- Geoffrey Chinot (ETH Zurich)&lt;/p&gt;
&lt;p&gt;Discussant:
- Prof. Gábor Lugosi, ICREA Research Professor at Pompeu Fabra University and Barcelona GSE Research Professor, and Blackwell Lecture speaker at the upcoming Bernoulli-IMS 2021 World Congress in Probability and Statistics&lt;/p&gt;
&lt;p&gt;The webinar is part of YoungStatS project of the Young Statisticians Europe initiative (FENStatS) supported by the Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics (IMS).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-06-30-concentration-inequality-in-machine-learning_files/webinar1.jpg&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Optional stopping with Bayes factors: possibilities and limitations</title>
      <link>https://youngstats.github.io/post/2021/06/10/optional-stopping-with-bayes-factors-possibilities-and-limitations/</link>
      <pubDate>Thu, 10 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/06/10/optional-stopping-with-bayes-factors-possibilities-and-limitations/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In recent years, a surprising number of scientific results have failed
to hold up to continued scrutiny. Part of this ‘replicability crisis’
may be caused by practices that ignore the assumptions of traditional
(frequentist) statistical methods &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-john-2012-measur-preval&#34; role=&#34;doc-biblioref&#34;&gt;John, Loewenstein, and Prelec 2012&lt;/a&gt;)&lt;/span&gt;. One of
these assumptions is that the experimental protocol should be completely
determined upfront. In practice, researchers often adjust the protocol
due to unforeseen circumstances or collect data until a point has been
proven. This practice, which is referred to as &lt;em&gt;optional stopping&lt;/em&gt;, can
cause true hypotheses to be wrongly rejected much more often than these
statistical methods promise.&lt;br /&gt;
Bayes factor hypothesis testing has long been advocated as an
alternative to traditional testing that can resolve several of its
problems; in particular, it was claimed early on that Bayesian methods
continue to be valid under optional stopping
&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-lindley-1957-statis-parad&#34; role=&#34;doc-biblioref&#34;&gt;Lindley 1957&lt;/a&gt;; &lt;a href=&#34;#ref-RaiffaS61&#34; role=&#34;doc-biblioref&#34;&gt;Raiffa and Schlaifer 1961&lt;/a&gt;; &lt;a href=&#34;#ref-edwards-1963-bayes-statis&#34; role=&#34;doc-biblioref&#34;&gt;Edwards, Lindman, and Savage 1963&lt;/a&gt;)&lt;/span&gt;. In
light of the replicability crisis, such claims have received much
renewed interest
&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-wagenmakers-2007-pract-solut&#34; role=&#34;doc-biblioref&#34;&gt;Wagenmakers 2007&lt;/a&gt;; &lt;a href=&#34;#ref-rouder-2014-option&#34; role=&#34;doc-biblioref&#34;&gt;Jeffrey N. Rouder 2014&lt;/a&gt;; &lt;a href=&#34;#ref-schonbrodt-2017-sequen-hypot&#34; role=&#34;doc-biblioref&#34;&gt;Schönbrodt et al. 2017&lt;/a&gt;; &lt;a href=&#34;#ref-yu-2013-when-decis&#34; role=&#34;doc-biblioref&#34;&gt;Yu et al. 2014&lt;/a&gt;; &lt;a href=&#34;#ref-sanborn-2013-frequen-implic&#34; role=&#34;doc-biblioref&#34;&gt;Sanborn and Hills 2014&lt;/a&gt;)&lt;/span&gt;.
But what do they mean mathematically? It turns out that different
authors mean quite different things by ‘Bayesian methods handle optional
stopping’; moreover, such claims are often shown to hold only in an
informal sense, or in restricted contexts. In the paper
&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hendriksen2020optional&#34; role=&#34;doc-biblioref&#34;&gt;Hendriksen, Heide, and Grünwald 2020&lt;/a&gt;)&lt;/span&gt; we give a systematic overview and
formalization of such claims, and explain their relevance for practice:
can we effectively rely on Bayes factor testing to do a good job under
optional stopping or not? As we shall see, the answer is subtle.
Secondly, we extend the reach of such claims to more general settings,
for which they have never been formally verified and for which
verification is not always trivial. In the paper &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-heide2020optional&#34; role=&#34;doc-biblioref&#34;&gt; Heide and Grünwald 2020&lt;/a&gt;)&lt;/span&gt;,
we explain claims about optional stopping for an audience of
methodologists and applied statisticians with the help of computer
simulations.&lt;br /&gt;
&lt;/p&gt;
&lt;div id=&#34;bayesian-inference&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;Bayesian inference&lt;/h1&gt;
&lt;p&gt;Bayesianism is about a certain interpretation of the concept
probability: as &lt;em&gt;degrees of belief&lt;/em&gt;. A Bayesian first expresses this
belief as a probability function. We call this the prior distribution,
and we denote it by &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{P}(\theta)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is the
parameter (or several parameters) of the model. After the specification
of the prior, we obtain the data &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; and the likelihood
&lt;span class=&#34;math inline&#34;&gt;\(\mathbb{P}(D | \theta)\)&lt;/span&gt;. Now we can compute the &lt;em&gt;posterior
distribution&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{P}(\theta | D)\)&lt;/span&gt; with the help of &lt;em&gt;Bayes’
theorem&lt;/em&gt;: &lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\mathbb{P}(\theta | D) = \frac{\mathbb{P}(D | \theta) \mathbb{P}(\theta)}{\mathbb{P}(D)}.\end{aligned}\]&lt;/span&gt;
Suppose we want to test a null hypothesis &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_0\)&lt;/span&gt; against an
alternative hypothesis &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_1\)&lt;/span&gt;. We can do this in a Bayesian way
with &lt;em&gt;Bayes factors&lt;/em&gt;: we start with the &lt;em&gt;prior odds&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\mathbb{P}(\mathcal{H}_1) / \mathbb{P}(\mathcal{H}_0)\)&lt;/span&gt;, our belief
before seeing the data. Often we believe that both hypotheses are
equally probable, then our prior odds are 1-to-1. Next we gather our
data &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;, and update our odds with the new knowledge, using Bayes’
theorem: &lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\text{posterior odds}(\mathcal{H}_1 \text{ vs. } \mathcal{H}_0) = \frac{\mathbb{P}(\mathcal{H}_1 | D)}{\mathbb{P}(\mathcal{H}_0 | D)} = \frac{\mathbb{P}(\mathcal{H}_1)}{\mathbb{P}(\mathcal{H}_0)} \frac{\mathbb{P}(D | \mathcal{H}_1)}{\mathbb{P}(D | \mathcal{H}_0)}.\end{aligned}\]&lt;/span&gt;
The posterior odds is our updated belief about which hypothesis is more
likely.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;three-notions-of-optional-stopping&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;Three notions of optional stopping&lt;/h1&gt;
&lt;p&gt;Validity under optional stopping is a desirable property of hypothesis
testing: we gather some data, look at the results, and decide whether we
stop of gather some additional data. Informally, we call ‘peeking at the
results to decide whether to collect more data’ &lt;em&gt;optional stopping&lt;/em&gt;, but
if we want to make more precise what it means if we say that a test can
handle optional stopping, it turns out that different approaches
(frequentist, subjective Bayesian and objective Bayesian) lead to
different interpretations and definitions. It tuns out that we can
discern three main mathematical concepts of handling optional stopping,
which we identify and formally define in the paper
&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hendriksen2020optional&#34; role=&#34;doc-biblioref&#34;&gt;Hendriksen, Heide, and Grünwald 2020&lt;/a&gt;)&lt;/span&gt;.&lt;br /&gt;
The first concept we call &lt;em&gt;subjective Bayesian optional stopping&lt;/em&gt; or
&lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;-independence. If one considers a purely subjective Bayesian
setting, appropriate if one truly believes one’s prior, then Bayesian
updating from prior to posterior is not affected by the employed
stopping rule: one ends up with the same posterior if one had decided
the sample size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; in advance, or if it had been determined, for
example, because one was satisfied with the result at this &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. In this
sense a subjective Bayesian procedure does not depend on the stopping
rule.&lt;br /&gt;
The second sense of optional stopping we call &lt;em&gt;calibration&lt;/em&gt;. As
&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-rouder-2014-option&#34; role=&#34;doc-biblioref&#34;&gt;Jeffrey N. Rouder 2014&lt;/a&gt;)&lt;/span&gt; writes: ‘If a replicate experiment yielded a
posterior odds of 3.5-to-1 in favor of the null, then we expect that the
null was 3.5 times as probable as the alternative to have produced the
data.’ In more mathematical language, this can be expressed as
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\text{post-odds} (\mathcal{H}_1 \text{ vs. } \mathcal{H}_0 | ``\text{post-odds} (\mathcal{H}_1 \text{ vs. } \mathcal{H}_0 | D ) = a&amp;quot;) = a.\end{aligned}\]&lt;/span&gt;
We say this equation expresses &lt;em&gt;calibration of the posterior odds&lt;/em&gt;. It
turns out that this calibration fails to hold if one does not adhere to
a purely subjective Bayesian view, in particular, it does not hold for
the &lt;em&gt;default&lt;/em&gt; priors the Bayesian psychology community is advocating
&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-wagenmakers-2007-pract-solut&#34; role=&#34;doc-biblioref&#34;&gt;Wagenmakers 2007&lt;/a&gt;; &lt;a href=&#34;#ref-rouder-2012-default-bayes&#34; role=&#34;doc-biblioref&#34;&gt;J. N. Rouder et al. 2012&lt;/a&gt;)&lt;/span&gt;. To get a
first idea of one of the issues: default priors sometimes depend on the
data. Then it is unclear what &lt;em&gt;optional stopping&lt;/em&gt; really means, because
if, using prior &lt;span class=&#34;math inline&#34;&gt;\(P_1(\theta)\)&lt;/span&gt; based on a sample of size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, one had
stopped at sample size &lt;span class=&#34;math inline&#34;&gt;\(n&amp;#39;&amp;lt; n\)&lt;/span&gt;, one should have really used prior
&lt;span class=&#34;math inline&#34;&gt;\(P&amp;#39;_1(\theta)\)&lt;/span&gt; based on sample of size &lt;span class=&#34;math inline&#34;&gt;\(n&amp;#39;\)&lt;/span&gt;...but then one would have
stopped at yet another sample size &lt;span class=&#34;math inline&#34;&gt;\(n&amp;#39;&amp;#39;\)&lt;/span&gt;, and so on. See our paper
&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-heide2020optional&#34; role=&#34;doc-biblioref&#34;&gt; Heide and Grünwald 2020&lt;/a&gt;)&lt;/span&gt; for an extensive discussion and many examples.&lt;br /&gt;
&lt;/p&gt;
&lt;p&gt;The third sense is a frequentist interpretation of handling optional
stopping, which is about controlling the Type I error of an experiment.
A Type I error occurs when we reject the null hypothesis when it is
true, also called &lt;em&gt;false positive&lt;/em&gt;. The frequentist interpretation of
handling optional stopping is that the Type I error guarantee holds if
we do not determine the sampling plan — and thus the stopping rule —
in advance, but we may stop when we see significant results. In the case
&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_0\)&lt;/span&gt; is &lt;em&gt;simple&lt;/em&gt; (containing just one hypothesis), there is a
well-known intriguing connection between Bayes factors and Type I error
probabilities: if we reject &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_0\)&lt;/span&gt; iff the posterior odds in
favor of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_0\)&lt;/span&gt; are smaller than some fixed level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;,
then we are guaranteed a Type I error of at most &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. And
interestingly, this holds not just for fixed sample sizes but even under
optional stopping. However, for &lt;em&gt;composite&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_0\)&lt;/span&gt; this does
not continue to hold. Except for the special case where &lt;em&gt;all&lt;/em&gt; free
parameters in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_0\)&lt;/span&gt; are nuisance parameters observing a group
structure and equipped with the corresponding right-Haar prior, and are
shared with &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_1\)&lt;/span&gt;, as we prove in &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hendriksen2020optional&#34; role=&#34;doc-biblioref&#34;&gt;Hendriksen, Heide, and Grünwald 2020&lt;/a&gt;)&lt;/span&gt;.
But for general priors and composite &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_0\)&lt;/span&gt;, this is typically
not the case.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-06-09-optional-stopping-with-bayes-factors-possibilities-and-limitations_files/screenshot.138.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 1: Posterior odds in an experiment of testing whether the mean of a normal distribution is 0
(&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_0\)&lt;/span&gt;), versus non-zero (&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_1\)&lt;/span&gt;), from 20; 000 replicate experiments. (a) The empirical sampling distribution
of the posterior odds as a histogram under &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_0\)&lt;/span&gt; (blue) and &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_1\)&lt;/span&gt; (pink). (b) Calibration plot: the observed
posterior odds as a function of the nominal posterior odds.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;One can give three distinct mathematical meanings to the notion of
&lt;em&gt;optional stopping&lt;/em&gt;. Whether or not we can say that ‘the Bayes factor
method can handle optional stopping’ in practice is a subtle matter,
depending on the specifics of the given situation: what models are used,
what priors, and what is the goal of the analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;authors&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Authors&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-06-09-optional-stopping-with-bayes-factors-possibilities-and-limitations_files/Allard.jpg&#34; height=&#34;70&#34; /&gt;
Allard Hendriksen, CWI Amsterdam&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-06-09-optional-stopping-with-bayes-factors-possibilities-and-limitations_files/Rianne.jpg&#34; height=&#34;70&#34; /&gt;
dr. Rianne de Heide, Leiden University &amp;amp; CWI Amsterdam&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-06-09-optional-stopping-with-bayes-factors-possibilities-and-limitations_files/Peter.jpg&#34; height=&#34;70&#34; /&gt;
prof. Peter Grünwald, Leiden University &amp;amp; CWI Amsterdam&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-edwards-1963-bayes-statis&#34; class=&#34;csl-entry&#34;&gt;
Edwards, Ward, Harold Lindman, and Leonard J. Savage. 1963. &lt;span&gt;“Bayesian Statistical Inference for Psychological Research.”&lt;/span&gt; &lt;em&gt;Psychological Review&lt;/em&gt; 70 (3): 193–242. &lt;a href=&#34;https://doi.org/10.1037/h0044139&#34;&gt;https://doi.org/10.1037/h0044139&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-heide2020optional&#34; class=&#34;csl-entry&#34;&gt;
 Heide, Rianne, and Peter D. Grünwald. 2020. &lt;span&gt;“Why Optional Stopping Can Be a Problem for Bayesians.”&lt;/span&gt; &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, Advance Publication&lt;/em&gt;, 1–18.
&lt;/div&gt;
&lt;div id=&#34;ref-hendriksen2020optional&#34; class=&#34;csl-entry&#34;&gt;
Hendriksen, Allard, Rianne Heide, and Peter Grünwald. 2020. &lt;span&gt;“Optional Stopping with Bayes Factors: A Categorization and Extension of Folklore Results, with an Application to Invariant Situations.”&lt;/span&gt; &lt;em&gt;Bayesian Analysis, Advance Publication&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-john-2012-measur-preval&#34; class=&#34;csl-entry&#34;&gt;
John, Leslie K, George Loewenstein, and Drazen Prelec. 2012. &lt;span&gt;“Measuring the Prevalence of Questionable Research Practices with Incentives for Truth Telling.”&lt;/span&gt; &lt;em&gt;Psychological Science&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-lindley-1957-statis-parad&#34; class=&#34;csl-entry&#34;&gt;
Lindley, D. V. 1957. &lt;span&gt;“A Statistical Paradox.”&lt;/span&gt; &lt;em&gt;Biometrika&lt;/em&gt; 44 (1/2): 187–92. &lt;a href=&#34;https://doi.org/10.2307/2333251&#34;&gt;https://doi.org/10.2307/2333251&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-RaiffaS61&#34; class=&#34;csl-entry&#34;&gt;
Raiffa, H., and R. Schlaifer. 1961. &lt;em&gt;Applied Statistical Decision Theory&lt;/em&gt;. Cambridge, MA: Harvard University Press.
&lt;/div&gt;
&lt;div id=&#34;ref-rouder-2012-default-bayes&#34; class=&#34;csl-entry&#34;&gt;
Rouder, J N, R D Morey, P L Speckman, and J M Province. 2012. &lt;span&gt;“Default &lt;span&gt;B&lt;/span&gt;ayes Factors for &lt;span&gt;ANOVA&lt;/span&gt; Designs.”&lt;/span&gt; &lt;em&gt;Journal of Mathematical Psychology&lt;/em&gt; 56 (5): 356–74.
&lt;/div&gt;
&lt;div id=&#34;ref-rouder-2014-option&#34; class=&#34;csl-entry&#34;&gt;
Rouder, Jeffrey N. 2014. &lt;span&gt;“Optional Stopping: No Problem for &lt;span&gt;B&lt;/span&gt;ayesians.”&lt;/span&gt; &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt; 21 (2): 301–8. &lt;a href=&#34;https://doi.org/10.3758/s13423-014-0595-4&#34;&gt;https://doi.org/10.3758/s13423-014-0595-4&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-sanborn-2013-frequen-implic&#34; class=&#34;csl-entry&#34;&gt;
Sanborn, Adam N., and Thomas T. Hills. 2014. &lt;span&gt;“The Frequentist Implications of Optional Stopping on &lt;span&gt;B&lt;/span&gt;ayesian Hypothesis Tests.”&lt;/span&gt; &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt; 21 (2): 283–300. &lt;a href=&#34;https://doi.org/10.3758/s13423-013-0518-9&#34;&gt;https://doi.org/10.3758/s13423-013-0518-9&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-schonbrodt-2017-sequen-hypot&#34; class=&#34;csl-entry&#34;&gt;
Schönbrodt, Felix D., Eric-Jan Wagenmakers, Michael Zehetleitner, and Marco Perugini. 2017. &lt;span&gt;“Sequential Hypothesis Testing with &lt;span&gt;B&lt;/span&gt;ayes Factors: Efficiently Testing Mean Differences.”&lt;/span&gt; &lt;em&gt;Psychological Methods&lt;/em&gt; 22 (2): 322–39. &lt;a href=&#34;https://doi.org/10.1037/met0000061&#34;&gt;https://doi.org/10.1037/met0000061&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-wagenmakers-2007-pract-solut&#34; class=&#34;csl-entry&#34;&gt;
Wagenmakers, Eric-Jan. 2007. &lt;span&gt;“A Practical Solution to the Pervasive Problems of p Values.”&lt;/span&gt; &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt; 14 (5): 779–804. &lt;a href=&#34;https://doi.org/10.3758/bf03194105&#34;&gt;https://doi.org/10.3758/bf03194105&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-yu-2013-when-decis&#34; class=&#34;csl-entry&#34;&gt;
Yu, Erica C., Amber M. Sprenger, Rick P. Thomas, and Michael R. Dougherty. 2014. &lt;span&gt;“When Decision Heuristics and Science Collide.”&lt;/span&gt; &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt; 21 (2): 268–82. &lt;a href=&#34;https://doi.org/10.3758/s13423-013-0495-z&#34;&gt;https://doi.org/10.3758/s13423-013-0495-z&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Spatiotemporal modeling and real-time prediction of origin-destination traffic demand</title>
      <link>https://youngstats.github.io/post/2021/06/09/spatiotemporal-modeling-and-real-time-prediction-of-origin/</link>
      <pubDate>Wed, 09 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/06/09/spatiotemporal-modeling-and-real-time-prediction-of-origin/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the past decades, intelligent transportation system (ITS) has brought
advanced technology that enables a data-rich environment and
unprecedented opportunities for traffic prediction, which is considered
as one of the most prevalent issues facing ITS (Li et al., 2015). We
discuss the online prediction of the origin-destination (OD) demand
count in traffic networks, which represents the number of trips between
certain combinations of an origin and a destination. The study of OD
demand prediction based on count data has a growing impact on many
traffic control and management policies (Ashok, 1996, Ashok and
Ben-Akiva, 2002, Li, 2005, Hazelton, 2008, Shao et al., 2014). For
example, dynamic OD demand prediction is critical in planning for the
charging services of the electrical vehicles (EV; Zhang et al., 2017). A
well-designed charging facility network is necessary to extend the
vehicle range and popularize the use of EVs. In particular, the dynamic
demand between nodes of the traffic network plays a key role in
determining the availability of the charging facilities, planning the
multi-period charging schedules, and meeting the customer needs at the
maximum extent (Zhang et al., 2017, Brandstatter et al., 2017). The
objective of this study is to appropriately model the stochastic OD
traffic demand counts considering the spatiotemporal correlations
between different routes and epochs, while incorporating physical
knowledge of the traffic network in the estimation. The estimation
results are expected to enhance the prediction accuracy and robustness
of the online traffic demand prediction for future epochs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Model and method&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We investigate a multivariate Poisson log-normal model with a
block-diagonal covariance matrix and incorporate domain knowledge of the
traffic network features to account for spatial correlations. Let
&lt;span class=&#34;math inline&#34;&gt;\(N_{\text{ijt}}\)&lt;/span&gt; denote the observed traffic demand (i.e., the count of
vehicles) for route &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; on day &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, at epoch &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. Based on the natural
characteristics of the demand counts, it is reasonable to model each
observation &lt;span class=&#34;math inline&#34;&gt;\(N_{\text{ijt}}\)&lt;/span&gt; with a Poisson log-linear model (Perrakis
et al., 2014, Xian et al. 2018) such that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[N_{\text{ijt}}\sim\text{Poisson}\left( \lambda_{\text{ijt}} \right),u_{\text{ijt}} = \log\lambda_{\text{ijt}}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{\text{ijt}}\)&lt;/span&gt; is the intensity of the Poisson process, and
&lt;span class=&#34;math inline&#34;&gt;\(u_{\text{ijt}}\)&lt;/span&gt; is the log transformation of the intensity. To
characterize the spatiotemporal correlations across different routes and
time points, we model &lt;span class=&#34;math inline&#34;&gt;\(u_{\text{ijt}}\)&lt;/span&gt; as a mixed-effect Gaussian
process based on &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; basis functions &lt;span class=&#34;math inline&#34;&gt;\(B_{k}(t)\)&lt;/span&gt; that&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math display&#34;&gt;\[u_{\text{ijt}} = \mu_{\text{jt}} + \sum_{k = 1}^{K}{\gamma_{\text{jk}}B_{k}(t)} + Z_{\text{ijt}}.\]&lt;/span&gt; (1)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\mu =}\left\lbrack \mu_{11},\mu_{12},\ \cdots,\ \mu_{\text{JT}} \right\rbrack\mathbf{&amp;#39;}\)&lt;/span&gt;
is the fixed effect coefficient that models the common characteristics
of the whole traffic network, and
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\gamma}_{k}\mathbf{=}\left\lbrack \gamma_{1k},\ \gamma_{2k},\ \cdots,\ \gamma_{\text{Jk}} \right\rbrack\)&lt;/span&gt;
is the random effect coefficient with prior distribution
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\gamma}_{k}\sim N(0,\ \mathbf{R}_{\theta_{y}})\)&lt;/span&gt; that
characterizes the uniqueness of different routes. Here
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{R}_{\theta_{y}}\)&lt;/span&gt; is the correlation matrix which takes into
consideration of the traffic network information, where
&lt;span class=&#34;math inline&#34;&gt;\(\left\lbrack \mathbf{R}_{\theta_{y}} \right\rbrack_{j_{1},j_{2}} = \sigma_{j_{1},\ j_{2}}\exp\left\{ - \theta_{y}\left| \mathbf{y}_{j_{1}} - \mathbf{y}_{j_{2}} \right|^{2} \right\}\)&lt;/span&gt;.
In this expression, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{y}_{j}\)&lt;/span&gt; denotes the unique features of
route &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, such as information about the origin and destination, the
maximum speed limit on a route, and the travel distance. The term
&lt;span class=&#34;math inline&#34;&gt;\(Z_{\text{ijt}}\)&lt;/span&gt; in model (1) is the random error that follows a
Gaussian distribution which has the covariance structure&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{cov}\left( Z_{ij_{1}t_{1}},Z_{ij_{2}t_{2}} \right) = \sigma_{j_{1},\ j_{2}}\exp\left\{ - \theta_{y}\left| \mathbf{y}_{j_{1}} - \mathbf{y}_{j_{2}} \right|^{2} \right\} \cdot \tau^{2}\exp\left\{ - \theta_{t}\left| t_{1} - t_{2} \right| \right\}.\]&lt;/span&gt; (2)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Which depends on both the features of routes &lt;span class=&#34;math inline&#34;&gt;\(j_{1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j_{2}\)&lt;/span&gt;, and
the time points &lt;span class=&#34;math inline&#34;&gt;\(t_{1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(t_{2}\)&lt;/span&gt;, which we refer to as the spatial
and temporal covariance structures, respectively.&lt;/p&gt;
&lt;p&gt;Denote the log-transformed intensity of the OD traffic demand on day &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;
as
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{u}_{i} = \left( u_{i11},u_{i12},\ldots,u_{\text{iJT}} \right)^{&amp;#39;}\)&lt;/span&gt;.
We can further derive that conditioning on parameters
&lt;span class=&#34;math inline&#34;&gt;\(\left( \mathbf{\mu},\ \theta_{y},\theta_{t},\mathbf{\sigma,\ }\tau^{2} \right)\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{u}_{i}\)&lt;/span&gt; follows normal distribution
&lt;span class=&#34;math inline&#34;&gt;\(N\left( \mathbf{\mu},\ \mathbf{\Sigma} \right)\)&lt;/span&gt;, where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathbf{\mu} = \left( \mu_{11},\mu_{12},\ \cdots,\ \mu_{\text{JT}} \right)^{&amp;#39;},\]&lt;/span&gt;&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathbf{\Sigma} = \mathbf{R}_{\theta_{y}}\bigotimes\left\lbrack \mathbf{R}_{B} + \tau^{2}\mathbf{R}_{\theta_{t}} \right\rbrack.\]&lt;/span&gt; (3)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here, the symbol &lt;span class=&#34;math inline&#34;&gt;\(\bigotimes\)&lt;/span&gt; denotes the Kronecker product,
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{R}_{B}\)&lt;/span&gt; is a fixed &lt;span class=&#34;math inline&#34;&gt;\(T \times T\)&lt;/span&gt; matrix with the &lt;span class=&#34;math inline&#34;&gt;\((t_{1},t_{2})\)&lt;/span&gt;
element equal to &lt;span class=&#34;math inline&#34;&gt;\(\sum_{k = 1}^{K}{B_{k}(t_{1})B_{k}(t_{2})}\)&lt;/span&gt;, and
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{R}_{\theta_{t}}\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(T \times T\)&lt;/span&gt; matrix with the
&lt;span class=&#34;math inline&#34;&gt;\((t_{1},t_{2})\)&lt;/span&gt; element equal to
&lt;span class=&#34;math inline&#34;&gt;\(\exp\left\{ - \theta_{t}|t_{1} - t_{2}| \right\}\)&lt;/span&gt;. Therefore, the large
covariance matrix is parametrized based on only the parameters
&lt;span class=&#34;math inline&#34;&gt;\(\theta_{y},\theta_{t},\mathbf{\sigma}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\tau^{2}\)&lt;/span&gt;. This
parsimonious model has several advantages, such as high interpretability
tailored to the traffic demand count data, increased stability of the
estimation results, and reduced computational burden for parameter
estimation. We treat &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{u}\)&lt;/span&gt; as a latent variable and further
employ the EM algorithm to obtain the maximum likelihood estimation
(MLE) for the parameters.&lt;/p&gt;
&lt;p&gt;In this way, we can fully explore the complicated spatiotemporal
correlation structure of the traffic network demand and automatically
cluster the routes with high correlations, without introducing a large
number of parameters that impact the estimation accuracy. Besides
transportation systems, the proposed method can be easily extended to
other network applications with count data through few modifications,
such as communication systems, supply chain management, smart grid, or
even three-dimensional networks (Wang et al., 2018).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case study&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We apply the proposed method to a real New York yellow taxi dataset
which is collected from June 1&lt;sup&gt;st&lt;/sup&gt; to July 31&lt;sup&gt;st&lt;/sup&gt; in 2017 (NYC taxi,
2017). The dataset records all yellow taxi trips during the
aforementioned time period including the pick-up and drop-off dates and
times, pick-up and drop-off locations, trip distances, and payment
information about the trips. We focus on the trips between the four
busiest zones in Manhattan and investigate the structure of the travel
demand counts on these zones as OD pairs. The details of the four taxi
zones are shown in Figure 1.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-06-10-spatiotemporal-modeling-and-real-time-prediction-of-origin_files/f1.jpg&#34; /&gt;&lt;/p&gt;
&lt;table style=&#34;width:97%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;14%&#34; /&gt;
&lt;col width=&#34;24%&#34; /&gt;
&lt;col width=&#34;58%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Index&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Borough&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Zones&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Manhattan&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Lincoln Square East&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Manhattan&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Times Square/Theatre District&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Manhattan&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Upper East Side North&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Manhattan&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Upper East Side South&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;Figure 1. Illustration of the taxi zones in the case study&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Figure 2 further shows the specific taxi demand prediction results of
two routes &lt;span class=&#34;math inline&#34;&gt;\((4,\ 3)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\((4,\ 4)\)&lt;/span&gt; for four test days. The solid black
line in this figure represents the true dynamic traffic demand counts,
where it can be observed that the true taxi demand indeed exhibits high
spatial and temporal variation and strong correlations for observations
between the routes and across different epochs. The solid red line in is
the predicted demand using the proposed method, and the dashed error
bars show the 90% confidence interval of the prediction based on the
variance derivation in equation (7), which significantly outperforms the
existing method shown in black dotted line.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-06-10-spatiotemporal-modeling-and-real-time-prediction-of-origin_files/f2.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 2. Taxi demand prediction results for routes &lt;span class=&#34;math inline&#34;&gt;\((4,\ 3)\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\((4,\ 4)\)&lt;/span&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Recent Advances in Functional Data Analysis</title>
      <link>https://youngstats.github.io/post/2021/04/29/fda-webinar/</link>
      <pubDate>Thu, 29 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/04/29/fda-webinar/</guid>
      <description>&lt;p&gt;The fourth &amp;ldquo;&lt;em&gt;One World webinar&lt;/em&gt;&amp;rdquo; organized by YoungStatS will take place
on June 30th, 2021. The topic of this webinar is on Functional Data
Analysis. Selected young European researchers active in this area of
research will present their contributions on spherical functional
autoregressions, additive models, and clustering methods for functional
data, with the focus on both theoretical developments and applications.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Wednesday, June 30th, 16:30 CEST&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Online&lt;/em&gt;, via Zoom. The registration form is available &lt;a href=&#34;https://docs.google.com/forms/d/1efH13r__c5v-GHC5MsxzTlS0Csps_f4YHr622717STY/viewform?edit_requested=true&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://people.epfl.ch/alessia.caponera/?lang=en&#34;&gt;Alessia Caponera&lt;/a&gt; (École polytechnique fédérale de Lausanne, Switzerland): &amp;ldquo;Asymptotics for spherical functional autoregressions&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.wiwi.hu-berlin.de/en/Professorships/vwl/statistik/team/volkmale&#34;&gt;Alexander Volkmann&lt;/a&gt; (Humboldt-Universität zu Berlin, Germany): &amp;ldquo;Multivariate Functional Additive Mixed Models&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.dii.unina.it/page.php?tabella=livello4&amp;amp;id_livello=272&amp;amp;flag=persona&amp;amp;livello1=1&amp;amp;livello2=53&amp;amp;livello3=25&amp;amp;lang=en&#34;&gt;Fabio Centofanti&lt;/a&gt; (University of Naples Federico II, Italy): &amp;ldquo;Sparse and Smooth Functional Data Clustering&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.math-evry.cnrs.fr/members/Jpark/welcome&#34;&gt;Juhyun Park&lt;/a&gt; (École nationale supérieure d&#39;informatique pour
l&#39;industrie et l&#39;entreprise, France)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;The webinar is part of YoungStatS project of the Young Statisticians Europe initiative (FENStatS) supported by the Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics (IMS).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For more information, please visit our
&lt;a href=&#34;https://youngstats.github.io/post/2020/09/10/webinars/&#34;&gt;website&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Composite-Based Structural Equation Modeling: Developments and Perspectives</title>
      <link>https://youngstats.github.io/post/2021/04/28/csem-webinar/</link>
      <pubDate>Wed, 28 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/04/28/csem-webinar/</guid>
      <description>&lt;p&gt;The third &amp;ldquo;&lt;em&gt;One World webinar&lt;/em&gt;&amp;rdquo; organized by YoungStatS will take place
on May 19th, 2021. The focus of this webinar will be on composite-based
structural equation modeling, particularly on partial least squares path
modeling (Wold, 1982; Lohmöller, 1989) and approaches to assess
composite models. The webinar will present some of the most interesting
and recent theoretical developments and applications from younger
scholars active in this area of research.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Wednesday, May 19th, 16:00 CEST&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Online&lt;/em&gt;, via Zoom. The registration form is available &lt;a href=&#34;https://docs.google.com/forms/d/19ozVQ71vZJi0_g_GYUC29w_xHnD9ub5HNC9sWbYhhvc/edit&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://econ.au.dk/research/researchcentres/creates/people/research-fellows/benjamin-d-liengaard/&#34;&gt;Benjamin Liengaard&lt;/a&gt; (Aarhus University, Denmark): &amp;lsquo;Measurement Invariance Testing with Latent Variable Scores using Partial Least Squares Path Modeling&amp;rsquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://nicholasdanks.com/&#34;&gt;Nicholas Danks&lt;/a&gt; (Trinity College Dublin, Ireland): &amp;lsquo;The Role of Prediction in Composite Modeling&amp;rsquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.florianschuberth.com/&#34;&gt;Florian Schuberth&lt;/a&gt; (University of Twente, The Netherlands): &amp;lsquo;Confirmatory Composite Analysis&amp;rsquo;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.henseler.com/&#34;&gt;Jörg Henseler&lt;/a&gt; (University of Twente, The Netherlands)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;The webinar is part of YoungStatS project of the Young Statisticians Europe initiative (FENStatS) supported by the Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics (IMS).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For more information, please visit our
&lt;a href=&#34;https://youngstats.github.io/post/2020/09/10/webinars/&#34;&gt;website&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A small step to understand Generative Adversarial Networks</title>
      <link>https://youngstats.github.io/post/2021/04/27/a-small-step-to-understand-gda/</link>
      <pubDate>Tue, 27 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/04/27/a-small-step-to-understand-gda/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In the last decade, there have been spectacular advances on the practical side of machine learning.
One of the most impressive may be the success of Generative Adversarial Networks (GANs) for image generation &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-GoPoMiXuWaOzCoBe14&#34; role=&#34;doc-biblioref&#34;&gt;Goodfellow et al. 2014&lt;/a&gt;)&lt;/span&gt;.
State of the art models are capable of producing &lt;a href=&#34;https://www.youtube.com/watch?v=XOxxPcy5Gr4&#34;&gt;portraits of fake persons&lt;/a&gt; that look perfectly authentic to you and me (see e.g. &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-SaGoZaChRaCg16&#34; role=&#34;doc-biblioref&#34;&gt;Salimans et al. 2016&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-Karras2018&#34; role=&#34;doc-biblioref&#34;&gt;Karras et al. 2018&lt;/a&gt;)&lt;/span&gt;).
Other domains such as inpainting, text to image and speech are also concerned by outstanding results (see &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-Go16&#34; role=&#34;doc-biblioref&#34;&gt;Goodfellow 2016&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-JaLiBo20&#34; role=&#34;doc-biblioref&#34;&gt;Jabbar, Li, and Bourahla 2020&lt;/a&gt;)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Since their introduction by &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-GoPoMiXuWaOzCoBe14&#34; role=&#34;doc-biblioref&#34;&gt;Goodfellow et al. 2014&lt;/a&gt;)&lt;/span&gt;, GANs have unleashed passions in the community of machine learning, leading to a large volume of variants and possible applications, often referred to as &lt;a href=&#34;https://github.com/hindupuravinash/the-gan-zoo&#34;&gt;the GAN Zoo&lt;/a&gt;.
However, despite increasingly spectacular applications, little was known few years ago about the statistical properties of GANs.&lt;/p&gt;
&lt;p&gt;This post sketches the paper entitled ``Some Theoretical Properties of GANs’’ (G. Biau, B. Cadre, M. Sangnier and U. Tanielian, The Annals of Statistics, 2020),
which aims at building a statistical analysis of GANs in order to better understand their mathematical mechanism.
In particular, it proves a non-asymptotic bound on the excess of Jensen-Shannon error and the asymptotic normality of the parametric estimator.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathematical-framework&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mathematical framework&lt;/h2&gt;
&lt;div id=&#34;overview-of-the-method&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Overview of the method&lt;/h3&gt;
&lt;p&gt;The objective of GANs is to randomly generate artificial contents similar to some data.
Put another way, they are aimed at sampling according to an unknown distribution &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt;, based solely on i.i.d. observations &lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt; drawn according to &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt;.
Obviously, a naive approach would be to:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Estimate the distribution &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt; by some &lt;span class=&#34;math inline&#34;&gt;\(\hat P\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Sample according to &lt;span class=&#34;math inline&#34;&gt;\(\hat P\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;However, both tasks are difficult in themselves.
In particular, density estimation is made arduous by the complexity and high dimensionality of the data involved in the domain, relegating both standard parametric and nonparametric approaches unworkable.
Thus, GANs offer a completely different way to achieve our goal, often compared to the struggle between a police team, trying to distinguish true banknotes (the observed data &lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt;) from false ones (the generated data), and a counterfeiters team (the generator), slaving to produce banknotes as credible as possible and to mislead the police.&lt;/p&gt;
&lt;p&gt;To be a bit more specific, there are two brilliant ideas at the core of GANs:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Sample data in a very straightforward manner thanks to the transform method:
let &lt;span class=&#34;math inline&#34;&gt;\(\mathscr G=\left \{G_{\theta}: \mathbb R^\ell \to \mathbb R^d, \theta \in \Theta \right\}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\ell\)&lt;/span&gt; is the dimension of what is called the latent space and &lt;span class=&#34;math inline&#34;&gt;\(\Theta \subset \mathbb R^p\)&lt;/span&gt;, be a class of measurable functions, called generators (in practice &lt;span class=&#34;math inline&#34;&gt;\(\mathscr G\)&lt;/span&gt; is often a class of neural networks with &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; parameters).
Now, let us sample &lt;span class=&#34;math inline&#34;&gt;\(Z \sim \mathcal N(0, I_\ell)\)&lt;/span&gt; and compute &lt;span class=&#34;math inline&#34;&gt;\(U = G_\theta(Z)\)&lt;/span&gt;.
Then, &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is an observation drawn according to the distribution &lt;span class=&#34;math inline&#34;&gt;\(P_\theta = G_\theta \# N(0, I_\ell)\)&lt;/span&gt; (the push-forward measure of the latent distribution (N(0, I_)) according to (G_)).
In other words, the statistical model for the estimation of &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt; has the form &lt;span class=&#34;math inline&#34;&gt;\(\mathscr P = \left\{ P_\theta = G_\theta \# N(0, I_\ell), \theta \in \Theta \right\}\)&lt;/span&gt; and it is definitely straightforward to sample according to &lt;span class=&#34;math inline&#34;&gt;\(P_\theta\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Assessing the proximity between &lt;span class=&#34;math inline&#34;&gt;\(P_\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt; by comparing two samples &lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n \overset{i.i.d.}{\sim} P^\star\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(U_1, \dots, U_n \overset{i.i.d.}{\sim} P_\theta\)&lt;/span&gt;.
What does comparing mean?
Assume the group of &lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt; is very difficult to ``separate’’ from the group of &lt;span class=&#34;math inline&#34;&gt;\(U_1, \dots, U_n\)&lt;/span&gt;, or put another way,
it is very difficult to distinguish the class of &lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt; from the class of &lt;span class=&#34;math inline&#34;&gt;\(U_1, \dots, U_n\)&lt;/span&gt;.
Would you be convinced that the two distributions &lt;span class=&#34;math inline&#34;&gt;\(P_\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt; are very close (at least for large &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;)?
That is exactly the point.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-distributions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Comparing distributions&lt;/h3&gt;
&lt;p&gt;At this point, Task 2 is still a bit blurry and deserves further details about how to quantify the difficulty (or the ease) of separating the two classes &lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(U_1, \dots, U_n\)&lt;/span&gt;.
This problem is actually closely related to supervised learning, and in particular to classification:
assume that a classifier, let us say &lt;span class=&#34;math inline&#34;&gt;\(h : \mathbb R^d \to \{0, 1\}\)&lt;/span&gt;, manages to perfectly discriminate the two classes: &lt;span class=&#34;math inline&#34;&gt;\(\mathbb P(h(X_1)=1) = \mathbb P(h(U_1)=0) = 1\)&lt;/span&gt;, then we can say that the two distributions &lt;span class=&#34;math inline&#34;&gt;\(P_\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt; are different.
Conversely, if the classifier is fooled, that is &lt;span class=&#34;math inline&#34;&gt;\(\mathbb P(h(X_1)=1) = \mathbb P(h(U_1)=0) = \frac 12\)&lt;/span&gt;, we may accept that the two distributions are identical.&lt;/p&gt;
&lt;p&gt;This classification setting is formalized as following:
let &lt;span class=&#34;math inline&#34;&gt;\((X, Y)\)&lt;/span&gt; be a pair of random variables taking values in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb R^d \times \{0, 1\}\)&lt;/span&gt; such that:
&lt;span class=&#34;math display&#34;&gt;\[
    X|Y=1 \sim P^\star
    \quad \text{and} \quad
    X|Y=0 \sim P_\theta,
    %\tag{M}
\]&lt;/span&gt;
and let &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D = \left \{D_{\alpha} : \mathbb R^d \to [0, 1], \alpha \in \Lambda \right\}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\Lambda \subset \mathbb R^q\)&lt;/span&gt;, be a parametric model of discriminators such that &lt;span class=&#34;math inline&#34;&gt;\(D_\alpha(x)\)&lt;/span&gt; is aimed at estimating &lt;span class=&#34;math inline&#34;&gt;\(\mathbb P(Y=1 | X=x)\)&lt;/span&gt; (put another way, the distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y|X=x\)&lt;/span&gt; is estimated by &lt;span class=&#34;math inline&#34;&gt;\(\mathcal B(D_\alpha(x))\)&lt;/span&gt;).
For a given discriminator &lt;span class=&#34;math inline&#34;&gt;\(D_\alpha\)&lt;/span&gt;, the corresponding classifier is &lt;span class=&#34;math inline&#34;&gt;\(h : x \in \mathbb R^d \mapsto \mathbb 1_{D_\alpha(x) &amp;gt; \frac 12}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The sample ({ (X_1, 1), , (X_n, 1), (U_1, 0), , (U_n, 0) }), previously build by putting together observed and generated data, fits the classification model and can serve for estimating a classifier by maximizing the conditional log-likelihood:
&lt;span class=&#34;math display&#34;&gt;\[
  \hat \alpha \in \operatorname*{arg\,max}_{\alpha \in \Lambda} \hat L(\theta, \alpha),
  \quad \text{where} \quad
  \hat L(\theta, \alpha) = \frac 1n \sum_{i=1}^n \log(D_\alpha(X_i)) + \frac 1n \sum_{i=1}^n \log(1-D_\alpha(U_i)).
\]&lt;/span&gt;
In addition, the maximal log-likelihood &lt;span class=&#34;math inline&#34;&gt;\(\sup_{\alpha \in \Lambda} \hat L(\theta, \alpha)\)&lt;/span&gt; reflects exactly the ease of discrimination of the two classes &lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(U_1, \dots, U_n\)&lt;/span&gt;, that is the proximity between &lt;span class=&#34;math inline&#34;&gt;\(P_\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt;.
Task 2 is thus performed by introducing a class of discriminators (which are often neural networks) and maximizing a log-likelihood.
The latter quantity also helps in adjusting &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; such that the distribution &lt;span class=&#34;math inline&#34;&gt;\(P_{\theta}\)&lt;/span&gt; of the generated data &lt;span class=&#34;math inline&#34;&gt;\(G_\theta(Z)\)&lt;/span&gt; becomes closer and closer to &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In statistical terms, &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt; can be estimated by &lt;span class=&#34;math inline&#34;&gt;\(P_{\hat \theta}\)&lt;/span&gt;, where:
&lt;span class=&#34;math display&#34;&gt;\[
  \hat \theta \in \operatorname*{arg\,min}_{\theta \in \Theta} \sup_{\alpha \in \Lambda} \hat L(\theta, \alpha),
\]&lt;/span&gt;
where, as described previously, the generated data is &lt;span class=&#34;math inline&#34;&gt;\(U_i = G_\theta (Z_i)\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(Z_1, \dots, Z_n \overset{i.i.d.}{\sim} \mathcal N(0, I_\ell)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The story of GANs is not that gleaming since, in practice, we never have access to &lt;span class=&#34;math inline&#34;&gt;\(P_{\hat \theta}\)&lt;/span&gt;, which may be a very complicated object, but only to the generator &lt;span class=&#34;math inline&#34;&gt;\(G_{\hat \theta}\)&lt;/span&gt;.
Anyway, our aim is to sample according to &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt;, which can be achieved (up to the estimation error) thanks to &lt;span class=&#34;math inline&#34;&gt;\(G_{\hat \theta}(Z)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(Z \sim \mathcal N(0, I_\ell)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Actually, in this work, &lt;span class=&#34;math inline&#34;&gt;\(P_\theta\)&lt;/span&gt; is just as a mathematical object helping to understand GANs.
To go into details, the forthcoming results are based on the assumption that all distributions in play are absolutely continuous with respect to a known measure &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; (typically the Hausdorff measure on some submanifold of &lt;span class=&#34;math inline&#34;&gt;\(\mathbb R^d\)&lt;/span&gt;) and probability density functions are noted with lowercase letters (in particular &lt;span class=&#34;math inline&#34;&gt;\(P^\star = p^\star d\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P_\theta = p_\theta d\mu\)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;div id=&#34;concerning-the-comparison-of-distributions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Concerning the comparison of distributions&lt;/h3&gt;
&lt;p&gt;In order to give a mathematical foundation to our intuition in Task 2, it may be useful to analyze the big sample case, where
&lt;span class=&#34;math display&#34;&gt;\[\hat L(\theta, \alpha) \approx \mathbb E [\hat L(\theta, \alpha)] = \mathbb E [\log(D_\alpha(X_1))] + \mathbb E [\log(1-D_\alpha\circ G_\theta(Z_1))].\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If the class of discriminators &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D = \left\{ D_\alpha, \alpha \in \Lambda \right\}\)&lt;/span&gt; is rich enough to contain the ``optimal’’ discriminator &lt;span class=&#34;math inline&#34;&gt;\(D_\theta^\star = \frac{p^\star}{p_\theta + p^\star}\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(\theta \in \Theta\)&lt;/span&gt;,
then
&lt;span class=&#34;math display&#34;&gt;\[\sup_{\alpha \in \Lambda} \mathbb E [\hat L(\theta, \alpha)] = 2 D_{JS}(P^\star, P_\theta) - \log 4,\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(D_{JS}\)&lt;/span&gt; is the Jensen-Shannon divergence &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-EnSc03&#34; role=&#34;doc-biblioref&#34;&gt;Endres and Schindelin 2003&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Two things can be learned from this first result (still assuming that &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D\)&lt;/span&gt; contains ``optimal’’ discriminators):&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Up to the approximation capacity of &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\sup_{\alpha \in \Lambda} \hat L(\theta, \alpha)\)&lt;/span&gt; does reflect the proximity between &lt;span class=&#34;math inline&#34;&gt;\(P_\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt; (thanks to an approximated divergence).&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt; cannot be better than &lt;span class=&#34;math inline&#34;&gt;\(\theta^\star \in \operatorname*{arg\,min}_{\theta \in \Theta} \sup_{\alpha \in \Lambda} \mathbb E [\hat L(\theta, \alpha)] = \operatorname*{arg\,min}_{\theta \in \Theta} D_{JS}(P^\star, P_\theta)\)&lt;/span&gt;, which leads to the approximation &lt;span class=&#34;math inline&#34;&gt;\(P_{\theta^\star}\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt; obtained by minimizing the Jensen-Shannon divergence.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;non-asymptotic-bound-on-jensen-shannon-error&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Non-asymptotic bound on Jensen-Shannon error&lt;/h3&gt;
&lt;p&gt;Thus, GANs drive the world downhill in two directions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A limited approximation capacity for the class of discriminators &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D\)&lt;/span&gt; (which may not contain the ``optimal’’ discriminator &lt;span class=&#34;math inline&#34;&gt;\(D_\theta^\star = \frac{p^\star}{p_\theta + p^\star}\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(\theta \in \Theta\)&lt;/span&gt;): &lt;span class=&#34;math inline&#34;&gt;\(\sup_{\alpha \in \Lambda} \mathbb E [\hat L(\theta, \alpha)] &amp;lt; 2 D_{JS}(P^\star, P_\theta) - \log 4\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;A finite sample approximation: the criterion maximized is &lt;span class=&#34;math inline&#34;&gt;\(\hat L(\theta, \alpha)\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(\mathbb E [\hat L(\theta, \alpha)]\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These limitations introduce two kinds of error in the estimation procedure:
an approximation error (or bias), induced by the richness of &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D\)&lt;/span&gt;,
and an estimation error (or variance) occurring from the finiteness of the sample.&lt;/p&gt;
&lt;p&gt;This can be formalized in the following manner:
assume some regularity conditions of the first order on the models &lt;span class=&#34;math inline&#34;&gt;\(\mathscr P\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mathscr G\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D\)&lt;/span&gt;
and assume that optimal discriminators &lt;span class=&#34;math inline&#34;&gt;\(D_\theta^\star\)&lt;/span&gt; can be approximated by &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D\)&lt;/span&gt; up to an error &lt;span class=&#34;math inline&#34;&gt;\(\epsilon&amp;gt;0\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(L^2\)&lt;/span&gt;-norm.
Then:
&lt;span class=&#34;math display&#34;&gt;\[
  \mathbb E [D_{JS}(P^\star, P_{\hat \theta})] - D_{JS}(P^\star, P_{\theta^\star}) = O \left( \epsilon^2 + \frac{1}{\sqrt n} \right).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This result explains quantitatively that the discriminators in GANs have to be tuned carefully:
on the one hand, poor discriminators induce an uncontrolled gap between &lt;span class=&#34;math inline&#34;&gt;\(\sup_{\alpha \in \Lambda} \mathbb E [\hat L(\theta, \alpha)]\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(D_{JS}(P^\star, P_\theta)\)&lt;/span&gt;;
on the other hand, very flexible discriminators may lead to overfitting the finite sample.&lt;/p&gt;
&lt;p&gt;The first assertion is illustrated in the next figure.
The numerical experiment has been set up with classes of fully connected neural networks for the generators and the discriminators (respectively &lt;span class=&#34;math inline&#34;&gt;\(\mathscr G\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D\)&lt;/span&gt;) and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; sufficiently large.
The depth of the generators is either 2 (blue bars) or 3 (green bars) and the depth of the discriminator ranges from 2 to 5 (from left to right).
As expected, it appears clearly that the more flexible the discriminators are (from left to right), the smaller &lt;span class=&#34;math inline&#34;&gt;\(D_{JS}(P^\star, P_{\hat \theta})\)&lt;/span&gt; is.
Obviously, this is also inversely correlated with the richness of the class of generators &lt;span class=&#34;math inline&#34;&gt;\(\mathscr G\)&lt;/span&gt; (at least in a first regime).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-04-27-a-small-step-to-understand-GDA_files/divergences.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;asymptotic-normality&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Asymptotic normality&lt;/h3&gt;
&lt;p&gt;As a second important result, it can be shown that the estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt; is asymptotically normal with convergence rate &lt;span class=&#34;math inline&#34;&gt;\(\sqrt n\)&lt;/span&gt;.
More formally, let us assume &lt;span class=&#34;math inline&#34;&gt;\(\bar \theta \in \operatorname*{arg\,min}_{\theta\in\Theta} \sup_{\alpha \in \Lambda} \mathbb E [\hat L(\theta, \alpha)]\)&lt;/span&gt; exists and is unique.
Let also assume some regularity conditions of the second order on the models &lt;span class=&#34;math inline&#34;&gt;\(\mathscr P\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mathscr G\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D\)&lt;/span&gt;,
well definiteness and smoothness of &lt;span class=&#34;math inline&#34;&gt;\(\theta \mapsto \operatorname*{arg\,max}_{\alpha \in \Lambda} \mathbb E [\hat L(\theta, \alpha)]\)&lt;/span&gt; around &lt;span class=&#34;math inline&#34;&gt;\(\bar \theta\)&lt;/span&gt;.
Then, there exists a covariance matrice &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; such that:
&lt;span class=&#34;math display&#34;&gt;\[
  \sqrt n \left( \hat \theta - \bar \theta \right) \xrightarrow{dist} \mathcal N(0, \Sigma).
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;GANs have been statistically analyzed from the estimation point of view.
Even though some simplifications were made (known dominating measure &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, uniqueness of some quantities) compared to the empirical setting based on deep neural networks,
the theoretical results show the importance of tuning correctly the architecture of the discriminators,
and exhibit an asymptotic behavior similar to that of a standard M-estimator.&lt;/p&gt;
&lt;p&gt;It remains to study the impact of the architecture of neural nets on the performance of GANs, as well as their behavior in an overparametrized regime.
But that’s a different story.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;This post is based on&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;G. Biau, B. Cadre, M. Sangnier and U. Tanielian. 2020. ``Some Theoretical Properties of GANs.’’ The Annals of Statistics 48(3): 1539-1566.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-EnSc03&#34; class=&#34;csl-entry&#34;&gt;
Endres, D. M., and J. E. Schindelin. 2003. &lt;span&gt;“A New Metric for Probability Distributions.”&lt;/span&gt; &lt;em&gt;IEEE Transactions on Information Theory&lt;/em&gt; 49: 1858–60.
&lt;/div&gt;
&lt;div id=&#34;ref-Go16&#34; class=&#34;csl-entry&#34;&gt;
Goodfellow, I. 2016. &lt;em&gt;NIPS 2016 Tutorial: Generative Adversarial Networks&lt;/em&gt;. arXiv:1701.00160.
&lt;/div&gt;
&lt;div id=&#34;ref-GoPoMiXuWaOzCoBe14&#34; class=&#34;csl-entry&#34;&gt;
Goodfellow, I., J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and J. Bengio. 2014. &lt;span&gt;“Generative Adversarial Nets.”&lt;/span&gt; In &lt;em&gt;Advances in Neural Information Processing Systems 27&lt;/em&gt;, edited by Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, 2672–80. Red Hook: Curran Associates, Inc.
&lt;/div&gt;
&lt;div id=&#34;ref-JaLiBo20&#34; class=&#34;csl-entry&#34;&gt;
Jabbar, A., X. Li, and O. Bourahla. 2020. &lt;em&gt;A Survey on Generative Adversarial Networks: Variants, Applications, and Training&lt;/em&gt;. arXiv:2006.05132.
&lt;/div&gt;
&lt;div id=&#34;ref-Karras2018&#34; class=&#34;csl-entry&#34;&gt;
Karras, T., T. Aila, S. Laine, and J. Lehtinen. 2018. &lt;span&gt;“Progressive Growing of GANs for Improved Quality, Stability, and Variation.”&lt;/span&gt; In &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-SaGoZaChRaCg16&#34; class=&#34;csl-entry&#34;&gt;
Salimans, T., I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. 2016. &lt;span&gt;“Improved Techniques for Training &lt;span&gt;GAN&lt;/span&gt;s.”&lt;/span&gt; In &lt;em&gt;Advances in Neural Information Processing Systems 29&lt;/em&gt;, edited by D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, 2234–42. Red Hook: Curran Associates, Inc.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Generalizing the Neyman-Pearson Lemma for multiple hypothesis testing problems</title>
      <link>https://youngstats.github.io/post/2021/04/13/generalizing-the-neyman-pearson-lemma-for-multiple-hypothesis-testing-problems/</link>
      <pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/04/13/generalizing-the-neyman-pearson-lemma-for-multiple-hypothesis-testing-problems/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Let us start by considering the optimal rejection policy for a single hypothesis testing problem. There are three elements to the problem. The objective: to maximize the power to reject the null hypothesis; The constraint: to control the type I error probability, so that it is at most a predefined &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;; The decision policy: for every realized sample define the decision &lt;span class=&#34;math inline&#34;&gt;\(D\in \{0,1\}\)&lt;/span&gt;, where the null hypothesis is rejected if &lt;span class=&#34;math inline&#34;&gt;\(D=1\)&lt;/span&gt; and retained otherwise. For simplicity, let &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; be the realized test statistic on which the decision &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is based, and let &lt;span class=&#34;math inline&#34;&gt;\(g(z\mid h=1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g(z\mid h=0)\)&lt;/span&gt; be, respectively, the non-null and the null density of &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;. The optimization problem is therefore:
&lt;span class=&#34;math display&#34;&gt;\[\max_{D:\mathbb R \rightarrow \{0,1\}} \int D(z)g(z\mid h=1)dz\\ s.t.\int D(z)g(z\mid h=0)dz\leq \alpha.  \]&lt;/span&gt; This infinite dimensional integer problem happens to have a simple solution provided by the Neyman-Pearson (NP) Lemma.&lt;/p&gt;
&lt;p&gt;In this era of big data, conducting a study with a single hypothesis is rare. In many disciplines, hundreds or thousands of null hypotheses are tested in each study. In order to avoid an inflation of false positive findings, it is critical to use multiple testing procedures that guarantee that a meaningful error measure is controlled at a predefined level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. The most popular measures are the family wise error rate (FWER), i.e.,
the probability of at least one type I error, and the false discovery rate (FDR), i.e., the expected false discovery proportion.&lt;/p&gt;
&lt;p&gt;Arguably, it is just as critical that the multiple testing procedure will have high statistical power, thus facilitating scientific discoveries. As with the single hypothesis testing problem, we take an optimization approach to the problem with &lt;span class=&#34;math inline&#34;&gt;\(K&amp;gt;1\)&lt;/span&gt; null hypotheses: we seek to find the &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; decision functions, &lt;span class=&#34;math inline&#34;&gt;\(\vec D: {\mathbb R}^K\rightarrow \{0,1 \}^K\)&lt;/span&gt;,
that maximize some notion of power while guaranteeing that the error measure is at most &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. In this post, we shall consider the problem of finding the optimal decision functions when testing multiple hypotheses in a couple of settings of interest.&lt;/p&gt;
&lt;p&gt;This post is based on our three recent papers on this topic: Rosset et al. (2018), where a general theoretical framework is presented; Heller and Rosset (2021), where the two-group model is discussed; and Heller, Krieger and Rosset (2021), where optimal clinical trial design is discussed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;an-optimal-policy-for-the-two-group-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An optimal policy for the two-group model&lt;/h2&gt;
&lt;p&gt;The two-group model, which is widely used in large scale inference problems, assumes a Bayesian setting, where the observed test statistics are generated independently from the mixture model &lt;span class=&#34;math display&#34;&gt;\[(1-\pi)g(z\mid h=0)+\pi g(z\mid h=1),\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt; follows the &lt;span class=&#34;math inline&#34;&gt;\(Bernoulli(\pi)\)&lt;/span&gt; distribution, and indicates as before whether the null is true &lt;span class=&#34;math inline&#34;&gt;\((h=0)\)&lt;/span&gt; or false &lt;span class=&#34;math inline&#34;&gt;\((h=1)\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\vec z\)&lt;/span&gt; be a vector of length &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; generated according to the two group model, and consider the following optimization problem:
&lt;span class=&#34;math display&#34;&gt;\[\max_{\vec D:{\mathbb R}^K \rightarrow \{0,1\}^K} \mathbb E(\vec h^t \vec D) \\ s.t. \ FDR(\vec D) \leq \alpha.\]&lt;/span&gt; One of our main results is that the solution is to threshold the test statistic &lt;span class=&#34;math display&#34;&gt;\[\mathbb P(h=0\mid z) = \frac{(1-\pi)g(z\mid h=0)}{(1-\pi)g(z\mid h=0)+\pi g(z\mid h=1)}, \]&lt;/span&gt; and that the threshold depends on the entire vector of test statistics &lt;span class=&#34;math inline&#34;&gt;\(\vec z\)&lt;/span&gt;. Thus for a realized vector of test statistics &lt;span class=&#34;math inline&#34;&gt;\(\vec z = (z_1, \ldots, z_K)\)&lt;/span&gt;, the decision vector &lt;span class=&#34;math inline&#34;&gt;\(\vec D\)&lt;/span&gt; can be described in the following form: &lt;span class=&#34;math display&#34;&gt;\[\vec D(\vec z) = (\mathbb I[\mathbb P(h=0\mid z_1)\leq t(\vec z)], \ldots, \mathbb I[\mathbb P(h=0\mid z_k)\leq t(\vec z)]),\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\mathbb I[\cdot]\)&lt;/span&gt; is the indicator function. This leads to practical algorithms which improve theoretically and empirically on previous solutions for the two-group model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;an-optimal-policy-for-the-design-of-clinical-trials&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An optimal policy for the design of clinical trials&lt;/h2&gt;
&lt;p&gt;Assume that &lt;span class=&#34;math inline&#34;&gt;\(K=2\)&lt;/span&gt; hypothesis testing problems are examined in a clinical trial (e.g., treatment effectiveness in two distinct subgroups). The federal agencies that approve drugs typically require strong FWER control at level &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 0.05\)&lt;/span&gt;. At the design stage, it is necessary to decide on the number of subjects that will be allocated. For &lt;span class=&#34;math inline&#34;&gt;\(K=1\)&lt;/span&gt;, this is typically done by computing the minimal number of subjects for achieving minimal power with a type I error probability of &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. For &lt;span class=&#34;math inline&#34;&gt;\(K=2\)&lt;/span&gt;, with a relevant notion of required power &lt;span class=&#34;math inline&#34;&gt;\(\Pi(\vec D)\)&lt;/span&gt;, a smaller number of subjects will need to be allocated, if the multiple testing procedure is the optimal policy, rather than an off-the-shelf policy. However, finding the optimal policy may be difficult since
the policy has to guarantee that for every data generation with at least one null parameter value, the probability of rejecting the null hypothesis corresponding to the null parameter value should be at most &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; (i.e., there may be an infinite number of constraints for strong FWER control at level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;For simplicity, assume we have two independent test statistics that are normally distributed with variance one and expectation zero if the null hypothesis is true but negative if the null hypothesis is false. Then, the optimization problem can be formalized as follows:&lt;br /&gt;
&lt;span class=&#34;math display&#34;&gt;\[\max_{\vec D:{\mathbb R}^2 \rightarrow \{0,1\}^2} \Pi(\vec D) \\ s.t. \  \mathbb P_{(0, \theta)}(D_1=0) \leq \alpha \ \forall \ \theta &amp;lt;0; \ \mathbb P_{( \theta, 0 )}(D_2=0) \leq \alpha \ \forall \ \theta &amp;lt;0; \ \mathbb P_{(0, 0)}(\max(D_1,D_2)=0) \leq \alpha.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We also add the common-sense restriction that &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-values above &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; are not rejected (for details, see Heller, Krieger, and Rosset 2021).&lt;br /&gt;
The resulting optimal rejection policy is attractive for relevant objectives &lt;span class=&#34;math inline&#34;&gt;\(\Pi(\vec D)\)&lt;/span&gt; that can be useful for clinical trials.
Below we provide one example optimal policy: the policy maximizing the average power when both test statistics have expectation -2 (left panel). For comparison, we provide the off-the-shelf popular Hochberg procedure (which coincides with the Hommel procedure for &lt;span class=&#34;math inline&#34;&gt;\(K=2\)&lt;/span&gt;). Both policies satisfy the strong FWER control guarantee at level &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 0.05\)&lt;/span&gt;, but the average power of the optimal policy is 3% higher (63% versus 60%) if the test statistics are normally distributed with expectation -2. The color coding is as follows: in red, reject only the second hypothesis; in black, reject only the first hypothesis; in green, reject both hypotheses.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-04-13-generalizing-the-neyman-pearson-lemma-for-multiple-hypothesis-testing-problems_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;this-post-is-based-on&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;This post is based on&lt;/h2&gt;
&lt;p&gt;Heller, R. Krieger, A. and Rosset, S. (2021) &lt;em&gt;Optimal multiple testing and design in clinical trials&lt;/em&gt;. arXiv:2104.01346&lt;/p&gt;
&lt;p&gt;Heller, R. and Rosset, S. (2020) &lt;em&gt;Optimal control of false discovery criteria in the two-group model&lt;/em&gt;. Journal of the Royal Statistical Society, Series B,
&lt;a href=&#34;https://doi.org/10.1111/rssb.12403&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1111/rssb.12403&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Rosset, S. Heller, R. Painsky, A. and Aharoni, E. (2018) &lt;em&gt;Optimal and Maximin Procedures for Multiple Testing Problems&lt;/em&gt;. arXiv: 1804.10256&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;about-the-authors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;About the authors&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-04-13-generalizing-the-neyman-pearson-lemma-for-multiple-hypothesis-testing-problems_files/headRuth.jpg&#34; height=&#34;75&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ruth Heller&lt;/strong&gt; is an Associate Professor of Statistics at Tel-Aviv University, Israel, &lt;a href=&#34;mailto:ruheller@gmail.com&#34; class=&#34;email&#34;&gt;ruheller@gmail.com&lt;/a&gt;. Her research interests are in multiple comparisons, nonparametrics, and observational studies.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-04-13-generalizing-the-neyman-pearson-lemma-for-multiple-hypothesis-testing-problems_files/headSaharon.png&#34; height=&#34;75&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Saharon Rosset&lt;/strong&gt; is a Professor of Statistics at Tel-Aviv University, Israel, &lt;a href=&#34;mailto:saharon@tauex.tau.ac.il&#34; class=&#34;email&#34;&gt;saharon@tauex.tau.ac.il&lt;/a&gt;. His research interests are in Statistical Learning theory and methods
and in Statistical Genetics.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Developments in Bayesian Nonparametrics (updated with slides)</title>
      <link>https://youngstats.github.io/post/2021/04/06/bnp-webinar/</link>
      <pubDate>Tue, 06 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/04/06/bnp-webinar/</guid>
      <description>&lt;p&gt;The second &lt;em&gt;&amp;ldquo;One World webinar&amp;rdquo;&lt;/em&gt; organized by YoungStatS will take place on April 21st.
The focus of this webinar will be on illustrating modern advances in Bayesian Nonparametrics data analysis, discussing challenging theoretical problems and stimulating case-studies within this active area of research.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Wednesday, April 21st, 16:30 CEST&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Online&lt;/em&gt;, via Zoom. The registration form is available &lt;a href=&#34;https://forms.gle/vfinGjQJMeqhq6HQ7&#34;&gt;here&lt;/a&gt;. Further details and the Zoom link will be sent to the registered addresses only.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://francescapanero.github.io/&#34;&gt;Francesca Panero&lt;/a&gt; (University of Oxford, UK): &lt;em&gt;&amp;ldquo;Sparse Spatial Random Graphs&amp;rdquo;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://martacatalano.github.io/&#34;&gt;Marta Catalano&lt;/a&gt; (University of Torino, Italy). &lt;em&gt;&amp;ldquo;Measuring dependence in the Wasserstein distance for Bayesian nonparametric models&amp;rdquo;&lt;/em&gt; [&lt;a href=&#34;https://youngstats.github.io/post/2021/04/06/bnp-webinar/dBNP_Catalano.pdf&#34;&gt;talk slides&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://salleuska.github.io/&#34;&gt;Sally Paganin&lt;/a&gt; (Harvard School of Public Health, USA) &lt;em&gt;&amp;ldquo;Informative model-based clustering via Centered Partition Processes&amp;rdquo;&lt;/em&gt;  [&lt;a href=&#34;https://youngstats.github.io/post/2021/04/06/bnp-webinar/dBNP_Paganin.pdf&#34;&gt;talk slides&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.google.com/unimib.it/camerlenghi-federico/&#34;&gt;Federico Camerlenghi&lt;/a&gt; (University of Milano Bicocca, Italy) [&lt;a href=&#34;https://youngstats.github.io/post/2021/04/06/bnp-webinar/dBNP_Discussion_Camerlenghi.pdf&#34;&gt;discussion slides&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;The webinar is part of YoungStatS project of the Young Statisticians Europe initiative (FENStatS) supported by the Bernoulli Society for Mathematical Statistics and Probability.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For more information, please visit our &lt;a href=&#34;https://youngstats.github.io/post/2020/09/10/webinars/&#34;&gt;website&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Analysis of a Two-Layer Neural Network via Displacement Convexity</title>
      <link>https://youngstats.github.io/post/2021/03/14/analysis-of-a-two-layer-neural-network-via-displacement-convexity/</link>
      <pubDate>Sun, 14 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/03/14/analysis-of-a-two-layer-neural-network-via-displacement-convexity/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;abstract&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;We consider the problem of learning a function defined on a compact domain, using linear combinations
of a large number of “bump-like” components (neurons). This idea lies at the core of a variety of methods
from two-layer neural networks to kernel regression, to boosting. In general, the resulting risk minimization
problem is non-convex and is solved by gradient descent or its variants. Nonetheless, little is known about
global convergence properties of these approaches. In this work, we show that, as the number of neurons
diverges and the bump width tends to zero, the gradient flow has a limit which is a viscous porous medium
equation. By virtue of a property named “displacement convexity,” we show an exponential dimension-free
convergence rate for gradient descent.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-14-analysis-of-a-two-layer-neural-network-via-displacement-convexity_files/screenshot.82.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This post is based on the paper &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-javanmard2020analysis&#34; role=&#34;doc-biblioref&#34;&gt;Javanmard et al. 2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fitting a function with a linear combination of components.&lt;/strong&gt; In
supervised learning, we are given data points
&lt;span class=&#34;math inline&#34;&gt;\(\{(y_j,{\boldsymbol{x}}_j)\}_{j\le n}\)&lt;/span&gt;, which are often assumed to be
independent and identically distributed. Here
&lt;span class=&#34;math inline&#34;&gt;\({\boldsymbol{x}}_j\in {\mathbb R}^d\)&lt;/span&gt; is a feature vector, and
&lt;span class=&#34;math inline&#34;&gt;\(y_j\in{\mathbb R}\)&lt;/span&gt; is a label or response variable. We would like to
fit a model &lt;span class=&#34;math inline&#34;&gt;\(\widehat{f}:{\mathbb R}^d\to{\mathbb R}\)&lt;/span&gt; to predict the
labels at new points &lt;span class=&#34;math inline&#34;&gt;\({\boldsymbol{x}}\in{\mathbb R}^d\)&lt;/span&gt;. One of the most
fruitful ideas in this context is to use functions that are linear
combinations of simple components:
&lt;span class=&#34;math display&#34;&gt;\[\widehat{f}({\boldsymbol{x}};{\boldsymbol{w}}) = \frac{1}{N}\sum_{i=1}^N \sigma({\boldsymbol{x}};{\boldsymbol{w}}_i)\, ,\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\sigma:{\mathbb R}^d\times{\mathbb R}^D\to{\mathbb R}\)&lt;/span&gt; is a
component function (a ‘neuron’ or ‘unit’ in the neural network
parlance), and
&lt;span class=&#34;math inline&#34;&gt;\({\boldsymbol{w}}=({\boldsymbol{w}}_1,\dots,{\boldsymbol{w}}_N)\in{\mathbb R}^{D\times N}\)&lt;/span&gt;
are the parameters to be learnt from data. Specific instantiations of
this idea include, e.g., two-layer neural networks with radial
activations, sparse deconvolution, kernel ridge regression, random
feature methods, and boosting.&lt;/p&gt;
&lt;p&gt;A common approach towards fitting parametric models is by risk
minimization:
&lt;span class=&#34;math display&#34;&gt;\[R_N({\boldsymbol{w}}) = {\mathbb E}\Big\{\Big[y-\frac{1}{N}\sum_{i=1}^N\sigma({\boldsymbol{x}};{\boldsymbol{w}}_i)\Big]^2\Big\}\, .\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Despite the impressive practical success of these methods, the risk
function &lt;span class=&#34;math inline&#34;&gt;\(R_N({\boldsymbol{w}})\)&lt;/span&gt; is highly non-convex and little is
known about the global convergence of algorithms that try to minimize
it. The main objective of this work is to introduce a nonparametric
underlying regression model for which a global convergence result can be
proved for stochastic gradient descent (SGD), in the limit of a large
number of neurons.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Setting and main result.&lt;/strong&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(\Omega\subset{\mathbb R}^d\)&lt;/span&gt; be a
compact convex set with smooth boundary, and let
&lt;span class=&#34;math inline&#34;&gt;\(\{(y_j,{\boldsymbol{x}}_j)\}_{j\ge 1}\)&lt;/span&gt; be i.i.d. with
&lt;span class=&#34;math inline&#34;&gt;\({\boldsymbol{x}}_j\sim {\sf Unif}(\Omega)\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\({\mathbb E}(y_j|{\boldsymbol{x}}_j)=f({\boldsymbol{x}}_j)\)&lt;/span&gt;, where the
function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is smooth. We try to fit these data using a combination of
bumps, namely
&lt;span class=&#34;math display&#34;&gt;\[\widehat{f}({\boldsymbol{x}};{\boldsymbol{w}})= \frac{1}{N}\sum_{i=1}^NK^\delta({\boldsymbol{x}}-{\boldsymbol{w}}_i)\, ,\]&lt;/span&gt;
where
&lt;span class=&#34;math inline&#34;&gt;\(K^\delta({\boldsymbol{x}}) = \delta^{-d}K({\boldsymbol{x}}/\delta)\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(K:{\mathbb R}^d\to{\mathbb R}_{\ge 0}\)&lt;/span&gt; is a first order kernel with
compact support. The weights &lt;span class=&#34;math inline&#34;&gt;\({\boldsymbol{w}}_i\in{\mathbb R}^d\)&lt;/span&gt;
represent the centers of the bumps. Our model is general enough to
include a broad class of radial-basis function (RBF) networks which are
known to be universal function approximators. To the best of our
knowledge, there is no result on the global convergence of stochastic
gradient descent for learning RBF networks, and we establish the first
result of this type.&lt;/p&gt;
&lt;p&gt;We prove that, for sufficiently large &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; and small &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;, gradient
descent algorithms converge to weights &lt;span class=&#34;math inline&#34;&gt;\({\boldsymbol{w}}\)&lt;/span&gt; with nearly
optimum prediction error, provided &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is strongly concave. Let us
emphasize that the resulting population risk &lt;span class=&#34;math inline&#34;&gt;\(R_N({\boldsymbol{w}})\)&lt;/span&gt; is
non-convex regardless of the concavity properties of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;. Our proof
unveils a novel mechanism by which global convergence takes place.
Convergence results for non-convex empirical risk minimization are
generally proved by carefully ruling out local minima in the cost
function. Instead we prove that, as &lt;span class=&#34;math inline&#34;&gt;\(N\to\infty\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\delta\to 0\)&lt;/span&gt;, the
gradient descent dynamics converges to a gradient flow in Wasserstein
space, and that the corresponding cost function is ‘displacement
convex.’ Breakthrough results in optimal transport theory guarantee
dimension-free convergence rates for this limiting dynamics
&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-carrillo2003kinetic&#34; role=&#34;doc-biblioref&#34;&gt;Carrillo, McCann, and Villani 2003&lt;/a&gt;)&lt;/span&gt;. In particular, we expect the cost function
&lt;span class=&#34;math inline&#34;&gt;\(R_N({\boldsymbol{w}})\)&lt;/span&gt; to have many local minima, which are however
completely neglected by gradient descent.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof idea.&lt;/strong&gt; Let us start by providing some high-level insights about
our approach. Think about each model parameter as a particle moving
under the effect of other particles according to the SGD updates. Now,
instead of studying the microscopic dynamics of this system of
particles, we analyze the macroscopic dynamics of the medium when the
number of particles (i.e., the size of the hidden layer of the neural
network) goes to infinity. These dynamics are formulated through a
partial differential equation (more specifically, a viscous porous
medium equation) that describes the evolution of the mass density over
space and time. The nice feature of this approach is that, while the SGD
trajectory is a random object, it shows that in the large particle size
limit, it concentrates around the deterministic solution of this partial
differential equation (PDE).&lt;/p&gt;
&lt;p&gt;For a rigorous analysis and implementation of this idea, we use
propagation-of-chaos techniques. Specifically, we show that, in the
large &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; limit, the evolution of the weights
&lt;span class=&#34;math inline&#34;&gt;\(\{{\boldsymbol{w}}_i\}_{i=1}^N\)&lt;/span&gt; under gradient descent can be replaced
by the evolution of a probability distribution &lt;span class=&#34;math inline&#34;&gt;\(\rho^{\delta}_t\)&lt;/span&gt; which
satisfies the viscous porous medium PDE (with Neumann boundary
conditions). This PDE can also be described as the Wasserstein &lt;span class=&#34;math inline&#34;&gt;\(W_2\)&lt;/span&gt;
gradient flow for the following effective risk
&lt;span class=&#34;math display&#34;&gt;\[R^{\delta}(\rho) = \frac{1}{|\Omega|} \, \int_{\Omega} \big[f({\boldsymbol{x}}) - K^\delta\ast \rho({\boldsymbol{x}})\big]^2{\rm d}{\boldsymbol{x}}\, ,\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(|\Omega|\)&lt;/span&gt; is the volume of the set &lt;span class=&#34;math inline&#34;&gt;\(\Omega\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\ast\)&lt;/span&gt; is the
usual convolution. The use of &lt;span class=&#34;math inline&#34;&gt;\(W_2\)&lt;/span&gt; gradient flows to analyze two-layer
neural networks was recently developed in several papers
&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mei2018mean&#34; role=&#34;doc-biblioref&#34;&gt;Mei, Montanari, and Nguyen 2018&lt;/a&gt;; &lt;a href=&#34;#ref-rotskoff2018neural&#34; role=&#34;doc-biblioref&#34;&gt;Rotskoff and Vanden-Eijnden 2019&lt;/a&gt;; &lt;a href=&#34;#ref-chizat2018global&#34; role=&#34;doc-biblioref&#34;&gt;Chizat and Bach 2018&lt;/a&gt;; &lt;a href=&#34;#ref-sirignano2018mean&#34; role=&#34;doc-biblioref&#34;&gt;Sirignano and Spiliopoulos 2020&lt;/a&gt;)&lt;/span&gt;.
However, we cannot rely on earlier results because of the specific
boundary conditions in our problem.&lt;/p&gt;
&lt;p&gt;Note that even though the cost &lt;span class=&#34;math inline&#34;&gt;\(R^{\delta}(\rho)\)&lt;/span&gt; is quadratic and
convex in &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;, its &lt;span class=&#34;math inline&#34;&gt;\(W_2\)&lt;/span&gt; gradient flow can have multiple fixed
points, and hence global convergence cannot be guaranteed. Indeed, the
mathematical property that controls global convergence of &lt;span class=&#34;math inline&#34;&gt;\(W_2\)&lt;/span&gt; gradient
flows is not ordinary convexity but &lt;em&gt;displacement convexity&lt;/em&gt;. Roughly
speaking, displacement convexity is convexity along geodesics of the
&lt;span class=&#34;math inline&#34;&gt;\(W_2\)&lt;/span&gt; metric. Note that the risk function &lt;span class=&#34;math inline&#34;&gt;\(R^{\delta}(\rho)\)&lt;/span&gt; is &lt;em&gt;not&lt;/em&gt;
even displacement convex. However, for small &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;, we can formally
approximate &lt;span class=&#34;math inline&#34;&gt;\(K^\delta\ast \rho\approx \rho\)&lt;/span&gt;, and hence hope to replace
the risk function &lt;span class=&#34;math inline&#34;&gt;\(R^{\delta}(\rho)\)&lt;/span&gt; with the simpler one
&lt;span class=&#34;math display&#34;&gt;\[R(\rho) = \frac{1}{|\Omega|}\int_{\Omega} \big[f({\boldsymbol{x}}) - \rho({\boldsymbol{x}})\big]^2{\rm d}{\boldsymbol{x}}\, .\]&lt;/span&gt;
Most of our technical work is devoted to making rigorous this
&lt;span class=&#34;math inline&#34;&gt;\(\delta\to 0\)&lt;/span&gt; approximation.&lt;/p&gt;
&lt;p&gt;Remarkably, the risk function &lt;span class=&#34;math inline&#34;&gt;\(R(\rho)\)&lt;/span&gt; is strongly displacement convex
(provided &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is strongly concave). A long line of work in PDE and
optimal transport theory establishes dimension-free convergence rates
for its &lt;span class=&#34;math inline&#34;&gt;\(W_2\)&lt;/span&gt; gradient flow &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-carrillo2003kinetic&#34; role=&#34;doc-biblioref&#34;&gt;Carrillo, McCann, and Villani 2003&lt;/a&gt;)&lt;/span&gt;. By putting
everything together, we are able to show that SGD converges
exponentially fast to a near-global optimum with a rate that is
controlled by the convexity parameter of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A numerical illustration.&lt;/strong&gt; We demonstrate in a simple numerical
example that the convergence rate predicted by our asymptotic theory is
in excellent agreement with simulations. In the left column of the
figure below, we plot the true function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; together with the neural
network estimate obtained by running stochastic gradient descent (SGD)
with &lt;span class=&#34;math inline&#34;&gt;\(N=200\)&lt;/span&gt; neurons at several points in time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. Different plots
correspond to different values of &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; with
&lt;span class=&#34;math inline&#34;&gt;\(\delta\in\{1/5, 1/10, 1/20\}\)&lt;/span&gt;. We observe that the network estimates
converge to a limit curve which is an approximation of the true function
&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;. As expected, the quality of the approximation improves as &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;
gets smaller. In the right column, we report the evolution of the
population risk and we compare it to the risk predicted by the limit PDE
(corresponding to &lt;span class=&#34;math inline&#34;&gt;\(\delta=0\)&lt;/span&gt;). The PDE curve appears to capture well the
evolution of SGD towards optimality.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-14-analysis-of-a-two-layer-neural-network-via-displacement-convexity_files/screenshot.81.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;authors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Authors&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-14-analysis-of-a-two-layer-neural-network-via-displacement-convexity_files/javanmard.jpg&#34; width=&#34;75&#34; /&gt;
Adel Javanmard, Data Science and Operations Department, Marshall School of Business, University of Southern California&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-14-analysis-of-a-two-layer-neural-network-via-displacement-convexity_files/mondelli.jpg&#34; width=&#34;75&#34; /&gt;
Marco Mondelli, Institute of Science and Technology (IST) Austria&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-14-analysis-of-a-two-layer-neural-network-via-displacement-convexity_files/montanari.jpg&#34; width=&#34;75&#34; /&gt;
Andrea Montanari, Department of Electrical Engineering and Department of Statistics, Stanford University&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-carrillo2003kinetic&#34; class=&#34;csl-entry&#34;&gt;
Carrillo, José A., Robert J. McCann, and Cédric Villani. 2003. &lt;span&gt;“Kinetic Equilibration Rates for Granular Media and Related Equations: Entropy Dissipation and Mass Transportation Estimates.”&lt;/span&gt; &lt;em&gt;Revista Matematica Iberoamericana&lt;/em&gt; 19 (3): 971–1018.
&lt;/div&gt;
&lt;div id=&#34;ref-chizat2018global&#34; class=&#34;csl-entry&#34;&gt;
Chizat, Lenaic, and Francis Bach. 2018. &lt;span&gt;“On the Global Convergence of Gradient Descent for over-Parameterized Models Using Optimal Transport.”&lt;/span&gt; In &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt;, 3040–50.
&lt;/div&gt;
&lt;div id=&#34;ref-javanmard2020analysis&#34; class=&#34;csl-entry&#34;&gt;
Javanmard, Adel, Marco Mondelli, Andrea Montanari, and others. 2020. &lt;span&gt;“Analysis of a Two-Layer Neural Network via Displacement Convexity.”&lt;/span&gt; &lt;em&gt;Annals of Statistics&lt;/em&gt; 48 (6): 3619–42.
&lt;/div&gt;
&lt;div id=&#34;ref-mei2018mean&#34; class=&#34;csl-entry&#34;&gt;
Mei, Song, Andrea Montanari, and Phan-Minh Nguyen. 2018. &lt;span&gt;“A Mean Field View of the Landscape of Two-Layer Neural Networks.”&lt;/span&gt; &lt;em&gt;Proceedings of the National Academy of Sciences&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.1073/pnas.1806579115&#34;&gt;https://doi.org/10.1073/pnas.1806579115&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-rotskoff2018neural&#34; class=&#34;csl-entry&#34;&gt;
Rotskoff, Grant M., and Eric Vanden-Eijnden. 2019. &lt;span&gt;“Trainability and Accuracy of Neural Networks: An Interacting Particle System Approach.”&lt;/span&gt; &lt;em&gt;Communications on Pure and Applied Mathematics&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-sirignano2018mean&#34; class=&#34;csl-entry&#34;&gt;
Sirignano, Justin, and Konstantinos Spiliopoulos. 2020. &lt;span&gt;“Mean Field Analysis of Neural Networks: A Law of Large Numbers.”&lt;/span&gt; &lt;em&gt;SIAM Journal on Applied Mathematics&lt;/em&gt; 80 (2): 725–52.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
