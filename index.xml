<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>YoungStatS</title>
    <link>https://youngstats.github.io/</link>
    <description>Recent content on YoungStatS</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Apr 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://youngstats.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Extrapolation to unseen domains: from theory to applications</title>
      <link>https://youngstats.github.io/post/2024/04/05/extrapolation-generalization-to-novel-domains-in-data-science/</link>
      <pubDate>Fri, 05 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2024/04/05/extrapolation-generalization-to-novel-domains-in-data-science/</guid>
      <description>


&lt;p&gt;&lt;strong&gt;Extrapolation to unseen domains: from theory to applications&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Monday, April 22nd, 2024, 8:00 PT / 11:00 ET / 17:00 CET&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2024-04-05-extrapolation-generalization-to-novel-domains-in-data-science_files/noname-crop.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;3rd joint webinar of the &lt;a href=&#34;https://imstat.org/ims-groups/ims-new-researchers-group/&#34;&gt;IMS New Researchers Group&lt;/a&gt;, &lt;a href=&#34;https://math.ethz.ch/sfs/news-and-events/young-data-science.html&#34;&gt;Young Data Science Researcher Seminar Zürich&lt;/a&gt; and the YoungStatS Project.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Monday, April 22nd, 2024, 8:00 PT / 11:00 ET / 17:00 CET&lt;/li&gt;
&lt;li&gt;Online, via &lt;a href=&#34;https://washington.zoom.us/j/92385046970&#34;&gt;Zoom&lt;/a&gt;. The registration form is available
&lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSdJ_BYx3B4mkeduSutoS5fsLjjm9bGGxzUWwOdcZeHoOUWWaA/viewform&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://msimchowitz.github.io/&#34;&gt;Max Simchowitz&lt;/a&gt;, Robot Locomotion Group, MIT: “&lt;em&gt;Statistical Learning under Heterogeneous Distribution Shift&lt;/em&gt;”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Abstract: What makes a trained predictor, e.g. neural network, more or less susceptible to
performance degradation under distribution shift? Spurious correlation, lack of diversity in
the training data, and brittleness of the trained model are all possible culprits. In this talk, we will investigate a less well-studied factor: that of the statistical complexity of the individual features themselves. We will show that, for a very general class of predictors with a certain additive structure, empirical risk minimization is less sensitive to distribution shifts in “simple features” than “complex” ones, where simplicity/complexity are measured in terms of natural statistical quantities. We demonstrate that this arises because standard ERM learns the dependence on the “simpler” feature more quickly, whilst avoiding the risk of overfitting to more “complex” features. We will conclude by drawing connections to the orthogonal machine learning literature, and validating our theory on various experimental domains (even those in which the additivity assumption fails to hold).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://lotfollahi.com/&#34;&gt;Mohammad Lotfollahi&lt;/a&gt;, Wellcome Sanger Institute, University of Cambridge: “&lt;em&gt;Generative machine learning to model cellular perturbations&lt;/em&gt;”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Abstract: The field of cellular biology has long sought to understand the intricate mechanisms that govern cellular responses to various perturbations, be they chemical, physical, or biological. Traditional experimental approaches, while invaluable, often face limitations in scalability and throughput, especially when exploring the vast combinatorial space of potential cellular states. Enter generative machine learning that has shown exceptional promise in modeling complex biological systems. This talk will highlight recent successes, address the challenges and limitations of current models, and discuss the future direction of this exciting interdisciplinary field. Through examples of practical applications, we will illustrate the transformative potential of generative ML in advancing our understanding of cellular perturbations and in shaping the future of biomedical research.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhijing-jin.com/fantasy/about/&#34;&gt;Zhijing Jin&lt;/a&gt;, Max Planck Institute and ETH Zürich: “&lt;em&gt;A Paradigm Shift in Addressing Distribution Shifts: Insights from Large Language Models&lt;/em&gt;”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Abstract: Traditionally, the challenge of distribution shifts - where the training data distribution differs from the test data distribution - has been a central concern in statistical learning and model generalization. Traditional methods have primarily focused on techniques such as domain adaptation, and transfer learning. However, the rise of large language models (LLMs) such as ChatGPT has ushered in a novel empirical success, triggering a significant “shift” in problem formulation and approach for traditional distribution shift problems. In this talk, I will start with two formulations for LLMs: (1) the engineering heuristics aimed at transforming “out-of-distribution” (OOD) problems into “in-distribution” scenarios, which is further accompanied by (2) the hypothesized “emergence of intelligence” through massive scaling of data and model parameters, which challenges our traditional views on distribution shifts. I will sequentially examine these aspects, first by presenting behavioral tests of these models’ generalization capabilities across unseen data, and then by conducting intrinsic checks to uncover the mechanisms LLMs learned. This talk seeks to provoke thoughts on several questions: Do the strategies of “making OOD problem IID” and facilitating the “emergence of intelligence” by scaling, truly stand up to scientific scrutiny? Furthermore, what do these developments imply for the field of statistical learning and the broader evolution of AI.&lt;/p&gt;
&lt;p&gt;Discussant: &lt;a href=&#34;https://people.math.ethz.ch/~nicolai/&#34;&gt;Nicolai Meinshausen&lt;/a&gt;, ETH Zürich&lt;/p&gt;
&lt;p&gt;YoungStatS project of the Young Statisticians Europe initiative (FENStatS) is supported by the Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics (IMS).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you missed this webinar, you can &lt;a href=&#34;https://www.youtube.com/watch?v=7TrQq3vd9Mc&#34;&gt;watch the recording on our YouTube channel&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Characterization-based approach for construction of goodness-of-fit test for Lévy distribution</title>
      <link>https://youngstats.github.io/post/2023/12/25/characterization-based-approach-for-construction-of-goodness-of-fit-test-for-l-vy-distribution/</link>
      <pubDate>Mon, 25 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2023/12/25/characterization-based-approach-for-construction-of-goodness-of-fit-test-for-l-vy-distribution/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The Lévy distribution, together with the Normal and Cauchy distribution,
belongs to the class of stable distributions, and it is among the only
three distributions for which the density can be derived in a closed
form. The density function of the two-parameter Lévy distribution is
expressed as follows: &lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
f(x; \lambda, \mu)=\sqrt{\dfrac{ \lambda}{2\pi}}\frac{e^{-\dfrac{ \lambda}{2(x-\mu)}}}{(x-\mu)^{\frac{3}{2}}},\;x\geq\mu,\; \lambda&amp;gt;0, \; \mu\in \mathbb{R}.
\end{equation*}\]&lt;/span&gt; We specifically consider the scenario where &lt;span class=&#34;math inline&#34;&gt;\(\mu=0\)&lt;/span&gt; due
to the difficulty in estimating both parameters. Despite being widely
applied in various fields such as physics, biology, medicine, and
finance, there exist only a limited number of specific goodness-of-fit
tests for the Lévy distribution. In this blog post, we present results
from &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-lukic2023characterization&#34;&gt;1&lt;/a&gt;]&lt;/span&gt; by introducing two new families of test
statistics. Following the idea from &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-revista&#34;&gt;2&lt;/a&gt;]&lt;/span&gt; (see also references
therein), the novel statistics rely on the V-empirical Laplace transform
and the characterization of the Lévy distribution &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-AhsNev1&#34;&gt;3&lt;/a&gt;]&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Characterization 1&lt;/strong&gt; Suppose that &lt;span class=&#34;math inline&#34;&gt;\(X, Y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; are independent and
identically distributed random variables with density &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; defined on
&lt;span class=&#34;math inline&#34;&gt;\((0, \infty)\)&lt;/span&gt;. Then
&lt;span class=&#34;math display&#34;&gt;\[Z\text{ and }\dfrac{aX + bY}{\big(\sqrt{a}+\sqrt{b}\big)^2}\text{, }0 &amp;lt; a, b &amp;lt; \infty\]&lt;/span&gt;
are identically distributed if and only if &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is a density of Lévy
distribution with arbitrary scale parameter &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The first application of this characterization in the development of a
goodness-of-fit test for the Lévy distribution was presented in
&lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-bhati2020jackknife&#34;&gt;4&lt;/a&gt;]&lt;/span&gt; for the specific case of &lt;span class=&#34;math inline&#34;&gt;\(a=b=1\)&lt;/span&gt;. They proposed a
test statistic given by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\begin{equation*} T_n^*=\int_{{\mathbb{R}^+}}\Big(\frac{1}{\binom{n}{2}}\sum\limits_{j&amp;lt;i}I\Big\{\frac{X_i+X_j}{4}\leq t\Big\}-F_n(t)\Big)dF_n(t). \end{equation*}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-lukic2023characterization&#34;&gt;1&lt;/a&gt;]&lt;/span&gt;, we extended the aforementioned statistic
to cover the case of arbitrary values of &lt;span class=&#34;math inline&#34;&gt;\(a, b\in \mathbb{N}\)&lt;/span&gt;.
Additionally, we investigated the asymptotic distributions of these
generalized test statistics.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;our-test-statistics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Our test statistics&lt;/h2&gt;
&lt;p&gt;The equivalence in distribution between two random variables can also be
established by equating their Laplace transforms. Considering this, our
tests are constructed either as the supremum of the difference or the
integrated difference of the corresponding V-empirical Laplace
transforms of the terms described in the Characterization 1. The
underlying rationale for this approach is that the test statistic will
have small values when the sample is drawn from the Lévy distribution.
The proposed test statistics are of the form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}\label{statJn}
J_{n,a}&amp;amp;=\sup\limits_{t&amp;gt;0}\Big \vert \Big (\frac{1}{n^2}\sum\limits_{i, j} e^{-\frac{t(Y_{i}+Y_{j})}{4}}-\frac{1}{n}\sum\limits_{i} e^{-tY_{i}}\Big )e^{-at} t^{\frac{3}{2}}\Big \vert =\sup\limits_{t\in [0, 1]}\Big \vert \Big (\frac{1}{n^2}\sum\limits_{i, j} t^{\frac{Y_{i}+Y_{j}}{4}}-\frac{1}{n}\sum\limits_{i} t^{Y_{i}}\Big )t^{a}\big (-\log t\big)^{\frac{3}{2}}\Big \vert ,\\
R_{n, a}&amp;amp; =\int_{\mathbb{R}^+}\Big (\frac{1}{n}\sum\limits_{i} e^{-tY_{i}}-\frac{1}{n^2}\sum\limits_{i, j} e^{-\frac{t(Y_{i}+Y_{j})}{4}}\Big )e^{-at} t^{\frac{3}{2}}dt=\frac{3\sqrt{\pi}}{4n^2}\sum_{i, j}\Bigg(\frac{1}{\big(a+\frac{Y_i+Y_j}{4}\big)^\frac{5}{2}}-\frac{1}{2\big(a+Y_i\big)^\frac{5}{2}}-\frac{1}{2\big(a+Y_j\big)^\frac{5}{2}}\Bigg),
\end{align}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(Y_k=\frac{X_k}{\hat{\lambda}}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\lambda}\)&lt;/span&gt; is
a suitably chosen estimate of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. We have considered maximum
likelihood and median-based estimates. It should be noted that any
consistent estimate of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; can be employed. The performance of the
tests naturally varies depending on the chosen estimate.&lt;/p&gt;
&lt;p&gt;We have determined the asymptotic distributions of the novel tests and
provided the 95th percentiles of empirical distributions for large
sample sizes, demonstrating a fast stabilization of the distribution.
These results are summarized in the following two theorems:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 1&lt;/strong&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(a\geq 1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots, X_n\)&lt;/span&gt; be i.i.d random
variables distributed according to the Lévy law with scale parameter
&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. Then the following holds:
&lt;span class=&#34;math inline&#34;&gt;\(\begin{equation*} \sqrt{n} J_{n, a}\overset{{D}}{\to} \sup\limits_{t\in [0, 1]}\mid \xi (t)\mid, \end{equation*}\)&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\xi(t)\)&lt;/span&gt; is a centred Gaussian random process, having the
following covariance function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\begin{align} K(s,t)=&amp;amp; s^a t^a (-\log (s))^{3/2} (-\log (t))^{3/2} \Big(-e^{-\sqrt{2} \big(\sqrt{-\log (s)}+\sqrt{(-\log (t))}\big)}-2 e^{- \sqrt{-2(\log(s)-\frac14\log(t))}+\sqrt{-\frac{\log (t)}{2}}}\\&amp;amp;-2 e^{- \sqrt{2(-\log (t)-{\frac14}\log(s))}+\sqrt{-\frac{\log (s)}{2}}}+4 e^{-\frac{\sqrt{-\log (s t)}+\sqrt{-\log (s)}+\sqrt{-\log (t)}}{\sqrt{2}}}+e^{ \sqrt{-2\log (s t)}}\Big). \end{align}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 2&lt;/strong&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(a\geq 1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots, X_n\)&lt;/span&gt; be i.i.d random
variables distributed according to the Lévy law with scale parameter
&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. Then, for every &lt;span class=&#34;math inline&#34;&gt;\(a&amp;gt;0\)&lt;/span&gt;, the asymptotic distribution of
&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{n}R_{n, a}\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(n\to\infty\)&lt;/span&gt; is normal
&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{N}(0, \sigma^2_R(a))\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_R(a)= 4E\zeta(X; a)^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The expression for &lt;span class=&#34;math inline&#34;&gt;\(\zeta\)&lt;/span&gt; is intricate, and for the exact formulation,
we refer the reader to &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-lukic2023characterization&#34;&gt;1&lt;/a&gt;]&lt;/span&gt; for the exact
expression.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;performance-of-novel-tests&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Performance of novel tests&lt;/h2&gt;
&lt;p&gt;For assessing the performance of test statistics, one can usually
consider their powers against a wide range of alternatives.&lt;/p&gt;
&lt;p&gt;We conducted a power analysis of the tests at a significance level of
&lt;span class=&#34;math inline&#34;&gt;\(\alpha=0.05\)&lt;/span&gt; using a Monte Carlo method with 10,000 replications (N =
10,000). The objective of our study was to compare the JEL and AJEL
approaches proposed &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-bhati2020jackknife&#34;&gt;4&lt;/a&gt;]&lt;/span&gt; with the classical approach, as
well as to determine the empirical power of the new tests. The test
powers were obtained using the Monte Carlo approach. Furthermore, the
supremum of the calculation for &lt;span class=&#34;math inline&#34;&gt;\(J_{n, a}\)&lt;/span&gt; was acquired using a grid
search on 1,000 equidistant points within the interval [0, 1].&lt;/p&gt;
&lt;p&gt;Our findings revealed that the JEL and AJEL approaches proposed in
&lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-bhati2020jackknife&#34;&gt;4&lt;/a&gt;]&lt;/span&gt; are less powerful than the classical approach when
the testing is conducted using the original version of
&lt;span class=&#34;math inline&#34;&gt;\(\vert I^{[1,1]}\vert\)&lt;/span&gt;. In almost all cases, both &lt;span class=&#34;math inline&#34;&gt;\(R_{a}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(J_{a}\)&lt;/span&gt;
outperform the JEL and AJEL methods. We conclude that the novel tests
demonstrate superior performance compared to the tests &lt;span class=&#34;math inline&#34;&gt;\(N_1^a\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(N_1^b\)&lt;/span&gt; proposed in &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-pitera2022goodness&#34;&gt;5&lt;/a&gt;]&lt;/span&gt;. When compared to EDF-based
tests, the performance of the novel tests is better in some cases and
comparable in others, both for median-based and maximum likelihood
estimators.&lt;/p&gt;
&lt;p&gt;In the case of large samples, the most natural way to compare tests is
through the notion of asymptotic efficiency.&lt;/p&gt;
&lt;p&gt;For a detailed review of the theory presented below, we refer the reader
to the comprehensive work of Nikitin &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-nikitinKnjiga&#34;&gt;6&lt;/a&gt;]&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{G}=\{g(x;\theta),\;\theta&amp;gt;0\}\)&lt;/span&gt; be a family of alternatives
density functions, such that &lt;span class=&#34;math inline&#34;&gt;\(g(x;0)\)&lt;/span&gt; has the Lévy distribution with
arbitrary scale parameter, and
&lt;span class=&#34;math inline&#34;&gt;\(\int_{ \mathbb{R}^+ }\frac{1}{x^2}g(x;\theta)dx&amp;lt;\infty\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; in
the neighbourhood of 0, and some additional regularity conditions for
U-statistics with non-degenerate kernels hold &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-nikitinMetron&#34;&gt;7&lt;/a&gt;, &lt;a href=&#34;#ref-meintanis2022bahadur&#34;&gt;8&lt;/a&gt;]&lt;/span&gt;. Let also &lt;span class=&#34;math inline&#34;&gt;\(\{T_n\}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\{V_n\}\)&lt;/span&gt; be two
sequences of test statistic that we want to compare.&lt;/p&gt;
&lt;p&gt;Then for any alternative distribution from &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{G}\)&lt;/span&gt; the relative
Bahadur efficiency of the &lt;span class=&#34;math inline&#34;&gt;\(\{T_n\}\)&lt;/span&gt; with respect to &lt;span class=&#34;math inline&#34;&gt;\(\{V_n\}\)&lt;/span&gt; can be
expressed as &lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
    e_{(T,V)}(\theta)=\frac{c_T(\theta)}{c_V(\theta)},
\end{align*}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(c_{T}(\theta)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c_V(\theta)\)&lt;/span&gt; are the Bahadur
exact slopes, functions proportional to the exponential rate of decrease
of each test size when the sample size increases. It is usually assumed
that &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; belongs to the neighborhood of 0, and in such cases, we
refer to the local relative Bahadur efficiency of considered sequences
of test statistics.&lt;/p&gt;
&lt;p&gt;It is well known that for the Bahadur slope function
Bahadur–Ragavacharri inequality holds &lt;span class=&#34;citation&#34;&gt;[&lt;a href=&#34;#ref-raghavachari1970theorem&#34;&gt;9&lt;/a&gt;]&lt;/span&gt;, that is
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
    c_T(\theta)\leq 2K(\theta),
\end{align*}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(K(\theta)\)&lt;/span&gt; is the minimal Kullback–Leibler distance
from the alternative to the class of null hypotheses. This justifies the
definition of the local absolute Bahadur efficiency by
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}\label{effT}
    eff(T)=\lim_{\theta\to 0}\frac{c_T(\theta)}{2K(\theta)}.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If the sequence &lt;span class=&#34;math inline&#34;&gt;\(\{T_{n}\}\)&lt;/span&gt; of test statistics under the alternative
converges in probability to some finite function &lt;span class=&#34;math inline&#34;&gt;\(b(\theta)&amp;gt;0\)&lt;/span&gt; and the
limit &lt;span class=&#34;math display&#34;&gt;\[\label{ldf}
\lim_{n\leftarrow\infty}n^{-1}\log P_{H_{0}}(T_{n}\geq t)=-f_{LD}(t)
\]&lt;/span&gt; exists for any &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; in an open interval &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt;, on which &lt;span class=&#34;math inline&#34;&gt;\(f_{LD}\)&lt;/span&gt; is
continuous and &lt;span class=&#34;math inline&#34;&gt;\(\{b(\theta),\theta&amp;gt;0\}\subset I\)&lt;/span&gt; then the Bahadur exact
slope is equal to &lt;span class=&#34;math display&#34;&gt;\[\begin{equation}\label{slope}
c_{T}(\theta)=2f_{LD}(b(\theta)).
\end{equation}\]&lt;/span&gt; However, in many instances, calculating the large
deviation function, and consequently the Bahadur slope, proves to be an
extremely challenging task.&lt;/p&gt;
&lt;p&gt;In situations where the function &lt;span class=&#34;math inline&#34;&gt;\(c_T\)&lt;/span&gt; cannot be computed as &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;
approaches zero, an alternative approach is to approximate the Bahadur
slope as &lt;span class=&#34;math inline&#34;&gt;\(c_T^*(\theta)\)&lt;/span&gt;. This approximate slope often closely coincides
with the exact one. To calculate the approximate slope, we do not
require the tail behavior of the distribution function of the statistics
&lt;span class=&#34;math inline&#34;&gt;\(T_n\)&lt;/span&gt; but instead need the tail behavior of its limiting distribution,
which is typically easier to determine.&lt;/p&gt;
&lt;p&gt;Specifically, if the limiting distribution function of &lt;span class=&#34;math inline&#34;&gt;\(T_n\)&lt;/span&gt; under the
null hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; is denoted as &lt;span class=&#34;math inline&#34;&gt;\(F_T\)&lt;/span&gt;, and its tail behavior is
given by &lt;span class=&#34;math inline&#34;&gt;\(\log(1-F_T(t)) = -\frac{a_T^* t^2}{2}(1+o(1))\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(a_T^*\)&lt;/span&gt;
is a positive real number, and the limit in probability of
&lt;span class=&#34;math inline&#34;&gt;\(\frac{T_n}{\sqrt{n}}\)&lt;/span&gt; is denoted as &lt;span class=&#34;math inline&#34;&gt;\(b_T^*(\theta) &amp;gt; 0\)&lt;/span&gt;, then the
approximate Bahadur slope is equal to
&lt;span class=&#34;math inline&#34;&gt;\(c_T^*(\theta) = a_T^*\cdot (b_T^*(\theta))^2\)&lt;/span&gt;. For the calculation of
the local approximate Bahadur slope, one can utilize Maclaurin
expansion.&lt;/p&gt;
&lt;p&gt;Our research findings, focusing solely on the case of the maximum
likelihood estimator, revealed that the tuning parameter &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;
significantly impacts the efficiency of &lt;span class=&#34;math inline&#34;&gt;\(R_a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(J_a\)&lt;/span&gt;. In all examined
scenarios, the Bahadur efficiency of &lt;span class=&#34;math inline&#34;&gt;\(J_a\)&lt;/span&gt; decreases as &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; increases.
However, this is not the case for the statistic &lt;span class=&#34;math inline&#34;&gt;\(R_a\)&lt;/span&gt;. Based on our
analysis, we concluded that the new statistics outperform the
Bhati-Kattumanil one, with &lt;span class=&#34;math inline&#34;&gt;\(R_a\)&lt;/span&gt; exhibiting superior performance in
terms of local approximate Bahadur efficiencies.&lt;/p&gt;
&lt;p&gt;We applied the novel tests on two real datasets. The first one contained the weighted rainfall data for the month of January in India. The second dataset consisted of the well yields near Bel Air, Hartford county, Maryland.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-12-25-characterization-based-approach-for-construction-of-goodness-of-fit-test-for-l-vy-distribution_files/lukic_milosevic.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure: Histogram of rainfall application data and the appropriate Lévy density. The theoretical Lévy densities are drawn using the maximum likelihood estimate of the scale parameter &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;about-the-authors&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;About the authors&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/in/%C5%BEikica-l-128041242/&#34;&gt;Žikica Lukić&lt;/a&gt;,
PhD student at the Faculty of Mathematics, University of Belgrade&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://poincare.matf.bg.ac.rs/~bojana/en/&#34;&gt;Bojana Milošević&lt;/a&gt;,
Associate professor at the Faculty of Mathematics, University of
Belgrade&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;based-on&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Based on&lt;/h1&gt;
&lt;p&gt;Žikica Lukić &amp;amp; Bojana Milošević (2023) Characterization-based approach
for construction of goodness-of-fit test for Lévy distribution,
Statistics, 57:5, 1087-1116, DOI:
&lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/02331888.2023.2238236&#34;&gt;10.1080/02331888.2023.2238236&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body&#34;&gt;
&lt;div id=&#34;ref-lukic2023characterization&#34; class=&#34;csl-entry&#34;&gt;
&lt;div class=&#34;csl-left-margin&#34;&gt;[1] &lt;/div&gt;&lt;div class=&#34;csl-right-inline&#34;&gt;&lt;span class=&#34;smallcaps&#34;&gt;Lukić&lt;/span&gt;, Ž. and &lt;span class=&#34;smallcaps&#34;&gt;Milošević&lt;/span&gt;, B. (2023). &lt;a href=&#34;https://doi.org/10.1080/02331888.2023.2238236&#34;&gt;&lt;span class=&#34;nocase&#34;&gt;Characterization-based approach for construction of goodness-of-fit test for L&lt;span class=&#34;nocase&#34;&gt;é&lt;/span&gt;vy distribution&lt;/span&gt;&lt;/a&gt;. &lt;em&gt;Statistics&lt;/em&gt; &lt;strong&gt;57&lt;/strong&gt; 1087–116.&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-revista&#34; class=&#34;csl-entry&#34;&gt;
&lt;div class=&#34;csl-left-margin&#34;&gt;[2] &lt;/div&gt;&lt;div class=&#34;csl-right-inline&#34;&gt;&lt;span class=&#34;smallcaps&#34;&gt;Cuparić&lt;/span&gt;, M., &lt;span class=&#34;smallcaps&#34;&gt;Milošević&lt;/span&gt;, B. and &lt;span class=&#34;smallcaps&#34;&gt;Obradović&lt;/span&gt;, M. (2022). &lt;a href=&#34;https://doi.org/10.1007/s13398-021-01184-3&#34;&gt;&lt;span class=&#34;nocase&#34;&gt;New consistent exponentiality tests based on V-empirical Laplace transforms with comparison of efficiencies&lt;/span&gt;&lt;/a&gt;. &lt;em&gt;Revista de la Real Academia de Ciencias Exactas, F&lt;span&gt;ı́&lt;/span&gt;sicas y Naturales. Serie A. Matem&lt;span&gt;á&lt;/span&gt;ticas&lt;/em&gt; &lt;strong&gt;116&lt;/strong&gt; 42.&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-AhsNev1&#34; class=&#34;csl-entry&#34;&gt;
&lt;div class=&#34;csl-left-margin&#34;&gt;[3] &lt;/div&gt;&lt;div class=&#34;csl-right-inline&#34;&gt;&lt;span class=&#34;smallcaps&#34;&gt;Ahsanullah&lt;/span&gt;, M. and &lt;span class=&#34;smallcaps&#34;&gt;Nevzorov&lt;/span&gt;, V. B. (2019). &lt;a href=&#34;https://doi.org/10.1515/eqc-2018-0031&#34;&gt;&lt;span class=&#34;nocase&#34;&gt;On Some Characterizations of the Levy Distribution&lt;/span&gt;&lt;/a&gt;. &lt;em&gt;Stochastics and Quality Control&lt;/em&gt; &lt;strong&gt;34&lt;/strong&gt; 53–7.&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-bhati2020jackknife&#34; class=&#34;csl-entry&#34;&gt;
&lt;div class=&#34;csl-left-margin&#34;&gt;[4] &lt;/div&gt;&lt;div class=&#34;csl-right-inline&#34;&gt;&lt;span class=&#34;smallcaps&#34;&gt;Bhati&lt;/span&gt;, D. and &lt;span class=&#34;smallcaps&#34;&gt;Kattumannil&lt;/span&gt;, S. K. (2020). &lt;a href=&#34;https://doi.org/10.1080/02664763.2019.1672630&#34;&gt;&lt;span class=&#34;nocase&#34;&gt;Jackknife empirical likelihood test for testing one-sided L&lt;span class=&#34;nocase&#34;&gt;é&lt;/span&gt;vy distribution&lt;/span&gt;&lt;/a&gt;. &lt;em&gt;Journal of Applied Statistics&lt;/em&gt; &lt;strong&gt;47&lt;/strong&gt; 1208–19.&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-pitera2022goodness&#34; class=&#34;csl-entry&#34;&gt;
&lt;div class=&#34;csl-left-margin&#34;&gt;[5] &lt;/div&gt;&lt;div class=&#34;csl-right-inline&#34;&gt;&lt;span class=&#34;smallcaps&#34;&gt;Pitera&lt;/span&gt;, M., &lt;span class=&#34;smallcaps&#34;&gt;Chechkin&lt;/span&gt;, A. and &lt;span class=&#34;smallcaps&#34;&gt;Wyłomańska&lt;/span&gt;, A. (2022). &lt;a href=&#34;https://doi.org/10.1007/s10260-021-00571-9&#34;&gt;&lt;span class=&#34;nocase&#34;&gt;Goodness-of-fit test for &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;-stable distribution based on the quantile conditional variance statistics&lt;/span&gt;&lt;/a&gt;. &lt;em&gt;Statistical Methods &amp;amp; Applications&lt;/em&gt; &lt;strong&gt;31&lt;/strong&gt; 387–424.&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-nikitinKnjiga&#34; class=&#34;csl-entry&#34;&gt;
&lt;div class=&#34;csl-left-margin&#34;&gt;[6] &lt;/div&gt;&lt;div class=&#34;csl-right-inline&#34;&gt;&lt;span class=&#34;smallcaps&#34;&gt;Nikitin&lt;/span&gt;, Ya. Yu. (1995). &lt;em&gt;&lt;a href=&#34;https://doi.org/10.1017/CBO9780511530081&#34;&gt;&lt;span class=&#34;nocase&#34;&gt;Asymptotic efficiency of nonparametric tests&lt;/span&gt;&lt;/a&gt;&lt;/em&gt;. Cambridge University Press, New York.&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-nikitinMetron&#34; class=&#34;csl-entry&#34;&gt;
&lt;div class=&#34;csl-left-margin&#34;&gt;[7] &lt;/div&gt;&lt;div class=&#34;csl-right-inline&#34;&gt;&lt;span class=&#34;smallcaps&#34;&gt;Nikitin&lt;/span&gt;, Ya. Y. and &lt;span class=&#34;smallcaps&#34;&gt;Peaucelle&lt;/span&gt;, I. (2004). &lt;a href=&#34;&#34;&gt;&lt;span class=&#34;nocase&#34;&gt;Efficiency and local optimality of nonparametric tests based on U-and V-statistics&lt;/span&gt;&lt;/a&gt;. &lt;em&gt;Metron&lt;/em&gt; &lt;strong&gt;62&lt;/strong&gt; 185–200.&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-meintanis2022bahadur&#34; class=&#34;csl-entry&#34;&gt;
&lt;div class=&#34;csl-left-margin&#34;&gt;[8] &lt;/div&gt;&lt;div class=&#34;csl-right-inline&#34;&gt;&lt;span class=&#34;smallcaps&#34;&gt;Meintanis&lt;/span&gt;, S., &lt;span class=&#34;smallcaps&#34;&gt;Milošević&lt;/span&gt;, B. and &lt;span class=&#34;smallcaps&#34;&gt;Obradović&lt;/span&gt;, M. (2022). &lt;a href=&#34;https://doi.org/10.1007/s00184-022-00891-0&#34;&gt;Bahadur efficiency for certain goodness-of-fit tests based on the empirical characteristic function&lt;/a&gt;. &lt;em&gt;Metrika&lt;/em&gt; 1–29.&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-raghavachari1970theorem&#34; class=&#34;csl-entry&#34;&gt;
&lt;div class=&#34;csl-left-margin&#34;&gt;[9] &lt;/div&gt;&lt;div class=&#34;csl-right-inline&#34;&gt;&lt;span class=&#34;smallcaps&#34;&gt;Raghavachari&lt;/span&gt;, M. (1970). &lt;a href=&#34;https://doi.org/10.1214/aoms/1177696813&#34;&gt;&lt;span class=&#34;nocase&#34;&gt;On a theorem of Bahadur on the rate of convergence of test statistics&lt;/span&gt;&lt;/a&gt;. &lt;em&gt;The Annals of Mathematical Statistics&lt;/em&gt; 1695–9.&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Merry Christmas and Happy New Year 2024!</title>
      <link>https://youngstats.github.io/post/2023/12/24/merry-christmas-and-happy-new-year-2024/</link>
      <pubDate>Sun, 24 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2023/12/24/merry-christmas-and-happy-new-year-2024/</guid>
      <description>


&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-12-24-merry-christmas-and-happy-new-year-2024_files/HappyNY2024.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Dear Followers of the YoungStatS project, Dear All!&lt;/p&gt;
&lt;p&gt;It has been another intense year for our project, including 4 novel One World YoungStatS webinars, 12 blogposts from leading authors in various areas of statistics and data science, probability and econometrics, and 3 short contributions for the IMS Bulletin.&lt;/p&gt;
&lt;p&gt;In particular, we wish to thank our supporters: The Federation of European National Statistical Societies (FENStatS), Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics (IMS).&lt;/p&gt;
&lt;p&gt;And we thank You, readers of our postings and participants at our webinars for your support and feedback thus far!&lt;/p&gt;
&lt;p&gt;YoungStatS project will resume after the New Year including webinars on random matrix theory, copula models, network econometric models, and regular blogposts. You would also be able to read shorter interviews with some of our contributors soon in the IMS Bulletin, as well as continue following our joint webinar series with the IMS New Researchers Group and Young Data Science Researcher Seminar Zürich.&lt;/p&gt;
&lt;p&gt;We wish you to have a great holiday time - Merry Christmas and a Happy New Year 2024!&lt;/p&gt;
&lt;p&gt;Editorial Members of the YoungStatS project.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Non-stationary wrapped Gaussian spatial response model</title>
      <link>https://youngstats.github.io/post/2023/11/22/non-stationary-wrapped-gaussian-spatial-response-model/</link>
      <pubDate>Wed, 22 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2023/11/22/non-stationary-wrapped-gaussian-spatial-response-model/</guid>
      <description>


&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;Circular data, i.e., data defined on the unit circle, can be found in many areas of science.
The unique nature of these data means that conventional methods for non-circular data are not valid for these.
At the same time, advances in geographical information and global positioning systems have generated large amounts of spatial data and, consequently, have increased the need for spatial models that accurately describe it.
An example of circular, spatial data are the average wind directions in Germany from April 1 to 10, 2019, plotted in space (left) and on a circular histogram (right):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-11-22-non-stationary-wrapped-gaussian-spatial-response-model_files/plot_calm-horz.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It was &lt;span class=&#34;citation&#34;&gt;Jona-Lasinio, Gelfand, and Jona-Lasinio (&lt;a href=&#34;#ref-jona2012spatial&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Wang and Gelfand (&lt;a href=&#34;#ref-wang2014modeling&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt; that first brought the wrapped and projected Gaussian distributions to space and space-time settings.
Nonetheless, today,&lt;strong&gt;circular data models still need to catch up on the latest advances in spatial statistics&lt;/strong&gt;.
In particular, models developed so far &lt;strong&gt;assume stationarity in the mean and the covariance of the response&lt;/strong&gt; modeled; i.e., these are constant in space.
Our work in &lt;span class=&#34;citation&#34;&gt;Marques, Kneib, and Klein (&lt;a href=&#34;#ref-marques2022non&#34;&gt;2022&lt;/a&gt;)&lt;/span&gt; aims to bridge this gap and allow for non-stationarity in the mean and covariance of distributions for circular responses defined on spatial domains.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-response-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The response distribution&lt;/h2&gt;
&lt;p&gt;To construct distributions for circular data, we can rely on the construction principle of wrapping the distribution of a random variable &lt;span class=&#34;math inline&#34;&gt;\(Y \in \mathbb{R}\)&lt;/span&gt; defined on the real line around the unit circle to make its domain adhere to the interval &lt;span class=&#34;math inline&#34;&gt;\([0,2\pi]\)&lt;/span&gt;.
The result is a circular – or &lt;em&gt;wrapped&lt;/em&gt; – random variable &lt;span class=&#34;math inline&#34;&gt;\(X \in \lbrack 0,2\pi)\)&lt;/span&gt;.
Then, the &lt;em&gt;unwrapped&lt;/em&gt; variable &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; can be expressed as &lt;span class=&#34;math display&#34;&gt;\[
Y = X + 2 \pi K
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(K \in \mathbb{Z}\)&lt;/span&gt; measures the number of “turns” &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; has be wrapped around the unit circle.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{s}\)&lt;/span&gt; denote a spatial index variable within a spatial domain &lt;span class=&#34;math inline&#34;&gt;\(S \subset \mathbb{R}^2\)&lt;/span&gt;.
&lt;span class=&#34;citation&#34;&gt;Jona-Lasinio, Gelfand, and Jona-Lasinio (&lt;a href=&#34;#ref-jona2012spatial&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt; first showed that this wrapping approach can be extended to the context of spatial data.
We extend their framework and consider
&lt;span class=&#34;math display&#34;&gt;\[ X(\mathbf{s}_i) = \beta_0 + \mathbf{z}(\mathbf{s}_i)&amp;#39; \boldsymbol{\beta}  +\gamma(\mathbf{s}_i, \mathbf{z}^\kappa(\mathbf{s}_i)) + 2 \pi K(\mathbf{s}_i) + \varepsilon_i ,\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(i=1, \ldots,n\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\gamma(\mathbf{s}_i)\)&lt;/span&gt; is a zero-mean Gaussian random field (GRF) and &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_i \sim N(0, \sigma^2_\varepsilon)\)&lt;/span&gt; are the i.i.d. error terms, with the following novelties:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The mean depends on &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{z}(\mathbf{s}_i)&amp;#39; \boldsymbol{\beta}\)&lt;/span&gt;, i.e. it is non-stationary,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The covariance depends on &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{z}^\kappa(\mathbf{s}_i)\)&lt;/span&gt;, i.e. it is non-stationary,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We include small-scale noise &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_i\)&lt;/span&gt;s.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We focus on 1. and 2.&lt;/p&gt;
&lt;div id=&#34;non-stationarity-in-the-mean&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;1. Non-stationarity in the mean&lt;/h3&gt;
&lt;p&gt;Linear covariates included in the mean of a circular response might induce a circular likelihood for the regression coefficients that has infinitely many maxima.
The typical solution is to use a link function for fixed effects with co-domain defined in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}\)&lt;/span&gt; and of length equal to the circular variable’s period of &lt;span class=&#34;math inline&#34;&gt;\(2\pi\)&lt;/span&gt;, e.g. the inverse tangent link function.&lt;/p&gt;
&lt;p&gt;We argue that:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The winding number &lt;span class=&#34;math inline&#34;&gt;\(K(\mathbf{s})\)&lt;/span&gt; can govern the behavior of the combination of GRF and fixed effects such that no link function is needed,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We use prior information that shrinks towards a stationary mean and enforces the length of the fixed effects to be approximately &lt;span class=&#34;math inline&#34;&gt;\(2\pi\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Concretely, &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 \sim N(0, 10)\)&lt;/span&gt; represents a diffuse prior for the intercept and we let
&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\beta} \vert \xi^2 \sim N(\mathbf{0}, \xi^2 \mathbf{I})\)&lt;/span&gt;. One can show that
if the base model is such that &lt;span class=&#34;math inline&#34;&gt;\(\xi^2 \to 0\)&lt;/span&gt; and the prior is constructed
according to the PC-prior principles &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-simpson2017penalising&#34;&gt;Simpson et al. 2017&lt;/a&gt;)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\xi^2\)&lt;/span&gt;
has a Weibull prior with shape &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{2}\)&lt;/span&gt; and scale &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, i.e., &lt;span class=&#34;math inline&#34;&gt;\(\xi^2 \sim \text{Weibull}(\frac{1}{2}, \lambda)\)&lt;/span&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-klein2016scale&#34;&gt;Klein and Kneib 2016&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For &lt;span class=&#34;math inline&#34;&gt;\(\alpha \in (0, 1)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is chosen such that
&lt;span class=&#34;math display&#34;&gt;\[
     P\left( \text{max}_{\mathbf{s} \in S} \mid
    \mathbf{z}(\mathbf{s})&amp;#39;\boldsymbol{\beta} \mid \leq\,  \pi \right)
    \geq 1 - \alpha,
\]&lt;/span&gt;
i.e., the probability that the maximum norm of the non-stationary effect in the mean of the response is smaller than &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; is larger than &lt;span class=&#34;math inline&#34;&gt;\(1 - \alpha\)&lt;/span&gt;. The value &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; in the expression is close to the recommendation of &lt;span class=&#34;math inline&#34;&gt;\(3\)&lt;/span&gt; in &lt;span class=&#34;citation&#34;&gt;Klein and Kneib (&lt;a href=&#34;#ref-klein2016scale&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;non-stationarity-in-the-covariance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2. Non-stationarity in the covariance&lt;/h3&gt;
&lt;p&gt;The stochastic partial differential equation (SPDE) approach &lt;span class=&#34;citation&#34;&gt;(see &lt;a href=&#34;#ref-lindgren2011explicit&#34;&gt;Lindgren, Rue, and Lindström 2011&lt;/a&gt;)&lt;/span&gt; allows us to model non-stationarity in the covariance of the GRF in a computationally efficient way.
Consider the parameters of the Matérn covariance with smoothness &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\kappa\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; controlling the range and the marginal variance of the GRF, respectively.&lt;/p&gt;
&lt;p&gt;We let
&lt;span class=&#34;math display&#34;&gt;\[
\log(\tau) =  \theta^\tau_0 \mbox{ and } \log(\kappa(\mathbf{s})) =
\theta^\kappa_0 + \mathbf{z}^\kappa(\mathbf{s})&amp;#39;\boldsymbol{\theta}^\kappa_z,
\]&lt;/span&gt;
where uniform priors for &lt;span class=&#34;math inline&#34;&gt;\(\theta^\tau_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\theta_0^\kappa\)&lt;/span&gt; guarantee that the marginal variance is in &lt;span class=&#34;math inline&#34;&gt;\([0.01, (2\pi)^2]\)&lt;/span&gt; and the spatial range is in &lt;span class=&#34;math inline&#34;&gt;\([0.01, 1]\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(S \subseteq [0, 1] \times [0,1]\)&lt;/span&gt;.
Once again, we use a simulation-based method to guarantee&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
P\left( \text{max}_{\mathbf{s} \in S} \mid
\mathbf{z}^\kappa(\mathbf{s})&amp;#39;\boldsymbol{\theta}^\kappa_z \mid \leq\,  c \right)
\geq 1 - \alpha_\kappa,
\]&lt;/span&gt;
such that &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; guarantees that the spatial range satisfies &lt;span class=&#34;math inline&#34;&gt;\(\rho(\mathbf{s}) \in [0.01, 1]\)&lt;/span&gt;, with typically &lt;span class=&#34;math inline&#34;&gt;\(c = 3\)&lt;/span&gt; &lt;span class=&#34;citation&#34;&gt;(see &lt;a href=&#34;#ref-klein2016scale&#34;&gt;Klein and Kneib 2016&lt;/a&gt;)&lt;/span&gt;. The resulting prior structure shrinks towards a model with stationary covariance.&lt;/p&gt;
&lt;p&gt;More concretely, we have &lt;span class=&#34;math inline&#34;&gt;\(\theta^\tau_0 \sim U(-10, 3)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\theta^\kappa_0 \sim U(1, 6)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\theta}_z^\kappa \vert \zeta^2 \sim N(\mathbf{0}, \zeta^2 \mathbf{I})\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(\zeta^2 \sim \text{Weibull}(\frac{1}{2}, \lambda_\kappa)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-full-hierarchical-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The full-hierarchical model&lt;/h2&gt;
&lt;p&gt;The SPDE is typically used in combination with integrated nested Laplace approximations (INLA). However, INLA does not perform well when combined with shrinkage priors for non-stationarity in the covariance as defined here. Thus, we use Markov Chain Monte Carlo with full-hierarchical model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
X(\mathbf{s}) &amp;amp;= \eta(\mathbf{s}) + 2 \pi K(\mathbf{s}) +
\varepsilon, \\
&amp;amp;= \beta_0 + \mathbf{z}(\mathbf{s})&amp;#39; \boldsymbol{\beta} + \gamma(\mathbf{s}, \mathbf{z}^\kappa(\mathbf{s})) + 2 \pi K(\mathbf{s}) +
\varepsilon,\\
\beta_0 &amp;amp;\sim N(0, 10) ,\mbox{ }  \boldsymbol{\beta} \vert \xi^2 \sim N(0, \xi^2 \mathbf{I}), \mbox{ }\boldsymbol{\gamma} \vert  \boldsymbol{\theta} \sim N(\mathbf{0}, \mathbf{Q}^{-1}(\boldsymbol{\theta})) \\
\theta_0^\tau &amp;amp;\sim U(-10, 3) ,\mbox{ }
\theta_0^\kappa \sim U(1, 6) ,\mbox{ }
\boldsymbol{\theta}^\kappa_z  \vert  \zeta^2 \sim N(\mathbf{0}, \zeta^2 \mathbf{I}) \\
\xi^2 &amp;amp;\sim PC\left(\pi, \alpha \right) ,\mbox{ }
\zeta^2 \sim PC\left(3, \alpha_\kappa \right) \\
\mathbf{\varepsilon}  \vert \sigma^2_\varepsilon &amp;amp;\sim N(\mathbf{0}, \sigma^2_\varepsilon \mathbf{I}) ,\mbox{ }
\sigma^2_\varepsilon \sim IG (0.001, 0.001)\\
    p(K(\mathbf{s})) &amp;amp;=
\begin{cases}
    \frac{1}{3},&amp;amp; \text{if } K(\mathbf{s}) \in \lbrace -1, 0, 1 \rbrace\\
    0,              &amp;amp; \text{otherwise}
\end{cases}
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;outcome&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Outcome&lt;/h2&gt;
&lt;p&gt;One can use our model to estimate wind directions in the dataset above and test the model’s performance.
We randomly select a holdout set consisting of &lt;span class=&#34;math inline&#34;&gt;\(20\%\)&lt;/span&gt; of the locations and use the remaining &lt;span class=&#34;math inline&#34;&gt;\(80\%\)&lt;/span&gt; as training data.
The covariates considered for &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{z}(\mathbf{s})\)&lt;/span&gt; are maximum wind speed, average air humidity, and temperature at 10 meters height. For &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{z^\kappa}(\mathbf{s})\)&lt;/span&gt; we consider altitude and latitude.&lt;/p&gt;
&lt;p&gt;The figure below shows the posterior predictive mean (left) and standard deviation (right) for each test location for a fully stationary, i.e., stationary mean and covariance and fully non-stationary models.
The black arrows represent the true wind directions (left) and the corresponding standard deviation of zero (right).
The figure shows that the fully non-stationary model reaches smaller bias than the fully stationary model throughout the whole domain. In the north where this bias is close to zero, our model also reaches lower uncertainty.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-11-22-non-stationary-wrapped-gaussian-spatial-response-model_files/results_calm.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;about-the-authors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;About the authors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.gla.ac.uk/schools/mathematicsstatistics/staff/isamanuelarosagoncalvesmarques/&#34;&gt;Isa Marques&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.uni-goettingen.de/en/264255.html&#34;&gt;Thomas Kneib&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tu-dortmund.de/en/university/newly-appointed-professors/prof-nadja-klein/&#34;&gt;Nadja Klein&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-jona2012spatial&#34; class=&#34;csl-entry&#34;&gt;
Jona-Lasinio, Giovanna, Alan Gelfand, and Mattia Jona-Lasinio. 2012. &lt;span&gt;“&lt;span class=&#34;nocase&#34;&gt;Spatial analysis of wave direction data using wrapped Gaussian processes&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;The Annals of Applied Statistics&lt;/em&gt; 6 (4): 1478–98.
&lt;/div&gt;
&lt;div id=&#34;ref-klein2016scale&#34; class=&#34;csl-entry&#34;&gt;
Klein, Nadja, and Thomas Kneib. 2016. &lt;span&gt;“Scale-Dependent Priors for Variance Parameters in Structured Additive Distributional Regression.”&lt;/span&gt; &lt;em&gt;Bayesian Analysis&lt;/em&gt; 11 (4): 1071–1106.
&lt;/div&gt;
&lt;div id=&#34;ref-lindgren2011explicit&#34; class=&#34;csl-entry&#34;&gt;
Lindgren, Finn, Håvard Rue, and Johan Lindström. 2011. &lt;span&gt;“&lt;span class=&#34;nocase&#34;&gt;An explicit link between Gaussian fields and Gaussian Markov random fields: the stochastic partial differential equation approach&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;Journal of the Royal Statistical Society: Series B (Statistical Methodology)&lt;/em&gt; 73 (4): 423–98.
&lt;/div&gt;
&lt;div id=&#34;ref-marques2022non&#34; class=&#34;csl-entry&#34;&gt;
Marques, Isa, Thomas Kneib, and Nadja Klein. 2022. &lt;span&gt;“A Non-Stationary Model for Spatially Dependent Circular Response Data Based on Wrapped Gaussian Processes.”&lt;/span&gt; &lt;em&gt;Statistics and Computing&lt;/em&gt; 32 (5): 73.
&lt;/div&gt;
&lt;div id=&#34;ref-simpson2017penalising&#34; class=&#34;csl-entry&#34;&gt;
Simpson, Daniel, Håvard Rue, Andrea Riebler, Thiago G Martins, Sigrunn H Sørbye, et al. 2017. &lt;span&gt;“Penalising Model Component Complexity: A Principled, Practical Approach to Constructing Priors.”&lt;/span&gt; &lt;em&gt;Statistical Science&lt;/em&gt; 32 (1): 1–28.
&lt;/div&gt;
&lt;div id=&#34;ref-wang2014modeling&#34; class=&#34;csl-entry&#34;&gt;
Wang, Fangpo, and Alan E Gelfand. 2014. &lt;span&gt;“&lt;span class=&#34;nocase&#34;&gt;Modeling space and space-time directional data using projected Gaussian processes&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 109 (508): 1565–80.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Populations of Unlabeled Networks: Graph Space Geometry and Generalized Geodesic Principal Components</title>
      <link>https://youngstats.github.io/post/2023/11/06/populations-of-unlabeled-networks-graph-space-geometry-and-generalized-geodesic-principal-components/</link>
      <pubDate>Mon, 06 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2023/11/06/populations-of-unlabeled-networks-graph-space-geometry-and-generalized-geodesic-principal-components/</guid>
      <description>


&lt;div id=&#34;overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;Sets of graphs arise in many different applications, from medicine to
finance, from urban planning to social science. Analysing sets of graphs
is far from trivial as they are strongly non Euclidean data type. There
are two main sets of graphs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;labelled: where there is the same sets of nodes across each
observation;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- --&gt;
&lt;ul&gt;
&lt;li&gt;unlabelled: where there is no clear correspondence in the nodes
across networks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Unlabelled graphs poses high challenges from both the embedding
prospective (which geometrical embedding is suitable for such graphs)
and the statistical perspective (how can we extend basic tools to such
embedding).&lt;/p&gt;
&lt;p&gt;In the past years, scholars have been proposing different embedding
strategies for unlabelled graphs. Among existing models: &lt;span class=&#34;citation&#34;&gt;Ginestet et al. (&lt;a href=&#34;#ref-ginestet2017&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;
proposes a model where networks’ Laplacian matrices are smoothly
injected into a sub-manifold of a Euclidean space;
&lt;span class=&#34;citation&#34;&gt;Simpson et al. (&lt;a href=&#34;#ref-simpson2013permutation&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Severn, Dryden, and Preston (&lt;a href=&#34;#ref-severn2020non&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Durante, Dunson, and Vogelstein (&lt;a href=&#34;#ref-durante2017nonparametric&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;
face the problem of generating and performing tests on a population of
networks; &lt;span class=&#34;citation&#34;&gt;Lunagómez, Olhede, and Wolfe (&lt;a href=&#34;#ref-lunagomez2021modeling&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; provide Bayesian modelling for discrete
labeled graphs, and &lt;span class=&#34;citation&#34;&gt;Chowdhury and Mémoli (&lt;a href=&#34;#ref-chowdhury2019gromov&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; studies a metric space of
networks up to weak isomorphism, which allows the grouping of similar
nodes. In &lt;span class=&#34;citation&#34;&gt;Calissano, Feragen, and Vantini (&lt;a href=&#34;#ref-calissano2023populations&#34;&gt;2023&lt;/a&gt;)&lt;/span&gt;, we characterize geometrically the
Graph Space quotient space introduced by &lt;span class=&#34;citation&#34;&gt;Jain and Obermayer (&lt;a href=&#34;#ref-jain2009structure&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt; and we
extended principal component analysis to sets of unlabelled graphs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;graph-space&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Graph Space&lt;/h2&gt;
&lt;p&gt;Consider &lt;span class=&#34;math inline&#34;&gt;\(G_1,\dots, G_k\)&lt;/span&gt; where each &lt;span class=&#34;math inline&#34;&gt;\(G_i=(N_i,E_i,a_i)\)&lt;/span&gt; sets of nodes
&lt;span class=&#34;math inline&#34;&gt;\(N_i\)&lt;/span&gt;, edges &lt;span class=&#34;math inline&#34;&gt;\(E_i\)&lt;/span&gt;, and a real valued attribute function
&lt;span class=&#34;math inline&#34;&gt;\(a_i :E_i \rightarrow \mathbb{R}\)&lt;/span&gt;. Each graph can be described as a set
of adjacency matrix &lt;span class=&#34;math inline&#34;&gt;\(x \in X=\mathbb{R}^{n\times n}\)&lt;/span&gt;. We can embed
unlabelled graphs in a quotient space &lt;span class=&#34;math inline&#34;&gt;\(X / T\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; set of
permutation matrices. Each graph is now represented by its equivalence
class of permuted graphs: &lt;span class=&#34;math display&#34;&gt;\[[x]=\{t^T x t: t\in T\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-11-06-populations-of-unlabeled-networks-graph-space-geometry-and-generalized-geodesic-principal-components_files/exampleequivalenceclasses_graphspace.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 1: Conceptual visualization of Graph Space.&lt;/p&gt;
&lt;p&gt;By equipping the total space with a metric &lt;span class=&#34;math inline&#34;&gt;\((X,d_X)\)&lt;/span&gt; we can define a
quotient metric on &lt;span class=&#34;math inline&#34;&gt;\(X / T\)&lt;/span&gt; as:
&lt;span class=&#34;math display&#34;&gt;\[d_{X/T}([x_1],[x_2])=min_{t\in T}d_X (t^T x_1 t, x_2)\]&lt;/span&gt; Such metric
corresponds in finding the optimal candidate in the equivalence class
which minimize the distance in the total space. The minimization problem
is known as the Graph Matching problem, which is a broadly studied
problem in optimization (see &lt;span class=&#34;citation&#34;&gt;Conte et al. (&lt;a href=&#34;#ref-conte2004thirty&#34;&gt;2004&lt;/a&gt;)&lt;/span&gt; for a review). As we select
&lt;span class=&#34;math inline&#34;&gt;\(d_X\)&lt;/span&gt; to be the Frobenius norm, we use the FAQ Graph matching (&lt;span class=&#34;citation&#34;&gt;Vogelstein et al. (&lt;a href=&#34;#ref-vogelstein2015fast&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-11-06-populations-of-unlabeled-networks-graph-space-geometry-and-generalized-geodesic-principal-components_files/graphspace_totalspace_metric.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 2: Conceptual visualization of the distance in the Graph Space.&lt;/p&gt;
&lt;div id=&#34;short-geometrical-characterization-of-graph-space&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Short Geometrical Characterization of Graph Space&lt;/h3&gt;
&lt;p&gt;The graph space is a metric space &lt;span class=&#34;math inline&#34;&gt;\((X / T,d_{X/T})\)&lt;/span&gt;, but it is not a
manifold: The equivalence classes are often not of the same dimensions
(symmetries or blocks of zeros can cause the permutation to leave the
graph unchanged), thus the action is not free and the space is not a
quotient manifold (see &lt;span class=&#34;citation&#34;&gt;Lee (&lt;a href=&#34;#ref-lee2013smooth&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt; for details on quotient
manifolds). However, the total space &lt;span class=&#34;math inline&#34;&gt;\(X=\mathbb{R}^{n\times n}\)&lt;/span&gt; is
Euclidean. To overcome the complexity of the Graph Space and perform
statistics on unlabelled graphs, we define an algorithm relying on the
total space &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; for the computations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;align-all-and-compute&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Align All and Compute&lt;/h3&gt;
&lt;p&gt;The Align All and Compute Algorithm (AAC) allows to compute intrinsic
statistics on Graph Space by computing the estimators on the total
space. We illustrate it here for the estimation of the Fréchet Mean
(&lt;span class=&#34;citation&#34;&gt;Fréchet (&lt;a href=&#34;#ref-frechet1948elements&#34;&gt;1948&lt;/a&gt;)&lt;/span&gt;). Consider a set of &lt;span class=&#34;math inline&#34;&gt;\(\{[x_1],\dots,[x_k]\}\)&lt;/span&gt; graphs:
&lt;span class=&#34;math display&#34;&gt;\[[\bar{x}]=min_{[x]\in X / T }\sum_{i=1}^{n}d_{X /T}([\bar{x}],[x_i])\]&lt;/span&gt;
Notice that the Fréchet Mean results into the arithmetic mean in the
case of Euclidean data.&lt;/p&gt;
&lt;p&gt;The AAC operates as follows:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AAC algorithm for the the Fréchet Mean&lt;/strong&gt;&lt;br /&gt;
- Input: &lt;span class=&#34;math inline&#34;&gt;\(\{[x_1],\dots,[x_k]\} \subset X/T\)&lt;/span&gt;; a threshold
&lt;span class=&#34;math inline&#34;&gt;\(\varepsilon &amp;gt; 0\)&lt;/span&gt;&lt;br /&gt;
- Initialization: Randomly select
&lt;span class=&#34;math inline&#34;&gt;\(\tilde{x}=\tilde{x_i}\in [x_i] \in \{ [x_1],\dots,[x_k]\}\)&lt;/span&gt;&lt;br /&gt;
- While &lt;span class=&#34;math inline&#34;&gt;\(s&amp;gt;\varepsilon\)&lt;/span&gt;:&lt;br /&gt;
Obtain &lt;span class=&#34;math inline&#34;&gt;\(\tilde{x_i}\)&lt;/span&gt; optimally aligned wrt &lt;span class=&#34;math inline&#34;&gt;\(\tilde{x}\)&lt;/span&gt; for
&lt;span class=&#34;math inline&#34;&gt;\(i =\{ 1,\dots, k\}\)&lt;/span&gt;&lt;br /&gt;
Compute the Fréchet Mean &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; of
&lt;span class=&#34;math inline&#34;&gt;\(\{\tilde{x_1},\tilde{x_2},\dots,\tilde{x_k}\} \in X\)&lt;/span&gt;&lt;br /&gt;
Compute &lt;span class=&#34;math inline&#34;&gt;\(s=d(\tilde{x},\bar{x})\)&lt;/span&gt;&lt;br /&gt;
Set &lt;span class=&#34;math inline&#34;&gt;\(\tilde{x}=\bar{x}\)&lt;/span&gt;&lt;br /&gt;
- Output: &lt;span class=&#34;math inline&#34;&gt;\([\bar{x}]\)&lt;/span&gt;, an estimate of the Fréchet Mean of
&lt;span class=&#34;math inline&#34;&gt;\(\{[x_1], \ldots, [x_k]\} \in X/T\)&lt;/span&gt;.&lt;br /&gt;
&lt;/p&gt;
&lt;p&gt;The Figure 3 represents the algorithm graphically:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-11-06-populations-of-unlabeled-networks-graph-space-geometry-and-generalized-geodesic-principal-components_files/AAC_Scheme_correct_fm.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 3: Conceptual visualization of the AAC algorithm. The star
represents the current estimation of the Frechet Mean.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;generalized-geodesic-principal-components&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generalized Geodesic Principal Components&lt;/h2&gt;
&lt;p&gt;Let’s move now to more complex estimators. To extend the Principal
Component Analysis to Graph Space: (1) we firstly extend the concept of
geodesic to Graph Space; (2) we define a way to align an equivalent
class to a geodesic (i.e. optimal positioning).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 1 (Generalized Geodesics):&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;Denote by&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\Gamma(X)\)&lt;/span&gt; &lt;em&gt;the set of all straight lines (geodesics) in&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;&lt;em&gt;. Following &lt;span class=&#34;citation&#34;&gt;Huckemann, Hotz, and Munk (&lt;a href=&#34;#ref-huckemann&#34;&gt;2010&lt;/a&gt;)&lt;/span&gt;, a curve&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; &lt;em&gt;is a generalized geodesic
on the Graph Space&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(X/T\)&lt;/span&gt;&lt;em&gt;, if it is a projection of a straight line on&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;&lt;em&gt;:&lt;/em&gt; &lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
    \Gamma(X/T)=\{\delta=\pi \circ \gamma:\gamma\in\Gamma(X)\}.
\end{equation}\]&lt;/span&gt; &lt;em&gt;Where&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\pi: X\rightarrow X/T\)&lt;/span&gt; &lt;em&gt;is the canonical
quotient space projection.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Since Graph Space is not an inner product space, we define orthogonality
as:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 2:&lt;/strong&gt; &lt;em&gt;Two generalized geodesics&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\delta_1,\delta_2\in\Gamma(X/T)\)&lt;/span&gt; &lt;em&gt;are orthogonal if they have
representatives in&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\delta_1=\pi\circ\gamma_1,\delta_2=\pi\circ\gamma_2, \gamma_1,\gamma_2\in\Gamma(X)\)&lt;/span&gt;
&lt;em&gt;which are orthogonal&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(&amp;lt;\gamma_1,\gamma_2&amp;gt;_X=0\)&lt;/span&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In order to bridge computations in Graph Space &lt;span class=&#34;math inline&#34;&gt;\(X/T\)&lt;/span&gt; with computations
in the total space &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, we introduce a concept of alignment in &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 3 (Optimal position):&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;Given&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\tilde{x} \in X\)&lt;/span&gt; &lt;em&gt;and&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(t \in T\)&lt;/span&gt;&lt;em&gt;, the point&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(t^T \tilde{x} t\)&lt;/span&gt;
&lt;em&gt;is in&lt;/em&gt; &lt;em&gt;if&lt;/em&gt; &lt;span class=&#34;math display&#34;&gt;\[
d_X(t^T \tilde{x}t,x)= d_{X/T}([\tilde{x}],[x]).
\]&lt;/span&gt; &lt;em&gt;That is, the equivalence class&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\([\tilde{x}] \in X/T\)&lt;/span&gt; &lt;em&gt;contains (at
least) one point&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(t^T \tilde{x} t \in [\tilde{x}]\)&lt;/span&gt; &lt;em&gt;which has minimal
distance to&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;em&gt;, and this point is in optimal position to&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;em&gt;. Next,
consider&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\([x] \in X/T\)&lt;/span&gt;&lt;em&gt;,&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(t \in T\)&lt;/span&gt; &lt;em&gt;and&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; &lt;em&gt;a generalized
geodesic in&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(X/T\)&lt;/span&gt; &lt;em&gt;with representative&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\gamma\in \Gamma(X)\)&lt;/span&gt;&lt;em&gt;. The
graph representative&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(t^T x t\in X\)&lt;/span&gt; &lt;em&gt;is in optimal position to&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\gamma\in\Gamma(X)\)&lt;/span&gt; &lt;em&gt;if&lt;/em&gt; &lt;span class=&#34;math display&#34;&gt;\[d_X(t^T x t ,\gamma)=d_{X/T}([x],\delta).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Having concepts of generalized geodesic, optimal position and
orthogonality, we now define a set of geodesic principal components:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 4:&lt;/strong&gt; &lt;em&gt;Consider the canonical projection of the Graph Space&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\pi \colon X \rightarrow X/T\)&lt;/span&gt; &lt;em&gt;of&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; &lt;em&gt;and consider a set&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\{[x_1],\dots, [x_k]\} \subset X/T\)&lt;/span&gt; &lt;em&gt;of graphs,&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\([x]\in X/T\)&lt;/span&gt;&lt;em&gt;, and&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\delta \in \Gamma(X/T)\)&lt;/span&gt;&lt;em&gt;. The Generalized Geodesic Principal Components
(GGPCs) for the set&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\{[x_1],\dots, [x_k]\}\)&lt;/span&gt; &lt;em&gt;are defined as:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;The first generalized geodesic principal component&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\delta_1 \in \Gamma(X/T)\)&lt;/span&gt; &lt;em&gt;is the generalized geodesic minimizing
the sum of squared residuals:&lt;/em&gt; &lt;span class=&#34;math display&#34;&gt;\[\begin{equation}\label{eq:wrtdelta}
    \delta_1 = \underset{\delta \in \Gamma(X/T)}{\operatorname{argmin}} \sum_{i=1}^{k}{(d_{X/T}^2([x_i],\delta))}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;The second generalized geodesic principal component&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\delta_2 \in \Gamma(X/T)\)&lt;/span&gt; &lt;em&gt;minimizes (2) over all&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\delta \in \Gamma(X/T)\)&lt;/span&gt;&lt;em&gt;, having at least one point in common with&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\delta_1\)&lt;/span&gt; &lt;em&gt;and being orthogonal to&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\delta_1\)&lt;/span&gt; &lt;em&gt;at all points in
common with&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\delta_1\)&lt;/span&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;The point&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\mu\in X/T\)&lt;/span&gt; &lt;em&gt;is called Principal Component Mean if it
minimizes&lt;/em&gt; &lt;span class=&#34;math display&#34;&gt;\[\begin{equation}\label{eq:wrtpoint}
     \sum_{i=1}^{k}{(d_{X/T}^2([x_i],[\mu])^2)}
\end{equation}\]&lt;/span&gt; &lt;em&gt;where&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\([\mu]\)&lt;/span&gt; &lt;em&gt;only runs over points&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\tilde{x}\)&lt;/span&gt;
&lt;em&gt;in common with&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\delta_1\)&lt;/span&gt; &lt;em&gt;and&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\delta_2\)&lt;/span&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;The&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(j^{th}\)&lt;/span&gt; &lt;em&gt;generalized geodesic principal component is a&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\delta_j \in \Gamma(X/T)\)&lt;/span&gt; &lt;em&gt;if it minimizes (2) over all generalized
geodesics that meet orthogonally&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\delta_1,\dots, \delta_{j-1}\)&lt;/span&gt;
&lt;em&gt;and cross&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;&lt;em&gt;.&lt;/em&gt;&lt;br /&gt;
&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Conceptualized in Figure 4, the actual estimation of the GPCA in Graph
Space is done via AAC: (1) Randomly pick some candidates in the
equivalence classes; (2) estimate the standard PCA in &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;; (3) Select
new optimally aligned candidates wrt the current PCA estimation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-11-06-populations-of-unlabeled-networks-graph-space-geometry-and-generalized-geodesic-principal-components_files/AAC_Scheme_correct_pca.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 4: Conceptual visualization of the AAC for the estimation of the
first generalized geodesic principal component.&lt;/p&gt;
&lt;p&gt;The AAC converge in finite time and to a local minima as proven in
Theorem 2 and 3 (&lt;span class=&#34;citation&#34;&gt;Calissano, Feragen, and Vantini (&lt;a href=&#34;#ref-calissano2023populations&#34;&gt;2023&lt;/a&gt;)&lt;/span&gt;). All the algorithms and the
framework is implemeted in the geomstats python package.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;an-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An Example&lt;/h2&gt;
&lt;p&gt;As an intuitive visual example with real data and associated vectors
attributes, we subsample &lt;span class=&#34;math inline&#34;&gt;\(20\)&lt;/span&gt; cases of the letter A from the well known
hand written letters dataset (&lt;span class=&#34;citation&#34;&gt;Kersting et al. (&lt;a href=&#34;#ref-KKMMN2016&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Riesen and Bunke (&lt;a href=&#34;#ref-riesen2008iam&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt;). As shown in the
left panel of Figure 5, every network has node attributes consisting of
the node’s &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;- and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;-coordinates, and binary &lt;span class=&#34;math inline&#34;&gt;\((0/1\)&lt;/span&gt;) edge attributes
indicating whether nodes are connected by lines.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-11-06-populations-of-unlabeled-networks-graph-space-geometry-and-generalized-geodesic-principal-components_files/Letters_Plot.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 5: &lt;strong&gt;Left:&lt;/strong&gt; A datum extracted from the &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; dataset. Every
unlabelled node has a bi-dimensional real valued attribute, while every
edge has a &lt;span class=&#34;math inline&#34;&gt;\({0,1}\)&lt;/span&gt; attribute. The Fréchet mean. &lt;strong&gt;Right:&lt;/strong&gt; Visualization
of the GGPCs. &lt;span class=&#34;math inline&#34;&gt;\({0.1,0.25,0.5,0.75,0.9}\)&lt;/span&gt; quantile of the projected scores
are shown for the first three GGPCs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Graph Space is an intuitive embedding for sets of graphs with unlabelled
sets of nodes. Even if its geometry is far from trivial, we can easily
estimate intrinsic statistics by using the Align All and Compute
algorithm. In &lt;span class=&#34;citation&#34;&gt;Calissano, Feragen, and Vantini (&lt;a href=&#34;#ref-calissano2023populations&#34;&gt;2023&lt;/a&gt;)&lt;/span&gt; we detailed the geometry of
Graph Space, we define the AAC algorithm and the Generalized Geodesic
Principal Components for a set of graphs. Regression with unlabelled
network outputs is also available in &lt;span class=&#34;citation&#34;&gt;Calissano, Feragen, and Vantini (&lt;a href=&#34;#ref-calissano2022graph&#34;&gt;2022&lt;/a&gt;)&lt;/span&gt;. All the
framework is available as part of the geomstats python package
(&lt;span class=&#34;citation&#34;&gt;Miolane et al. (&lt;a href=&#34;#ref-miolane2020geomstats&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-calissano2022graph&#34; class=&#34;csl-entry&#34;&gt;
Calissano, Anna, Aasa Feragen, and Simone Vantini. 2022. &lt;span&gt;“Graph-Valued Regression: Prediction of Unlabelled Networks in a Non-Euclidean Graph Space.”&lt;/span&gt; &lt;em&gt;Journal of Multivariate Analysis&lt;/em&gt; 190: 104950.
&lt;/div&gt;
&lt;div id=&#34;ref-calissano2023populations&#34; class=&#34;csl-entry&#34;&gt;
———. 2023. &lt;span&gt;“Populations of Unlabelled Networks: Graph Space Geometry and Generalized Geodesic Principal Components.”&lt;/span&gt; &lt;em&gt;Biometrika&lt;/em&gt;, asad024.
&lt;/div&gt;
&lt;div id=&#34;ref-chowdhury2019gromov&#34; class=&#34;csl-entry&#34;&gt;
Chowdhury, Samir, and Facundo Mémoli. 2019. &lt;span&gt;“The Gromov–Wasserstein Distance Between Networks and Stable Network Invariants.”&lt;/span&gt; &lt;em&gt;Information and Inference: A Journal of the IMA&lt;/em&gt; 8 (4): 757–87.
&lt;/div&gt;
&lt;div id=&#34;ref-conte2004thirty&#34; class=&#34;csl-entry&#34;&gt;
Conte, Donatello, Pasquale Foggia, Carlo Sansone, and Mario Vento. 2004. &lt;span&gt;“Thirty Years of Graph Matching in Pattern Recognition.”&lt;/span&gt; &lt;em&gt;International Journal of Pattern Recognition and Artificial Intelligence&lt;/em&gt; 18 (03): 265–98.
&lt;/div&gt;
&lt;div id=&#34;ref-durante2017nonparametric&#34; class=&#34;csl-entry&#34;&gt;
Durante, Daniele, David B Dunson, and Joshua T Vogelstein. 2017. &lt;span&gt;“Nonparametric &lt;span&gt;B&lt;/span&gt;ayes Modeling of Populations of Networks.”&lt;/span&gt; &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 112 (520): 1516–30.
&lt;/div&gt;
&lt;div id=&#34;ref-frechet1948elements&#34; class=&#34;csl-entry&#34;&gt;
Fréchet, Maurice. 1948. &lt;span&gt;“Les &lt;span class=&#34;nocase&#34;&gt;é&lt;/span&gt;l&lt;span&gt;é&lt;/span&gt;ments Al&lt;span&gt;é&lt;/span&gt;atoires de Nature Quelconque Dans Un Espace Distanci&lt;span&gt;é&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;Annales de l’institut Henri Poincar&lt;span&gt;é&lt;/span&gt;&lt;/em&gt; 10 (4): 215–310.
&lt;/div&gt;
&lt;div id=&#34;ref-ginestet2017&#34; class=&#34;csl-entry&#34;&gt;
Ginestet, C. E., J. Li, P. Balachandran, S. Rosenberg, and E. D. Kolaczyk. 2017. &lt;span&gt;“Hypothesis Testing for Network Data in Functional Neuroimaging.”&lt;/span&gt; &lt;em&gt;The Annals of Applied Statistics&lt;/em&gt; 11 (2): 725–50. &lt;a href=&#34;https://doi.org/10.1214/16-AOAS1015&#34;&gt;https://doi.org/10.1214/16-AOAS1015&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-huckemann&#34; class=&#34;csl-entry&#34;&gt;
Huckemann, S., T. Hotz, and A. Munk. 2010. &lt;span&gt;“Intrinsic Shape Analysis: Geodesic &lt;span&gt;PCA&lt;/span&gt; for &lt;span&gt;R&lt;/span&gt;iemannian Manifolds Modulo Isometric &lt;span&gt;L&lt;/span&gt;ie Group Actions.”&lt;/span&gt; &lt;em&gt;Statist. Sinica&lt;/em&gt; 20 (1): 1–58.
&lt;/div&gt;
&lt;div id=&#34;ref-jain2009structure&#34; class=&#34;csl-entry&#34;&gt;
Jain, B. J., and K. Obermayer. 2009. &lt;span&gt;“Structure Spaces.”&lt;/span&gt; &lt;em&gt;Journal of Machine Learning Research&lt;/em&gt; 10 (Nov): 2667–2714.
&lt;/div&gt;
&lt;div id=&#34;ref-KKMMN2016&#34; class=&#34;csl-entry&#34;&gt;
Kersting, K., Kriege N. M., C. Morris, Mutzel. P., and M. Neumann. 2016. &lt;span&gt;“Benchmark Data Sets for Graph Kernels.”&lt;/span&gt; &lt;a href=&#34;http://graphkernels.cs.tu-dortmund.de&#34;&gt;http://graphkernels.cs.tu-dortmund.de&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-lee2013smooth&#34; class=&#34;csl-entry&#34;&gt;
Lee, John M. 2013. &lt;em&gt;Smooth Manifolds&lt;/em&gt;. Springer.
&lt;/div&gt;
&lt;div id=&#34;ref-lunagomez2021modeling&#34; class=&#34;csl-entry&#34;&gt;
Lunagómez, Simón, Sofia C Olhede, and Patrick J Wolfe. 2021. &lt;span&gt;“Modeling Network Populations via Graph Distances.”&lt;/span&gt; &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 116 (536): 2023–40.
&lt;/div&gt;
&lt;div id=&#34;ref-miolane2020geomstats&#34; class=&#34;csl-entry&#34;&gt;
Miolane, Nina, Nicolas Guigui, Alice Le Brigant, Johan Mathe, Benjamin Hou, Yann Thanwerdas, Stefan Heyder, et al. 2020. &lt;span&gt;“Geomstats: A Python Package for Riemannian Geometry in Machine Learning.”&lt;/span&gt; &lt;em&gt;Journal of Machine Learning Research&lt;/em&gt; 21 (223): 1–9.
&lt;/div&gt;
&lt;div id=&#34;ref-riesen2008iam&#34; class=&#34;csl-entry&#34;&gt;
Riesen, K., and H. Bunke. 2008. &lt;span&gt;“IAM Graph Database Repository for Graph Based Pattern Recognition and Machine Learning.”&lt;/span&gt; In &lt;em&gt;Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR)&lt;/em&gt;, 287–97. Springer.
&lt;/div&gt;
&lt;div id=&#34;ref-severn2020non&#34; class=&#34;csl-entry&#34;&gt;
Severn, Katie E, Ian L Dryden, and Simon P Preston. 2020. &lt;span&gt;“Non-Parametric Regression for Networks.”&lt;/span&gt; &lt;em&gt;arXiv Preprint arXiv:2010.00050&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-simpson2013permutation&#34; class=&#34;csl-entry&#34;&gt;
Simpson, Sean L, Robert G Lyday, Satoru Hayasaka, Anthony P Marsh, and Paul J Laurienti. 2013. &lt;span&gt;“A Permutation Testing Framework to Compare Groups of Brain Networks.”&lt;/span&gt; &lt;em&gt;Frontiers in Computational Neuroscience&lt;/em&gt; 7: 171.
&lt;/div&gt;
&lt;div id=&#34;ref-vogelstein2015fast&#34; class=&#34;csl-entry&#34;&gt;
Vogelstein, Joshua T, John M Conroy, Vince Lyzinski, Louis J Podrazik, Steven G Kratzer, Eric T Harley, Donniell E Fishkind, R Jacob Vogelstein, and Carey E Priebe. 2015. &lt;span&gt;“Fast Approximate Quadratic Programming for Graph Matching.”&lt;/span&gt; &lt;em&gt;PLOS One&lt;/em&gt; 10 (4): e0121002.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Stochastic Fluid Dynamics</title>
      <link>https://youngstats.github.io/post/2023/10/14/stochastic-fluid-dynamics/</link>
      <pubDate>Sat, 14 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2023/10/14/stochastic-fluid-dynamics/</guid>
      <description>


&lt;p&gt;&lt;strong&gt;Stochastic Fluid Dynamics&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Wednesday, November 15th, 6:00 PT / 9:00 ET / 15:00 CET&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-10-14-stochastic-fluid-dynamics_files/stochasticfluids_cover.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The study of fluid dynamics equations with white forcing is a classical topic in SPDEs and ergodic theory. Recently a new wave of interest, with a shift in focus towards transport noise, has risen, due to its connections with Stochastic Geometric Mechanics, Turbulence, Climate Modelling, Stochastic Parametrization and Uncertainty Quantification.&lt;/p&gt;
&lt;p&gt;In this webinar, selected young researchers will present their recent works on the topic.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wednesday, November 15th, 6:00 PT / 9:00 ET / 15:00 CET&lt;/li&gt;
&lt;li&gt;Online, via Zoom. The registration form is available &lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLScxPLuU4ItGqBbrcADlr-bub5xMs55KwaKrSLA2Fc3ehud5mA/viewform&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.google.com/view/dalonsoo&#34;&gt;Diego Alonso-Orán&lt;/a&gt;, Universidad de la Laguna, Tenerife, Spain, &lt;embed&gt; link to &lt;a href=&#34;https://drive.google.com/file/d/19wp11lCGPUW1sHrGwVrZ1KmzRpqvRZ4B/view&#34;&gt;Presentation&lt;/a&gt;. &lt;/embed&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Title: Regularity results for a stochastic transport equation with non-local velocity&lt;/p&gt;
&lt;p&gt;Abstract: In this talk, I will present different results regarding the well-posedness of a one-dimensional transport equation with non-local velocity under random perturbations. More precisely, we will discuss local and global existence of smooth solutions as well as the formation of finite time singularities depending on the nature of the noise. Time permitting, I will also pose some future research directions and open problems.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.imperial.ac.uk/people/daniel.goodair16&#34;&gt;Daniel Goodair&lt;/a&gt;, Imperial College London, UK, &lt;embed&gt; link to &lt;a href=&#34;https://drive.google.com/file/d/1Tl_kiGA8lHRKbV-9mTnCgumpm3oDlKG6/view&#34;&gt;Presentation&lt;/a&gt;. &lt;/embed&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Title: Pushing the Boundaries of Stochastic Fluid Dynamics&lt;/p&gt;
&lt;p&gt;Abstract: From the theoretical perspective, the recent introduction of transport noise into fluid dynamics models invites two interesting questions: do we retain the analytical properties of the deterministic equations for a wide class of such noise, and can we improve upon these properties for a specific choice of noise within the class. My talk will be focused on the former, with a brief eye to the latter. In particular, I will discuss the existence of solutions in the presence of a physical boundary. This will be guided by recent difficulties and successes surrounding the Navier-Stokes Equation with Stochastic Lie Transport, addressing the challenges arising in a bounded domain and what standard techniques can and cannot cope with. I will conclude with the mention of a conjecture as to how new developments in enhanced dissipation could provide a high probability stochastic answer to a famously open problem regarding the inviscid limit.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://people.utwente.nl/e.luesink&#34;&gt;Erwin Luesink&lt;/a&gt;, University of Twente, Netherlands, &lt;embed&gt; link to &lt;a href=&#34;https://drive.google.com/file/d/1DvSx20qyDyt2xd7cjmlvCf8g68mXap3s/view&#34;&gt;Presentation&lt;/a&gt;. &lt;/embed&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Title: Stochastic fluid dynamics and its connections to geometry&lt;/p&gt;
&lt;p&gt;Abstract: Many equations in stochastic fluid dynamics can be derived through a geometric framework. This framework is known as stochastic geometric mechanics and guides also the numerical discretisation of models in stochastic fluid dynamics. In this talk I will discuss several versions of this framework and use them to derive stochastic models in fluid dynamics. For a special case, namely for stochastic fluids on the sphere, I show how the geometric framework helps to guide the numerical discretisation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://miloviviani.wordpress.com/&#34;&gt;Milo Viviani&lt;/a&gt;, Scuola Normale Superiore di Pisa, Italy, &lt;embed&gt; link to &lt;a href=&#34;https://slides.com/miloviviani-1/lie-jordan-splitting-iteration-methods-for-systems-of-linear-equations&#34;&gt;Presentation&lt;/a&gt;. &lt;/embed&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Title: Zero-Noise Selection for Point Vortex Dynamics after Collapse&lt;/p&gt;
&lt;p&gt;Abstract: In this talk, we discuss the continuation of point vortex dynamics after a vortex collapse by means of a regularization procedure consisting in introducing a small stochastic diffusive term, that corresponds to a vanishing viscosity. By a careful numerical investigation, we show that in contrast with deterministic regularization, in which a cutoff interaction selects in the limit a single trajectory of the system after collapse, the zero-noise method produces a probability distribution supported by trajectories satisfying relevant conservation laws of the point vortex system.&lt;/p&gt;
&lt;p&gt;Discussant: Umberto Pappalettera, University of Bielefeld, Germany&lt;/p&gt;
&lt;p&gt;The webinar is part of YoungStatS project of the Young Statisticians Europe initiative (FENStatS) supported by the Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics (IMS).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you missed this webinar, you can &lt;a href=&#34;https://www.youtube.com/watch?v=7Vl6aZVAiHg&#34;&gt;watch the recording on our YouTube channel&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Locally Sparse Functional Regression</title>
      <link>https://youngstats.github.io/post/2023/10/09/locally-sparse-functional-regression/</link>
      <pubDate>Mon, 09 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2023/10/09/locally-sparse-functional-regression/</guid>
      <description>


&lt;div id=&#34;overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this post we present a new estimation procedure for functional linear
regression useful when the regression surface – or curve – is supposed
to be exactly zero within specific regions of its domain. Our approach
involves regularization techniques, merging a B-spline representation of
the unknown coefficient function with a peculiar overlap group lasso
penalty. The methodology is illustrated on the well-known Swedish
mortality dataset and can be employed by &lt;span class=&#34;math inline&#34;&gt;\({\tt R}\)&lt;/span&gt;
users through the package &lt;span class=&#34;math inline&#34;&gt;\({\tt fdaSP}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;We consider the framework on which a functional response
&lt;span class=&#34;math inline&#34;&gt;\(y_i(s)\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(s \in \mathcal{S}\)&lt;/span&gt; is observed together with a functional covariate
&lt;span class=&#34;math inline&#34;&gt;\(x_i(t)\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(t \in \mathcal{T}\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(i=1, \dots, n\)&lt;/span&gt;. The simplest
modeling strategy for function-on-function regression is the &lt;em&gt;concurrent
model,&lt;/em&gt; which assumes that &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{T} = \mathcal{S}\)&lt;/span&gt; and the covariate &lt;span class=&#34;math inline&#34;&gt;\(x(\cdot)\)&lt;/span&gt; influences
&lt;span class=&#34;math inline&#34;&gt;\(y(s)\)&lt;/span&gt; only through its values &lt;span class=&#34;math inline&#34;&gt;\(x(s)\)&lt;/span&gt; at the domain point &lt;span class=&#34;math inline&#34;&gt;\(s \in \mathcal{S}\)&lt;/span&gt;.
The relation when both variables are centered is given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y_i(s) = x_i(s)\psi(s) + e_i(s)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\psi(s)\)&lt;/span&gt; is the regression function and
&lt;span class=&#34;math inline&#34;&gt;\(e_i(s)\)&lt;/span&gt; is a functional zero-mean random error.
The more general approach, named &lt;em&gt;nonconcurrent functional linear
model,&lt;/em&gt; allows &lt;span class=&#34;math inline&#34;&gt;\(y_i(s)\)&lt;/span&gt; to entirely depend on the
functional regressor in the following way&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y_i(s) = \int_{\mathcal{T}} x_i(t) \psi(t, s) dt + e_i(s) .
\]&lt;/span&gt; The model allows &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{T} \neq
\mathcal{S}\)&lt;/span&gt; and the bivariate function &lt;span class=&#34;math inline&#34;&gt;\(\psi(t,
s)\)&lt;/span&gt; represents the impact of &lt;span class=&#34;math inline&#34;&gt;\(x(\cdot)\)&lt;/span&gt; evaluated at &lt;span class=&#34;math inline&#34;&gt;\(t \in \mathcal{T}\)&lt;/span&gt; on
&lt;span class=&#34;math inline&#34;&gt;\(y_i(s)\)&lt;/span&gt; and usually is a “dense” function. Our
goal is to introduce a &lt;em&gt;nonconcurrent functional linear model&lt;/em&gt; that
allows for local sparsity patterns. Specifically, we want that
&lt;span class=&#34;math inline&#34;&gt;\(\psi(t, s) = 0\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\((t, s) \in
\mathcal{D}_0\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}_0\)&lt;/span&gt; being a suitable subset of the domain
&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\psi(\cdot)\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D} = \mathcal{S} \times
\mathcal{T}\)&lt;/span&gt;, thus, inducing locally sparse
Hilbert-Schmidt operators. Note that the set
&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}_0\)&lt;/span&gt; is unknown and need to be
estimated from data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;locally-sparse-functional-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Locally sparse functional model&lt;/h2&gt;
&lt;div id=&#34;b-splines-and-sparsity&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;B-splines and sparsity&lt;/h3&gt;
&lt;p&gt;In order to represent functional objects using basis expansion, we
select a basis &lt;span class=&#34;math inline&#34;&gt;\(\{\theta_l(s), l = 1, \dots , L\}\)&lt;/span&gt; of dimension &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; in the space of square
integrable functions on &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{S}\)&lt;/span&gt; and a basis
&lt;span class=&#34;math inline&#34;&gt;\(\{\varphi_m(t), m = 1, \dots ,M\}\)&lt;/span&gt; of
dimension &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; in the space of square integrable
functions on &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{T}\)&lt;/span&gt;. Exploiting a tensor
product expansion of these two, we represent the regression coefficient
&lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt; as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{eqnarray} \psi(t,s) &amp;amp; = \sum_{m=1}^M \sum_{l=1}^L
\psi_{ml} \varphi_m(t) \theta_l(s) = &amp;amp; (\varphi_1(t), \dots,
\varphi_M(t)) \left( \begin{matrix} \psi_{1,1} &amp;amp; \cdots &amp;amp;
\psi_{1,L} \\ \vdots &amp;amp; \ddots&amp;amp; \vdots \\ \psi_{M,1} &amp;amp; \cdots
&amp;amp; \psi_{M,L} \end{matrix}\right) \left( \begin{matrix}
\theta_{1}(s)\\ \vdots \\ \theta_{L}(s) \end{matrix}\right) =
\boldsymbol{\varphi}(t)^T \boldsymbol{\Psi}
\boldsymbol{\theta}(s), \label{eq:tensorproduct}
\end{eqnarray}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\psi_{ml} \in \mathbb{R}\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(l=1,
\dots, L\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(m=1, \dots, M\)&lt;/span&gt;.
A key point is to assume that elements in this representation are
B-splines (De Boor 1978) of order &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(L −
d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(M − d\)&lt;/span&gt; interior knots,
respectively. Suitable zero patterns in the B-spline basis coefficients
of &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Psi}\)&lt;/span&gt; induce sparsity in
&lt;span class=&#34;math inline&#34;&gt;\(\psi(t, s)\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\tau_1 &amp;lt; \dots &amp;lt; \tau_m
&amp;lt; \dots &amp;lt; \tau_{M−d+2}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_1 &amp;lt;
\dots &amp;lt; \sigma_l &amp;lt; \dots &amp;lt; \sigma_{L−d+2}\)&lt;/span&gt;
denote the knots defining the tensor product splines, with &lt;span class=&#34;math inline&#34;&gt;\(\tau_1,
\tau_{M−d+2}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_1,
\sigma_{L−d+2}\)&lt;/span&gt; being the boundaries of the two
domains, and let &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}_{ml} \in \mathcal{D}\)&lt;/span&gt; be the rectangular subset of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt; defined as &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}_{ml} = (\tau_m, \tau_{m+1})
\times (\sigma_l, \sigma_{l+1})\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(m = 1,
\dots ,M − d + 1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(l = 1, \dots , L − d +
1\)&lt;/span&gt;. To obtain &lt;span class=&#34;math inline&#34;&gt;\(\psi(t, s) = 0\)&lt;/span&gt;
for each &lt;span class=&#34;math inline&#34;&gt;\((t, s) \in \mathcal{D}_{ml}\)&lt;/span&gt;, it is
sufficient that all the coefficients &lt;span class=&#34;math inline&#34;&gt;\(\psi_{m&#39;l&#39;}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(m&#39; = m, \dots ,m + d − 1\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(l&#39; = l, \dots , l+d−1\)&lt;/span&gt; are jointly zero. The
following figure further clarifies this concept.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-10-09-locally-sparse-functional-regression_files/Image1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Bivariate example with tensor product cubic B-splines (d=4). The top
row shows different coefficient matrix patterns, while the bottom row
shows the corresponding spline, where dots represent knots and the set D
is highlighted in red. The first 2 columns show a coefficient matrix
with isolated zeros and a (d-1) x (d-1) block of zeros, respectively.
None of the two is able to produce a sparse function. In the last
column, conversely, an entire d x d block of coefficients is null and
the resulting function is indeed sparse.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The previous figure suggests that, in general, &lt;span class=&#34;math inline&#34;&gt;\(\psi(t, s)\)&lt;/span&gt; equals zero in the region identified by two pairs of
consecutive knots if the related &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; ×
&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; block of coefficients of
&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Psi}\)&lt;/span&gt; is entirely set to zero.
Therefore, &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Psi}\)&lt;/span&gt; should be suitably
partitioned in several blocks of dimensions &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; ×
&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; on which a joint sparsity penalty is induced.&lt;/p&gt;
&lt;p&gt;Note that in the simpler situation where a functional covariate
&lt;span class=&#34;math inline&#34;&gt;\(x_i(t)\)&lt;/span&gt; and a scalar response &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; are observed we have the following simplifications. The
functional linear model is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y_i = \int x_i(t) \psi(t) dt + e_i, \]&lt;/span&gt; the
regression function is expanded as &lt;span class=&#34;math inline&#34;&gt;\(\psi(t) = \sum_{m=1}^M
\psi_m \varphi_m(t)\)&lt;/span&gt; and the set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}_m
= (\tau_m, \tau_{m+1})\)&lt;/span&gt; is an interval of the real
line. To obtain &lt;span class=&#34;math inline&#34;&gt;\(\psi(t) = 0\)&lt;/span&gt; for each &lt;span class=&#34;math inline&#34;&gt;\(t \in
\mathcal{D}_{m}\)&lt;/span&gt;, it is sufficient that all the
coefficients &lt;span class=&#34;math inline&#34;&gt;\(\psi_{m&#39;}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(m&#39; = m,
\dots ,m + d − 1\)&lt;/span&gt; are jointly zero and
&lt;span class=&#34;math inline&#34;&gt;\(\psi(t)\)&lt;/span&gt; equals zero in the interval identified
by two pairs of consecutive knots if the related &lt;span class=&#34;math inline&#34;&gt;\(d-\)&lt;/span&gt;dimensional subvector of coefficients is entirely set to zero.
This behaviour is illustrated in the next figure.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-10-09-locally-sparse-functional-regression_files/Image2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Univariate cubic B-spline basis and resulting spline functions. Dashed
curves correspond to bases with a zero-valued coefficient. Only in the
case when d=4 consecutive coefficients are zero the resulting spline
function is null on a set of positive Lebesgue measure (in red).&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;overlap-group-lasso&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Overlap group Lasso&lt;/h3&gt;
&lt;p&gt;Having in mind the above mentioned B-splines sparsity properties, in
order to estimate a locally-sparse regression surface, we minimize the
following objective function&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{1}{2} \sum_{i=1}^n \int \bigg( y_i(s) - \int
x_i(t)\psi(t,s) dt \bigg)^2 ds + \lambda
\Omega(\boldsymbol{\Psi}), \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;but, instead of specifying the functional form for the penalty
&lt;span class=&#34;math inline&#34;&gt;\(\Omega(\cdot)\)&lt;/span&gt; as the widely-employed Lasso
(Tibshirani, 1996) or group-Lasso (Yuan and Lin, 2006) that would not
work properly (see paper for details), we propose to use something truly
tailored for the problem. First, instead of a disjoint partition, we
define an overlapping sequence of blocks of size &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; × &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. Specifically, we introduce the
block index &lt;span class=&#34;math inline&#34;&gt;\(b = 1, \dots , B\)&lt;/span&gt; with the total
number of blocks denoted by &lt;span class=&#34;math inline&#34;&gt;\(B = (M − d + 1) \times (L − d +
1)\)&lt;/span&gt;. Notably, there is a block for each set
&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}_{ml}\)&lt;/span&gt;. This overlapping group
structure allows &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}_0\)&lt;/span&gt; to be the union
of any set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}_{ml}\)&lt;/span&gt; by moving a block
of minimum size. An example of overlapping covering when
&lt;span class=&#34;math inline&#34;&gt;\(d=4\)&lt;/span&gt; (cubic splines) is shown in the following
figure.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-10-09-locally-sparse-functional-regression_files/Image3.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Overlapping covering of the coefficient matrix (L = M = 12) with the
first 4 blocks of size d × d, d = 4. Colors according to the balancing
vector.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This construction suggests specifying a penalty &lt;span class=&#34;math inline&#34;&gt;\(\Omega\)&lt;/span&gt; for overlapping groups of coefficients, which has attracted
significant interest in the last decade, see Jacob, Obozinski and Vert
(2009), Jenatton, Audibert and Bach (2011) and Lim and Hastie (2015).
Being interested in the sparsity structure of the matrix of coefficients
&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Psi}\)&lt;/span&gt; rather than its support we
particularize the previous problem as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{1}{2} \sum_{i=1}^n \int \left( y_i(s) - \int x_i(t)
\psi(t,s) dt \right)^2 ds + \lambda \sum_{b=1}^{B+1} || c_{b}
\odot \boldsymbol{\psi} ||_2, \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\lambda &amp;gt; 0\)&lt;/span&gt; is a fixed penalization term,
and &lt;span class=&#34;math inline&#34;&gt;\(\Omega\)&lt;/span&gt; specifies in the sum of &lt;span class=&#34;math inline&#34;&gt;\(B +
1\)&lt;/span&gt; Euclidean norms &lt;span class=&#34;math inline&#34;&gt;\(\lVert c_b \odot
\boldsymbol{\psi} \rVert_2\)&lt;/span&gt;, where
&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\psi} = \text{vec}(\boldsymbol{\Psi})\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\odot\)&lt;/span&gt; represents the Hadamard
product. The index &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; denotes the block of
coefficients in &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Psi}\)&lt;/span&gt;, with the
first &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; blocks being consistent with the
aforementioned construction and the last block containing all
coefficients in &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Psi}\)&lt;/span&gt;. Vectors of
size &lt;span class=&#34;math inline&#34;&gt;\(ML\)&lt;/span&gt;, denoted by &lt;span class=&#34;math inline&#34;&gt;\(c_b\)&lt;/span&gt;,
are needed to extract the correct subset of entries of
&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Psi}\)&lt;/span&gt; and contain also known
constants that equally balance the penalization of the coefficients.
This &lt;em&gt;balancing&lt;/em&gt; is needed because the parameters close to the
boundaries appear in fewer groups than the central ones. Note that this
penalty constitutes a special case of the norm defined by Jenatton,
Audibert, and Bach (2011).&lt;/p&gt;
&lt;p&gt;In the case of a scalar response, the objective function becomes&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{1}{2} \sum_{i=1}^n \left( y_i - \int x_i(t) \psi(t)
dt \right)^2 + \lambda \sum_{b=1}^{B+1} || c_{b} \odot
\boldsymbol{\psi} ||_2 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\psi}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c_b\)&lt;/span&gt; are vectors of dimension &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; and each of
the first &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; blocks contains &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; consecutive coefficients, as in the following example.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-10-09-locally-sparse-functional-regression_files/Image4.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Overlapping covering of the coefficient vector (M = 12) when the
response is scalar with the first 4 blocks of size d = 4. Colors
according to the balancing vector.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;computational-considerations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Computational Considerations&lt;/h2&gt;
&lt;p&gt;To develop an efficient computational strategy, we introduce the
empirical counterparts of the quantities described in the previous
section assuming to observe a sample of response curves
&lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(i=1, \dots,n\)&lt;/span&gt; on
a common grid of &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; points, i.e. &lt;span class=&#34;math inline&#34;&gt;\(y_i =
(y_i(s_1), \dots, y_i(s_G))^T\)&lt;/span&gt;. Let also
&lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; be the related functional covariate observed
on a possibly different but— common across &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;—grid of points, that for simplicity and without loss of
generality, we assume of length &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;. Let
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(n \times G\)&lt;/span&gt; matrix with &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; in the rows. Let
&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Phi}\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Theta}\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(M \times
G\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(L \times G\)&lt;/span&gt; matrices
defined as &lt;span class=&#34;math display&#34;&gt;\[ \boldsymbol{\Phi} = \begin{pmatrix} \varphi_1(t_1)
&amp;amp; \cdots &amp;amp; \varphi_1(t_G) \\ \vdots&amp;amp;&amp;amp;\vdots \\ \varphi_m(t_1) &amp;amp;
\cdots &amp;amp; \varphi_m(t_G) \\ \vdots&amp;amp;&amp;amp;\vdots \\ \varphi_M(t_1) &amp;amp;
\cdots &amp;amp; \varphi_M(t_G) \\ \end{pmatrix}, \quad \quad
\boldsymbol{\Theta} = \begin{pmatrix} \theta_1(s_1) &amp;amp; \dots &amp;amp;
\theta_1(s_G) \\ \vdots&amp;amp;&amp;amp;\vdots \\ \theta_l(s_1) &amp;amp; \cdots &amp;amp;
\theta_l(s_G) \\ \vdots&amp;amp;&amp;amp;\vdots \\ \theta_L(s_1) &amp;amp; \cdots &amp;amp;
\theta_L(s_G) \\ \end{pmatrix}, \]&lt;/span&gt; and let
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\({\bf E}\)&lt;/span&gt;
be the &lt;span class=&#34;math inline&#34;&gt;\(n\times G\)&lt;/span&gt; matrices obtained as
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y} = (y_1, \dots, y_n)^T\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{E} = (e_1, \dots, e_n)^T\)&lt;/span&gt; , with
&lt;span class=&#34;math inline&#34;&gt;\(e_i = (e_i(s_1), \dots, e_i(s_G))^T\)&lt;/span&gt;. The
function-on-function linear regression model can be equivalently written
in matrix form as &lt;span class=&#34;math display&#34;&gt;\[ \mathbf{Y} = \mathbf{X} \boldsymbol{\Phi}^T
\boldsymbol{\Psi} \boldsymbol{\Theta} + {\bf E}. \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Applying the vectorization operator on each side of the equality above,
we obtain the new optimization problem&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{1}{2}\Vert \mathbf{y} - \mathbf{Z}
\boldsymbol{\psi}\Vert_2^2 + \lambda \sum_{b=1}^{B+1} \Vert
\mathbf{D}_{b}\boldsymbol{\psi} \Vert_2, \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{y}=\text{vec}(\mathbf{Y})\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\psi}=\text{vec}(\boldsymbol{\Psi})\)&lt;/span&gt; is the vector of coefficients of dimension &lt;span class=&#34;math inline&#34;&gt;\(LM\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Z} = \boldsymbol{\Theta}^T \otimes
\mathbf{X} \boldsymbol{\Phi}^T\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{D}_b=\mathrm{diag}(c_b)\)&lt;/span&gt; is a diagonal
matrix whose elements correspond to the elements of the vector
&lt;span class=&#34;math inline&#34;&gt;\(c_b\)&lt;/span&gt;. The minimization of the quantity above is
challenging because of the non-separability of the overlap group Lasso
penalty. We propose a Majorization-Minimization (MM) algorithm (Lange,
2016) to obtain the solution, although other choices are viable e.g.,
the Alternating Direction Method of Multipliers (ADMM), see Boyd et
al. (2011). The MM procedure works in two steps: in the first step a
majorizing function
&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{Q}(\boldsymbol{\psi}\vert\widehat{\boldsymbol{\psi}}^{k})\)&lt;/span&gt; based on the actual estimate is determined and in the second
step this function is minimized. Alternating between the two guarantees
the convergence to a solution of the original problem. The curious
reader can find the expression of
&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{Q}(\boldsymbol{\psi}\vert\widehat{\boldsymbol{\psi}}^{k})\)&lt;/span&gt; in the original paper, here we just point out that the function
is quadratic and admits an explicit solution in the form of generalized
ridge regression. A further speed-up, when the dimension of the problem
is high, is made exploiting the Sherman-Morrison-Woodbury matrix
identity, also known as the matrix inversion lemma.&lt;/p&gt;
&lt;p&gt;When the response is scalar, we have the following modifications. The
design matrix is &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Z} = \mathbf{X}
\boldsymbol{\Phi}^T\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{y}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\psi}\)&lt;/span&gt; are vectors of
dimension respectively &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; and the diagonal matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{D}_{b}\)&lt;/span&gt;
is of dimension &lt;span class=&#34;math inline&#34;&gt;\(M \times M\)&lt;/span&gt;. The optimization
problem is still valid and the same class of algorithms can be employed
to obtain the solution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;swedish-mortality-revisited&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Swedish mortality revisited&lt;/h2&gt;
&lt;p&gt;We apply the proposed locally sparse estimator to the Swedish mortality
dataset, where the aim is to predict the log-hazard function on a given
year from the same quantity on the previous year. We implement the model
with &lt;span class=&#34;math inline&#34;&gt;\(d = 4, M = L = 20\)&lt;/span&gt; basis functions on each
dimension and select the optimal value of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; by means of cross-validation. A perspective plot of the
estimated surface is depicted in the last figure of the post. The
estimate shows a marked positive diagonal confirming the positive
influence on the log-hazard rate at age &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; of the
previous year’s curve evaluated on a neighborhood of &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;. At the same time, the flat zero regions outside the diagonal
suggest that there is no influence of the curves evaluated at distant
ages. Our estimate is more regular than previous approaches and its
qualitative interpretation sharper and easier. Refer, for example, to
Figure 10.11 of Ramsay, Hooker, and Graves (2009).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-10-09-locally-sparse-functional-regression_files/Image5.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Estimated regression surface for the Swedish mortality dataset.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This result witnesses the practical relevance of adopting the proposed
approach. Indeed, the resulting estimate, while being reminiscent of a
concurrent model—inheriting its ease of interpretation—gives further
insights and improves the fit, representing the desired intermediate
solution between the concurrent and nonconcurrent models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tt-r-package&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;math inline&#34;&gt;\({\tt R}\)&lt;/span&gt; package&lt;/h2&gt;
&lt;p&gt;The method is implemented in &lt;span class=&#34;math inline&#34;&gt;\({\tt R}\)&lt;/span&gt; through the
package &lt;span class=&#34;math inline&#34;&gt;\({\tt fdaSP}\)&lt;/span&gt;, available on CRAN. The
function &lt;span class=&#34;math inline&#34;&gt;\({\tt f2fSP}\)&lt;/span&gt; can be used for a
functional response regression model while &lt;span class=&#34;math inline&#34;&gt;\({\tt f2sSP}\)&lt;/span&gt; for a scalar response one. The two counterparts
&lt;span class=&#34;math inline&#34;&gt;\(\texttt{f2fSP_cv}\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(\texttt{f2sSP_cv}\)&lt;/span&gt; are useful to select the
tuning parameter by means of cross-validation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;this-post-is-based-on&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;This post is based on&lt;/h2&gt;
&lt;p&gt;Bernardi, M., Canale, A. and Stefanucci, M. (2022). Locally Sparse
Function-on-Function Regression, Journal of Computational and Graphical
Statistics, 32:3, 985-999, DOI:
&lt;a href=&#34;https://doi.org/10.1080/10618600.2022.2130926&#34;&gt;10.1080/10618600.2022.2130926&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Tibshirani, R. (1996), “Regression Shrinkage and Selection via the
Lasso,” &lt;em&gt;Journal of the Royal Statistical Society,&lt;/em&gt; Series B, 58,
267–288.&lt;br /&gt;
Yuan, M., and Lin, Y. (2006), “Model Selection and Estimation in
Regression with Grouped Variables,” &lt;em&gt;Journal of the Royal Statistical
Society,&lt;/em&gt; Series B, 68, 49–67.&lt;br /&gt;
Jacob, L., Obozinski, G., and Vert, J.-P. (2009), “Group Lasso with
Overlap and Graph Lasso,” in &lt;em&gt;Proceedings of the 26th Annual
International Conference on Machine Learning,&lt;/em&gt; pp. 433–440.&lt;br /&gt;
Jenatton, R., Audibert, J.-Y., and Bach, F. (2011), “Structured Variable
Selection with Sparsity-Inducing Norms,” &lt;em&gt;Journal of Machine Learning
Research,&lt;/em&gt; 12, 2777–2824.&lt;br /&gt;
Lim, M., and Hastie, T. (2015), “Learning Interactions via Hierarchical
Group-Lasso Regularization,” &lt;em&gt;Journal of Computational and Graphical
Statistics,&lt;/em&gt; 24, 627–654.&lt;br /&gt;
Lange, K. (2016), &lt;em&gt;“MM optimization algorithms.”&lt;/em&gt; Society for Industrial
and Applied Mathematics, Philadelphia, PA.&lt;br /&gt;
Boyd, S., Parikh, N., Chu, E., Peleato, B. and Eckstein, J. (2011),
“Distributed optimization and statistical learning via the alternating
direction method of multipliers,” &lt;em&gt;Foundations and Trends® in Machine
Learning,&lt;/em&gt; 3, 1–122.&lt;br /&gt;
Ramsay, J. O., Hooker, G., and Graves, S. (2009), &lt;em&gt;Functional Data
Analysis with R and Matlab,&lt;/em&gt; NewYork: Springer.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;about-the-authors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;About the authors&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://scholar.google.it/citations?user=TejvXeQAAAAJ&amp;amp;hl=en&#34;&gt;Mauro Bernardi&lt;/a&gt;
is Associate Professor at University of Padua.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://tonycanale.github.io/&#34;&gt;Antonio Canale&lt;/a&gt; is Associate Professor
at University of Padua.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://marcostefanucci.github.io/&#34;&gt;Marco Stefanucci&lt;/a&gt; is Assistant
Professor at University of Rome Tor Vergata.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linear-cost unbiased estimator for large crossed random effect models via couplings</title>
      <link>https://youngstats.github.io/post/2023/09/27/linear-cost-unbiased-estimator-for-large-crossed-random-effect-models-via-couplings/</link>
      <pubDate>Wed, 27 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2023/09/27/linear-cost-unbiased-estimator-for-large-crossed-random-effect-models-via-couplings/</guid>
      <description>


&lt;style&gt;
body {
text-align: justify
fig.align = &#39;center&#34;}
&lt;/style&gt;
&lt;p&gt;In the following we show how it is possible to obtain &lt;strong&gt;parallelizable, unbiased and computationally cheap&lt;/strong&gt; estimates of Crossed random effects models with a &lt;strong&gt;linear cost&lt;/strong&gt; in the number of datapoints (and paramaters) exploiting &lt;strong&gt;couplings&lt;/strong&gt;.&lt;/p&gt;
&lt;div id=&#34;crossed-random-effects-models-crem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Crossed random effects models (CREM)&lt;/h2&gt;
&lt;p&gt;CREM model a continuous response variables &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; as depending on the sum of unknown effects of &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; categorical predictors. Think of the &lt;span class=&#34;math inline&#34;&gt;\(Y_n\)&lt;/span&gt; as the ratings given to university courses, along with some factors potentially impacting such a score, e.g. student identity, code of the course, department teaching it, professors ecc. Aim of the model is investigating the effect of each of those factors on the overall score. In their simplest version (i.e. linear, intercept-only case), the model takes the form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
	\label{eq:crem}
	\mathcal{L}(y_n) =  f \left(\mu +\sum_{k=1}^K a_{i_{k[n]}}^{(k)},\tau_0^{-1}\right) \text{ for } n=1,...,N,
\end{equation}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; indicates the density of some distribution whose mean is the sum of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, a global mean, and &lt;span class=&#34;math inline&#34;&gt;\(a^{(k)}_{i_{k[n]}}\)&lt;/span&gt;, i.e. the unknown effects of the student identity, the department teaching it ecc.&lt;/p&gt;
&lt;p&gt;We are interested in studying how the cost of estimating the unknown effects scales as the number of observations &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; and of parameters &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; grows to &lt;span class=&#34;math inline&#34;&gt;\(\infty\)&lt;/span&gt;. Our goal is an algorithm whose complexity scales linearly in &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, and we call such algorithms “scalable”. Both in the Frequentist and in the Bayesian literature, these models are difficult to estimate: works of &lt;span class=&#34;citation&#34;&gt;Gao and Owen (&lt;a href=&#34;#ref-gao_16&#34; role=&#34;doc-biblioref&#34;&gt;2016a&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Gao and Owen (&lt;a href=&#34;#ref-gao19&#34; role=&#34;doc-biblioref&#34;&gt;2016b&lt;/a&gt;)&lt;/span&gt; showed how the “vanilla” implementation of GLS and of Gibbs samplers have a computational cost that grows at best as &lt;span class=&#34;math inline&#34;&gt;\(O(N^\frac{3}{2})\)&lt;/span&gt;. Recent works by &lt;span class=&#34;citation&#34;&gt;Papaspiliopoulos, Roberts, and Zanella (&lt;a href=&#34;#ref-papaspiliopoulos2018scalable&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Ghosh, Hastie, and Owen (&lt;a href=&#34;#ref-gosh_back&#34; role=&#34;doc-biblioref&#34;&gt;2022&lt;/a&gt;)&lt;/span&gt; proposed, respectively, a collapsed Gibbs sampler and a “backfitting” iterative algorithm that exhibit computational costs linear in the number of observations; in particular the MCMC induced by the collapsed scheme is proved to have a mixing time that is &lt;span class=&#34;math inline&#34;&gt;\(O(1)\)&lt;/span&gt; under certain asymptotic regimes.&lt;/p&gt;
&lt;p&gt;It is possible to further improve the MCMC estimates exploiting couplings: as showed in &lt;span class=&#34;citation&#34;&gt;Jacob, O’Leary, and Atchadé (&lt;a href=&#34;#ref-jacob2019unbiased&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Glynn and Rhee (&lt;a href=&#34;#ref-glynn_rhee&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;, coupling two MCMC chains allows to derive unbiased estimators of posterior quantities, provided that the two coupled chains are exactly equal after a finite number of iterations. Furthermore, the same construction provides theoretical foundations for the early stopping of the chains (once met) and allows for the parallelization of independent experiments.&lt;/p&gt;
&lt;p&gt;The extra computational cost one has to pay is represented by the product between the cost of each iteration and the expected number of iterations needed for coalescence. As for the former, it is possible to devise many coupling algorithms for which the cost of each iteration is easily computable and linear in the number of observations. As for the expected meeting time, for chains arising from a Gibbs sampling scheme targeting Gaussian distributions, it is possible to show that is directly related to the mixing time of the single Markov chain, and indeed it differs from the latter only by a logarithmic factor up to some constants (see the Sections below for more details). Hence chains that mix fast also meet in a small number of iteration and therefore provide unbiased estimates with low computational cost.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;couplings-for-estimation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Couplings for estimation&lt;/h2&gt;
&lt;p&gt;Theoretically speaking, given &lt;span class=&#34;math inline&#34;&gt;\(X,Y\)&lt;/span&gt; random variables distributed according to &lt;span class=&#34;math inline&#34;&gt;\(P,Q\)&lt;/span&gt; respectively, a coupling of the two is random variables &lt;span class=&#34;math inline&#34;&gt;\((X, Y)\)&lt;/span&gt; on the joint space such that the marginal distribution of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; and the marginal distribution of Y is &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;.
Clearly, given two marginal distributions, there are infinitely many joint distributions with those as marginals. Below some of the possible couplings of a &lt;span class=&#34;math inline&#34;&gt;\(N(1,1)\)&lt;/span&gt; (on the x-axis) and &lt;span class=&#34;math inline&#34;&gt;\(N(0,1)\)&lt;/span&gt; on the y-axis. Starting clockwise from top left: maximal independent (i.e. a coupling maximizing the probability of equal draws), maximal reflection, independent (bivariate independent normal) and &lt;span class=&#34;math inline&#34;&gt;\(W2\)&lt;/span&gt;-optimal (maximally correlated draws).&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://youngstats.github.io/post/2023-09-27-linear-cost-unbiased-estimator-for-large-crossed-random-effect-models-via-couplings_files/gaussian_coupling.jpg&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Couplings of Gaussian distributions&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Couplings might be used for obtaining unbiased estimators in MCMC inference, as shown in &lt;span class=&#34;citation&#34;&gt;Jacob, O’Leary, and Atchadé (&lt;a href=&#34;#ref-jacob2019unbiased&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;.
Given a target probability distribution &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; and an integrable function &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt;, we are interested in estimating: &lt;span class=&#34;math display&#34;&gt;\[\mathbb{E}_{\pi}[h(\boldsymbol{\Theta})] = \int h(\boldsymbol{\theta}) \pi(d\boldsymbol{\theta}).\]&lt;/span&gt;
Usually, one would sample a chain according to some Markov kernel &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;, designed to leave the chain &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; invariant, i.e. a Gibbs kernel or random walk Metropolis &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-Metropolis&#34; role=&#34;doc-biblioref&#34;&gt;Hastings 1970&lt;/a&gt;)&lt;/span&gt;. Below the sample path of such a chain.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://youngstats.github.io/post/2023-09-27-linear-cost-unbiased-estimator-for-large-crossed-random-effect-models-via-couplings_files/Slide1.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Sample path of a single MC&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Instead of waiting until convergence, one could run two coupled Markov chains &lt;span class=&#34;math inline&#34;&gt;\((\boldsymbol{\Theta}^1_t)_{t\ge -1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\((\boldsymbol{\Theta}^2_t)_{t \ge0}\)&lt;/span&gt;, which marginally starts from some base distribution &lt;span class=&#34;math inline&#34;&gt;\(\pi_0\)&lt;/span&gt; and evolves according to the same kernel &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;, but some correlation is induced in order to let the chains meet after an almost surely finite number of iterations. Basically at iteration &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; instead of sampling from &lt;span class=&#34;math inline&#34;&gt;\(X_{t+1} \sim P(X_t, \cdot)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_{t+1} \sim P(Y_t, \cdot)\)&lt;/span&gt; independently, we sample from a coupling of the two distributions.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://youngstats.github.io/post/2023-09-27-linear-cost-unbiased-estimator-for-large-crossed-random-effect-models-via-couplings_files/Slide2.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Sample path of coupled MCs&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Then, for any fixed &lt;span class=&#34;math inline&#34;&gt;\(m \ge k\)&lt;/span&gt;, we can run coupled chains for &lt;span class=&#34;math inline&#34;&gt;\(\max(m, T)\)&lt;/span&gt; iterations and &lt;span class=&#34;math inline&#34;&gt;\(H_{k:m}\)&lt;/span&gt; is an unbiased estimator:
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
	H_{k:m} &amp;amp; = \frac{1}{m-k+1} \sum_{l=k}^m h(\boldsymbol{\Theta}^1_l) + \sum_{l=k+1}^{\tau} \min \left(1, \frac{l-k}{m-k+1}\right) \left(h(\boldsymbol{\Theta}^1_l)-h(\boldsymbol{\Theta}^2_{l})\right) \\ &amp;amp;= MCMC_{k:m} + BC_{k:m}.
\end{align*}\]&lt;/span&gt;
The form of the estimator includes two terms: the first term corresponds to a standard MCMC average with &lt;span class=&#34;math inline&#34;&gt;\(m-k+1\)&lt;/span&gt; total iterations and &lt;span class=&#34;math inline&#34;&gt;\(k-1\)&lt;/span&gt; burn-in steps, and the other term is a “bias correction”, the part that corrects the bias present in the MCMC average.&lt;/p&gt;
&lt;p&gt;For more details give a look up at &lt;a href=&#34;https://sites.google.com/site/pierrejacob/cmclectures&#34; class=&#34;uri&#34;&gt;https://sites.google.com/site/pierrejacob/cmclectures&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In order two yield small meeting times, we implement a “two-step” coupling strategy: whenever the chains are “far away” (in some notion that will be clarified later) use a coupling whose aim is to bring the realizations closer to each other; whenever “close enough”, choose a coupling maximizing the meeting probabilities. The heuristic for this construction is that whenever a maximal coupling fails, components are sampled far away in the space, thus reducing the coalescence probability for the next steps.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bound-on-coupling-time&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bound on coupling time&lt;/h2&gt;
&lt;p&gt;Consider &lt;span class=&#34;math inline&#34;&gt;\((\boldsymbol{\theta}_t)_{t\ge 1}=(\boldsymbol{\theta}^1_t, \boldsymbol{\theta}^2_t)_{t\ge 0}\)&lt;/span&gt;, two &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;-reversible Markov chains arising from a Gibbs sampler targeting &lt;span class=&#34;math inline&#34;&gt;\(\pi=N(\boldsymbol{\mu},\Sigma)\)&lt;/span&gt;. If a “two-step” strategy is implemented, with maximal reflection and &lt;span class=&#34;math inline&#34;&gt;\(W2\)&lt;/span&gt; optimal couplings, then for every &lt;span class=&#34;math inline&#34;&gt;\(\delta &amp;gt;0\)&lt;/span&gt;, the meeting time &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; is bounded by
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
		\mathbb{E}[T| \boldsymbol{\theta}_0] \le 5 + 3 \max \left(n^*_\delta, T_{rel} \left[\frac{\ln(T_{rel})}{2} + C_0 + C_\varepsilon \right] (1+\delta)  \right),
	\end{equation}\]&lt;/span&gt;on}
where &lt;span class=&#34;math inline&#34;&gt;\(C_0\)&lt;/span&gt; denotes a constant solely depending on &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\theta}^1_0, \boldsymbol{\theta}^2_0\)&lt;/span&gt; and the posterior variance &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(C_\varepsilon\)&lt;/span&gt; depends on the fixed parameters &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(n^*_\delta = inf_{n_0} \{ n_0 \ge 1: \forall n \ge n_0 \; 1-\| B^n \|^\frac{1}{n} \ge \frac{1-\rho(B)}{1+\delta} \}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Given the above, if one is able to design a single MCMC chain mixing in say &lt;span class=&#34;math inline&#34;&gt;\(O(1)\)&lt;/span&gt;, then the extra cost of an unbiased estimate is nothing more than a &lt;span class=&#34;math inline&#34;&gt;\(\ln\left( O(1) \right)\)&lt;/span&gt; plus a constant.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulations&lt;/h2&gt;
&lt;p&gt;We simulate data coming from the model for different asymptotic regimes and parameter specification. We study the behaviour of the meeting times as the dimensionality of the model increase. We implement the “two step” Algorithm, using maximal reflection and &lt;span class=&#34;math inline&#34;&gt;\(W2\)&lt;/span&gt;-optimal couplings.&lt;/p&gt;
&lt;p&gt;We consider two asymptotic regimes, called &lt;em&gt;outfill asymptotic&lt;/em&gt;, where both the number of parameters &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and the number of observation &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; increase, but with different speeds according to the chosen setting.
Consider first a model with &lt;span class=&#34;math inline&#34;&gt;\(K=2\)&lt;/span&gt; factors and &lt;span class=&#34;math inline&#34;&gt;\(I_1= I_2 = \{50, 100,250,500,750,1000 \}\)&lt;/span&gt; different possible levels. Suppose that the number of observations grows quadratically wrt the number of parameters, i.e. &lt;span class=&#34;math display&#34;&gt;\[ N= O(p^2)\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[ p \rightarrow +\infty.\]&lt;/span&gt;
Below we report the average meeting time for the sampler with fixed and free variances, alongside with the bound for the collapsed Gibbs sampler with fixed variances, for different values of &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://youngstats.github.io/post/2023-09-27-linear-cost-unbiased-estimator-for-large-crossed-random-effect-models-via-couplings_files/new_reg1_k2.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Average meeting times for Gaussian CREMS&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Results of the simulations suggest that the expected number of iterations converges to &lt;span class=&#34;math inline&#34;&gt;\(O(1)\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; increases, while diverges for the plain vanilla Gibbs sampler (not even plotted because of different scale). It is also clear that the bound closely resembles the estimated average meeting times.&lt;/p&gt;
&lt;p&gt;We consider now an even sparser asymptotic regime, where instead the number of observation grows at the same rate of the number of parameters.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{N}{p} \approx \alpha \]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[ p \rightarrow +\infty.\]&lt;/span&gt;
Estimates for the mean number of iterations for the fixed and free variance case are plotted below, alongside the corresponding theoretical bounds.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://youngstats.github.io/post/2023-09-27-linear-cost-unbiased-estimator-for-large-crossed-random-effect-models-via-couplings_files/new_reg2_k2.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Average meeting times for Gaussian CREMS&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Lastly, we want to highlight that while the bound presented in the Theorem above only applies for Gibbs samplers targeting Gaussian distributions, the methodology is still valid for general target distributions and provide small meeting times. We report below the average meeting times of chains targeting a CREM with Laplace response.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
	\mathcal{L}(y_n) = Laplace \left(\mu +\sum_{k=1}^K a_{i_{k[n]}}^{(k)} \right) \text{ for } n=1,...,N,
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://youngstats.github.io/post/2023-09-27-linear-cost-unbiased-estimator-for-large-crossed-random-effect-models-via-couplings_files/laplace_mh.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Average meeting times for Laplace CREMS&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;about-the-author&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;About the author&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://paoloceriani.github.io&#34;&gt;Paolo Ceriani&lt;/a&gt;, PhD student, Bocconi University, Italy.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.google.com/site/gzanellawebpage/home&#34;&gt;Giacomo Zanella&lt;/a&gt;, Assistant Professor, Bocconi University, Italy.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-gao_16&#34; class=&#34;csl-entry&#34;&gt;
Gao, Katelyn, and Art Owen. 2016a. &lt;span&gt;“Efficient Moment Calculations for Variance Components in Large Unbalanced Crossed Random Effects Models.”&lt;/span&gt; &lt;em&gt;Electronic Journal of Statistics&lt;/em&gt; 11 (January). &lt;a href=&#34;https://doi.org/10.1214/17-EJS1236&#34;&gt;https://doi.org/10.1214/17-EJS1236&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-gao19&#34; class=&#34;csl-entry&#34;&gt;
———. 2016b. &lt;span&gt;“Estimation and Inference for Very Large Linear Mixed Effects Models.”&lt;/span&gt; &lt;em&gt;Statistica Sinica&lt;/em&gt;, October. &lt;a href=&#34;https://doi.org/10.5705/ss.202018.0029&#34;&gt;https://doi.org/10.5705/ss.202018.0029&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-gosh_back&#34; class=&#34;csl-entry&#34;&gt;
Ghosh, Swarnadip, Trevor J. Hastie, and Art B. Owen. 2022. &lt;span&gt;“Backfitting for Large Scale Crossed Random Effects Regressions.”&lt;/span&gt; &lt;em&gt;The Annals of Statistics&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-glynn_rhee&#34; class=&#34;csl-entry&#34;&gt;
Glynn, Peter W., and Chang-han Rhee. 2014. &lt;span&gt;“Exact Estimation for Markov Chain Equilibrium Expectations.”&lt;/span&gt; &lt;em&gt;Journal of Applied Probability&lt;/em&gt; 51A: 377–89. &lt;a href=&#34;http://www.jstor.org/stable/43284129&#34;&gt;http://www.jstor.org/stable/43284129&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-Metropolis&#34; class=&#34;csl-entry&#34;&gt;
Hastings, W. K. 1970. &lt;span&gt;“Monte Carlo Sampling Methods Using Markov Chains and Their Applications.”&lt;/span&gt; &lt;em&gt;Biometrika&lt;/em&gt; 57 (1): 97–109. &lt;a href=&#34;http://www.jstor.org/stable/2334940&#34;&gt;http://www.jstor.org/stable/2334940&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-jacob2019unbiased&#34; class=&#34;csl-entry&#34;&gt;
Jacob, Pierre E., John O’Leary, and Yves F. Atchadé. 2020. &lt;span&gt;“Unbiased Markov Chain Monte Carlo Methods with Couplings.”&lt;/span&gt; &lt;em&gt;Journal of the Royal Statistical Society: Series B (Statistical Methodology)&lt;/em&gt; 82 (3): 543–600. https://doi.org/&lt;a href=&#34;https://doi.org/10.1111/rssb.12336&#34;&gt;https://doi.org/10.1111/rssb.12336&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-papaspiliopoulos2018scalable&#34; class=&#34;csl-entry&#34;&gt;
Papaspiliopoulos, O, G O Roberts, and G Zanella. 2019. &lt;span&gt;“&lt;span class=&#34;nocase&#34;&gt;Scalable inference for crossed random effects models&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;Biometrika&lt;/em&gt; 107 (1): 25–40. &lt;a href=&#34;https://doi.org/10.1093/biomet/asz058&#34;&gt;https://doi.org/10.1093/biomet/asz058&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Algorithmic Fairness</title>
      <link>https://youngstats.github.io/post/2023/09/19/algorithmic-fairness/</link>
      <pubDate>Tue, 19 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2023/09/19/algorithmic-fairness/</guid>
      <description>


&lt;p&gt;&lt;strong&gt;Algorithmic Fairness&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Tuesday, October 3rd, 2023, 7:30 PT / 10:30 ET / 16:30 CET&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-09-19-algorithmic-fairness_files/fairness_algo_img.jpeg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;2nd joint webinar of the &lt;a href=&#34;https://imstat.org/ims-groups/ims-new-researchers-group/&#34;&gt;IMS New Researchers Group&lt;/a&gt;, &lt;a href=&#34;https://math.ethz.ch/sfs/news-and-events/young-data-science.html&#34;&gt;Young Data Science Researcher Seminar Zürich&lt;/a&gt; and the YoungStatS Project.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tuesday, October 3rd, 2023, 7:30 PT / 10:30 ET / 16:30 CET&lt;/li&gt;
&lt;li&gt;Online, &lt;a href=&#34;https://washington.zoom.us/j/93421065642&#34;&gt;via Zoom&lt;/a&gt;. The registration form is available
&lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSdKUvtyyJZZb7dkF0RXnqak5OqBzvwCSUaoYxwvQKdAA3rQaw/viewform&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://linjunz.github.io/&#34;&gt;Linjun Zhang&lt;/a&gt;, Rutgers University: “&lt;em&gt;Fair conformal prediction&lt;/em&gt;”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Abstract: Multi-calibration is a powerful and evolving concept originating in the field of algorithmic fairness. For a predictor f that estimates the outcome y given covariates x, and for a function class C, multi-calibration requires that the predictor f(x) and outcome y are indistinguishable under the class of auditors in C. Fairness is captured by incorporating demographic subgroups into the class of functions C. Recent work has shown that, by enriching the class C to incorporate appropriate propensity reweighting functions, multi-calibration also yields target-independent learning, wherein a model trained on a source domain performs well on unseen, future target domains (approximately) captured by the re-weightings. The multi-calibration notion is extended, and the power of an enriched class of mappings is explored. HappyMap, a generalization of multi-calibration, is proposed, which yields a wide range of new applications, including a new fairness notion for uncertainty quantification (conformal prediction), a novel technique for conformal prediction under covariate shift, and a different approach to analyzing missing data, while also yielding a unified understanding of several existing seemingly disparate algorithmic fairness notions and target-independent learning approaches. A single HappyMap meta-algorithm is given that captures all these results, together with a sufficiency condition for its success.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://moonfolk.github.io/&#34;&gt;Mikhail Yurochkin&lt;/a&gt;, IBM Research and MIT-IBM Watson AI Lab: “&lt;em&gt;Operationalizing Individual Fairness&lt;/em&gt;”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Abstract: Societal applications of ML proved to be challenging due to algorithms replicating or even exacerbating biases in the training data. In response, there is a growing body of research on algorithmic fairness that attempts to address these issues, primarily via group definitions of fairness. In this talk, I will illustrate several shortcomings of group fairness and present an algorithmic fairness pipeline based on individual fairness (IF). IF is often recognized as the more intuitive notion of fairness: we want ML models to treat similar individuals similarly. Despite the benefits, challenges in formalizing the notion of similarity and enforcing equitable treatment prevented the adoption of IF. I will present our work addressing these barriers via algorithms for learning the similarity metric from data and methods for auditing and training fair models utilizing the intriguing connection between individual fairness and adversarial robustness. Finally, I will demonstrate applications of IF with Large Language Models.&lt;/p&gt;
&lt;p&gt;Discussant: &lt;a href=&#34;https://raziehnabi.com/&#34;&gt;Razieh Nabi&lt;/a&gt;, Emory University&lt;/p&gt;
&lt;p&gt;YoungStatS project of the Young Statisticians Europe initiative (FENStatS) is supported by the Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics (IMS).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you missed this webinar, you can &lt;a href=&#34;https://www.youtube.com/watch?v=faPooozTKGk&#34;&gt;watch the recording on our YouTube channel&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Illustration of Graphical Gaussian Process models to analyze highly multivariate spatial data</title>
      <link>https://youngstats.github.io/post/2023/07/07/illustration-of-graphical-gaussian-process-models-to-analyze-highly-multivariate-spatial-data/</link>
      <pubDate>Fri, 07 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2023/07/07/illustration-of-graphical-gaussian-process-models-to-analyze-highly-multivariate-spatial-data/</guid>
      <description>



&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Abundant multivariate spatial data from the natural and environmental sciences demands research on the joint distribution of multiple spatially dependent variables (&lt;span class=&#34;citation&#34;&gt;Wackernagel (&lt;a href=&#34;#ref-wackernagel2013multivariate&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Cressie and Wikle (&lt;a href=&#34;#ref-creswikle11&#34;&gt;2011&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Banerjee and Gelfand (&lt;a href=&#34;#ref-ban14&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;). Here, our goal is to estimate associations over spatial locations for each variable and those among the variables.&lt;/p&gt;
&lt;p&gt;In this document, we will present an example of a simulated multivariate spatial data and how we can use Graphical Gaussian processes (GGP) (&lt;span class=&#34;citation&#34;&gt;Dey and Banerjee (&lt;a href=&#34;#ref-dey2021ggp&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt;) to model the data. First, we’ll introduce the general multivariate spatial model, and then we will introduce a variable graph and how to simulate a Graphical Gaussian process (Matérn) for that variable graph. Next, we will lay out the estimation steps of GGP parameters and how the estimated parameters compare against the truth.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model&lt;/h1&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(y(s)\)&lt;/span&gt; denote a &lt;span class=&#34;math inline&#34;&gt;\(q\times 1\)&lt;/span&gt; vector of spatially-indexed dependent outcomes for any location &lt;span class=&#34;math inline&#34;&gt;\(s \in \mathcal D \subset \mathbb{R}^d\)&lt;/span&gt; with typically &lt;span class=&#34;math inline&#34;&gt;\(d=2\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(3\)&lt;/span&gt;. On our spatial domain &lt;span class=&#34;math inline&#34;&gt;\(\mathcal D\)&lt;/span&gt;, a multivariate spatial regression model provides a marginal spatial regression model for each outcome as follows:
&lt;span class=&#34;math display&#34; id=&#34;eq:mgp&#34;&gt;\[\begin{equation}
y_i(s) = x_i(s)^{T}\beta_i + w_i(s) + \epsilon_i(s)\;,\quad i=1,2,\ldots,q,\; s \in \mathcal D
\tag{1}
\end{equation}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(y_i(s)\)&lt;/span&gt; is the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th element of &lt;span class=&#34;math inline&#34;&gt;\(y(s)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(x_i(s)\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(p_i\times 1\)&lt;/span&gt; vector of predictors, &lt;span class=&#34;math inline&#34;&gt;\(\beta_i\)&lt;/span&gt; is the &lt;span class=&#34;math inline&#34;&gt;\(p_i\times 1\)&lt;/span&gt; vector of slopes, &lt;span class=&#34;math inline&#34;&gt;\(w_i(s)\)&lt;/span&gt; is a spatially correlated process and &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i(s) \stackrel{ind}{\sim} N(0,\tau^2_i)\)&lt;/span&gt; is the random error in outcome &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. We usually assume that &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i(s)\)&lt;/span&gt; are independent across &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, whereas &lt;span class=&#34;math inline&#34;&gt;\(w(s)=(w_1(s), w_2(s),\ldots, w_q(s))^T\)&lt;/span&gt; is a zero-centred multivariate Gaussian process (GP) with a zero mean and a cross-covariance function that introduces dependency across space and among the &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; variables. Let &lt;span class=&#34;math inline&#34;&gt;\(\mathcal S_i\)&lt;/span&gt; represent the collection of places where the &lt;span class=&#34;math inline&#34;&gt;\(i-\)&lt;/span&gt;th variable was observed. The reference set of locations for our approach is &lt;span class=&#34;math inline&#34;&gt;\(\mathcal L = \cup_i\mathcal S_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The definition of &lt;span class=&#34;math inline&#34;&gt;\(w(s)\)&lt;/span&gt; as the &lt;span class=&#34;math inline&#34;&gt;\(q \times 1\)&lt;/span&gt; multivariate graphical Matérn GP (&lt;span class=&#34;citation&#34;&gt;Dey and Banerjee (&lt;a href=&#34;#ref-dey2021ggp&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt;) with respect to a decomposable variable graph &lt;span class=&#34;math inline&#34;&gt;\({\mathcal G}_{\mathcal V}\)&lt;/span&gt; yields the distribution of each &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt;. The marginal parameters for each component Matérn process &lt;span class=&#34;math inline&#34;&gt;\(w_i(\cdot)\)&lt;/span&gt; are denoted by &lt;span class=&#34;math inline&#34;&gt;\(\{\phi_{ii}, \sigma_{ii} | i = 1.\ldots,q\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;It is sufficient to limit the intra-site covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma=(\sigma_{ij})\)&lt;/span&gt; to be of the following form to assure a valid multivariate Matérn cross-covariance function (Theorem 1 of &lt;span class=&#34;citation&#34;&gt;Apanasovich and Sun (&lt;a href=&#34;#ref-apanasovich2012valid&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt;):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:constraints&#34;&gt;\[\begin{equation}
\begin{array}{cc}
   \nu_{ij} =&amp;amp; \frac 12 (\nu_{ii} + \nu_{jj}) + \Delta_A (1 - A_{ij}) \mbox{ where } \Delta_A \geq 0, A=(A_{ij}) \mbox{ for all } i \geq 0, A_{ii}=1 \\
    \sum_{i,j} c_ic_j\phi_{ij} \leq 0  &amp;amp;\mbox{ is a conditionally non-negative definite matrix } \\
    \sigma_{ij} =&amp;amp; b_{ij} \frac{\Gamma(\frac 12 (\nu_{ii}+\nu_{jj} + d))\Gamma(\nu_{ij})}{\phi_{ij}^{2\Delta_A+\nu_{ii}+\nu_{jj}}\Gamma(\nu_{ij} + \frac d2)} \mbox{ where } \Delta_A \geq 0, \mbox{ and } B=(b_{ij}) &amp;gt; 0, \mbox{ i.e., is p.d.}
\end{array}
\tag{2}
\end{equation}\]&lt;/span&gt;
This is equal to &lt;span class=&#34;math inline&#34;&gt;\(\Sigma = (B \odot (\gamma_{ij}))\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{ij}\)&lt;/span&gt;’s are constants collecting the components in &lt;a href=&#34;#eq:constraints&#34;&gt;(2)&lt;/a&gt; involving just &lt;span class=&#34;math inline&#34;&gt;\(\nu_{ij}\)&lt;/span&gt;’s and &lt;span class=&#34;math inline&#34;&gt;\(\phi_{ij}\)&lt;/span&gt;’s, and &lt;span class=&#34;math inline&#34;&gt;\(\odot\)&lt;/span&gt; indicates the Hadamard (element-wise) product.&lt;/p&gt;
&lt;p&gt;In the following example, we’ll assume &lt;span class=&#34;math inline&#34;&gt;\(\Delta_A=0, \nu_{ij} = \frac{\nu_{ii}+\nu_{jj}}{2}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\phi_{ij}^2 = \frac{\phi_{ii}^2 + \phi_{jj}^2}{2}\)&lt;/span&gt;. The constraints in &lt;a href=&#34;#eq:constraints&#34;&gt;(2)&lt;/a&gt; simplifies to -&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:constraints-simp&#34;&gt;\[\begin{equation}
\begin{array}{cc}
    \sigma_{ij} =&amp;amp; (\sigma_{ii} \sigma_{jj})^{\frac{1}{2}} * \frac{\phi_{ii}^{\nu_{ii}}\phi_{jj}^{\nu_{jj}}}{\phi_{ij}^{\nu_{ij}}}  \frac{\Gamma(\nu_{ij})}{\Gamma(\nu_{ii})^{\frac 12} \Gamma(\nu_{ij})^{\frac 12}} r_{ij} \mbox{ where }, \mbox{ and } R=(r_{ij}) &amp;gt; 0, \mbox{ i.e., is p.d.}
\end{array}
\tag{3}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We also take &lt;span class=&#34;math inline&#34;&gt;\(\nu_{ii} = \nu_{jj} = 0.5\)&lt;/span&gt; in our case and hence, we only need to estimate the marginal scale (&lt;span class=&#34;math inline&#34;&gt;\(\phi_{ii}\)&lt;/span&gt;) and variance parameters (&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{ii}\)&lt;/span&gt;) and cross-correlation parameters &lt;span class=&#34;math inline&#34;&gt;\(r_{ij}\)&lt;/span&gt;. We also assume variable graph (&lt;span class=&#34;math inline&#34;&gt;\({\mathcal G}_{\mathcal V}\)&lt;/span&gt;) to be decomposable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simulation&lt;/h1&gt;
&lt;p&gt;As a first step of simulation, we need to fix a decomposable variable graph (&lt;span class=&#34;math inline&#34;&gt;\({\mathcal G}_{\mathcal V}\)&lt;/span&gt;). Depending on the perfect ordering of cliques in the graph, the density function of the full process can be factorized. Using this property, we can calculate the variance-covarinace matrix of the GGP and use it to simulate observation from a multivariate GGP.&lt;/p&gt;
&lt;div id=&#34;decomposable-variable-graph&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Decomposable variable graph&lt;/h2&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\mathcal G_\mathcal V\)&lt;/span&gt; is decomposable, it has a perfect clique sequence &lt;span class=&#34;math inline&#34;&gt;\(\{K_1,K_2,\cdots,K_p\}\)&lt;/span&gt; with separators &lt;span class=&#34;math inline&#34;&gt;\(\{S_2,\ldots,S_m\}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(w(\mathcal L)\)&lt;/span&gt;’s joint density may be factorized as follows.
&lt;span class=&#34;math display&#34; id=&#34;eq:ggp-fact-2&#34;&gt;\[\begin{equation}
    f_M(w(\mathcal L)) = \frac{\Pi_{m=1}^{p} f_C(w_{K_m}( \mathcal L))}{\Pi_{m=2}^{p} f_C(w_{S_m}( \mathcal L))}\;,
\tag{4}
\end{equation}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(S_m=F_{m-1} \cap K_m\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(F_{m-1}= K_1 \cup \cdots \cup K_{m-1}\)&lt;/span&gt;, and
&lt;span class=&#34;math inline&#34;&gt;\(f_A\)&lt;/span&gt; denotes the the density of a GP over &lt;span class=&#34;math inline&#34;&gt;\(\mathcal L\)&lt;/span&gt; with covariance function &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(A \in \{M,C\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Here, we assume we have &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt; variables with each being observed at &lt;span class=&#34;math inline&#34;&gt;\(250\)&lt;/span&gt; locations uniformly chosen from the &lt;span class=&#34;math inline&#34;&gt;\((0,1) \times (0,1)\)&lt;/span&gt; square. We assume the variable graph to be as follows.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-07-07-illustration-of-graphical-gaussian-process-models-to-analyze-highly-multivariate-spatial-data_files/unnamed-chunk-3-1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We calculate the perfect ordering of the cliques of the graph above and list the cliques and separators below. To visualize the decomposition, we also plot the junction tree (definition below) of the graph -&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;junction graph&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; of a decomposable graph &lt;span class=&#34;math inline&#34;&gt;\(\mathcal G_\mathcal V\)&lt;/span&gt; is a complete graph with the cliques of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal G_\mathcal V\)&lt;/span&gt; as its nodes. Every edge in the junction graph is denoted as a link, which is also the intersection of the two cliques, and can be empty. A &lt;strong&gt;spanning tree&lt;/strong&gt; of a graph is defined as a subgraph comprising all the vertices of the original graph and is a tree (acyclic graph). If a spanning tree &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; of the junction graph of &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; satisfies the following property: for any two cliques &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; of the graph, every node in the unique path between &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; in the tree contains &lt;span class=&#34;math inline&#34;&gt;\(C \cap D\)&lt;/span&gt;. Then &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; is called the &lt;strong&gt;junction tree&lt;/strong&gt; for the graph &lt;span class=&#34;math inline&#34;&gt;\(\mathcal G_\mathcal V\)&lt;/span&gt;. Here, the junction tree of the graph has the perfectly ordered cliques as its nodes and the separators denoted as edges.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-07-07-illustration-of-graphical-gaussian-process-models-to-analyze-highly-multivariate-spatial-data_files/unnamed-chunk-4-1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-07-07-illustration-of-graphical-gaussian-process-models-to-analyze-highly-multivariate-spatial-data_files/unnamed-chunk-5-1.png&#34; /&gt;
Hence, in this case, the likelihood of the decomposable GGP would be as follows -
&lt;span class=&#34;math display&#34; id=&#34;eq:ggp-fact-ex&#34;&gt;\[\begin{equation}
    f_M(w(\mathcal L)) = \frac{f_C(w_{(1,2,3)}(\mathcal L)) f_C(w_{(2,3,4)}(\mathcal L)) f_C(w_{(4,5,6)}(\mathcal L)) f_C(w_{(6,7,8)}(\mathcal L)) f_C(w_{(8,9,10)}(\mathcal L))}{f_C(w_{(2,3)}(\mathcal L)) f_C(w_{(4)}(\mathcal L)) f_C(w_{(6)}(\mathcal L)) f_C(w_{(8)}(\mathcal L))}\;,
\tag{5}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulating-latent-ggp&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulating latent GGP&lt;/h2&gt;
&lt;p&gt;First we fix the marginal and cross-covariance parameters of the process.&lt;/p&gt;
&lt;p&gt;Now, the precision matrix of the GGP &lt;span class=&#34;math inline&#34;&gt;\(w(\mathcal L)\)&lt;/span&gt; satisfies (Lemma 5.5 of &lt;span class=&#34;citation&#34;&gt;Lauritzen (&lt;a href=&#34;#ref-lauritzen1996graphical&#34;&gt;1996&lt;/a&gt;)&lt;/span&gt;)
&lt;span class=&#34;math display&#34; id=&#34;eq:m-decomp&#34;&gt;\[\begin{equation}
M(\mathcal L,\mathcal L)^{-1} = \sum_{m=1}^{p} [{C}_{[K_m \boxtimes \mathcal{G_L}]}^{-1}]^{\mathcal V \times \mathcal L} - \sum_{m=2}^{p} [{C}_{[S_m \boxtimes \mathcal{G_L}]}^{-1}] ^{\mathcal V \times \mathcal L}\;, %= \sum_{m=1}^{p} [{M}_{[K_m \boxtimes \mathcal{G_L}]}^{-1}] ^\mathcal V - \sum_{m=2}^{p} [{M}_{[S_m \boxtimes \mathcal{G_L}]}^{-1}] ^\mathcal V\;
\tag{6}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;#eq:m-decomp&#34;&gt;(6)&lt;/a&gt; and &lt;a href=&#34;#eq:ggp-fact-2&#34;&gt;(4)&lt;/a&gt; shows that inverting the full cross-covariance matrix only requires inverting the clique and separator specific covariance matrices. Hence, the computational complexity for calculating likelihood of a multivariate GGP boils down to &lt;span class=&#34;math inline&#34;&gt;\(O(n^3 c^3)\)&lt;/span&gt; (here, &lt;span class=&#34;math inline&#34;&gt;\(O(250^3 * 27)\)&lt;/span&gt;) where &lt;span class=&#34;math inline&#34;&gt;\(c= 3\)&lt;/span&gt; is the maximum size of a clique in the perfect clique ordering of the graph. On the contrary, the likelihood of a full GGP would need &lt;span class=&#34;math inline&#34;&gt;\(O(n^3 q^3)\)&lt;/span&gt; complexity, i.e. &lt;span class=&#34;math inline&#34;&gt;\(O(250^3 * 1000)\)&lt;/span&gt; in our case.&lt;/p&gt;
&lt;p&gt;We use the &lt;a href=&#34;#eq:m-decomp&#34;&gt;(6)&lt;/a&gt; to calculate the covariance of the latent GGP and we use it to simulate the latent process.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulating-multivariate-spatial-outcome&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulating multivariate spatial outcome&lt;/h2&gt;
&lt;p&gt;Now we use &lt;a href=&#34;#eq:mgp&#34;&gt;(1)&lt;/a&gt; to simulate our multivariate outcome &lt;span class=&#34;math inline&#34;&gt;\(y_i(.), i= 1, \cdots, 10\)&lt;/span&gt;. In order to do that, we fix some covariates &lt;span class=&#34;math inline&#34;&gt;\(x_i(\cdot)\)&lt;/span&gt; and simulate error process &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{i}(\cdot)\)&lt;/span&gt; independently from &lt;span class=&#34;math inline&#34;&gt;\(N(0,\tau^2_{ii})\)&lt;/span&gt;. We take &lt;span class=&#34;math inline&#34;&gt;\(\tau_{ii}^2=\tau^2=0.25\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For analyzing the prediction accuracy of our method, we randomly pick &lt;span class=&#34;math inline&#34;&gt;\(20\%\)&lt;/span&gt; of the locations for each outcome variable and consider them missing. We will only be working with the training set to fit the model and judge our prediction accuracy on the test set.&lt;/p&gt;
&lt;p&gt;Now we visualize the surface of the training data by variables below.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data analysis&lt;/h1&gt;
&lt;p&gt;The analysis of our simulated data can be broken down in the following steps.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Marginal parameter estimation&lt;/strong&gt;: We estimate the marginal scale (&lt;span class=&#34;math inline&#34;&gt;\(\phi_{ii}\)&lt;/span&gt;), variance (&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{ii}\)&lt;/span&gt;) and smoothness parameters (&lt;span class=&#34;math inline&#34;&gt;\(\nu_{ii}\)&lt;/span&gt;) from the component Gaussian processes. We also estimate the error variance (&lt;span class=&#34;math inline&#34;&gt;\(\tau^2_{ii}\)&lt;/span&gt;) for each marginal processes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Initialize Gibbs sampling&lt;/strong&gt;: For this step, we need the following components:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Processing the variable graph&lt;/strong&gt;: We fix our variable graph for the estimation process. First we calculate cliques, separators of the graph. Then we color the nodes and edges of the graph for parallel computation purposes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Starting cross-correlation&lt;/strong&gt;: We use the estimated marginal parameters and an initial correlation parameter to start off.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Gibbs sampling&lt;/strong&gt;: We run our Gibbs sampler in two steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Sampling latent spatial processes&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sampling latent correlations&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;marginal-parameter-estimation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Marginal parameter estimation&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We first estimate the marginal process parameters using &lt;strong&gt;BRISC&lt;/strong&gt; package in R. We estimate scale (&lt;span class=&#34;math inline&#34;&gt;\(\phi_{ii}\)&lt;/span&gt;) and variance (&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{ii}\)&lt;/span&gt;) parameters. We fix the marginal smoothness (&lt;span class=&#34;math inline&#34;&gt;\(\nu_ii\)&lt;/span&gt;) and cross-smoothness (&lt;span class=&#34;math inline&#34;&gt;\(\nu_{ij}\)&lt;/span&gt;) parameters at &lt;span class=&#34;math inline&#34;&gt;\(0.5\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Same as the true cross-covariance, we calculate the cross-scale parameters as: &lt;span class=&#34;math inline&#34;&gt;\(\phi_{ij}= \sqrt{\frac{\phi_{ii}^2 + \phi_{jj}^2}{2}}\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;We estimate the marginal regression coefficients (&lt;span class=&#34;math inline&#34;&gt;\(\beta_i\)&lt;/span&gt;).&lt;/li&gt;
&lt;li&gt;We estimate the marginal error variance (&lt;span class=&#34;math inline&#34;&gt;\(\tau_{ii}^2\)&lt;/span&gt;).&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;initialize-gibbs-sampling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Initialize Gibbs sampling&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Process the variable graph&lt;/strong&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;First, we color the nodes of the variable graph. This will allow us to simulate the latent processes belonging to the same color in parallel.&lt;/p&gt;
&lt;p&gt;We also construct a new graph &lt;span class=&#34;math inline&#34;&gt;\(\mathcal G_E(\mathcal G_V)=(E_\mathcal V,E^*)\)&lt;/span&gt; which denotes this graph on the set of edges &lt;span class=&#34;math inline&#34;&gt;\(E_\mathcal V\)&lt;/span&gt;, i.e., there is an edge &lt;span class=&#34;math inline&#34;&gt;\(((i,j),(i&amp;#39;,j&amp;#39;))\)&lt;/span&gt; in this new graph &lt;span class=&#34;math inline&#34;&gt;\(\mathcal G_E(\mathcal G_V)\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\{i,i&amp;#39;,j,j&amp;#39;\}\)&lt;/span&gt; are in some clique &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal G_V\)&lt;/span&gt;. This allows us to facilitate parallel update of cross-correlation parameters corresponding to edges in the same color.&lt;/p&gt;
&lt;p&gt;These two procedures are examples of chromatic Gibbs sampling.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-07-07-illustration-of-graphical-gaussian-process-models-to-analyze-highly-multivariate-spatial-data_files/unnamed-chunk-13-1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;2.&lt;strong&gt;Calculate initial cross-covariance matrix&lt;/strong&gt;: Start with an initial cross-correlation matrix. We Take a convex combination: &lt;span class=&#34;math inline&#34;&gt;\(0.5*diag(q) + 0.5*cor(Y.train)\)&lt;/span&gt;. Then we use this initial cross-correlation parameters and estimated marginal parameters to store the cross covariance matrices for cliques and separators only.&lt;/p&gt;
&lt;p&gt;The largest matrix we need to store is of size &lt;span class=&#34;math inline&#34;&gt;\(nq_c \times nq_c\)&lt;/span&gt; matrix where &lt;span class=&#34;math inline&#34;&gt;\(q_c\)&lt;/span&gt; is the size of maximum clique or separator).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gibbs-sampling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gibbs sampling&lt;/h2&gt;
&lt;p&gt;We iteratively perform the following steps until we reach a desired number of samples (&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;).&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Sampling latent processes&lt;/strong&gt; : Using random draws from multivariate normal distribution&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sampling correlations&lt;/strong&gt;: Metropolis-hastings&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Jumping between graphs&lt;/strong&gt;: Reversible jump MCMC.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;sampling-latent-processes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sampling latent processes&lt;/h3&gt;
&lt;p&gt;To update the latent random effects &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;, let &lt;span class=&#34;math inline&#34;&gt;\(\mathcal L=\{s_1,\ldots,s_n\}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(o_i=\mbox{diag}(I(s_1 \in \mathcal S_i), \ldots, I(s_n \in \mathcal S_i))\)&lt;/span&gt; denote the vector of missing observations for the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th outcome. With &lt;span class=&#34;math inline&#34;&gt;\(X_{i}(\mathcal L) = (x_i(s_1),\ldots,x_i(s_n))^T\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(y_i(\mathcal L)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(w_i(\mathcal L)\)&lt;/span&gt; defined similarly, we obtain
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
\begin{split}
p(w_i(\mathcal L) | \cdot) &amp;amp; \sim N\left(\mathcal M_i^{-1}\mu_i, \mathcal M_i^{-1}\right)\;, \mbox{ where } \\
\mathcal M_i &amp;amp; =\frac{1}{\tau_i^2}\mbox{diag}(o_i) + \sum_{K \ni i}M_{\{i\} \times \mathcal L|(K\setminus \{i\}) \times \mathcal L}^{-1} - \sum_{S \ni i}M^{-1}_{\{i\} \times \mathcal L|(S\setminus \{i\}) \times \mathcal L}\;, \\
\mu_i &amp;amp;= \frac{(y_i(\mathcal L) - x_i(\mathcal L)^{T}\beta_i)\odot o_i}{\tau_i^2} + \\
&amp;amp; \qquad \sum_{K \ni i}  T_{i}(K) w({(K\setminus \{i\}) \times \mathcal L}) - \sum_{S \ni i} T_i(S) w({(S\setminus \{i\}) \times \mathcal L})\;, \\
T_{i}(A) &amp;amp;= M_{\{i\} \times \mathcal L|(A\setminus \{i\}) \times \mathcal L}^{-1}  M_{\{i\} \times \mathcal L,(A\setminus \{i\}) \times \mathcal L}M_{(A\setminus \{i\}) \times \mathcal L}^{-1}, \mbox{ for } A \in \{K,S\}.\\
% U_i(S) &amp;amp;= M_{\{i\} \times \mathcal L|(S\setminus \{i\}) \times \mathcal L}^{-1}  M_{(S\setminus \{i\}) \times \mathcal L,\{i\} \times \mathcal L}M_{(S\setminus \{i\}) \times \mathcal L}^{-1}.
\end{split}\label{eqn: gibbs-sam-latent}
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;%We denote &lt;span class=&#34;math inline&#34;&gt;\(\mathscr T_i= \mathcal L \setminus \mathscr S_i\)&lt;/span&gt;.
% for a clique &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; in variable graph &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{G_V}\)&lt;/span&gt;, the set - &lt;span class=&#34;math inline&#34;&gt;\(K \times \mathcal L = K \times \mathscr S\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(S \times \mathcal L= S \times \mathscr S\)&lt;/span&gt;,
%&lt;span class=&#34;math inline&#34;&gt;\(i_{K}=\{K \setminus i\} \times \mathscr S\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(i \times \mathscr S_i = i_{\mathscr S}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(i \times \mathscr T_i= i_{\mathscr T}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(i_{K} \cup i_{\mathscr T}= i_{K\mathscr T}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(i_{K} \cup i_{\mathscr S}= i_{K \times \mathcal L}\)&lt;/span&gt; . Also, for a clique &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; in containing a variable &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, let’s denote &lt;span class=&#34;math inline&#34;&gt;\(M_{i_{{\mathscr S}.K}} = M_{i_{\mathscr S}} - M_{i_{\mathscr S}, i_{K\mathscr T}}M_{i_{K\mathscr T}}^{-1}M_{i_{K\mathscr T},i_{\mathscr S}}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(M_{i_{{\mathscr T}.K}} = M_{i_{\mathscr T}} - M_{i_{\mathscr T}, i_{K \times \mathcal L}}M_{i_{K \times \mathcal L}}^{-1}M_{i_{K \times \mathcal L},i_{\mathscr T}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sampling-cross-correlation-parameters&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sampling cross-correlation parameters&lt;/h3&gt;
&lt;p&gt;Requires only checking positive-definiteness of the clique-specific cross-covariance matrix and inverting it. The largest matrix inversion across all these updates is of the order &lt;span class=&#34;math inline&#34;&gt;\(nc \times nc\)&lt;/span&gt;, corresponding to the largest clique. The largest matrix that needs storing is also of dimension &lt;span class=&#34;math inline&#34;&gt;\(nc \times nc\)&lt;/span&gt;. These result in appreciable reduction of computations from any multivariate Matérn model that involves &lt;span class=&#34;math inline&#34;&gt;\(nq \times nq\)&lt;/span&gt; matrices and positive-definiteness checks for &lt;span class=&#34;math inline&#34;&gt;\(q \times q\)&lt;/span&gt; matrices at every iteration.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;For every correlation parameter corresponding to edges in the current graph, we draw a new correlation value from the proposal distribution.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We accept or reject the proposal based on the Metropolis-Hastings acceptance probability.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Results&lt;/h1&gt;
&lt;p&gt;We create three plots below among true and estimated parameters for - (1) product of marginal scale and variance parameter (&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{ii}\)&lt;/span&gt;), (2) cross-correlation parameter (&lt;span class=&#34;math inline&#34;&gt;\(r_{ij}\)&lt;/span&gt;) and (3) product of cross-covariance and cross-scale parameter (&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{ij}*\phi_{ij}\)&lt;/span&gt;). The points on all the plots align on the &lt;span class=&#34;math inline&#34;&gt;\(y=x\)&lt;/span&gt; line thus showing the accuracy of our estimation. We also create a plot across &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt; variables showing true test data values and predicted values from our model. The points align on &lt;span class=&#34;math inline&#34;&gt;\(y=x\)&lt;/span&gt; line and mean-square error varied from &lt;span class=&#34;math inline&#34;&gt;\(0.403\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(1.064\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-07-07-illustration-of-graphical-gaussian-process-models-to-analyze-highly-multivariate-spatial-data_files/unnamed-chunk-15-1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-07-07-illustration-of-graphical-gaussian-process-models-to-analyze-highly-multivariate-spatial-data_files/unnamed-chunk-15-2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-07-07-illustration-of-graphical-gaussian-process-models-to-analyze-highly-multivariate-spatial-data_files/unnamed-chunk-15-3.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-07-07-illustration-of-graphical-gaussian-process-models-to-analyze-highly-multivariate-spatial-data_files/unnamed-chunk-15-4.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;about-the-authors&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;About the authors&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://debangandey.rbind.io&#34;&gt;Debangan Dey&lt;/a&gt;, National Institute of Mental Health.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://abhidatta.com/&#34;&gt;Abhirup Datta&lt;/a&gt;, Dept. of Biostatistics, Johns Hopkins Bloomberg School of Public Health.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sudipto.bol.ucla.edu/&#34;&gt;Sudipto Banerjee&lt;/a&gt;, Dept. of Biostatistics, UCLA Fielding School of Public Health.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;bibliography&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;Bibliography&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-apanasovich2012valid&#34; class=&#34;csl-entry&#34;&gt;
Apanasovich, Marc G Genton, Tatiyana V, and Ying Sun. 2012. &lt;span&gt;“A Valid Matern Class of Cross-Covariance Functions for Multivariate Random Fields with Any Number of Components.”&lt;/span&gt; &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 107 (497): 180–93.
&lt;/div&gt;
&lt;div id=&#34;ref-ban14&#34; class=&#34;csl-entry&#34;&gt;
Banerjee, B. P. Carlin, S., and A. E. Gelfand. 2014. &lt;em&gt;Hierarchical Modeling and Analysis for Spatial Data&lt;/em&gt;. Chapman &amp;amp; Hall/CRC, Boca Raton, FL.
&lt;/div&gt;
&lt;div id=&#34;ref-creswikle11&#34; class=&#34;csl-entry&#34;&gt;
Cressie, Noel A. C., and Christopher K. Wikle. 2011. &lt;em&gt;Statistics for Spatio-Temporal Data&lt;/em&gt;. Wiley Series in Probability and Statistics. Wiley, Hoboken, NJ.
&lt;/div&gt;
&lt;div id=&#34;ref-dey2021ggp&#34; class=&#34;csl-entry&#34;&gt;
Dey, Abhirup Datta, Debangan, and Sudipto Banerjee. 2021. &lt;span&gt;“Graphical Gaussian Process Models for Highly Multivariate Spatial Data.”&lt;/span&gt; &lt;em&gt;Biometrika&lt;/em&gt; December.
&lt;/div&gt;
&lt;div id=&#34;ref-lauritzen1996graphical&#34; class=&#34;csl-entry&#34;&gt;
Lauritzen, Steffen L. 1996. &lt;em&gt;Graphical Models&lt;/em&gt;. Vol. 17. Clarendon Press.
&lt;/div&gt;
&lt;div id=&#34;ref-wackernagel2013multivariate&#34; class=&#34;csl-entry&#34;&gt;
Wackernagel, Hans. 2013. &lt;em&gt;Multivariate Geostatistics: An Introduction with Applications&lt;/em&gt;. Springer Science &amp;amp; Business Media.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
