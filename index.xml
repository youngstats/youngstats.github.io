<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>YoungStatS</title>
    <link>https://youngstats.github.io/</link>
    <description>Recent content on YoungStatS</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 22 Oct 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://youngstats.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Some Recent Developments in Mixture Cure Model Methodology for Survival Analysis</title>
      <link>https://youngstats.github.io/post/2022/10/22/some-recent-developments-in-mixture-cure-model-methodology-for-survival-analysis/</link>
      <pubDate>Sat, 22 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/10/22/some-recent-developments-in-mixture-cure-model-methodology-for-survival-analysis/</guid>
      <description>


&lt;p&gt;In certain clinical trials or observational studies, either prospective or based on historically accumulated data, individuals are or have been {} for a period of time and their status at some endpoint reported.
See for example the (Surveillance, Epidemiology, and End Results, US National Cancer Institute) data base, which contains a massive amount of data with extended followup on a wide range of cancers — an important source for historical data.&lt;br /&gt;
In another context, in , the times of occurrence of four endpoints (overall survival, disease-specific survival, disease-free interval, or progression-free interval) for 11,160 patients across 33 cancer types were obtained from follow-up data files, with a view to making recommendations to clinicians regarding their patient’s status.&lt;/p&gt;
&lt;p&gt;The data confronting the statistician consists of observations like this, on
the time to the occurrence of some event such as death, or the recurrence of a disease, etc. For definiteness, suppose we are analysing overall survival, and the measurement is the life-lengths of a sample of individuals.
A particular characteristic of this kind of data is that it is commonly
{}. This happens when an individual’s complete lifetime is not observed, either because s/he left the study early for some reason, or was still alive at the end of the study (and all real-life studies must be terminated at some finite time).
The censored observations must be taken into account in any analysis; to ignore them would introduce bias, in that, typically, some of the longer lifetimes would have been ignored.&lt;/p&gt;
&lt;p&gt;Methods for the analysis of such survival data have long been known.
See for example .
A good place to start is simply to look at the data, literally, in the form of the
{} (KME, ), which is a nonparametric estimator of the survival function (the tail, or complement, of the distribution describing the lifetimes) which takes into account the censoring.
%&lt;a href=&#34;https://en.wikipedia.org/wiki/Kaplan%E2%80%93Meier_estimator&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Kaplan%E2%80%93Meier_estimator&lt;/a&gt;
An example KME plot is the first thing we see when looking at the current Wikipedia entry () for the KME.
Notable about this example and many others we can see in the literature is that the survivor function is {}; it does not reach zero at its right endpoint.
Equivalently, in such cases, the KME as a cdf has total mass less than 1.
This is so for all of the data sets in .
%, and we give other examples below.&lt;/p&gt;
&lt;p&gt;A KME which levels off or &lt;code&gt;plateaus&#39;&#39; at its right hand end because the largest or perhaps a number of the largest lifetimes are censored  may indicate the presence of a proportion of individuals in the population who will not suffer the event, no matter how long they are followed up. We refer to them as&lt;/code&gt;cured of’’ or &lt;code&gt;immune to&#39;&#39; the event, and methods are now well  developed to deal with this kind of data, generally known as {\it  cure model } methods.  As well as providing significant extra information beyond that of a standard survival analysis, ignoring the presence of  cures in an analysis can lead to biased and misleading conclusions, sometimes  with  profound consequences for diagnostic prognostications and evaluations.   %An important application  is reported in %additional point is that  statistics such as  %$Q_n$ and $\wt\alpha_n$  can be used not only to  test for sufficient follow-up but  % also to provide measures of how much follow-up there is in a sample. %Both these aspects are  %  \cite{Liu:etal:2018},  %where   testing for and measurement of sufficient follow-up in the TCGA pan-cancer clinical data resource are done on a very extensive scale  %in order to provide recommendations to cancer researchers wishing to assess the adequacy of clinical follow-up in a medical situation.  %In \cite{Liu:etal:2018},  follow-up data files for 11,160 patients %across 33 cancer types were  % processed.   %Median follow-up times as well as median times %to event (or censorship) based on the observed times for  %four endpoints (overall survival, disease-specific survival, disease-free interval, or progression-free interval) were %calculated. %%In a very detailed analysis,  %The authors used  $Q_n$  %% and $\wt\alpha_n$  % in \cite{shen:2000} %to classify all $33 \times 4$ resulting  KMEs  as having sufficient or insufficient follow-up (or noted cases in which  tests were inconclusive)  in order to give  endpoint usage recommendations for each cancer type. %%%%%%%%%%Their results are summarized in their Table 3.  %They stress: {\it  %For each endpoint, it is very important to have a %sufficiently long follow-up time to capture the events of interest, %and the minimum follow-up time needed depends on both the %aggressiveness of the disease and the type of endpoint} (Liu et al., %\cite{Liu2018}, p.401). % % %In the analysis of time-to-event data, it is not uncommon to encounter  %survival curves which  plateau (level off) at the right hand end. % % %This may indicate the presence of a proportion of individuals in the population who will not suffer the event, no matter how long they are followed up. %We refer to them as&lt;/code&gt;cured of’’ or ``immune to’’ the event, and methods are now well developed to deal with this kind of data, generally known as cure model methods.
% As well as providing significant extra information beyond that of a standard survival analysis, ignoring the presence of cures in an analysis can lead to biased and misleading conclusions, sometimes with profound consequences for diagnostic prognostications and evaluations.
%&lt;br /&gt;
%Cure models have received large and growing attention in the last few decades and it seems timely now to provide a review of their development leading up to present day applications.
Various versions of cure models have been formulated over the years but here we concentrate on a version which seems easiest to us to formulate, analyse and interpret — {}.&lt;/p&gt;
&lt;p&gt;The first recognition of the need for and implementation of a cure model seems to have been by .
He collected data from a number of centres in England, for various sites of the disease and
treatment methods, and
noticed that, while
the distributions of life-lengths
(measured from the beginning of treatment) of those dying
appeared to follow quite well a lognormal distribution,
{}
Accordingly, he proposed a model in which
{}
He went on to fit by maximum likelihood a lognormal distribution with mass at infinity –
a mixture cure model –
to followup data on 121 women with breast cancer, finding a significant ``cured’’ proportion in the data.
%% We revisit Boag’s data and analysis in Sections &lt;span class=&#34;math inline&#34;&gt;\(\ref{DD}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\ref{prob}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Fig. &lt;span class=&#34;math inline&#34;&gt;\(\ref{uncensKMEBoag}\)&lt;/span&gt; shows the KME of the survival distribution
with 95% confidence intervals, and a Weibull mixture distribution fitted,
for Boag’s 121 breast cancer patients.
The KME jumps only at uncensored (death) times, remaining constant at
censored times.
For this data it clearly levels off at a value less than 1, consistent with Boag’s observation of a possible cured component,
%The KME contains further evidence about the existence of a cured component.
with a tendency to
remain constant at lifetimes greater than 90 months, except for one late death at
120.6 months. The length of the level stretch at the righthand end of the KME is indicative of the amount of followup in the data.&lt;/p&gt;
&lt;p&gt;The KME in Fig. &lt;span class=&#34;math inline&#34;&gt;\(\ref{uncensKMEBoag}\)&lt;/span&gt; is very typical of the kind that can be seen in much of the medical literature. It displays clearly the main issues we want to address:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bullet\)&lt;/span&gt;
has the KME levelled off at a value {}
less than 1 thereby indicating the possible presence of immunes in the population? and&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bullet\)&lt;/span&gt;
has the KME levelled off {} for us to be confident of this?
%&lt;br /&gt;
%
% These are statistical questions, and to answer them we must apply
% statistical methods.
% The first step in a statistical
% analysis of survival data with possible cures is to assess and test for their existence in the population.
% The KME in Fig. &lt;span class=&#34;math inline&#34;&gt;\(\ref{KMEsBoag}\)&lt;/span&gt; is very typical of the kind that can be seen in much of the medical literature; see, e.g., .
% Other useful information in the sample is the KME of the censoring distribution;&lt;br /&gt;
% Fig.&lt;span class=&#34;math inline&#34;&gt;\(\ref{censKMEBoag}\)&lt;/span&gt; shows this for Boag’s data with a Weibull distribution fitted.
% We discuss Boag’s data and the Lui et al. analysis
% further in Section &lt;span class=&#34;math inline&#34;&gt;\(\ref{??}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Since the prospect of a cure is surely the hope of many or most medical procedures, the importance of Boag’s insight can hardly be overstated.
Following his groundbreaking paper a number of researchers, including
,
,
,
,
, %,
, % ,
,
,
,
,
followed up with various aspects and analyses of the model, but the first systematic treatment of what is now called the long term survivor or cure mixture model seems to have been in .
That book combines nonparametric and parametric theoretical formulations and proofs with many practical applications and examples of the model.&lt;/p&gt;
&lt;p&gt;There has been an upsurge in interest in the model since the 1990s, with many applications areas explored, especially in medical statistics, and some substantial theoretical advances made.
Correspondingly, computational facilities have improved tremendously, and with modern capabilities a wide variety of parametric models of censored data with long term survivors can now
be fitted routinely with the statistical package R (); see
,
and .
%%&lt;a href=&#34;https://cran.r-project.org/web/packages/flexsurvcure/flexsurvcure.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/flexsurvcure/flexsurvcure.pdf&lt;/a&gt;
%(See and their references for goodness-of-fit tests for parametric models with censored data.)
There have also been a number of review/overview and methodological articles:
,
, , ,
%%%DONT USE ….
%Felizzi, F., Paracha, N., Pöhlmann, J. et al. Mixture Cure Models in Oncology: A Tutorial and Practical Guidance. PharmacoEconomics Open 5, 143–155 (2021).
and the book by . % summarising some of these aspects.&lt;/p&gt;
&lt;p&gt;More recent work of the present authors concerns some aspects left unresolved in ,
as well as some quite new points of view, which we discuss below.&lt;/p&gt;
&lt;p&gt;%It seems appropriate now to present an overview drawing together earlier and some more recent developments as well as pointing out areas where further work is needed.
%Besides those we mention, the literature has now grown too large and diverse to summarise completely here, and we confine ourselves to a selection reflecting our own main interests.
%
%
%The next subsection introduces the notation to be used throughout.
%
%Expressions have been obtained for the distributions of these quantities under certain assumptions, and in the next section we explain these.&lt;/p&gt;
%%
&lt;p&gt;%We adopt the notation in and ,
%which differs slightly from that in .
% For the distributional results to follow we use the notation in ,
%which should be read in conjunction with the present paper.
A tractable and reasonably realistic model for the data is an independent and identically distributed (iid) censoring model with right censoring.
In it, a sample of size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; consists of observations on the sequence of
iid 2-vectors
&lt;span class=&#34;math inline&#34;&gt;\(\big(T_i=T_i^*\wedge U_i, C_i={\bf 1}(T_i^*\le U_i);\, 1\le i\le n\big)\)&lt;/span&gt;.
%where the &lt;span class=&#34;math inline&#34;&gt;\(T_i\)&lt;/span&gt; represent the censored survival times and the &lt;span class=&#34;math inline&#34;&gt;\(C_i\)&lt;/span&gt; are the censor indicators.
%It will be convenient to let &lt;span class=&#34;math inline&#34;&gt;\(M(n):= M(n) =\max_{1\leq i\leq n}T_i\)&lt;/span&gt;.
%We formally set &lt;span class=&#34;math inline&#34;&gt;\(T_i^*=\infty\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(C_i = 0\)&lt;/span&gt; for immune individuals.
%We have two independent sequences of iid positive random variables
%&lt;span class=&#34;math inline&#34;&gt;\((T_i^*)_{1\le i\le n}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\((U_i)_{1\le i\le n}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(n\in\N:=\{1,2,\ldots\}\)&lt;/span&gt;, having continuous cumulative distribution functions &lt;span class=&#34;math inline&#34;&gt;\(F^*\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\([0,\infty)\)&lt;/span&gt;, not degenerate at 0.
The &lt;span class=&#34;math inline&#34;&gt;\(T_i^*\)&lt;/span&gt; with continuous
cumulative distribution function (cdf) &lt;span class=&#34;math inline&#34;&gt;\(F^*\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\([0,\infty)\)&lt;/span&gt;
represent the times of occurrence of the event under study.
%, such as the death of a person, or the onset of a disease, etc.
The &lt;span class=&#34;math inline&#34;&gt;\(U_i\)&lt;/span&gt;, iid with continuous cdf &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\([0,\infty)\)&lt;/span&gt;,
are censoring random variables, independent of the &lt;span class=&#34;math inline&#34;&gt;\(T_i^*\)&lt;/span&gt;.
In a sample
%from a population containing long-term survivors
we observe the censored random variables &lt;span class=&#34;math inline&#34;&gt;\(T_i=T_i^*\wedge U_i\)&lt;/span&gt;
%, these being potential lifetimes censored at a limit of follow-up represented for individual &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; by the random variable &lt;span class=&#34;math inline&#34;&gt;\(U_i\)&lt;/span&gt;.
with censor indicators $ C_i={}(T_i^*U_i)$.&lt;/p&gt;
&lt;p&gt;In the general mixture cure model, the censoring distribution &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; of the &lt;span class=&#34;math inline&#34;&gt;\(U_i\)&lt;/span&gt; is always assumed proper (total mass 1), but the distribution &lt;span class=&#34;math inline&#34;&gt;\(F^*\)&lt;/span&gt; of the &lt;span class=&#34;math inline&#34;&gt;\(T_i^*\)&lt;/span&gt; may be improper,
%, with mass &lt;span class=&#34;math inline&#34;&gt;\(1-p\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(0\le p&amp;lt;1\)&lt;/span&gt;, at infinity. We assume &lt;span class=&#34;math inline&#34;&gt;\(F^*\)&lt;/span&gt; to be
of the form
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}    \label{FandF0}
    F^*(t)=pF(t),
\end{equation}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(0&amp;lt;p\le 1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; is a proper distribution.
&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; is the distribution of the lifetimes of susceptible individuals in the population; only these can experience the event of interest and have a potentially uncensored failure time.
The remainder are immune to the event of interest or cured of it.
The presence of cured subjects is signalled by a value of &lt;span class=&#34;math inline&#34;&gt;\(p&amp;lt;1\)&lt;/span&gt;, in which case the distribution &lt;span class=&#34;math inline&#34;&gt;\(F^*\)&lt;/span&gt; is improper, with total mass &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. %Then &lt;span class=&#34;math inline&#34;&gt;\(1-p\)&lt;/span&gt; is the proportion of immune or cured individuals in the population.&lt;/p&gt;
&lt;p&gt;We do not know whether a particular censored lifetime in the sample is from a cured or immune individual (uncensored lifetimes are obviously not from immunes); but
%in aggregate, the presence of cured individuals may be signalled by
%an ``improper’’ sample Kaplan-Meier estimator (KME, ),
% that is, one having total mass less than 1.
% This situation arises when the largest survival time in the sample is censored,
% and there may further be an interval of constancy of the KME at its right hand end due to a number of the largest observations being censored.
observations on cured or immune individuals are always censored; those on susceptibles may or may not be according as the corresponding &lt;span class=&#34;math inline&#34;&gt;\(T_i^*&amp;gt;U_i\)&lt;/span&gt; or not.&lt;/p&gt;
&lt;p&gt;%The notation &lt;span class=&#34;math inline&#34;&gt;\(\Fbar^*(t)= 1-F^*(t)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(t\ge 0\)&lt;/span&gt;, is used for the survival function (tail function) of &lt;span class=&#34;math inline&#34;&gt;\(F^*\)&lt;/span&gt;, and similarly &lt;span class=&#34;math inline&#34;&gt;\(\Fbar(t)= 1-F(t)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Gbar(t)= 1-G(t)\)&lt;/span&gt;.
% Let &lt;span class=&#34;math inline&#34;&gt;\(H(t):=P(T_1\le t)\)&lt;/span&gt; be the distribution of the observed survival times &lt;span class=&#34;math inline&#34;&gt;\(T_i=T_i^*\wedge U_i\)&lt;/span&gt;, with tail &lt;span class=&#34;math inline&#34;&gt;\(\Hbar(t)=1-H(t)=P(T_i^*\wedge U_i&amp;gt;t ) = \Fbar^*(t)\Gbar(t)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(t\ge 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;%
%We restrict our discussion to the {} cure model in this survey.
%Other models incoporating long term survivors have been proposed;
%we discuss them just briefly in Subsection &lt;span class=&#34;math inline&#34;&gt;\(\ref{omixmod}\)&lt;/span&gt;.
%But the mixture model is easy to formulate and easy for practitioners to interpret, and it generalises easily to competing risks setups, see Subsection &lt;span class=&#34;math inline&#34;&gt;\(\ref{??}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;%We take a practical point of view whereby the data has prominence and the methodological developments flow from the inferences to be drawn from it.
%So suppose we have at hand a single sample of survival data which is to be analysed statistically.
%The first thing a statistician will want to do is look at the data, and for this we recommend the empirical distribution function estimator (KME) of the
%lifetimes.&lt;/p&gt;
&lt;p&gt;%%See Subsection &lt;span class=&#34;math inline&#34;&gt;\(\ref{KME}\)&lt;/span&gt; for the definition.
The KME is a highly informative data display which shows clearly in visual form the features we want to investigate.
To define it, denote the ordered sample lifetimes as &lt;span class=&#34;math inline&#34;&gt;\(T_n^{(1)}&amp;lt; T_n^{(2)}&amp;lt; \cdots &amp;lt;T_n^{(n)}\)&lt;/span&gt;,
with associated censor indicators
&lt;span class=&#34;math inline&#34;&gt;\(C_n^{(1)}, C_n^{(2)}, \ldots, C_n^{(n)}\)&lt;/span&gt;.
%%NOTE THIS ORDERING IS THE SAME AS IN .
Let $M(n)=T_n^{(n)}=&lt;em&gt;{1in}T_i $ be the largest survival time.
%and let &lt;span class=&#34;math inline&#34;&gt;\(M_u(n)\)&lt;/span&gt; be the largest observed {} survival time.
An explicit definition of the KME is
F_n(t):= 1-&lt;/em&gt;{1in: , T_n^{(i)} t}^n (1- ),
 {} 0&amp;lt;tM(n),
with &lt;span class=&#34;math inline&#34;&gt;\(\wh F_n(0):=0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\wh F_n(t):=\wh F_n(M(n))\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(t&amp;gt; M(n)\)&lt;/span&gt;.
In &lt;span class=&#34;math inline&#34;&gt;\(\eqref{km1}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(n-i+1\)&lt;/span&gt; % := &lt;em&gt;{j=1}^n {}&lt;/em&gt;{T_j&amp;gt;T_i}$
is the number of subjects ``at risk’’ at times just prior to &lt;span class=&#34;math inline&#34;&gt;\(T_n^{(i)}\)&lt;/span&gt;.
Recall we assume &lt;span class=&#34;math inline&#34;&gt;\(F^*\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; are continuous so there are no tied survival times in the data with probability 1.
Let&lt;br /&gt;
p_n:= F_n(M(n))
be the value of the KME at its right extreme.
%See , Ch. 3 and 4, for properties of &lt;span class=&#34;math inline&#34;&gt;\(\wh F_n(t)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\wh p_n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In a sample we observe data values &lt;span class=&#34;math inline&#34;&gt;\((t_i, c_i)_{1\le i\le n}\)&lt;/span&gt; for
&lt;span class=&#34;math inline&#34;&gt;\((T_i,C_i)_{1\le i\le n}\)&lt;/span&gt;, order them as
&lt;span class=&#34;math inline&#34;&gt;\(t_n^{(1)}&amp;lt; t_n^{(2)}&amp;lt; \cdots &amp;lt;t_n^{(n)}\)&lt;/span&gt;, and define associated censor indicators
&lt;span class=&#34;math inline&#34;&gt;\(c_n^{(1)}, c_n^{(2)}, \ldots, c_n^{(n)}\)&lt;/span&gt;.
Then $t_n^{(n)}=_{1in}t_i $ is the largest observed survival time.
%and let &lt;span class=&#34;math inline&#34;&gt;\(M_u(n)\)&lt;/span&gt; be the largest observed {} survival time.
The sample KME is the same function with observed data values substituted for the random quantities, and we obtain a sample estimate of &lt;span class=&#34;math inline&#34;&gt;\(\wh p_n\)&lt;/span&gt; by substituting appropriately in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{defpnh}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;%As an example, Fig. &lt;span class=&#34;math inline&#34;&gt;\(\ref{uncensKMEBoag}\)&lt;/span&gt; shows the KME of the survival (lifetime) distribution
% with 95% confidence intervals, and a Weibull mixture distribution fitted,
% for Boag’s 121 breast cancer patients.
% (The Weibull is a slightly better fit than the lognormal as used by Boag).
% The KME jumps only at uncensored (death) times, remaining constant at
%censored times. For this data it clearly levels off at a value less than 1, consistent with Boag’s observation of a possible cured component, and we see clearly the main issues we want to address:
%
% &lt;span class=&#34;math inline&#34;&gt;\(\bullet\)&lt;/span&gt;
% has the KME levelled off at a value {} less than 1 thereby indicating the possible presence of immunes in the population? and
%&lt;br /&gt;
% &lt;span class=&#34;math inline&#34;&gt;\(\bullet\)&lt;/span&gt;
% has the KME levelled off {} for us to be confident of this?
%&lt;br /&gt;
%
% These are statistical questions, and to answer them we must apply
% statistical methods.
%&lt;br /&gt;
% The first step in a statistical
% analysis of survival data with possible cures is to assess and test for their existence in the population.&lt;/p&gt;
&lt;p&gt;A nonparametric estimate of the population proportion dying is given by the maximum value of the KME, that is, &lt;span class=&#34;math inline&#34;&gt;\(\wh p_n\)&lt;/span&gt; as defined by &lt;span class=&#34;math inline&#34;&gt;\(\eqref{defpnh}\)&lt;/span&gt;,
and its complement is the estimated cure proportion, which
as can be seen in Fig.&lt;span class=&#34;math inline&#34;&gt;\(\ref{uncensKMEBoag}\)&lt;/span&gt; for Boag’s data is 0.30 with a 95% confidence interval (CI) (calculated using the estimate)
of $ [0.19, 0.48]$. This interval excludes 0, in general agreement with Boag’s observation of a possible cured component.
This confidence interval assessment though indicative is not strictly correct usage, however, as the restriction of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt; should be taken into account, as should the fact that &lt;span class=&#34;math inline&#34;&gt;\(\wh p_n\)&lt;/span&gt; is calculated from the KME at a random (not deterministic) time.&lt;/p&gt;
&lt;p&gt;When a parametric mixture model such as the Weibull is fitted, a rigorous test for &lt;span class=&#34;math inline&#34;&gt;\(H_0: p=1\)&lt;/span&gt; (no immunes present) is available
(see Section 5.3, p.109, of ), and a nonparametric test using &lt;span class=&#34;math inline&#34;&gt;\(\wh p_n\)&lt;/span&gt; is outlined in Section 4.2, p.76, p.109, of (with percentage points in Table A.1 of the book),
but we still do not have complete understanding of the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\wh p_n\)&lt;/span&gt; under the null hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;% An ``improper’’ sample KME
% is suggestive but not definitive evidence of the presence of cured individuals in the population.
%Even in the absence of cures, it’s possible for the
%right extreme of the KME to be less than 1 just by chance; calculate this probability under the assumption of iid censoring.
% (for which, see the next section).
%So we need rigorous tests for whether the right extreme of the KME is significantly less than 1, and
%how this is related to the length of the level stretch at the righthand end of the KME.&lt;/p&gt;
&lt;p&gt;The KME contains further evidence about the existence of a cured component.
We see in Fig. &lt;span class=&#34;math inline&#34;&gt;\(\ref{uncensKMEBoag}\)&lt;/span&gt; a tendency for the KME to
remain constant at lifetimes greater than 90 months, except for one late death at
120.6 months. The length of the level stretch at the righthand end of the KME is indicative of the amount of followup in the data.
A statistic &lt;span class=&#34;math inline&#34;&gt;\(Q_n\)&lt;/span&gt; is suggested in for assessing ``sufficient followup’’.&lt;/p&gt;
%
% The KME in Fig. &lt;span class=&#34;math inline&#34;&gt;\(\ref{uncensKMEBoag}\)&lt;/span&gt; is very typical of the kind that can be seen in much of the medical literature; see, e.g., .
% Other useful information in the sample is the KME of the censoring distribution; Fig.&lt;span class=&#34;math inline&#34;&gt;\(\ref{censKMEBoag}\)&lt;/span&gt; shows this for Boag’s data with a Weibull distribution fitted.
% We discuss Boag’s data and the Lui et al. analysis
% further in Section &lt;span class=&#34;math inline&#34;&gt;\(\ref{??}\)&lt;/span&gt;.
%
% \begin{figure}[h]
%
%
% \begin{subfigure}{5cm}
%
%
% \end{subfigure}
%
% \begin{subfigure}{5cm}
%
%
&lt;p&gt;%\end{subfigure}
%\end{figure}&lt;/p&gt;
These ideas are related to the magnitudes of the largest survival time observed,
and the largest {} survival time observed, and the numbers of observations in the two time intervals defined by these.
%Much of the methodology is set out in , which can be read as background to the present paper.
%But some issues are left unresolved in that book, which we address in the present paper, along with some newly derived results.&lt;br /&gt;
%%
&lt;p&gt;A key structural result obtained in
is that, conditional on the value of the largest uncensored survival time, and knowing the number of censored observations exceeding this time,
the sample partitions into two independent subsamples, each subsample having
the distribution of an iid sample of censored survival times, of reduced size, from truncated random variables.
This result provides valuable insight and intuition into the construction of samples of censored survival data,
and facilitates the calculation of explicit finite sample formulae, for example, for
the joint distribution of the largest and the largest {} survival time observed, and for &lt;span class=&#34;math inline&#34;&gt;\(Q_n\)&lt;/span&gt;.
Further, the asymptotic distributions of these statistics can then be worked out under conditions related to those familiar from extreme value theory.
Our recent research is very much in this line.
See
(adjusting for insufficient follow-up),
(extremes of censored and uncensored lifetimes),
(splitting the sample at the largest uncensored observation,
testing for sufficient followup, estimating the probability of being cured).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bullet\)&lt;/span&gt;
It’s very common in survival analysis to encounter a KME which has levelled off at a value less than 1. This may indicate the presence of immune or cured individuals in the population — but not always — even in the absence of cures, it’s possible for the
right extreme of the KME to be less than 1 just by chance.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bullet\)&lt;/span&gt; A significance test is available for the hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H_0: p=1\)&lt;/span&gt; when a well-fitting parametric model has been found for the data.
A wide variety of models can be fitted routinely with R (); see
, and .
These cover a class of generalised F models and, as a submodel, an extended generalised gamma model, which between them include as submodels most of the usual survival distributions such as the exponential, Weibull, lognormal, Gumbel,
log-logistic, Burr, etc.
Methods of distinguishing between them in a data analysis with the cure model are set out in .&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bullet\)&lt;/span&gt;&lt;br /&gt;
A nonparametric test for &lt;span class=&#34;math inline&#34;&gt;\(H_0: p=1\)&lt;/span&gt; is available too, but at present we have to rely on simulated, tabulated, percentage points for the distribution.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bullet\)&lt;/span&gt;
An important point is whether the KME has levelled off {} at its right endpoint.
The &lt;span class=&#34;math inline&#34;&gt;\(Q_n\)&lt;/span&gt; statistic has been developed to measure and test for this.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bullet\)&lt;/span&gt;
We’ve confined our discussion to the one-sample case.
In practice, we usually have one or more groups (treatment groups, or otherwise), and/or covariates, and want to examine the effects of these.
Much of is concerned with methods for handling this.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bullet\)&lt;/span&gt;
We’ve also confined our discussion to medical data and survival analysis.
But the methodology applies to many other kinds of time-to-event data.
A wide variety of examples can be found in a web search.
use much criminological data (time to re-arrest of a released prisoner, etc.) to illustrate the methods.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bullet\)&lt;/span&gt;
Ignoring the possible presence of cured, immune or long-term survivors in a population not only risks losing valuable information but can result in bias and misleading conclusions.
An important point is that including the possibility of long-term survivors in {} survival analysis can do no damage;
if their presence is allowed for but found not to be significant, no harm is done.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; $\bullet$
 The mixture cure model can be regarded as a special case of a competing risks analysis where death or failure of an individual  may be due to a number of possible causes; see \cite{MZ2002}.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The issue of sufficient followup is clearly relevant in this context, but has not been addressed at all, to our knowledge.&lt;/p&gt;
&lt;p&gt;\end{document}&lt;/p&gt;
%%
&lt;p&gt;%The Kaplan-Meier empirical distribution function estimator (KME) of the survival distribution of the individuals is defined as&lt;/p&gt;
&lt;p&gt;The right extremes of the survival and censoring distributions
play a special role in our analysis.
Let $_{F^&lt;em&gt;}= {t&amp;gt;0:F^&lt;/em&gt;(t)=1} $
%&lt;span class=&#34;math inline&#34;&gt;\(\tau_{F}= \inf\{t&amp;gt;0:F(t)=1\} = \inf\{t&amp;gt;0:\)&lt;/span&gt;F^*(t)=p} $
(with the inf of the empty set equal to &lt;span class=&#34;math inline&#34;&gt;\(\infty\)&lt;/span&gt;) be the right extreme of the survival distribution &lt;span class=&#34;math inline&#34;&gt;\(F^*\)&lt;/span&gt;,
and similarly &lt;span class=&#34;math inline&#34;&gt;\(\tau_{F}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\tau_{G}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tau_H\)&lt;/span&gt;
%&lt;span class=&#34;math inline&#34;&gt;\(\tau_{F}= \inf\{u&amp;gt;0:F(u)=1\}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tau_G= \inf\{u&amp;gt;0:G(u)=1\}\)&lt;/span&gt;&lt;br /&gt;
are the right extremes of &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt;.
The quantity &lt;span class=&#34;math inline&#34;&gt;\(\tau_{F^*}\)&lt;/span&gt; represents the largest survival time of an individual in the population,
but in a sample we can only observe times up to a maximum of &lt;span class=&#34;math inline&#34;&gt;\(\tau_H: =\min(\tau_{F^*}, \tau_G)\)&lt;/span&gt;, due to the censoring.
We always have &lt;span class=&#34;math inline&#34;&gt;\(H(\tau_H)=1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(G(\tau_G)=1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(F(\tau_{F})=1\)&lt;/span&gt;.
When &lt;span class=&#34;math inline&#34;&gt;\(p=1\)&lt;/span&gt;, so that &lt;span class=&#34;math inline&#34;&gt;\(F^*\equiv F\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(F^*\)&lt;/span&gt; has total mass 1 and &lt;span class=&#34;math inline&#34;&gt;\(\tau_{F^*}=\tau_{F}\)&lt;/span&gt;;
when &lt;span class=&#34;math inline&#34;&gt;\(p&amp;lt;1\)&lt;/span&gt; we have &lt;span class=&#34;math inline&#34;&gt;\(\tau_{F^*}=\infty\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\tau_{F}\le \tau_{F^*}\)&lt;/span&gt;,
with the possibility that &lt;span class=&#34;math inline&#34;&gt;\(\tau_{F}&amp;lt; \tau_{F^*}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The relation &lt;span class=&#34;math inline&#34;&gt;\(\tau_F\le \tau_G\)&lt;/span&gt; expresses that there is enough followup to allow the largest possible susceptible survival times to be observed; in the contrary situation, &lt;span class=&#34;math inline&#34;&gt;\(\tau_G&amp;lt;\tau_F\)&lt;/span&gt;, censoring is so heavy that the data is truncated at a level below the maximum possible survival time.
Besides expressing that followup is ``sufficient’’ in this sense,
the condition &lt;span class=&#34;math inline&#34;&gt;\(\tau_F\le \tau_G\)&lt;/span&gt; arises in a number of other theoretical results.
For example, the KME is biased downwards, but the bias is asymptotically negligible (tends to 0 in large samples) if and only if &lt;span class=&#34;math inline&#34;&gt;\(\tau_F\le \tau_G\)&lt;/span&gt;
(provided &lt;span class=&#34;math inline&#34;&gt;\(F^*\)&lt;/span&gt; is continuous at &lt;span class=&#34;math inline&#34;&gt;\(\tau_H\)&lt;/span&gt; in case &lt;span class=&#34;math inline&#34;&gt;\(\tau_H&amp;lt;\infty\)&lt;/span&gt;; see , Theorem 3.13).
Under this same continuity condition, &lt;span class=&#34;math inline&#34;&gt;\(\tau_F\le \tau_G\)&lt;/span&gt;
is necessary and sufficient condition for the KME &lt;span class=&#34;math inline&#34;&gt;\(\wh F_n\)&lt;/span&gt; to be consistent for &lt;span class=&#34;math inline&#34;&gt;\(F^*\)&lt;/span&gt; on the whole line; , Thms. 3.8 and 4.2.&lt;/p&gt;
&lt;p&gt;Convergence of the integral
&lt;em&gt;{{0&amp;lt;t&amp;lt; &lt;/em&gt;{H}}}
is a required assumption in Theorem 4.2.3 of giving the asymptotic distribution of the KME, and in Theorem 4.3 of
.
The ``sufficient followup’’ condition &lt;span class=&#34;math inline&#34;&gt;\(\tau_F\le \tau_G\)&lt;/span&gt; is necessary for the convergence of the integral in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{intg}\)&lt;/span&gt; and consequently plays an important role in many of the large-sample results in and in the literature.&lt;/p&gt;
&lt;p&gt;% and , Theorem 4.3 as…&lt;/p&gt;
&lt;p&gt;This discussion highlights the need for information on, or assumptions about,
the right hand endpoints of &lt;span class=&#34;math inline&#34;&gt;\(F^*\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;, and, especially, whether they are finite or not.
Most practical is to assume &lt;span class=&#34;math inline&#34;&gt;\(\tau_G&amp;lt;\infty\)&lt;/span&gt; since observation must always cease at some finite point.
In many cases the assumption &lt;span class=&#34;math inline&#34;&gt;\(\tau_{F}&amp;lt;\infty\)&lt;/span&gt; may also be natural.
Certainly in real survival data no individual lives forever, but we would set &lt;span class=&#34;math inline&#34;&gt;\(\tau_{F}=\infty\)&lt;/span&gt; for example when studying the occurrence of an infectious disease where an immune individual would never contract the disease no matter how long the follow-up. This can certainly be the case in epidemics such as the COVID virus pandemic, for example; and in an analysis with children immune to malaria is given.&lt;/p&gt;
&lt;p&gt;Regardless of the situation, in modelling exercises it is not uncommon to use a distribution
with infinite right endpoint as the lifetime distribution; e.g., an exponential, Weibull, lognormal, or Gumbel, or the generalised gamma distribution fitted in Section &lt;span class=&#34;math inline&#34;&gt;\(\ref{DA}\)&lt;/span&gt;.
In doing so we accept that the probability of seeing an extremely long lifetime under the assumed model is negligible, so the theoretical approximation is good enough for practical purposes.
Alternatively, we could truncate the survival distribution at a (large) finite value, thus creating a distribution with &lt;span class=&#34;math inline&#34;&gt;\(\tau_F&amp;lt;\infty\)&lt;/span&gt;, as is often done in simulations; but then in practice there arises the question of where the truncation should be.
We do not address this issue here.&lt;/p&gt;
&lt;p&gt;%Theorem 3.2 in Maller and Zhou (1996) proposes a consistent estimator for &lt;span class=&#34;math inline&#34;&gt;\(\tau_H\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;We discuss the issue of the finiteness or otherwise of the right extremes further in Section &lt;span class=&#34;math inline&#34;&gt;\(\ref{??}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In this section we review some of the methodological advances that have been made in recent years and how they can be used to calculate distributions of statistics we are interested in.
Calculation of exact distributions (under the iid censoring model) allows for a rigorous investigation
of their properties and makes unnecessary the need for simulations of percentage points.
%, though in practice the unknown distributions
%(or their parameters) implicit in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{Qdis}\)&lt;/span&gt;
%–&lt;span class=&#34;math inline&#34;&gt;\(\eqref{Qdis4}\)&lt;/span&gt; %%and &lt;span class=&#34;math inline&#34;&gt;\(\eqref{12a}\)&lt;/span&gt;
%must be estimated or postulated.&lt;/p&gt;
&lt;p&gt;A key structural result obtained in
is that, conditional on the value of the largest uncensored survival time, and knowing the number of censored observations exceeding this time,
the sample partitions into two independent subsamples, each subsample having
the distribution of an iid sample of censored survival times, of reduced size, from truncated random variables.
This result provides valuable insight and intuition into the construction of samples of censored survival data,
and facilitates the calculation of explicit finite sample formulae.&lt;/p&gt;
&lt;p&gt;%
%To state the splitting result, we adopt the convention that for a
%non-negative random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and Borel set &lt;span class=&#34;math inline&#34;&gt;\(B\subset %\R_+\)&lt;/span&gt;
% with &lt;span class=&#34;math inline&#34;&gt;\(P(B)&amp;gt;0\)&lt;/span&gt;,
% $(X|XB) $ is a random variable with distribution
%&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}\label{e: conditDistn}
%  P(X\in A|X\in B)=P(X\in A\cap B)/P(X\in B),
%  \quad A\subset \R_+,\,A
%\text{ Borel}.\end{equation}

Recall the notation in Section \ref{nii}.
The splitting theorem tells us that
the sample $S_n:=\{T_i,1\le i\le n\}$  partitions into disjoint sets as follows:
\be \label{e:partition}
  S_n
  %=\{T_i,1\le i\le n\}=
%  &amp;amp;\{M_u(n) \}\cup  \{T_i:i\leq n\; \&amp;amp;\;
%                           T_i&amp;lt;M_u(n)\}\nonumber\\
%&amp;amp;  \qquad \qquad
%  \cup
%                           \{T_i:i\leq n \;\&amp;amp; \; T_i&amp;gt;M_u(n)\}\nonumber\\
  =S_n^&amp;lt; \cup \{M_u(n)\} \cup S_n^&amp;gt;
\ee
(we keep $n\ge 3$),
where the component sets are
\be\label{none}
S_n^&amp;lt; :=  \{T_i:i\leq n\ {\rm and}\  T_i&amp;lt;M_u(n)\}
\ee
and
\be\label{non}
S_n^&amp;gt; :=   \{T_i:i\leq n\ {\rm and}\   T_i&amp;gt;M_u(n)\}.
\ee
They have the following properties.
On $\{M_u(n)&amp;gt;0\}$, let
\be\label{Nd1}
N_c^&amp;gt;(M_u(n)):= \{{\rm number\ of\ censored\ observations\ exceeding}\  M_u(n)\},
\ee
%so that  
%\ben
%\{N_c^&amp;gt;(M_u(n))=r\}= \{C_{(n-r)}=1, C_{(n-r+1)} =\cdots=C_{(n)}=0\}, \ 1\le  r\le n-1.
%\een
where by convention we set
\ben %\label{Nd2}
\{N_c^&amp;gt;(M_u(n))=0\}
=\{M_u(n)= M(n)\}
=
\{{\rm largest\ observation\ uncensored} \}
%\{C_{(n)}=1\},
\een
and
\ben %\label{Nd3}
\{N_c^&amp;gt;(M_u(n))=n\}=
\{{\rm all}\ n\ {\rm observations\ censored} \}.
% = \{C_1=\cdots=C_n=0\}.
\een
%On $\{N_c^&amp;gt;(M_u(n))=0\}$ we set $M_u(n)=M(n)$, and
On $\{N_c^&amp;gt;(M_u(n))=n\}$ set $M_u(n)=0$.
%Note that $M_u(n)=T_{n- N_c^&amp;gt;(M_u(n))}$, so  $n- N_c^&amp;gt;(M_u(n))$ is the index of the largest uncensored observation among the ordered observations.
Then,  conditional on knowing that $ M_u(n)=t&amp;gt;0$ and  $N_c^&amp;gt;(M_u(n))=r$,
%%$r$  censored  observations exceed $ M_u(n)$, $0\le r\le n-1$,
the set $S_n^&amp;lt;$ consists of $n-r-1$ iid variables with distribution that
of $T_1$,  conditional on $T_1&amp;lt;t$;
     and the set $S_n^&amp;gt;$ consists of $r$ iid variables with tail function
     \begin{equation}\label{e:censort_tail}
P(T_1^{&amp;gt;,c}(t) &amp;gt;x):=    \frac{\int_x^\infty \Fbar^*(s) G(ds)}{\int_t^\infty \Fbar^*(s)
       G(ds)},\quad x\geq t,\end{equation}\]&lt;/span&gt;
which is the distribution tail of a censored observation conditional on being bigger than &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;.
%% (See Eq. (2.13) in .)
Furthermore, &lt;span class=&#34;math inline&#34;&gt;\(S_n^&amp;lt;\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(S_n^&amp;gt;\)&lt;/span&gt; are conditionally independent given
$ M_u(n)=t$ and &lt;span class=&#34;math inline&#34;&gt;\(N_c^&amp;gt;(M_u(n))=r\)&lt;/span&gt;.
Note that observed lifetimes less
than &lt;span class=&#34;math inline&#34;&gt;\(M_u(n)\)&lt;/span&gt; may be either censored or uncensored but observed
lifetimes greater than &lt;span class=&#34;math inline&#34;&gt;\(M_u(n)\)&lt;/span&gt; are necessarily censored.&lt;/p&gt;
% is contained in the next theorem and proved in Section &lt;span class=&#34;math inline&#34;&gt;\(\ref{pfs}\)&lt;/span&gt;. Theorem &lt;span class=&#34;math inline&#34;&gt;\(\ref{th1}\)&lt;/span&gt; also contains the distribution in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{j0}\)&lt;/span&gt; needed for later calculations.
%Let $_{F}= {t&amp;gt;0:F(t)&amp;lt;1} $ be the right endpoint of the support of the cdf &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;, and similarly define &lt;span class=&#34;math inline&#34;&gt;\(\tau_G\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\tau_H\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tau_{F^*}\)&lt;/span&gt;.&lt;br /&gt;
%
%
&lt;p&gt;&lt;br /&gt;
(i) 
%Note that &lt;span class=&#34;math inline&#34;&gt;\(\eqref{JD}\)&lt;/span&gt; is consistent with the fact that &lt;span class=&#34;math inline&#34;&gt;\(0\le M_u(n)\le M(n)\le \tau_H\)&lt;/span&gt;, always,
%and that
There is no probability mass outside the region &lt;span class=&#34;math inline&#34;&gt;\([0,\tau_H]\times [0,\tau_H]\)&lt;/span&gt; so the distribution in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{JD}\)&lt;/span&gt; equals 1 for
$ t&amp;gt;_H$, &lt;span class=&#34;math inline&#34;&gt;\(x&amp;gt;\tau_H\)&lt;/span&gt;.
Likewise the distribution in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{4g}\)&lt;/span&gt; equals 1 for
$ t&amp;gt;_H$.&lt;/p&gt;
&lt;p&gt;Note also that Lines 2 and 3
on the RHS of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{JD}\)&lt;/span&gt; include the value for &lt;span class=&#34;math inline&#34;&gt;\(t=0\)&lt;/span&gt;;
there is mass on the interval &lt;span class=&#34;math inline&#34;&gt;\(\{t=0\}\times [0\le x\le \tau_H]\)&lt;/span&gt;, as given by the first line on the RHS of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{JD}\)&lt;/span&gt;.
%Illustrative plots of the distributions of &lt;span class=&#34;math inline&#34;&gt;\(M(n)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(M_u(n)\)&lt;/span&gt;
%are in (supplementary to ).&lt;/p&gt;
&lt;p&gt;(ii) 
$M_u(n) $ has the distribution of the maximum of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; iid copies of a rv with distribution &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\([0,\infty)\)&lt;/span&gt;.
The distribution has mass &lt;span class=&#34;math inline&#34;&gt;\(\big(\int_{z=0}^{\tau_H} \Fbar^* (z)\rmd G(z)\big)^n\)&lt;/span&gt; at 0 corresponding to all observations being censored.
%(It may seem pedantic to include these degenerate cases but they are important for checking that distributions are proper (have total mass 1).)&lt;/p&gt;
&lt;p&gt;The right extreme &lt;span class=&#34;math inline&#34;&gt;\(\tau_J\)&lt;/span&gt; of the distribution &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; may be strictly less than &lt;span class=&#34;math inline&#34;&gt;\(\tau_G\)&lt;/span&gt;; in fact, we have &lt;span class=&#34;math inline&#34;&gt;\(\tau_J=\tau_{F}\wedge \tau_G\)&lt;/span&gt;.
%, as is derived in the proof of Theorem &lt;span class=&#34;math inline&#34;&gt;\(\ref{th4a}\)&lt;/span&gt;.
No uncensored observation, including the sample maximum of the uncensored observations, can exceed the smaller of &lt;span class=&#34;math inline&#34;&gt;\(\tau_{F}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tau_G\)&lt;/span&gt;.
% consistent with the fact that &lt;span class=&#34;math inline&#34;&gt;\(\tau_J=\tau_{F}\wedge \tau_G\)&lt;/span&gt;.
Note that, in general, &lt;span class=&#34;math inline&#34;&gt;\(\tau_J\ne \tau_H=\tau_{F^*}\wedge \tau_G\)&lt;/span&gt;.
We always have &lt;span class=&#34;math inline&#34;&gt;\(H(\tau_H)=1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(G(\tau_G)=1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(F(\tau_{F})=1\)&lt;/span&gt;;
when &lt;span class=&#34;math inline&#34;&gt;\(p=1\)&lt;/span&gt;, so that &lt;span class=&#34;math inline&#34;&gt;\(F^*\equiv F\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(F^*\)&lt;/span&gt; has total mass 1 and &lt;span class=&#34;math inline&#34;&gt;\(\tau_{F^*}=\tau_{F}\)&lt;/span&gt;;
when &lt;span class=&#34;math inline&#34;&gt;\(p&amp;lt;1\)&lt;/span&gt; we have &lt;span class=&#34;math inline&#34;&gt;\(\tau_{F^*}=\infty\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\tau_{F}\le \tau_{F^*}\)&lt;/span&gt;,
with the possibility that &lt;span class=&#34;math inline&#34;&gt;\(\tau_{F}&amp;lt; \tau_{F^*}\)&lt;/span&gt;.
%See Figure &lt;span class=&#34;math inline&#34;&gt;\(\ref{fig:2}\)&lt;/span&gt; %%and &lt;span class=&#34;math inline&#34;&gt;\(\ref{fig:7}\)&lt;/span&gt;
%for plots of the distribution of &lt;span class=&#34;math inline&#34;&gt;\(M_u(n)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;(iii)  %&lt;span class=&#34;math inline&#34;&gt;\(\eqref{4h}\)&lt;/span&gt; is correct of course since
&lt;span class=&#34;math inline&#34;&gt;\(M(n)\)&lt;/span&gt; has the distribution of the maximum of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; iid copies of a rv with distribution &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\([0,\tau_H]\)&lt;/span&gt;; namely,
P(M(n)x)=H^n(x), 0x_H.
%
%(iv) The conditional distributions of &lt;span class=&#34;math inline&#34;&gt;\(M(n)\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(M_u(n)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N_c^&amp;gt;(M_u(n))\)&lt;/span&gt;, and of &lt;span class=&#34;math inline&#34;&gt;\(M(n)\)&lt;/span&gt; given
%&lt;span class=&#34;math inline&#34;&gt;\(M_u(n)\)&lt;/span&gt;, are stated in Section &lt;span class=&#34;math inline&#34;&gt;\(\ref{pfs}\)&lt;/span&gt; following the proof of Theorem &lt;span class=&#34;math inline&#34;&gt;\(\ref{th1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;%Since &lt;span class=&#34;math inline&#34;&gt;\(M(n)\)&lt;/span&gt; is the maximum of the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; i.i.d rvs &lt;span class=&#34;math inline&#34;&gt;\((T_i)_{1\le i \le n}\)&lt;/span&gt; with distribution &lt;span class=&#34;math inline&#34;&gt;\(H(t)\)&lt;/span&gt;, we know immediately that
% &lt;span class=&#34;math inline&#34;&gt;\(P(M(n)\le t)= H^n(t)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(t&amp;gt;0\)&lt;/span&gt;.
Also important are the length of the time interval between the largest uncensored survival time and the largest survival time, and the ratio of those times. For them
we have the following distributions.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
(i)  
Setting &lt;span class=&#34;math inline&#34;&gt;\(u=0\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{12a}\)&lt;/span&gt; we see that the distribution of the difference $M(n) - M_u(n) $ has mass at 0 of
P( M(n) - M_u(n) =0 )=P( M(n) = M_u(n) )
=
n_{t=0}^{_H}H^{n-1}(t) (t) F&lt;sup&gt;&lt;em&gt;(t).
This is the probability that the largest observation is uncensored (, Eq. (3.21), p.49).
%while setting &lt;span class=&#34;math inline&#34;&gt;\(u=\tau_H\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{12a}\)&lt;/span&gt; and observing that
%
%H(t)= ^&lt;/em&gt;(t)G(t)+(t)F&lt;/sup&gt;*(t),
%
%we can check that the total mass is 1 by doing the integration
%
%&amp;amp;&amp;amp;
%n_{t=0}&lt;sup&gt;{&lt;em&gt;H}
% (&lt;/em&gt;{z=t}&lt;/sup&gt;{&lt;em&gt;H} ^&lt;em&gt;(z) G(z) +H(t))^{n-1} (t) F^&lt;/em&gt;(t)
%&amp;amp;&amp;amp;
%=
% (&lt;/em&gt;{z=t}^{_H} ^*(z) G(z) +H(t))^{n} |_{t=0}^{_H}
%=
%H&lt;sup&gt;n(&lt;em&gt;H) - (&lt;/em&gt;{z=0}&lt;/sup&gt;{_H} ^*(z) G(z))^n
%
%&amp;amp;&amp;amp;=
%1- (_{z=0}^{_H} ^*(z) G(z))^n.
%
%Taking &lt;span class=&#34;math inline&#34;&gt;\(u=\tau_H\)&lt;/span&gt; in the second term on the RHS of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{12a}\)&lt;/span&gt;
%and adding this to the RHS of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{12d}\)&lt;/span&gt; we get 1.
See %Figures &lt;span class=&#34;math inline&#34;&gt;\(\ref{fig:4}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\ref{fig:5}\)&lt;/span&gt;&lt;br /&gt;
for illustrative plots of the distributions in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{12a}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eqref{12b}\)&lt;/span&gt;
(supplementary to ).
%of &lt;span class=&#34;math inline&#34;&gt;\(M(n)-M_u(n)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(M_u(n)/M(n)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;%(ii) 
%The denominator in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{12b}\)&lt;/span&gt; multiplied by &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;
%is the expression on the LHS of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{12d}\)&lt;/span&gt; and equal to &lt;span class=&#34;math inline&#34;&gt;\(P(M_u(n)&amp;gt;0)\)&lt;/span&gt; as can be seen from &lt;span class=&#34;math inline&#34;&gt;\(\eqref{4g}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;%In Maller and Zhou (1996) and etc. we calculated distributions of various quantities of interest related to the above rvs, and, also, for example, the probability that the largest observation is censored,
%&lt;span class=&#34;math inline&#34;&gt;\(P(C_{(n)}=1)\)&lt;/span&gt;.
%Under some assumptions we found their limits as &lt;span class=&#34;math inline&#34;&gt;\(n\to\infty\)&lt;/span&gt;, and how these depend on the interplay between &lt;span class=&#34;math inline&#34;&gt;\(F^*\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; and their tails, &lt;span class=&#34;math inline&#34;&gt;\(\Fbar^*\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Gbar\)&lt;/span&gt;.&lt;br /&gt;
%In the present paper we concentrate on &lt;span class=&#34;math inline&#34;&gt;\(M(n)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(M_u(n)\)&lt;/span&gt;, and the difference between them.&lt;/p&gt;
&lt;p&gt;%%%% 
%\begin{example}
%\end{example}&lt;/p&gt;
&lt;p&gt;%The results in Subsection &lt;span class=&#34;math inline&#34;&gt;\(\ref{ssT}\)&lt;/span&gt; are obtained in Section &lt;span class=&#34;math inline&#34;&gt;\(\ref{pfs}\)&lt;/span&gt; as special cases of the formulae for the joint distributions of &lt;span class=&#34;math inline&#34;&gt;\(M(n)\)&lt;/span&gt;, $M_u(n) $ and &lt;span class=&#34;math inline&#34;&gt;\(N_c^&amp;gt;(M_u(n))\)&lt;/span&gt; which we derive there.&lt;/p&gt;
&lt;p&gt;Besides the definition of &lt;span class=&#34;math inline&#34;&gt;\(N_c^&amp;gt;(M_u(n))\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{Nd1}\)&lt;/span&gt;,
we need notation for the numbers of censored observations
smaller or greater than &lt;span class=&#34;math inline&#34;&gt;\(M_u(n)\)&lt;/span&gt;.
%%, the largest uncensored survival time in the sample.
Let
N_u(n)
:=
{{} },
and when &lt;span class=&#34;math inline&#34;&gt;\(N_u(n)&amp;gt;1\)&lt;/span&gt;, define
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}\label{Nd4}
&amp;amp;
N_u^&amp;lt;(M_u(n))\cr
&amp;amp;:=
\{{\rm number\ of\ uncensored\ observations\ strictly \ less\ than}\  M_u(n)\}
\end{align}\]&lt;/span&gt;
and
N_c^&amp;lt;(M_u(n)):= {{}&lt;br /&gt;
M_u(n)}.
On &lt;span class=&#34;math inline&#34;&gt;\(\{N_u(n)=1\}\)&lt;/span&gt;, set &lt;span class=&#34;math inline&#34;&gt;\(N_u^&amp;lt;(M_u(n))=N_c^&amp;lt;(M_u(n))=0\)&lt;/span&gt;.
When &lt;span class=&#34;math inline&#34;&gt;\(N_u(n)=0\)&lt;/span&gt;, we do not define
&lt;span class=&#34;math inline&#34;&gt;\(N_u^&amp;lt;(M_u(n))\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(N_c^&amp;lt;(M_u(n))\)&lt;/span&gt;.
% (and of course there’s no need for a notation like &lt;span class=&#34;math inline&#34;&gt;\(N_u^&amp;gt;(M_u(n))\)&lt;/span&gt;
%since such a number would always be 0.)
Let
N_c(n):
=
{{} }.
We also use the notation &lt;span class=&#34;math inline&#34;&gt;\(\NN_n:=\{1,2,\ldots,n \}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(n=1,2,\ldots,\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;With these definitions and conventions,
on &lt;span class=&#34;math inline&#34;&gt;\(\{N_u(n)\ge 1\}\)&lt;/span&gt; the
&lt;span class=&#34;math inline&#34;&gt;\(N_u^&amp;lt;(M_u(n))\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(N_c^&amp;lt;(M_u(n))\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N_c^&amp;gt;(M_u(n))\)&lt;/span&gt;
take values in
%
&lt;span class=&#34;math inline&#34;&gt;\(\N_{n-1}\cup\{0\}\)&lt;/span&gt;,
satisfying
&lt;span class=&#34;math inline&#34;&gt;\(N_u^&amp;lt;(M_u(n))+N_c^&amp;lt;(M_u(n))+ N_c^&amp;gt;(M_u(n))=n-1\)&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(N_c(n)= N_c^&amp;gt;(M_u(n))+N_c^&amp;lt;(M_u(n))\)&lt;/span&gt;,
and we have
{N_u(n)=0}=
{{} n {} } =
{M_u(n)=0}.
%
%That analysis can be expanded to obtain more generally the
%joint distribution of &lt;span class=&#34;math inline&#34;&gt;\(M(n)\)&lt;/span&gt;, $M_u(n) $, $ N_c^&amp;gt;(M_u(n))$ and &lt;span class=&#34;math inline&#34;&gt;\(N_c^&amp;lt;(M_u(n))\)&lt;/span&gt;
%(and then $ N_u^&amp;lt;(M_u(n))=n-1 -N_c&lt;sup&gt;&amp;gt;(M_u(n))-N_c&lt;/sup&gt;&amp;lt;(M_u(n))&lt;span class=&#34;math inline&#34;&gt;\(). %This allows derivation of the joint distribution of %\)&lt;/span&gt;N_c^&amp;gt;(M_u(n))$, &lt;span class=&#34;math inline&#34;&gt;\(N_c^&amp;lt;(M_u(n))\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N_u^&amp;lt;(M_u(n))\)&lt;/span&gt;, variables which are also useful in addressing questions of sufficient follow-up.
%%(We could, for example, analyse the number of censored observations exceeding &lt;span class=&#34;math inline&#34;&gt;\(M_u(n)\)&lt;/span&gt;, as a proportion of the total number.)
%
%We omit the details of this more general analysis here, %
%but give a main&lt;/p&gt;
&lt;p&gt;The next result concerns
the vector &lt;span class=&#34;math inline&#34;&gt;\((N_c^&amp;gt;(M_u(n)), N_c^&amp;lt;(M_u(n)), N_u^&amp;lt;(M_u(n)))\)&lt;/span&gt;.
This vector is not as might be thought at first multinomially distributed, but it is, conditional on the value of &lt;span class=&#34;math inline&#34;&gt;\(M_u(n)\)&lt;/span&gt;.
We prove it as another application of the splitting property, again illustrating
the simplicity of exposition gained by conditioning on &lt;span class=&#34;math inline&#34;&gt;\(M_u(n)\)&lt;/span&gt;.
We need some more notation.
Define the functions
p_c^&amp;gt;(t)
&amp;amp;=&amp;amp;
{ _{y=t}&lt;sup&gt;{&lt;em&gt;H} ^* (y)G(y)+H(t)},
p_c^&amp;lt;(t)
&amp;amp;=&amp;amp;
{ &lt;/em&gt;{y=t}&lt;/sup&gt;{&lt;em&gt;H} ^* (y)G(y)+H(t)},
p_u^&amp;lt;(t)
&amp;amp;=&amp;amp;
{ &lt;/em&gt;{y=t}^{_H} ^* (y)G(y)+H(t)},
which are non-negative and %%(using &lt;span class=&#34;math inline&#34;&gt;\(\eqref{F^*GH}\)&lt;/span&gt;)
add to 1 for each &lt;span class=&#34;math inline&#34;&gt;\(t\in (0,\tau_H)\)&lt;/span&gt;.
%The integrals in the numerators of &lt;span class=&#34;math inline&#34;&gt;\(p_u^&amp;lt;(t)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p_c^&amp;lt;(t)\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{pdefs}\)&lt;/span&gt; are sub-distribution functions related to censored and uncensored survival times, as will be seen in Subsection &lt;span class=&#34;math inline&#34;&gt;\(\ref{s4}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;{&lt;em&gt;H}
% (&lt;/em&gt;{z=t}^{_H} ^&lt;em&gt;(z) G(z) +H(t))^{n-1} (t) F^&lt;/em&gt;(t)}.
%
\end{theorem}&lt;/p&gt;
&lt;p&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(t&amp;gt;0\)&lt;/span&gt; in Theorem &lt;span class=&#34;math inline&#34;&gt;\(\ref{th2}\)&lt;/span&gt;, the conditioning on &lt;span class=&#34;math inline&#34;&gt;\(M_u(n)=t\)&lt;/span&gt; implies
&lt;span class=&#34;math inline&#34;&gt;\(M_u(n)&amp;gt;0\)&lt;/span&gt;, thus &lt;span class=&#34;math inline&#34;&gt;\(N_u(n)\ge 1\)&lt;/span&gt;, and there is at least one uncensored observation.
Thus &lt;span class=&#34;math inline&#34;&gt;\(N_u^&amp;lt;(M_u(n))+N_c^&amp;lt;(M_u(n))+N_c^&amp;gt;(M_u(n))=n-1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;An immediate application of Theorem &lt;span class=&#34;math inline&#34;&gt;\(\ref{th2}\)&lt;/span&gt; is to derive the finite sample distribution of the statistic &lt;span class=&#34;math inline&#34;&gt;\(Q_n\)&lt;/span&gt;, used as a test for sufficient followup.
We exhibit this in Section &lt;span class=&#34;math inline&#34;&gt;\(\ref{suff}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The KME was of course not available to Boag in 1949 and he used a parametric approach, inferring the existence of cures in his population from his sample estimate of the proportion cured
(&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;, in his notation, obtained from a lognormal mixture fit – see our Section &lt;span class=&#34;math inline&#34;&gt;\(\ref{intro}\)&lt;/span&gt;)
and its standard error.
(There is also an issue of the correctness of a confidence interval assessment which ignores the restriction of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt;. Section 5.3 of contains a discussion of this.)
The advent of the KME in 1958 was a major advance in the visualisation and analysis of survival data, and especially with reference to assessing the presence or otherwise of cures.
But how to measure, and more importantly, rigorously estimate and test, for
what we see in the KME?&lt;/p&gt;
&lt;p&gt;That the medical literature did and still does wrestle with this problem is illustrated for example by
the ``mini-review’’ paper of
, from which we quote:&lt;br /&gt;
%Analysis of Survival Curves: Statistical Methods Accounting for the Presence of Long-Term Survivors
%Vera Damuzzo 1, Laura Agnoletto 2, Luca Leonardi 3, Marco Chiumente 4, Daniele Mengato 5, Andrea Messori 6
%Affiliations expand
%PMID: 31231609 PMCID: PMC6558210 DOI: 10.3389/fonc.2019.00453
%Free PMC article
%Abstract
{}&lt;/p&gt;
&lt;p&gt;The problem of course is that with cures (possibly) present, the KME is improper and theoretical quantities such as the mean survival time or expected
``area-under-the-curve’’ are, formally, infinite, and distributions of their sample estimates will reflect this. (See the Appendix to Chapter 3 in for some discussion.)
A remedy is to restrict their calculation to the (proper) survival distribution of the susceptibles, but this then begs the question of how to estimate this.&lt;/p&gt;
&lt;p&gt;We can start by estimating the proportion of cured subjects in the population.
This proportion is the complement of the susceptible proportion, of which perhaps the simplest and most intuitive estimate is &lt;span class=&#34;math inline&#34;&gt;\(\wh p_n\)&lt;/span&gt;, the maximum value of the KME.
The properties of &lt;span class=&#34;math inline&#34;&gt;\(\wh p_n\)&lt;/span&gt; are explored extensively in ,
though there are still some unknown features; we discuss one such in Subsection &lt;span class=&#34;math inline&#34;&gt;\(\ref{LLC1}\)&lt;/span&gt;.
With an estimate of the proportion of cured subjects, we can rescale the KME or a fitted parametric distribution to estimate the survival distribution of the susceptibles.&lt;/p&gt;
&lt;p&gt;As foreshadowed in Section &lt;span class=&#34;math inline&#34;&gt;\(\ref{ext}\)&lt;/span&gt;, the sufficient followup condition &lt;span class=&#34;math inline&#34;&gt;\(\tau_F\le \tau_G\)&lt;/span&gt; plays an important role. In the present context, we know that
&lt;span class=&#34;math inline&#34;&gt;\(\wh p_n\)&lt;/span&gt; is consistent for &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; iff &lt;span class=&#34;math inline&#34;&gt;\(\tau_F\le \tau_G\)&lt;/span&gt; (Thm.~4.1 of ), assuming &lt;span class=&#34;math inline&#34;&gt;\(F^*\)&lt;/span&gt; is continuous at &lt;span class=&#34;math inline&#34;&gt;\(\tau_{_H}\)&lt;/span&gt; in case &lt;span class=&#34;math inline&#34;&gt;\(\tau_{_H}&amp;lt;\infty\)&lt;/span&gt;. This result holds for all &lt;span class=&#34;math inline&#34;&gt;\(0&amp;lt;p\le 1\)&lt;/span&gt;.
When &lt;span class=&#34;math inline&#34;&gt;\(0&amp;lt;p&amp;lt;1\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(\wh p_n\)&lt;/span&gt; is asymptotically normally distributed, as stated in the next section.&lt;/p&gt;
&lt;p&gt;%&lt;span class=&#34;math inline&#34;&gt;\(\wh F_n^*\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\wh F_n\)&lt;/span&gt; are consistent for &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; iff &lt;span class=&#34;math inline&#34;&gt;\(\tau_F\le \tau_G\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\([0,infty)\)&lt;/span&gt;
%(Thms. 3.8 and 4.2 of )&lt;/p&gt;
&lt;p&gt;Thm.~4.3 of gives a definitive result in the case &lt;span class=&#34;math inline&#34;&gt;\(p&amp;lt;1\)&lt;/span&gt;.
To state it we need some more notation.
Let &lt;span class=&#34;math inline&#34;&gt;\(Z(t)\)&lt;/span&gt; be a stochastic process on &lt;span class=&#34;math inline&#34;&gt;\([0, \tau_{_H})\)&lt;/span&gt; with independent increments such that for each &lt;span class=&#34;math inline&#34;&gt;\(t&amp;lt; \tau_{_H}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Z(t)\)&lt;/span&gt; is normally distributed with mean 0 and finite variance
&lt;span class=&#34;math inline&#34;&gt;\(v(t)\)&lt;/span&gt;, where
v(t) :=_{[0, t]} { dF&lt;sup&gt;&lt;em&gt;(s)((1-F^&lt;/em&gt;(s))&lt;/sup&gt;2 (1-G(s))}.
%%The function &lt;span class=&#34;math inline&#34;&gt;\(v(t)\)&lt;/span&gt; is finite and increasing on &lt;span class=&#34;math inline&#34;&gt;\([0,\tau_{H})\)&lt;/span&gt;.
We keep &lt;span class=&#34;math inline&#34;&gt;\(p&amp;lt;1\)&lt;/span&gt; throughout this subsection.
This means that &lt;span class=&#34;math inline&#34;&gt;\(\tau_{F^*}=\infty\)&lt;/span&gt; and consequently
&lt;span class=&#34;math inline&#34;&gt;\(\tau_H=\tau_{F^*}\wedge \tau_G= \tau_G\)&lt;/span&gt;.
%So we can replace &lt;span class=&#34;math inline&#34;&gt;\(\tau_H\)&lt;/span&gt; by $ &lt;em&gt;G$ in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{3.60}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eqref{3.61}\)&lt;/span&gt;
%and similar expressions.
Further, since &lt;span class=&#34;math inline&#34;&gt;\(F(t)\)&lt;/span&gt; attributes no mass to values &lt;span class=&#34;math inline&#34;&gt;\(t&amp;gt;\tau_F\)&lt;/span&gt;,
the integral in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{3.58a}\)&lt;/span&gt; can be written
v(t) =p&lt;/em&gt;{[0, t]} {dF(s)((1-pF(s))^2 (1-G(s))},  0t&amp;lt;_F.
Assuming in addition that the integral in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{intg}\)&lt;/span&gt; is finite,
we also have that &lt;span class=&#34;math inline&#34;&gt;\(v(\tau_G)\)&lt;/span&gt; is finite, and recall that this implies the sufficient followup
condition &lt;span class=&#34;math inline&#34;&gt;\(\tau_F\le \tau_G\)&lt;/span&gt;.
%the integral in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{3.61}\)&lt;/span&gt; need only run to &lt;span class=&#34;math inline&#34;&gt;\(\tau_F\)&lt;/span&gt;, and&lt;/p&gt;
&lt;p&gt;The next theorem, Theorem 4.3 of , is based on Theorem 4.2.3 of .&lt;/p&gt;
&lt;p&gt;To apply Theorem &lt;span class=&#34;math inline&#34;&gt;\(\ref{ASK}\)&lt;/span&gt; in practice, we need a sample estimate for the population quantity &lt;span class=&#34;math inline&#34;&gt;\(v(\tau_G)\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{asp&amp;lt;1}\)&lt;/span&gt;.
We can obtain this from Thm.~4.4 and the discussion following it
in .
A consistent estimator of &lt;span class=&#34;math inline&#34;&gt;\(v(t)\)&lt;/span&gt; is, under our assumptions,
v_n(t):= &lt;em&gt;{i:t_i&amp;gt;t} ,  t,
and correspondingly a
consistent estimator of &lt;span class=&#34;math inline&#34;&gt;\(v(\tau_G)\)&lt;/span&gt; is
v_n:= &lt;/em&gt;{i=1}^{n-1} .
The largest uncensored survival time &lt;span class=&#34;math inline&#34;&gt;\(M_u(n) \upto \tau_F\)&lt;/span&gt; in probability as &lt;span class=&#34;math inline&#34;&gt;\(n\to \infty\)&lt;/span&gt;, so &lt;span class=&#34;math inline&#34;&gt;\(v(t)\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{3.58b}\)&lt;/span&gt; can be evaluated at time &lt;span class=&#34;math inline&#34;&gt;\(M_u(n)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In the case &lt;span class=&#34;math inline&#34;&gt;\(p=1\)&lt;/span&gt;, the asymptotic distribution of &lt;span class=&#34;math inline&#34;&gt;\(\wh p_n\)&lt;/span&gt;,
the maximum value of the Kaplan-Meier estimator in a sample of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;,
was unknown at the time was written.
It still remains a seemingly difficult problem.
Here we give some partial but enlightening results.&lt;/p&gt;
&lt;p&gt;% Using the notation in Section &lt;span class=&#34;math inline&#34;&gt;\(\ref{nii}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\wh p_n\)&lt;/span&gt; can be written as
%
%p_n= 1-_{i=1}^n (1- ),
%
%where %% &lt;span class=&#34;math inline&#34;&gt;\(NAR_i := \sum_{j=1}^n {\bf 1}_\{T_j&amp;gt;T_i\}\)&lt;/span&gt;
%the denominator in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{p1}\)&lt;/span&gt;
%is the number of subjects at risk at time &lt;span class=&#34;math inline&#34;&gt;\(T_i\)&lt;/span&gt;.
%Using the ordered lifetimes
%Order the lifetimes &lt;span class=&#34;math inline&#34;&gt;\((T_i)_{1\le i\le n}\)&lt;/span&gt; as
%&lt;span class=&#34;math inline&#34;&gt;\(T_n^{(1)}&amp;lt; T_n^{(2)}&amp;lt; \cdots &amp;lt;T_n^{(n)}\)&lt;/span&gt;
%with associated censor indicators
%&lt;span class=&#34;math inline&#34;&gt;\(C_n^{(1)}, C_n^{(2)}, \ldots, C_n^{(n)}\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;From &lt;span class=&#34;math inline&#34;&gt;\(\eqref{km1}\)&lt;/span&gt; we can write the complement of &lt;span class=&#34;math inline&#34;&gt;\(\wh p_n\)&lt;/span&gt; as
1- p_n= 1-F_n(T_n^{(n)} )=
&lt;em&gt;{i=1}^n (1- )
= &lt;/em&gt;{i=1}^n (1- ),
where recall that &lt;span class=&#34;math inline&#34;&gt;\(T_n^{(1)}&amp;lt; T_n^{(2)}&amp;lt; \cdots &amp;lt;T_n^{(n)}\)&lt;/span&gt; are the ordered lifetimes
with associated censor indicators
&lt;span class=&#34;math inline&#34;&gt;\(C_n^{(1)}, C_n^{(2)}, \ldots, C_n^{(n)}\)&lt;/span&gt;.
Now, provided &lt;span class=&#34;math inline&#34;&gt;\(C_n^{(n)}=0\)&lt;/span&gt;, we can take logs and turn the product into a sum.
Since we are only interested in data for which the largest observation is censored, we will
condition on the event &lt;span class=&#34;math inline&#34;&gt;\(\{C_n^{(n)}=0\}\)&lt;/span&gt; throughout this discussion.
Then from &lt;span class=&#34;math inline&#34;&gt;\(\eqref{p2}\)&lt;/span&gt; we get
|(1- p_n)|
= &lt;em&gt;{i=2}^n |(1- )|
= &lt;/em&gt;{i=2}^n a_i C_n^{(n-i+1)},
where &lt;span class=&#34;math inline&#34;&gt;\(a_i:= |\log(1-1/i)|\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(i\ge 2\)&lt;/span&gt;.
By Theorem 4.1 of , &lt;span class=&#34;math inline&#34;&gt;\(\wh p_n \topr 1\)&lt;/span&gt;,
so &lt;span class=&#34;math inline&#34;&gt;\(|\log(1- \wh p_n)|\topr \infty\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(n\to\infty\)&lt;/span&gt;, and we need to determine the rate of this divergence.
Despite its quite simple representation, it seems rather hard to analyse &lt;span class=&#34;math inline&#34;&gt;\(\eqref{p3}\)&lt;/span&gt;
in full generality.
%\footnote{&lt;/p&gt;
&lt;p&gt;The model, also known as the proportional hazards model
(not to be confused with Cox’s proportional hazards model),
assumes
(t) = (^*(t))^
for some &lt;span class=&#34;math inline&#34;&gt;\(\beta&amp;gt;0\)&lt;/span&gt;.
The K-G model has been criticised as being unrealistic (), nevertheless there is a substantial literature in which it is used to gain theoretical insight into the behaviour of survival models (e.g., , , Verver etc.??), and also successfully in real data analyses (e.g.??).
We use it here similarly to get some valuable insight.
Applied in the next lemma, the KG property greatly simplifies the analysis of &lt;span class=&#34;math inline&#34;&gt;\(\wh p_n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
%It is well known and easily checked
showed that &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; are independent in the Koziol-Green model
(see also ),
%W. R. Allen, 1963. ” Letter to the Editor—A Note on Conditional Probability of Failure When Hazards are Proportional ,” Operations Research, INFORMS, vol. 11 (4), pages 658-659,
hence
&lt;span class=&#34;math inline&#34;&gt;\((T_i)_{1\le i\le n}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\((C_i)_{1\le i\le n}\)&lt;/span&gt; are independent.
%Y. Y. Chen, M. Hollander &amp;amp; N. A. Langberg (1982) Small-Sample Results for the Kaplan-Meier Estimator, Journal of the American Statistical Association, 77:377, 141-144,
% DOI: 10.1080/01621459.1982.10477777
Since the ordering which relabels &lt;span class=&#34;math inline&#34;&gt;\((T_i)_{1\le i\le n}\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\((T_n^{(i)})_{1\le i\le n}\)&lt;/span&gt; is
independent of &lt;span class=&#34;math inline&#34;&gt;\((C_i)_{1\le i\le n}\)&lt;/span&gt;, it follows that
&lt;span class=&#34;math inline&#34;&gt;\((T_n^{(i)})_{1\le i\le n}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\((C_n^{(i)})_{1\le i\le n}\)&lt;/span&gt; are independent.
Specifically, let &lt;span class=&#34;math inline&#34;&gt;\((k_n(i))_{1\le i\le n}\)&lt;/span&gt; be the random permutation which transforms
&lt;span class=&#34;math inline&#34;&gt;\((T_i)_{1\le i\le n}\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\((T_n^{(i)})_{1\le i\le n}\)&lt;/span&gt;, i.e.,
&lt;span class=&#34;math inline&#34;&gt;\(T_n^{(i)}= T_{k_n(i)}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(1\le i\le n\)&lt;/span&gt;.
The &lt;span class=&#34;math inline&#34;&gt;\(k_n(i)\)&lt;/span&gt; are independent of &lt;span class=&#34;math inline&#34;&gt;\((C_i)_{1\le i\le n}\)&lt;/span&gt;, and, for &lt;span class=&#34;math inline&#34;&gt;\(t_i\ge 0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(c_i\in \{0,1\}\)&lt;/span&gt;,
&amp;amp;&amp;amp;P(T_n^{(i)}t_i, , C_n^{(i)}=c_i, , 1in)
=
P(T_{k_n(i)} t_i, , C_{k_n(i)}=c_i, , 1in)
&amp;amp;&amp;amp;
&amp;amp;=&amp;amp;
&lt;em&gt;{k_1,, k_n}
P(T&lt;/em&gt;{k_i} t_i, , C_{k_i}=c_i, , k_n(i)=k_i, ,1in),
where the summation is over unequal integers &lt;span class=&#34;math inline&#34;&gt;\(k_i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(1\le i\le n\)&lt;/span&gt;.
By independence the last summation equals
&amp;amp;&amp;amp;
&lt;em&gt;{j_1,, j_n}
P(T&lt;/em&gt;{k_i} t_i, , k_n(i)=k_i, ,1in)
P(C_{k_i}=c_i, , 1in)
&amp;amp;&amp;amp;
&amp;amp;=&amp;amp;
P(T_{k_n(i)} t_i, , 1in) _{i=1}^n P(C_1=c_i).
Finally, on noting that
P(C_1=1) =P(T_1^*U_1) = _0^{_h} (t) F^*(t) =
,
the lemma follows. &lt;/p&gt;
&lt;p&gt;As a corollary to the lemma, we get from &lt;span class=&#34;math inline&#34;&gt;\(\eqref{p3}\)&lt;/span&gt;
|(1- p_n)|
_{i=2}^n a_i C_n^{(i)},
where &lt;span class=&#34;math inline&#34;&gt;\(a_i= |\log(1-1/i)|\)&lt;/span&gt; and the &lt;span class=&#34;math inline&#34;&gt;\((C_n^{(i)})\)&lt;/span&gt; are iid, &lt;span class=&#34;math inline&#34;&gt;\(2\le i\le n\)&lt;/span&gt;, for each &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;.
Using this, we can give a quite explicit representation for the limiting distribution of
the centered sequence
$|(1- p_n)| - E|(1- p_n)| $ in the Koziol-Green model.
We have the following result.
%%%Let &lt;span class=&#34;math inline&#34;&gt;\(a_i:= |\log(1-1/i)|\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(i\ge 2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
We use Lemma 3.16, p.47, of (see also his Theorem 3.18, p.48),
which states that the sum &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i\ge 1} \xi_i\)&lt;/span&gt;
of independent random variables &lt;span class=&#34;math inline&#34;&gt;\(\xi_1, \xi_2, \ldots\)&lt;/span&gt; having mean 0 and satisfying
&lt;span class=&#34;math inline&#34;&gt;\(\sum_{i\ge 1} E (\xi_i^2)&amp;lt;\infty\)&lt;/span&gt;, converges a.s.
Apply this with
&lt;span class=&#34;math inline&#34;&gt;\(Y_i= C_n^{(i)}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\xi_i:= a_i (Y_i-E(Y_i) )\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(i\ge 1\)&lt;/span&gt;.
Since the &lt;span class=&#34;math inline&#34;&gt;\(Y_i\)&lt;/span&gt; are iid Bernoulli and &lt;span class=&#34;math inline&#34;&gt;\(a_i\le 2/(i-1)\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(i\ge 2\)&lt;/span&gt;, the convergence of the series
&lt;span class=&#34;math inline&#34;&gt;\(\sum_{i\ge 1} E\big(a_i (Y_i-E(Y_i)\big)^2\)&lt;/span&gt;
is clear and we deduce that the rv &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{disy}\)&lt;/span&gt; is finite a.s. and clearly has the specified variance.
The infinite divisibility of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; follows from…
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
(i)  
&lt;span class=&#34;math inline&#34;&gt;\(Y- E(Y)\)&lt;/span&gt; has characteristic function
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}\label{cfy}
Ee^{\rmi \theta (Y-E(Y))}
&amp;amp;=
\prod_{j\ge 1}  
Ee^{\rmi \theta a_j(Y_j-EY_j)} \cr
&amp;amp;=
\prod_{j\ge 1} \frac{1}{\beta+1} \Big( e^{-\rmi \theta \log(1-1/j)}    +  \beta \Big)
e^{\rmi \theta \log(1-1/j)/(\beta+1) }    \cr
  &amp;amp;=
  \prod_{j\ge 1}  \frac{1}{\beta+1}
  \Big( (1-1/j)^{-\rmi \theta\beta/(\beta+1)}    +   \beta (1-1/j)^{-\rmi \theta/(\beta+11)}  \Big). \ \
\end{align}\]&lt;/span&gt;\end{align}&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
(i)  
Some properties of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; are easily deduced from &lt;span class=&#34;math inline&#34;&gt;\(\eqref{disy}\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\eqref{cfy}\)&lt;/span&gt;.
(e.g., moments??)&lt;/p&gt;
&lt;p&gt;(ii)&lt;br /&gt;
An extension of the KG model is to allow &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{kgm}\)&lt;/span&gt; to depend on &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;
(), but we cannot generalise Lemma &lt;span class=&#34;math inline&#34;&gt;\(\ref{lem2}\)&lt;/span&gt;
to this situation because the independence of &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; is also necessary for the KG model as stated in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{kgm}\)&lt;/span&gt;
().
%Zheng, G. and Gastwirth, J. L. (2001). On the Fisher information in randomly censored data.
%Statist. Probab. Lett. 52, 421-426.&lt;/p&gt;
&lt;p&gt;A common assumption in many theoretical and practical survival analyses
is of a uniform distribution for &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; and an exponential distribution for &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;:
(t) =(1-t/){}_{{t}}
(t) =e^{-t}, t,
where &lt;span class=&#34;math inline&#34;&gt;\(\tau\equiv \tau_G=\tau_H&amp;gt;0\)&lt;/span&gt; and $ &amp;gt;0$.&lt;br /&gt;
This setup was used in to simulate percentage points of the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\wh p_n\)&lt;/span&gt; in the case &lt;span class=&#34;math inline&#34;&gt;\(p=1\)&lt;/span&gt; for certain typical choices of &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The tails in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{uc1}\)&lt;/span&gt; are very far from the proportionality of survival and censoring distribution tails as
in the Koziol-Green model, and the setup does not enjoy
the independence of survival and censoring variables obtaining in that model.
With less information we can prove less, but still we can show that
the centered $|(1- p_n)| $ is stochastically bounded in this situation, suggesting that
Theorem &lt;span class=&#34;math inline&#34;&gt;\(\ref{ASKG}\)&lt;/span&gt; gives the right order of magnitude in this situation too.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
%Then a similar argument as before using Chebychev’s inequality shows that
%
%&amp;amp;&amp;amp;
%|(1- p_n)| - E|(1- p_n)|
%= O_P(1),
%
%so the centered $|(1- p_n)| $ is stochastically bounded.
We use the notation and results in Section &lt;span class=&#34;math inline&#34;&gt;\(\ref{LLC}\)&lt;/span&gt;.&lt;br /&gt;
Refer to formula &lt;span class=&#34;math inline&#34;&gt;\(\eqref{p3}\)&lt;/span&gt;, recall that &lt;span class=&#34;math inline&#34;&gt;\(a_i= |\log(1-1/i)|\)&lt;/span&gt; and write, for &lt;span class=&#34;math inline&#34;&gt;\(j&amp;gt;i\)&lt;/span&gt;,
%%
E( |(1- )| |(1- )|)
&amp;amp;=&amp;amp;
E(a_ia_j {}&lt;em&gt;{{C_n&lt;sup&gt;{(i)}=1,C_n&lt;/sup&gt;{(j)}=1}})
&amp;amp;=&amp;amp;
a_ia_j P(C_n&lt;sup&gt;{(i)}=1,C_n&lt;/sup&gt;{(j)}=1).  &lt;br /&gt;
Thus from &lt;span class=&#34;math inline&#34;&gt;\(\eqref{p3}\)&lt;/span&gt;
&amp;amp;&amp;amp;
E
|(1- p_n)|^2
= E&lt;/em&gt;{i=1}^n |(1- )|^2
&amp;amp;&amp;amp;
&amp;amp;&amp;amp; +
2 &lt;em&gt;{i=1}^{n-1} &lt;/em&gt;{j=i+1}^n
E( |(1- )| |(1- )|)
&amp;amp;&amp;amp;
&amp;amp;&amp;amp;=
&lt;em&gt;{i=1}^n a_i^2 P(C_n^{(i)}=1 ) +
2&lt;/em&gt;{i=1}^{n-1} &lt;em&gt;{j=i+1}^n
a_ia_j
P(C_n^{(i)} =1, , C_n^{j}=1 ).
Also
&amp;amp;&amp;amp;
E^2|(1- p_n)|=(&lt;/em&gt;{i=1}^n a_i P(C_n^{(i)}=1 ) )^2
&amp;amp;&amp;amp;
&amp;amp;&amp;amp; =
&lt;em&gt;{i=1}^n a_i^2 P&lt;sup&gt;2(C_n&lt;/sup&gt;{(i)}=1 )+
2&lt;/em&gt;{i=1}^{n-1} &lt;em&gt;{j=i+1}^n
a_ia_j
P(C_n^{(i)} =1) P(C_n^{j}=1 ).
Subtracting &lt;span class=&#34;math inline&#34;&gt;\(\eqref{k12}\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(\eqref{k11}\)&lt;/span&gt; we see that
&amp;amp;&amp;amp;{}( |(1- p_n)|)=
E|(1- p_n)|^2 -E^2|(1- p_n)|
&amp;amp;&amp;amp;
&amp;amp;&amp;amp;
&lt;/em&gt;{i=1}^n P(C_n^{(i)}=1 ) (1- P(C_n^{(i)}=1 ))
+
2&lt;em&gt;{i=1}^{n-1} &lt;/em&gt;{j=i+1}^n
P(C_n^{(i)} =1, , C_n^{j}=1 ).
&amp;amp;&amp;amp;
&lt;/p&gt;
&lt;p&gt;%
%From &lt;span class=&#34;math inline&#34;&gt;\(\eqref{c6}\)&lt;/span&gt;
%
%&amp;amp;&amp;amp;
%&lt;em&gt;{i=1}^n
%
%=
%&lt;/em&gt;{i=1}^n {n i} &lt;em&gt;{t=0}^{}
% ^{i-1}(t) H^{n-i}(t) (t) t
% &amp;amp;&amp;amp;
% &amp;amp;&amp;amp;=
% &lt;/em&gt;{t=0}^{}
%(1- (1-t/)e^{-t} )^n t
%&amp;amp;&amp;amp;
%&amp;amp;&amp;amp;
%,  {} n.
%
To estimate the bivariate term in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{uc5a}\)&lt;/span&gt;, reverse the order of summation and consider first the summation over &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;.
Using &lt;span class=&#34;math inline&#34;&gt;\(\eqref{c6}\)&lt;/span&gt; we calculate
%
&amp;amp;&amp;amp;
&lt;em&gt;{i=1}^{j-1}
&amp;amp;&amp;amp;=
&lt;/em&gt;{i=1}^{j-1}
&amp;amp;&amp;amp;
&lt;em&gt;{t_1=0}^{} &lt;/em&gt;{t_2=0}^{t_1}
^{i-1}(t_1) H^{n-j}(t_2) ((t_2)- (t_1) )^{j-i-1} K(t_1)K(t_2)
&amp;amp;&amp;amp;
&amp;amp;&amp;amp;=
&lt;em&gt;{i=1}^{j-1}
&amp;amp;&amp;amp;
^2 &lt;/em&gt;{t_1=0}^{} &lt;em&gt;{t_2=0}^{t_1}
( (1-t_1/)e^{-t_1} )^{i} (1- (1-t_2/)e^{-t_2} )^{n-j}
&amp;amp;&amp;amp;
&amp;amp;&amp;amp;
( (1-t_2/)e^{-t_2} - (1-t_1/)e^{-t_1} )^{j-i-1}
(1-t_2/)e^{-t_2} t_1 t_2.
Performing the summation we get
%%
&amp;amp;&amp;amp;
&lt;/em&gt;{i=1}^{j-1}
&amp;amp;&amp;amp;
&amp;amp;&amp;amp;=
&lt;em&gt;{t_1=0}^{} &lt;/em&gt;{t_2=0}^{t_1}
(1- (1-t_2/)e^{-t_2} )^{n-j} (1-t_2/)e^{-t_2}
&amp;amp;&amp;amp;
&amp;amp;&amp;amp; (
( (1-t_2/)e^{-t_2} )^{j-1}
- ( (1-t_2/)e^{-t_2} - (1-t_1/)e^{-t_1} )^{j-1} )
t_1 t_2.
&amp;amp;&amp;amp;
Discard the subtracted term, divide the remainder by &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; and add over &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; to get
&amp;amp;&amp;amp;
&lt;em&gt;{j=2}^{n}
&lt;/em&gt;{i=1}^{j-1}
&amp;amp;&amp;amp;
&amp;amp;&amp;amp;
^2 &lt;em&gt;{j=2}^{n} {n j}
&lt;/em&gt;{t_1=0}^{} &lt;em&gt;{t_2=0}^{t_1}
(1- (1-t_2/)e^{-t_2} )^{n-j}&lt;br /&gt;
( (1-t_2/)e^{-t_2} )^{j} t_1 t_2
&amp;amp;&amp;amp;
&amp;amp;&amp;amp;
^2 &lt;/em&gt;{t_1=0}^{} &lt;em&gt;{t_2=0}^{t_1}
t_1 t_2 = ^2 ^2.
Thus from &lt;span class=&#34;math inline&#34;&gt;\(\eqref{uc5a}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eqref{uc3a}\)&lt;/span&gt; we see that
&lt;span class=&#34;math inline&#34;&gt;\({\rm Var}\big( |\log(1- \wh p_n)|\big)\)&lt;/span&gt;
is bounded above by
&lt;/em&gt;{i} P(C_n^{(i)}=1 ) (1- P(C_n^{(i)}=1 )
+^2 ^2
and hence is finite.
Then by Chebychev’s inequality we conclude that &lt;span class=&#34;math inline&#34;&gt;\(\eqref{OP1}\)&lt;/span&gt; holds and
the centered $|(1- p_n)| $ is stochastically bounded.
&lt;/p&gt;
%
&lt;p&gt;%&lt;span class=&#34;math inline&#34;&gt;\(\eqref{k9}\)&lt;/span&gt; already gives some useful information concerning &lt;span class=&#34;math inline&#34;&gt;\(\wh p_n\)&lt;/span&gt;.
%We weaken the independence assumption to
Assuming the pairwise independence of the &lt;span class=&#34;math inline&#34;&gt;\(C_n^{(i)}\)&lt;/span&gt;, i.e., that
P(C_n&lt;sup&gt;{(i)}=1,C_n&lt;/sup&gt;{(j)}=1)
= P(C_n&lt;sup&gt;{(i)}=1)P(C_n&lt;/sup&gt;{(j)}=1),   j&amp;gt;i,
allows some calculations like in the previous subsection to be carried out, and we can again show that
the centered $|(1- p_n)| $ is stochastically bounded.
We omit the details of this as it’s not clear when the pairwise independence assumption holds.&lt;/p&gt;
&lt;p&gt;%This already gives very useful information concerning &lt;span class=&#34;math inline&#34;&gt;\(\wh p_n\)&lt;/span&gt;.
%Note that, then,
%
%&amp;amp;&amp;amp;
%E (C_n&lt;sup&gt;{(i)}C_n&lt;/sup&gt;{(j)}) =
%P(C_n&lt;sup&gt;{(i)}=1,C_n&lt;/sup&gt;{(j)}=1)
%&amp;amp;&amp;amp;
%&amp;amp;&amp;amp;
%= P(C_n&lt;sup&gt;{(i)}=1)P(C_n&lt;/sup&gt;{(j)}=1)
%= E(C_n^{(i)}) E(C_n^{(j)}),   j&amp;gt;i.
%
%Recall that &lt;span class=&#34;math inline&#34;&gt;\(a_i=|\log(1-1/i)|\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(i\ge 1\)&lt;/span&gt;. Then for &lt;span class=&#34;math inline&#34;&gt;\(j&amp;gt;i\)&lt;/span&gt;
%
%E( |(1- )| |(1- )|)
%&amp;amp;=&amp;amp;
%E(a_ia_j {}&lt;em&gt;{{C_n&lt;sup&gt;{(i)}=1,C_n&lt;/sup&gt;{(j)}=1}})
%&amp;amp;=&amp;amp;
%a_ia_j P(C_n&lt;sup&gt;{(i)}=1,C_n&lt;/sup&gt;{(j)}=1),  &lt;br /&gt;
%
%while from &lt;span class=&#34;math inline&#34;&gt;\(\eqref{p3}\)&lt;/span&gt;
%
%&amp;amp;&amp;amp;
%E
%|(1- p_n)|^2
%= E&lt;/em&gt;{i=1}^n |(1- )|^2
%&amp;amp;&amp;amp;
%&amp;amp;&amp;amp; +
%2 &lt;em&gt;{i=1}^{n-1} &lt;/em&gt;{j=i+1}^n
% E( |(1- )| |(1- )|)
% &amp;amp;&amp;amp;
% &amp;amp;&amp;amp;=
%&lt;em&gt;{i=1}^n a_i^2 P(C_n^{(i)}=1 ) +
% 2&lt;/em&gt;{i=1}^{n-1} &lt;em&gt;{j=i+1}^n
% a_ia_j
%P(C_n^{(i)} =1, , C_n^{j}=1 ).
%
%In view of &lt;span class=&#34;math inline&#34;&gt;\(\eqref{p2}\)&lt;/span&gt; this equals
%
%&lt;/em&gt;{i=1}^n a_i^2 P(C_n^{(i)}=1 ) +
% 2&lt;em&gt;{i=1}^{n-1} &lt;/em&gt;{j=i+1}^n
% a_ia_j
%P(C_n^{(i)} =1) P(C_n^{j}=1 ).
%
%Also
%
%&amp;amp;&amp;amp;
%E^2|(1- p_n)|=(&lt;em&gt;{i=1}^n a_i P(C_n^{(i)}=1 ) )^2
%&amp;amp;&amp;amp;
%&amp;amp;&amp;amp; =
%&lt;/em&gt;{i=1}^n a_i^2 P&lt;sup&gt;2(C_n&lt;/sup&gt;{(i)}=1 )+
% 2&lt;em&gt;{i=1}^{n-1} &lt;/em&gt;{j=i+1}^n
% a_ia_j
%P(C_n^{(i)} =1) P(C_n^{j}=1 ).
%
%Subtract &lt;span class=&#34;math inline&#34;&gt;\(\eqref{k12}\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(\eqref{k12b}\)&lt;/span&gt; and use &lt;span class=&#34;math inline&#34;&gt;\(a_i=-\log(1-1/i)\le 1/i\)&lt;/span&gt; to get
%
%{}( |(1- p_n)|)
%&amp;amp;=&amp;amp;
%&lt;em&gt;{i=1}^n a_i^2 P(C_n^{(i)}=1 )(1- P(C_n^{(i)}=1 ) )
%&amp;amp;&amp;amp;
%&amp;amp;=&amp;amp;
%
%&lt;/em&gt;{i=1}^n a_i^2
%
%_{i=1}^n c&amp;lt; .
%&amp;amp;&amp;amp;
%
%Consequently
%
%E(|(1- p_n)| - E|(1- p_n)| )^2
%=
%{}( |(1- p_n)|) c&amp;lt; ,
%
%
%and by Chebychev’s inequality we conclude that
%
%&amp;amp;&amp;amp;
%|(1- p_n)| - E|(1- p_n)|
%= O_P(1),
%
%thus the centered $|(1- p_n)| $ is stochastically bounded.&lt;/p&gt;
In Section &lt;span class=&#34;math inline&#34;&gt;\(\ref{ext}\)&lt;/span&gt; we quantified the idea of ``sufficient followup’’ as the condition &lt;span class=&#34;math inline&#34;&gt;\(\tau_F\le \tau_G\)&lt;/span&gt; .
%
&lt;p&gt;As a test statistic for sufficient follow-up we focus on the statistic &lt;span class=&#34;math inline&#34;&gt;\(Q_n\)&lt;/span&gt; proposed in .
This is defined as follows.
In a sample of size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, let &lt;span class=&#34;math inline&#34;&gt;\(M(n)\)&lt;/span&gt; be the largest observed survival time and let &lt;span class=&#34;math inline&#34;&gt;\(M_u(n)\)&lt;/span&gt; be the largest observed {} survival time.
%%With all survival times necessarily in &lt;span class=&#34;math inline&#34;&gt;\([0,M(n)]\)&lt;/span&gt;, a number
Let &lt;span class=&#34;math inline&#34;&gt;\(N_u(n)\)&lt;/span&gt; be the number of
uncensored survival times, necessarily in &lt;span class=&#34;math inline&#34;&gt;\([0,M_u(n)]\)&lt;/span&gt;, let
&lt;span class=&#34;math inline&#34;&gt;\(N_c^&amp;lt;(n)\)&lt;/span&gt; be the number of censored survival times in &lt;span class=&#34;math inline&#34;&gt;\([0,M_u(n))\)&lt;/span&gt;, and let &lt;span class=&#34;math inline&#34;&gt;\(N_c^&amp;gt;(n)\)&lt;/span&gt; be
the number of censored survival times in &lt;span class=&#34;math inline&#34;&gt;\((M_u(n), M(n)]\)&lt;/span&gt;.
Thus there is a total of &lt;span class=&#34;math inline&#34;&gt;\(N_c(n)=N_c^&amp;lt;(n)+N_c^&amp;gt;(n)=n-N_u(n)\)&lt;/span&gt; censored survival times in the sample.
Set &lt;span class=&#34;math inline&#34;&gt;\(\Delta_n:= 2M_u(n)-M(n)\)&lt;/span&gt;.
As in , p.81,
%
define
Q_n
&amp;amp;=&amp;amp; #{{}[_n, M_u(n)) }
&amp;amp;&amp;amp;
&amp;amp;=&amp;amp;
#{{} 2M_u(n)-M(n) }.
(Note that we exclude &lt;span class=&#34;math inline&#34;&gt;\(M_u(n)\)&lt;/span&gt; itself when counting the number of
uncensored observations greater than &lt;span class=&#34;math inline&#34;&gt;\(\Delta_n\)&lt;/span&gt;.)], and for any 8 and 8’ E 6,
%if p’ F, (t) = pF, (t) for any t E [0, TI, then (p’, 8’) = (p, 8).
%2. In the two components mixture case: For any p =(&lt;sub&gt;,q)&lt;/sub&gt; and
%p’ = qOT E Y2 and for any x = (OT, &amp;lt;T)T and xi = (elT, E A
%such that 8 # 5,
%if pIT (3,. (t) = pT Om (t) for any t E [0, T] then
%(p’, x’) E fl if p # 0 and q # 0.
%(p’, q’; 8’) = (p, 0; 8) if q = 0 when F, # F n 2.
%min [lip’ - p, q’; 8’ - 811, ii p’, q’ - p; 5’ - 0111 = 0 if q = 0, when F, E
%9 n 2 and 8’ # &amp;lt;‘.
%(p’q’; r) = (0, q; 5) if p = 0, when F; E F n 2.
%min[/p’-q,qr;8’-(!.lpt,q’,-q;(’-&amp;lt;/]=Oifp=O, when F&amp;lt;E
%9 n X and 8’ # 5’.
%370 M. LEMDANI AND 0. PONS
%We describe here all the possible cases involving a susceptibility
%ratio problem with one or two factors. The generalization to more
%than two factors is straightforward. We summarize the identifiability
%assumption for all these situations by the following equivalence
%if P’~ O, (t) = pT 0, (t) for any t E [O,Z], then (p’, r”)?’ - (p,
%We shall then consider two equivalent parameters as two versions
%of the same parameter, up to a permutation of their components.
%Assumption A, implies a similar identifiability result for the hazard
%function. It will be used in the following form
%LEMMA 2Under C, and A,, if lim,+, suptEl-, .,I i ,m,,n (t) - j.,;,(t)l = 0.
%then there is a cersion of (p,,~,), which converges to a limit
%(P,x”)-(p,~) as n-tx.
%Proof Suppose that limn_, &lt;sub&gt;up,,&lt;/sub&gt;,,,,)~ (t) - .,, (t)J = 0. By condition %, and the dominated convergence theorem, for any t E[O,T],
%lim ,,+, log [l - p: @,” (t)] = log [I - pTO, (t)] and therefore lim ,
%p: Ox, (t) = pT 0, (t). By boundedness of 9, and using if necessary a
%permutation of the components of (p,, r,),, from any subsequence
%(p,, r,), of (p,, r,), one can extract a convergent subsequence (p, , r, ),
%having a limit (p&lt;em&gt;, r&lt;/em&gt;) that satisifes P*~ @,, (t) = PT O,(t) for any
%t E [0, z]. Under A,, this yields the required result.
%We can now prove the following consistency property, where the
%type of convergence depends on the model and on the value of p, in
%the case of k components, as described above.
%THEOREM 1 Under the assumptions C,, A,, A,, and under H,, (b,, 4,)
%concerges in probability to a limit equicalent to (p,, r,) as n -t x.
%Proof For p = (p; r), let K, (&amp;amp;) = n- ’ [log L, (p; x) - log L,(p,; x,)]
%and let
%(i) = [: fog [=] 2, (s) - i” ;., (s) + 1 ] p: q, (r)C(s) ds.
%We have
%MIXTURE MODELS FOR CENSORED SURVIVAL DATA 371
%From proposition 1, lim, supp 1 K, (Ap) – KO (&amp;amp;) 1 = 0. Moreover, according to the concavity of the logarithm function, KO is maximal for . A
%r. = /., and of course KO (Lo) = 0. It follows that K, (3,8,) -0 in probability as n + x. Then,
%hence KO (it,,) + 0 in probability as 11 + x. Then, using again the concavity of logarithm function we conclude that &lt;sub&gt;up,i&lt;/sub&gt;~~,, (t) 5 0 which,]
%Suppose &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; is Uniform&lt;span class=&#34;math inline&#34;&gt;\([0,\tau_{F}]\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; is Uniform&lt;span class=&#34;math inline&#34;&gt;\([0,\tau_G]\)&lt;/span&gt;.
Recall we assume that &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; are continuous distributions.
We have the following limiting distributions in cases of interest.
 Assume &lt;span class=&#34;math inline&#34;&gt;\(\tau_{F}&amp;lt; \tau_G&amp;lt;\infty\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(0&amp;lt;p&amp;lt;1\)&lt;/span&gt;,
so that &lt;span class=&#34;math inline&#34;&gt;\(\tau_J=\tau_{F}&amp;lt; \tau_H= \tau_G&amp;lt;\tau_{F^*}=\infty\)&lt;/span&gt;.
Suppose in addition that, as &lt;span class=&#34;math inline&#34;&gt;\(z\dto 0\)&lt;/span&gt;,
(&lt;em&gt;G-z)=a_G (1+o(1)) z^L_G(z)
 {}&lt;br /&gt;
&lt;em&gt;0(&lt;/em&gt;{F}-z)= a&lt;/em&gt;{F}(1+o(1)) z^L_{F}(z),
where &lt;span class=&#34;math inline&#34;&gt;\(a_G, a_{F}, \gamma, \beta\)&lt;/span&gt; are positive constants and &lt;span class=&#34;math inline&#34;&gt;\(L_G(z)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(L_{F}(z)\)&lt;/span&gt; are slowly varying as &lt;span class=&#34;math inline&#34;&gt;\(z\dto 0\)&lt;/span&gt;.
Then the random variables
&lt;span class=&#34;math inline&#34;&gt;\(a_n(\tau_G- M(n))\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b_n(\tau_{F}- M_u(n))\)&lt;/span&gt; are asymptotically independently %exponentially
distributed with marginal cdfs, respectively,
&amp;amp;&amp;amp;
1- e^{- (1-p) a_Gu^}
 {} 
1 - e^{- pa_{F}(_{F})v^},  u, v,
for any choice of norming sequences satisfying $a_n(nL_G(1/a_n)) ^{1/} $
and
&lt;span class=&#34;math inline&#34;&gt;\(b_n\sim n^{1/\beta}L_{F}(1/b_n)\)&lt;/span&gt;, as &lt;span class=&#34;math inline&#34;&gt;\(n\to\infty\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt; Assume &lt;span class=&#34;math inline&#34;&gt;\(\tau_{F}&amp;lt;\tau_G&amp;lt;\infty\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p=1\)&lt;/span&gt;,
so that &lt;span class=&#34;math inline&#34;&gt;\(F\equiv F\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tau_J=\tau_{F}= \tau_{F^*}=\tau_H&amp;lt;\tau_G&amp;lt;\infty\)&lt;/span&gt;.
Suppose in addition that, as &lt;span class=&#34;math inline&#34;&gt;\(z\dto 0\)&lt;/span&gt;,
(&lt;em&gt;G-z)=a (1+o(1)) z^L(z)
 {}&lt;br /&gt;
&lt;em&gt;0(&lt;/em&gt;{F}-z)= a(1+o(1)) z^L(z),
where &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; are positive constants and &lt;span class=&#34;math inline&#34;&gt;\(L(z)\)&lt;/span&gt; is slowly varying as &lt;span class=&#34;math inline&#34;&gt;\(z\dto 0\)&lt;/span&gt;.
Choose &lt;span class=&#34;math inline&#34;&gt;\(a_n\)&lt;/span&gt; to satisfy
$a_n(nL(1/a_n)) ^{1/} $.
%(and now $b_n(nL&lt;/em&gt;{F}(1/b_n)) ^{1/} $).
Then, for $ 0uv$,
&lt;em&gt;{n}
P( a_n(&lt;/em&gt;{F}- M(n))u, , a_n(&lt;em&gt;{F}- M_u(n))v )=1- e^{- a (&lt;/em&gt;{F}) u^}.
%%,  u,v.
%%=1- e^{- a_{F} u^(&lt;em&gt;{F})}.
%%&amp;amp;&amp;amp;
%&amp;amp;&amp;amp;=
% 1
%%%%%%%%%%%+ e^{- a&lt;/em&gt;{F} (u&lt;sup&gt;,v&lt;/sup&gt;) (&lt;em&gt;{F})}
%- e^{- a&lt;/em&gt;{F} u^(&lt;em&gt;{F})},  u,v.
%%%%%%%%%%%%- e^{- a&lt;/em&gt;{F} v^(_{F})},  u,v.
&lt;/p&gt;
&lt;p&gt; Assume &lt;span class=&#34;math inline&#34;&gt;\(\tau_G&amp;lt;\tau_{F}\le \infty\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(0&amp;lt;p\le 1\)&lt;/span&gt;,
so that &lt;span class=&#34;math inline&#34;&gt;\(\tau_J= \tau_H= \tau_G&amp;lt;\tau_{F}\le \tau_{F^*}\le \infty\)&lt;/span&gt;,
and assume that
the first relation in
&lt;span class=&#34;math inline&#34;&gt;\(\eqref{FGD}\)&lt;/span&gt; holds.
Suppose in addition that, in a neighbourhood of &lt;span class=&#34;math inline&#34;&gt;\(\tau_G\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; has a density &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; which is positive and continuous at &lt;span class=&#34;math inline&#34;&gt;\(\tau_G\)&lt;/span&gt;.
Take &lt;span class=&#34;math inline&#34;&gt;\(a_n\)&lt;/span&gt; to satisfy
$a_n(nL_G(1/a_n)) ^{1/} $ and &lt;span class=&#34;math inline&#34;&gt;\(b_n\)&lt;/span&gt; to satisfy &lt;span class=&#34;math inline&#34;&gt;\(b_n\sim (nL_{F}(1/b_n))^{\frac{1}{1+\gamma}}\)&lt;/span&gt;.
Then &lt;span class=&#34;math inline&#34;&gt;\(a_n(\tau_G- M(n))\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b_n(\tau_G- M_u(n))\)&lt;/span&gt; are asymptotically independently distributed with
%%marginal exponential and Weibull distributions; specifically, having
marginal cdfs, respectively,
&amp;amp;&amp;amp;
1- e^{-a_G (1-pF(_G))u^}
 {} 
1 - e^{-p a_G f(_G) v^{1+}/(1+)},  u,v.
%Furthermore, this result remains true under the same assumptions when &lt;span class=&#34;math inline&#34;&gt;\(\tau_{F}=\infty\)&lt;/span&gt;.
%Thus, &lt;span class=&#34;math inline&#34;&gt;\(n(\tau_J- M(n))\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{n}(\tau_H- M_u(n))\)&lt;/span&gt; are asymptotically independently distributed with exponential and Weibull distributions, respectively.&lt;br /&gt;
%
% 
%The result in Case 3 remains true under the same assumptions when &lt;span class=&#34;math inline&#34;&gt;\(p=1\)&lt;/span&gt;, specifically,
% &lt;span class=&#34;math inline&#34;&gt;\(a_n(\tau_G- M(n))\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b_n (\tau_G- M_u(n))\)&lt;/span&gt; are then asymptotically independently distributed with marginal distributions
%$ 1- e^{- a_G_0(_G)u^}$, &lt;span class=&#34;math inline&#34;&gt;\(u&amp;gt;0\)&lt;/span&gt;, and
%&lt;span class=&#34;math inline&#34;&gt;\(1 - e^{-a_G f(\tau_G) v^{1+\gamma}}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(v&amp;gt;0\)&lt;/span&gt;,
% respectively, for the same choices of &lt;span class=&#34;math inline&#34;&gt;\(a_n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b_n\)&lt;/span&gt;.
%The result also remains true under the same assumptions when &lt;span class=&#34;math inline&#34;&gt;\(\tau_{F}=\infty\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;
In any case we have &lt;span class=&#34;math inline&#34;&gt;\(M(n)\topr \tau_H\)&lt;/span&gt; and $ M_u(n)_J$ as &lt;span class=&#34;math inline&#34;&gt;\(n\to\infty\)&lt;/span&gt;.
\end{theorem}&lt;/p&gt;
%
%
&lt;p&gt;\end{document}&lt;/p&gt;
&lt;p&gt;\end{document}&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Inference on Adaptively Collected Data</title>
      <link>https://youngstats.github.io/post/2022/10/11/inference-on-adaptively-collected-data/</link>
      <pubDate>Tue, 11 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/10/11/inference-on-adaptively-collected-data/</guid>
      <description>


&lt;div id=&#34;abstract&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;It is increasingly common for data to be collected adaptively, where
experimental costs are reduced progressively by assigning promising
treatments more frequently. However, adaptivity also poses great
challenges on post-experiment inference, since observations are
dependent, and standard estimates can be skewed and heavy-tailed. We
propose a treatment-effect estimator that is consistent and
asymptotically normal, allowing for constructing frequentist confidence
intervals and testing hypotheses.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-10-11-inference-on-adaptively-collected-data_files/eye_catching.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Adaptive data collection can optimize sample efficiency during the
course of the experiment for particular objectives, such as identifying
the best treatment &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-russo2020simple&#34; role=&#34;doc-biblioref&#34;&gt;D. Russo 2020&lt;/a&gt;)&lt;/span&gt; or improving operational
performance &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-agrawal2013thompson&#34; role=&#34;doc-biblioref&#34;&gt;Agrawal and Goyal 2013&lt;/a&gt;)&lt;/span&gt; . To achieve these efficiency gains,
the experimenter—rather than staying with a fixed randomization
rule—updates the data-collection policy (which maps individual
characteristics/contexts to treatments/actions) in response to observed
outcomes over the course of the experiment. In this way, the
experimenter can resolve uncertainty about some aspects of the data
generating process at the expense of learning little about others
&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-murphy2005experimental&#34; role=&#34;doc-biblioref&#34;&gt;Murphy 2005&lt;/a&gt;)&lt;/span&gt;. A common family of the design algorithms are
bandit algorithms &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-lai1985asymptotically&#34; role=&#34;doc-biblioref&#34;&gt;Lai and Robbins 1985&lt;/a&gt;)&lt;/span&gt;, where treatment assignments
are selected to trade off exploration and exploitation to maximize the
cumulative performance over time.&lt;/p&gt;
&lt;p&gt;The increasing popularity of adaptive experiments results in the growing
availability of data collected from such designs. A natural query
arises: can we reuse the data to answer a variety of questions that may
not be originally targeted by the experiments? However, adaptivity also
poses great statistical challenges if the post-experiment objective
differs significantly from the original, and standard approaches used to
analyze independently collected data can be plagued by bias, excessive
variance, or both. This post seeks to address the problem of offline
policy evaluation, which is to estimate the expected benefit of one
treatment assignment policy—often termed as &lt;em&gt;policy value&lt;/em&gt;—with data
that was collected using another potentially different policy. For
example, in personalized healthcare, doctors may use electronic medical
records to evaluate how particular groups of patients will respond to
heterogenenous treatments (e.g., different types of drugs/therapies or
different dosage levels of the same drug) &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-bertsimas2017personalized&#34; role=&#34;doc-biblioref&#34;&gt;Bertsimas et al. 2017&lt;/a&gt;)&lt;/span&gt;,
whereas in targeted advertising, retailers may want to understand how
alternative product promotions (either in mail or online) affect
different consumer segments &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-schnabel2016recommendations&#34; role=&#34;doc-biblioref&#34;&gt;Schnabel et al. 2016&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-formulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Problem formulation&lt;/h2&gt;
&lt;p&gt;Consider that samples are collected by a multi-armed bandit algorithm,
where each observation is represented by a tuple &lt;span class=&#34;math inline&#34;&gt;\((W_t, Y_t)\)&lt;/span&gt;. The
random variables &lt;span class=&#34;math inline&#34;&gt;\(W_t \in \{1, 2, \dots, K\}\)&lt;/span&gt; are called the arms,
treatments or interventions. The reward or outcome &lt;span class=&#34;math inline&#34;&gt;\(Y_t\)&lt;/span&gt; represents the
individual’s response to the treatment, for which we use the potential
outcome framework and denote &lt;span class=&#34;math inline&#34;&gt;\(Y_t(w)\)&lt;/span&gt; as the random variable
representing the outcome that would be observed if individual &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; were
assigned to a treatment &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;. We here consider a stationary setting of
the potential outcomes, where &lt;span class=&#34;math inline&#34;&gt;\((Y_t(1),Y_t(2),\dots, Y_t(K))\)&lt;/span&gt; is sampled
from a fixed distribution. The set of observations up to a certain time
&lt;span class=&#34;math inline&#34;&gt;\(H_t := \{(W_s, Y_s) \}_{s=1}^T\)&lt;/span&gt; is called a history. The treatment
assignment probabilities (also known as &lt;em&gt;propensities&lt;/em&gt;)
&lt;span class=&#34;math inline&#34;&gt;\(e_t(w) := \mathbb{P}[W_t = w | H_{t-1}]\)&lt;/span&gt; are updated over time by the
experimenter, in response to the observations &lt;span class=&#34;math inline&#34;&gt;\(H_{t-1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Our goal is to estimate the value of an arm &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;, denoted by
&lt;span class=&#34;math inline&#34;&gt;\(Q(w):=\mathbb{E}[Y_t(w)]\)&lt;/span&gt;. We will provide consistent and
asymptotically normal test statistics for &lt;span class=&#34;math inline&#34;&gt;\(Q(w)\)&lt;/span&gt;, so that we can
construct confidence intervals around the estimations to test
hypotheses. We would like to do that even in data-poor situations in
which the experimenter did not collect many samples around the arm &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;our-approach&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Our approach&lt;/h2&gt;
&lt;p&gt;The main challenging in evaluating an arm &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; with observational data is
known as the &lt;em&gt;overlap&lt;/em&gt; issue between the target arm and the
data-collection mechanism, when the arm assignments made during data
collection differ substantially from the target arm &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;. This issue
becomes more severe when data is collected adaptively, since overlap
with the target arm can deteriorate as the experimenter shifts the
data-collection mechanism in response to what they observe. As a result,
estimates from standard estimators can be skewed and heavy-tailed.&lt;/p&gt;
&lt;p&gt;Our approach to recover the asymptotic normality is done in three steps.
First, we construct an unbiased arm evaluation score of each sample,
which is a transformation of the observed outcome. Second, we average
these scores with non-uniform and data-adaptive weights, obtaining a new
estimator with controlled variance. Finally, by dividing the estimator
by its estimated standard error we obtain a test statistic that is
consistent and asymptotically normal.&lt;/p&gt;
&lt;div id=&#34;step-1-constructing-an-unbiased-arm-evaluation-score-for-each-observation&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Step 1: Constructing an unbiased arm evaluation score for each observation&lt;/h4&gt;
&lt;p&gt;The arm evaluation scoring rule should address the sampling bias issue,
which is notorious in adaptively collected data &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-nie2018adaptively&#34; role=&#34;doc-biblioref&#34;&gt;Nie et al. 2018&lt;/a&gt;)&lt;/span&gt;.
One natural method is to re-weight observed outcomes based on importance
sampling, which results in an inverse propensity score weighted (IPW)
estimator: &lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
  \label{eq:ipw}
   \widehat{Q}_T^{IPW}(w) := \frac{1}{T}\sum_{t=1}^T\widehat{\Gamma}_t^{IPW}(w), \ \mbox{where} \  \widehat{\Gamma}_t^{IPW}(w) := \frac{\mathbb{I}\{W_t=w\}}{e_t(w)} Y_t.
\end{equation}\]&lt;/span&gt; With the observation that
&lt;span class=&#34;math inline&#34;&gt;\(\mathbb{P}({W_t = w | H_{t-1}, \, Y_t(w)} = \mathbb{P}({W_t = w | H_{t-1}} = e(w;H_{t-1})\)&lt;/span&gt;,
one can immediately see the unbiasedness of
&lt;span class=&#34;math inline&#34;&gt;\(\widehat{\Gamma}_t^{IPW}(w)\)&lt;/span&gt; that has
&lt;span class=&#34;math inline&#34;&gt;\(\mathbb{E}[\widehat{\Gamma}_t^{IPW}(w)|H_{t-1}]=Q(w)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The augmented inverse propensity weighted (AIPW) estimator generalizes
the above by incorporating regression adjustment
: &lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
  \label{eq:aipw}
  \begin{split}
  &amp;amp;\widehat{Q}_T^{AIPW}(w) := \frac{1}{T}\sum_{t=1}^T \widehat{\Gamma}_t^{AIPW}(w), \ \mbox{where}\ \widehat{\Gamma}_t^{AIPW}(w) :=  \hat{\mu}_t(w) + \frac{\mathbb{I}\{W_t=w\}}{e_t(w)} \left(  Y_t - \hat{\mu}_t(w)\right).
  \end{split}
\end{equation}\]&lt;/span&gt; The symbol &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}_t(w)\)&lt;/span&gt; denotes an estimator of the
conditional mean function &lt;span class=&#34;math inline&#34;&gt;\(\mu(w) = \mathbb{E}[Y_t(w)]\)&lt;/span&gt; based on the
history &lt;span class=&#34;math inline&#34;&gt;\(H_{t-1}\)&lt;/span&gt;, but it need not be a good one—it could be biased or
even inconsistent. The second term of &lt;span class=&#34;math inline&#34;&gt;\(Y_t - \hat{\mu}_t(w)\)&lt;/span&gt; acts as a
bias-correction term: adding it preserves unbiasedness but also reduces
the variance, since the residual—when &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}_t(w)\)&lt;/span&gt; is a reasonable
estimator of &lt;span class=&#34;math inline&#34;&gt;\(\mu(w)\)&lt;/span&gt;—potentially has a smaller absolute mean as
compared to the raw outcome &lt;span class=&#34;math inline&#34;&gt;\(Y_t\)&lt;/span&gt;. We will hereby use the AIPW score
&lt;span class=&#34;math inline&#34;&gt;\(\widehat{\Gamma}_t^{AIPW}(w)\)&lt;/span&gt; for each observation &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-averaging-arm-evaluations-with-adaptive-weights&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Step 2: Averaging arm evaluations with adaptive weights&lt;/h4&gt;
&lt;p&gt;When data is collected non-adaptively but by a fixed randomization rule,
AIPW estimator is semiparametrically efficient &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hahn1998role&#34; role=&#34;doc-biblioref&#34;&gt;Hahn 1998&lt;/a&gt;)&lt;/span&gt;. However,
adaptivity makes the sampling distribution non-normal and heavy-tailed,
and the variance of the AIPW scores &lt;span class=&#34;math inline&#34;&gt;\(\widehat{\Gamma}_t^{AIPW}(w)\)&lt;/span&gt; can
vary hugely over the course of experiment. In fact, the conditional
variances &lt;span class=&#34;math inline&#34;&gt;\(\mbox{Var}(\widehat{\Gamma}_t^{AIPW}(w)|H_{t-1})\)&lt;/span&gt; depend
primarily on the behavior of the inverse propensities &lt;span class=&#34;math inline&#34;&gt;\(1/e_t(w)\)&lt;/span&gt;, which
may explode to infinity or fail to converge.&lt;/p&gt;
&lt;p&gt;To address this difficulty, we consider a generalization of the AIPW
estimator, which non-uniformly averages the unbiased scores
&lt;span class=&#34;math inline&#34;&gt;\(\widehat{\Gamma}^{AIPW}_t(w)\)&lt;/span&gt; using a sequence of &lt;em&gt;adaptive weights&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(h_t(w)\)&lt;/span&gt;. The resulting estimator is the &lt;em&gt;adaptively-weighted AIPW
estimator&lt;/em&gt;: &lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  \label{eq:aw}
  \widehat{Q}^{h}_T(w) = \frac{\sum_{t=1}^T h_t(w) \widehat{\Gamma}_t^{AIPW}(w)}{\sum_{t=1}^T h_t(w)}.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Adaptive weights &lt;span class=&#34;math inline&#34;&gt;\(h_t(w)\)&lt;/span&gt; provide an additional degree of flexibility in
controlling the variance and the tail of the sampling distribution. When
chosen appropriately, these weights compensate for erratic trajectories
of the assignment probabilities &lt;span class=&#34;math inline&#34;&gt;\(e_t(w)\)&lt;/span&gt;, stabilizing the variance of
the estimator. A natural choice of adaptive weights is to approximate
the inverse standard deviation of the &lt;span class=&#34;math inline&#34;&gt;\(\widehat{\Gamma}_t^{AIPW}(w)\)&lt;/span&gt;. In
this way each re-weighted term &lt;span class=&#34;math inline&#34;&gt;\(h_t\widehat{\Gamma}_t^{AIPW}(w)\)&lt;/span&gt; has
comparable variance, such that averaging these object may lead to a
normal limiting distribution. We shall refer it to
&lt;strong&gt;constant-allocation&lt;/strong&gt; weighting scheme since each weighted element
&lt;span class=&#34;math inline&#34;&gt;\(h_t\widehat{\Gamma}_t^{AIPW}(w)\)&lt;/span&gt; contributes to roughly the same share
of variance in the final estimator. Weights of this type were proposed
by &lt;span class=&#34;citation&#34;&gt;Luedtke and Laan (&lt;a href=&#34;#ref-luedtke2016statistical&#34; role=&#34;doc-biblioref&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt; for the purpose of estimating the expected
value of non-unique optimal policies that possibly depend on observable
covariates.&lt;/p&gt;
&lt;p&gt;More generally, to get an adaptively-weighted AIPW estimator
&lt;span class=&#34;math inline&#34;&gt;\(\widehat{Q}^{h}_T(w)\)&lt;/span&gt; that is consistent and asymptotically normal, we
require the following assumptions on our weighting schemes stated below.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1&lt;/strong&gt; (Infinite sampling).
&lt;span class=&#34;math inline&#34;&gt;\(\big(\sum_{t=1}^T h_t(w)]\big)^2 \,\big/\, \mathbb{E}\big[ \sum_{t=1}^T \frac{h^2_t(w)}{e_t(w)} \big] \xrightarrow[T \to \infty]{p} \infty.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2&lt;/strong&gt; (Variance convergence).
&lt;span class=&#34;math inline&#34;&gt;\(\sum_{t=1}^T \frac{h^2_t(w)}{e_t(w) } \,\bigg/\, \mathbb{E}\big[ \sum_{t=1}^T \frac{h^2_t(w)}{e_t(w)} \big]\xrightarrow[T \to \infty]{L_p} 1\)&lt;/span&gt;,
for some &lt;span class=&#34;math inline&#34;&gt;\(p&amp;gt;0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 3&lt;/strong&gt; (Bounded moments).
&lt;span class=&#34;math inline&#34;&gt;\(\sum_{t=1}^T \frac{h^{2 + \delta}(w)}{e^{1 + \delta}(w) } \,\Big/\, \mathbb{E}\Big[ \Big(\sum_{t=1}^T \frac{h^2_t(w)}{e_t(w)} \Big)^{1 +\delta/2}\Big]\xrightarrow[T \to \infty]{p} 0\)&lt;/span&gt;,
for some &lt;span class=&#34;math inline&#34;&gt;\(\delta&amp;gt;0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Above, Assumption 1 requires that the effective sample size after
adaptive weighting goes to infinity, which implies that the estimator
converges. Assumption 3 is a Lyapunov-type regularity condition on the
weights, which controls higher moments of the distribution. Assumption 2
is the more subtle condition that standard estimators such as AIPW
estimator (i.e., &lt;span class=&#34;math inline&#34;&gt;\(h_t(w) \equiv 1\)&lt;/span&gt;) often fail to satisfy. We refer
interesting readers to our paper &lt;span class=&#34;citation&#34;&gt;Hadad et al. (&lt;a href=&#34;#ref-hadad2021confidence&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; for a recipe on
building weights that satisfy Assumption 2. In particular, we note a
&lt;strong&gt;two-point allocation&lt;/strong&gt; weighting scheme when the assignment
probabilities &lt;span class=&#34;math inline&#34;&gt;\(e_t\)&lt;/span&gt; reflect the experimenter’s belief on arm optimality
(as is the case for Thompson sampling). This weighting scheme allows us
to downweight samples with small propensities more boldly but still
preserve the asymptotic normality, and therefore often merits smaller
variance and tighter confidence intervals as compared to the
constant-allocation scheme.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3-estimating-standard-error-and-constructing-a-test-statistic&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Step 3: Estimating standard error and constructing a test statistic&lt;/h4&gt;
&lt;p&gt;With the evaluation weights discussed in step 2, when normalized by an
estimate of its standard deviation, the adaptively-weighted AIPW
estimator has a centered and normal asymptotic distribution. Similar
``self-normalization’’ schemes are often key to martingale central
limit theorems &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-pena2008self&#34; role=&#34;doc-biblioref&#34;&gt;Peña, Lai, and Shao 2008&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;. Suppose that we observe arms &lt;span class=&#34;math inline&#34;&gt;\(W_t\)&lt;/span&gt; and rewards
&lt;span class=&#34;math inline&#34;&gt;\(Y_t=Y_t(W_t)\)&lt;/span&gt;, and that the underlying potential outcomes
&lt;span class=&#34;math inline&#34;&gt;\((Y_t(w))_{w \in \mathcal{W}}\)&lt;/span&gt; are independent and identically
distributed with nonzero variance, and satisfy
&lt;span class=&#34;math inline&#34;&gt;\(\mathbb{E}|Y_{t}(w)|^{2+\delta} &amp;lt; \infty\)&lt;/span&gt; for some &lt;span class=&#34;math inline&#34;&gt;\(\delta &amp;gt; 0\)&lt;/span&gt; and all
&lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;. Suppose that the assignment probabilities &lt;span class=&#34;math inline&#34;&gt;\(e_t(w)\)&lt;/span&gt; are strictly
positive and let &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}_t(w)\)&lt;/span&gt; be any history-adapted estimator of
&lt;span class=&#34;math inline&#34;&gt;\(Q(w)\)&lt;/span&gt; that is bounded and that converges almost-surely to some constant
&lt;span class=&#34;math inline&#34;&gt;\(\mu_{\infty}(w)\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(h_t(w)\)&lt;/span&gt; be non-negative history-adapted weights
satisfying Assumptions 1-3. Suppose that either &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}_t(w)\)&lt;/span&gt; is
consistent or &lt;span class=&#34;math inline&#34;&gt;\(e_t(w)\)&lt;/span&gt; has a limit &lt;span class=&#34;math inline&#34;&gt;\(e_{\infty}(w) \in [0, \, 1]\)&lt;/span&gt;, i.e.,
either &lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
     \label{eq:e_mu_alternative}
      \hat{\mu}_t(w) \xrightarrow[t \to \infty]{a.s.} Q(w)  \quad \text{or}
      \quad e_t(w)  \xrightarrow[t \to \infty]{a.s.} e_{\infty}(w)
\end{equation}\]&lt;/span&gt; Then, for any arm &lt;span class=&#34;math inline&#34;&gt;\(w \in \mathcal{W}\)&lt;/span&gt;, the
adaptively-weighted estimator &lt;span class=&#34;math inline&#34;&gt;\(\widehat{Q}_T^{h}(w)\)&lt;/span&gt; is consistent for
the arm value &lt;span class=&#34;math inline&#34;&gt;\(Q(w)\)&lt;/span&gt;, and the following studentized statistic is
asymptotically normal: &lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
  \begin{aligned}
    \label{eq:clt}
    &amp;amp;\frac{\widehat{Q}_T^{h}(w) - Q(w)}{\widehat{V}_T^{h}(w)^{\frac{1}{2}}} \xrightarrow{d} \mathcal{N}(0, 1),
    \ \ \ \text{where} \ \widehat{V}_T^{h}(w) := \frac{\sum_{t=1}^T h^2_t(w) \left( \widehat{\Gamma}_t(w) - \widehat{Q}_T(w) \right)^2}{\left( \sum_{t=1}^T h_t(w) \right)^2}.
  \end{aligned}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;simulations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulations&lt;/h2&gt;
&lt;p&gt;We consider three simulation settings, each with &lt;span class=&#34;math inline&#34;&gt;\(K = 3\)&lt;/span&gt; arms that yield
rewards observed with additive &lt;span class=&#34;math inline&#34;&gt;\(\text{uniform}[-1, 1]\)&lt;/span&gt; noise. The
settings vary in the difference between the arm values. In the
&lt;em&gt;no-signal&lt;/em&gt; case, we set arm values to &lt;span class=&#34;math inline&#34;&gt;\(Q(w) = 1\)&lt;/span&gt; for all
&lt;span class=&#34;math inline&#34;&gt;\(w \in \{1, 2, 3\}\)&lt;/span&gt;; in the &lt;em&gt;low signal&lt;/em&gt; case, we set
&lt;span class=&#34;math inline&#34;&gt;\(Q(w) = 0.9+ 0.1w\)&lt;/span&gt;; and &lt;em&gt;high signal&lt;/em&gt; case we set &lt;span class=&#34;math inline&#34;&gt;\(Q(w) = 0.5 + 0.5w\)&lt;/span&gt;.
During the experiment, treatment is assigned by a modified Thompson
sampling method: first, tentative assignment probabilities are computed
via Thompson sampling with normal likelihood and normal
prior&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-russo2018tutorial&#34; role=&#34;doc-biblioref&#34;&gt;D. J. Russo et al. 2018&lt;/a&gt;)&lt;/span&gt;; they are then adjusted to impose the lower
bound &lt;span class=&#34;math inline&#34;&gt;\(e_t(w) \geq (1/K)t^{-0.7}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We show comparison among four point estimators of arm values &lt;span class=&#34;math inline&#34;&gt;\(Q(w)\)&lt;/span&gt;: the
sample mean, the AIPW estimator with uniform weights (labeled as
“unweighted AIPW”), and the adaptively-weighted AIPW estimator with
constant and two-point allocation schemes. For the AIPW-based
estimators, we use the same formula given in our theorem to construct
confidence intervals. For the sample mean we use the usual variance
estimate
&lt;span class=&#34;math inline&#34;&gt;\(\smash{\widehat{V}^{AVG}(w) := T_w^{-2} \sum_{t: W_t = w}^{T} (Y_t - \widehat{Q}^{AVG}_T(w))^2}\)&lt;/span&gt;.
Approximate normality is not theoretically justified for the unweighted
AIPW estimator or for the sample mean. We also consider non-asymptotic
confidence intervals for the sample mean, based on the method of
time-uniform confidence sequences described in &lt;span class=&#34;citation&#34;&gt;Howard et al. (&lt;a href=&#34;#ref-howard2021uniform&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt;. We
refer interesting readers to our paper for more results on estimating
other arms as well as the contrast between arms &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hadad2021confidence&#34; role=&#34;doc-biblioref&#34;&gt;Hadad et al. 2021&lt;/a&gt; )&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The below panel demonstrates that we are able to estimate the “good” arm
value &lt;span class=&#34;math inline&#34;&gt;\(Q(3)\)&lt;/span&gt; with considerable ease in high- and low-signal settings, in
that all methods produce point estimates with negligible bias and small
root mean-squared error, and moreover attain roughly correct coverage
for large &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;. Conversely, estimating the “bad” arm value &lt;span class=&#34;math inline&#34;&gt;\(Q(1)\)&lt;/span&gt; is
challenging. Although the AIPW estimator is unbiased, it performs very
poorly in terms of RMSE and confidence interval width. In the low and
high signal case, its problem is that it does not take into account the
fact that the bad arm is undersampled, so its variance is high. Of our
two adaptively-weighted AIPW estimators, two-point allocation better
controls the variance of bad arm estimates by more aggressively
downweighting `unlikely’ observations associated with large inverse
propensity weights; this results in smaller RMSE and tighter confidence
intervals. For the sample-mean estimator, naive confidence intervals
based on the normal approximation are invalid, with severe
under-coverage when there’s little or no signal. On the other hand, the
non-asymptotic confidence sequences of &lt;span class=&#34;citation&#34;&gt;Howard et al. (&lt;a href=&#34;#ref-howard2021uniform&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; are
conservative and often wide.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-10-11-inference-on-adaptively-collected-data_files/arm_values.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;extensions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Extensions&lt;/h2&gt;
&lt;p&gt;One direct extension is to apply the above adaptive weighting technique
to evaluating policies on populations with hetegeneous outcomes, using
data that is collected adaptively via contextual bandit algorithms. In
&lt;span class=&#34;citation&#34;&gt;Zhan, Hadad, et al. (&lt;a href=&#34;#ref-zhan2021off&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt;, we carefully choose adaptive weights to accommodate the
variances of AIPW scores that may differ not only over time but also
across the context space. The resulting estimator further reduces
estimation variance.&lt;/p&gt;
&lt;p&gt;Aside from policy evaluation, learning the optimal treatment assignment
policies using adaptive data is also desirable, which enables
personalized decision making in a wide variety of domains. In
&lt;span class=&#34;citation&#34;&gt;Zhan, Ren, et al. (&lt;a href=&#34;#ref-zhan2021policy&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt;, we establish the first-of-the-kind regret lower bound
that characterizes the fundamental difficulty of policy learning with
adaptive data. We then propose an algorithm based on re-weighted AIPW
estimators and show that it is minimax optimal with the best weighting
scheme.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;about-the-author-and-article&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;About the author and article&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ruohanzhan.github.io/&#34;&gt;Ruohan Zhan&lt;/a&gt;: postdoctoral fellow at
the Stanford Graduate School of Business, incoming Assistant
Professor at the Hong Kong University of Science and Technology.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Based on:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Vitor Hadad, David A. Hirschberg, Ruohan Zhan, Stefan Wager and Susan
Athey. Confidence intervals for policy evaluation in adaptive
experiments. Proceedings of the National Academy of Sciences, April 5,
2021, 118 (15) e2014602118, &lt;a href=&#34;https://doi.org/10.1073/pnas.2014602118&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1073/pnas.2014602118&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;bibliography&#34; class=&#34;section level3 unnumbered&#34;&gt;
&lt;h3&gt;Bibliography&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-agrawal2013thompson&#34; class=&#34;csl-entry&#34;&gt;
Agrawal, Shipra, and Navin Goyal. 2013. &lt;span&gt;“Thompson Sampling for Contextual Bandits with Linear Payoffs.”&lt;/span&gt; In &lt;em&gt;International Conference on Machine Learning&lt;/em&gt;, 127–35. PMLR.
&lt;/div&gt;
&lt;div id=&#34;ref-bertsimas2017personalized&#34; class=&#34;csl-entry&#34;&gt;
Bertsimas, Dimitris, Nathan Kallus, Alexander M Weinstein, and Ying Daisy Zhuo. 2017. &lt;span&gt;“Personalized Diabetes Management Using Electronic Medical Records.”&lt;/span&gt; &lt;em&gt;Diabetes Care&lt;/em&gt; 40 (2): 210–17.
&lt;/div&gt;
&lt;div id=&#34;ref-hadad2021confidence&#34; class=&#34;csl-entry&#34;&gt;
Hadad, Vitor, David A Hirshberg, Ruohan Zhan, Stefan Wager, and Susan Athey. 2021. &lt;span&gt;“Confidence Intervals for Policy Evaluation in Adaptive Experiments.”&lt;/span&gt; &lt;em&gt;Proceedings of the National Academy of Sciences&lt;/em&gt; 118 (15).
&lt;/div&gt;
&lt;div id=&#34;ref-hahn1998role&#34; class=&#34;csl-entry&#34;&gt;
Hahn, Jinyong. 1998. &lt;span&gt;“On the Role of the Propensity Score in Efficient Semiparametric Estimation of Average Treatment Effects.”&lt;/span&gt; &lt;em&gt;Econometrica&lt;/em&gt;, 315–31.
&lt;/div&gt;
&lt;div id=&#34;ref-howard2021uniform&#34; class=&#34;csl-entry&#34;&gt;
Howard, Steven R, Aaditya Ramdas, Jon McAuliffe, and Jasjeet Sekhon. 2021. &lt;span&gt;“Time-Uniform, Nonparametric, Non-Asymptotic Confidence Sequences.”&lt;/span&gt; &lt;em&gt;The Annals of Statistics&lt;/em&gt; Forthcoming.
&lt;/div&gt;
&lt;div id=&#34;ref-lai1985asymptotically&#34; class=&#34;csl-entry&#34;&gt;
Lai, Tze Leung, and Herbert Robbins. 1985. &lt;span&gt;“Asymptotically Efficient Adaptive Allocation Rules.”&lt;/span&gt; &lt;em&gt;Advances in Applied Mathematics&lt;/em&gt; 6 (1): 4–22.
&lt;/div&gt;
&lt;div id=&#34;ref-luedtke2016statistical&#34; class=&#34;csl-entry&#34;&gt;
Luedtke, Alexander R, and Mark J van der Laan. 2016. &lt;span&gt;“Statistical Inference for the Mean Outcome Under a Possibly Non-Unique Optimal Treatment Strategy.”&lt;/span&gt; &lt;em&gt;Annals of Statistics&lt;/em&gt; 44 (2): 713.
&lt;/div&gt;
&lt;div id=&#34;ref-murphy2005experimental&#34; class=&#34;csl-entry&#34;&gt;
Murphy, Susan A. 2005. &lt;span&gt;“An Experimental Design for the Development of Adaptive Treatment Strategies.”&lt;/span&gt; &lt;em&gt;Statistics in Medicine&lt;/em&gt; 24 (10): 1455–81.
&lt;/div&gt;
&lt;div id=&#34;ref-nie2018adaptively&#34; class=&#34;csl-entry&#34;&gt;
Nie, Xinkun, Xiaoying Tian, Jonathan Taylor, and James Zou. 2018. &lt;span&gt;“Why Adaptively Collected Data Have Negative Bias and How to Correct for It.”&lt;/span&gt; In &lt;em&gt;Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics&lt;/em&gt;, edited by Amos Storkey and Fernando Perez-Cruz, 84:1261–69. New York: PMLR. &lt;a href=&#34;http://proceedings.mlr.press/v84/nie18a.html&#34;&gt;http://proceedings.mlr.press/v84/nie18a.html&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-pena2008self&#34; class=&#34;csl-entry&#34;&gt;
Peña, Victor H de la, Tze Leung Lai, and Qi-Man Shao. 2008. &lt;em&gt;Self-Normalized Processes: Limit Theory and Statistical Applications&lt;/em&gt;. Berlin Heidelberg: Springer-Verlag.
&lt;/div&gt;
&lt;div id=&#34;ref-russo2020simple&#34; class=&#34;csl-entry&#34;&gt;
Russo, Daniel. 2020. &lt;span&gt;“Simple Bayesian Algorithms for Best-Arm Identification.”&lt;/span&gt; &lt;em&gt;Operations Research&lt;/em&gt; 68 (6): 1625–47. &lt;a href=&#34;https://doi.org/10.1287/opre.2019.1911&#34;&gt;https://doi.org/10.1287/opre.2019.1911&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-russo2018tutorial&#34; class=&#34;csl-entry&#34;&gt;
Russo, Daniel J., Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen. 2018. &lt;span&gt;“A Tutorial on Thompson Sampling.”&lt;/span&gt; &lt;em&gt;Found. Trends Mach. Learn.&lt;/em&gt; 11 (1): 1–96. &lt;a href=&#34;https://doi.org/10.1561/2200000070&#34;&gt;https://doi.org/10.1561/2200000070&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-schnabel2016recommendations&#34; class=&#34;csl-entry&#34;&gt;
Schnabel, Tobias, Adith Swaminathan, Ashudeep Singh, Navin Chandak, and Thorsten Joachims. 2016. &lt;span&gt;“Recommendations as Treatments: Debiasing Learning and Evaluation.”&lt;/span&gt; In &lt;em&gt;International Conference on Machine Learning&lt;/em&gt;, 1670–79. PMLR.
&lt;/div&gt;
&lt;div id=&#34;ref-zhan2021off&#34; class=&#34;csl-entry&#34;&gt;
Zhan, Ruohan, Vitor Hadad, David A Hirshberg, and Susan Athey. 2021. &lt;span&gt;“Off-Policy Evaluation via Adaptive Weighting with Data from Contextual Bandits.”&lt;/span&gt; In &lt;em&gt;Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp;amp; Data Mining&lt;/em&gt;, 2125–35.
&lt;/div&gt;
&lt;div id=&#34;ref-zhan2021policy&#34; class=&#34;csl-entry&#34;&gt;
Zhan, Ruohan, Zhimei Ren, Susan Athey, and Zhengyuan Zhou. 2021. &lt;span&gt;“Policy Learning with Adaptively Collected Data.”&lt;/span&gt; &lt;em&gt;arXiv Preprint arXiv:2105.02344&lt;/em&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Nonparametric inference based on statistical depth</title>
      <link>https://youngstats.github.io/post/2022/10/03/nonparametric-inference-based-on-statistical-depth/</link>
      <pubDate>Mon, 03 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/10/03/nonparametric-inference-based-on-statistical-depth/</guid>
      <description>


&lt;p&gt;&lt;strong&gt;Nonparametric inference based on statistical depth&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Monday, October 17th, 7:00 PT / 10:00 ET / 16:00 CET&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-10-03-nonparametric-inference-based-on-statistical-depth_files/cover2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The notion of center of an object, be it a set of observations, a physical object or a random variable, is difficult to define. This motivated the development of general ways to measure centrality via depth functions. Such mappings allow for comparing relative centrality of two locations and, consequently, providing a center-outward ordering. Many such mappings have been introduced in the literature in recent decades and this is subject to intense research in, among other, nonparametric statistics and functional data analysis.&lt;/p&gt;
&lt;p&gt;In the webinar, selected statisticians will present their recent works and elaborate on different aspects of this topic.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Monday, October 17th, 7:00 PT / 10:00 ET / 16:00 CET&lt;/li&gt;
&lt;li&gt;Online, via Zoom. The registration form is available &lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSdsX2FS-Dpmris0W1yfUbwYOUdXfA5qoY2T4ujP5XAuLGLFwA/viewform?vc=0&amp;amp;c=0&amp;amp;w=1&amp;amp;flr=0&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://dimitri.konen.web.ulb.be/&#34;&gt;Dimitri Konen&lt;/a&gt;, Université Libre de Bruxelles (ULB), Belgium, &lt;embed&gt; link to &lt;a href=&#34;https://drive.google.com/file/d/10D3bNPuAmzuUzJmaL1tE7788p_G_pJTu/view&#34;&gt;Presentation&lt;/a&gt;. &lt;/embed&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www2.karlin.mff.cuni.cz/~nagy/index.php&#34;&gt;Stanislav Nagy&lt;/a&gt;, Faculty of Mathematics and Physics, Charles University, Czech Republic, &lt;embed&gt; link to &lt;a href=&#34;https://drive.google.com/file/d/1yrouPnanWb29fQxj7-8yDrZpM37_WNZa/view&#34;&gt;Presentation&lt;/a&gt;. &lt;/embed&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://perso.telecom-paristech.fr/mozharovskyi/&#34;&gt;Pavlo Mozharovskyi&lt;/a&gt;, LTCI, Telecom Paris, Institut Polytechnique de Paris, France&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant: &lt;a href=&#34;https://sites.google.com/site/germainvanbever&#34;&gt;Germain Van Bever&lt;/a&gt;, Université de Namur, Belgium, &lt;embed&gt; link to &lt;a href=&#34;https://drive.google.com/file/d/1L-hbQ8Y0rh0TXeS9muNoiyJzhRL7oI3J/view&#34;&gt;Presentation&lt;/a&gt;. &lt;/embed&gt;&lt;/p&gt;
&lt;p&gt;The webinar is part of YoungStatS project of the Young Statisticians Europe initiative (FENStatS) supported by the Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics (IMS).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you missed this webinar, you can &lt;a href=&#34;https://www.youtube.com/watch?v=FxSWsoOi5jE&#34;&gt;watch the recording on our YouTube channel&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Recent challenges in model specification testing based on different data structures</title>
      <link>https://youngstats.github.io/post/2022/09/29/recent-challenges-in-model-specification-testing-based-on-different-data-structures/</link>
      <pubDate>Thu, 29 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/09/29/recent-challenges-in-model-specification-testing-based-on-different-data-structures/</guid>
      <description>


&lt;p&gt;&lt;strong&gt;Recent challenges in model specification testing based on different
data structures&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Wednesday, November 9th, 8:00 PT / 11:00 ET / 17:00 CET&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-09-29-recent-challenges-in-model-specification-testing-based-on-different-data-structures_files/Slika1-horz.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Model specification testing is one of the essential methodological tasks
in statistics. Recently, with the development of different data
structures, envisioning concepts from classical data setups to other
environments becomes very important.&lt;/p&gt;
&lt;p&gt;In the webinar, selected young statisticians will present their recent
works and elaborate on different aspects of this topic.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wednesday, November 9th, 8:00 PT / 11:00 ET / 17:00 CET&lt;/li&gt;
&lt;li&gt;Online, via Zoom. The registration form is available
&lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLScwfeWQzP-LQjweX0G9dg50G9BzTmmdVqqpmwlFkG94_3pgpQ/viewform?vc=0&amp;amp;c=0&amp;amp;w=1&amp;amp;flr=0&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://warwick.ac.uk/fac/sci/statistics/staff/academic-research/berrett/&#34;&gt;Thomas
Berrett&lt;/a&gt;,
University of Warwick, United Kingdom: &lt;em&gt;Hypothesis testing under
local differential privacy&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.math.uni-frankfurt.de/~gerstenb/&#34;&gt;Julian G.
Gerstenberg&lt;/a&gt;, Goethe
Universität Frankfurt, Institute for Mathematics, Germany: &lt;em&gt;On the
exchangeability theory and its applications in non-parametric
statistics&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Marija-Cuparic&#34;&gt;Marija
Cuparić&lt;/a&gt;,
University of Belgrade, Serbia: &lt;em&gt;Goodness-of-fit tests for randomly
censored data&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Charl-Pretorius&#34;&gt;Charl
Pretorius&lt;/a&gt;,
University of the Free State, Bloemfontein, South Africa:
&lt;em&gt;Goodness-of-fit tests for multivariate&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;&lt;em&gt;-stable
distributions with application to MGARCH models&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant: &lt;a href=&#34;http://poincare.matf.bg.ac.rs/~bojana/en/&#34;&gt;Bojana
Milošević&lt;/a&gt;, University of
Belgrade, Serbia&lt;/p&gt;
&lt;p&gt;The webinar is part of YoungStatS project of the Young Statisticians
Europe initiative (FENStatS) supported by the Bernoulli Society for
Mathematical Statistics and Probability and the Institute of
Mathematical Statistics (IMS).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Non-Homogeneous Poisson Process Intensity Modeling and Estimation using Measure Transport</title>
      <link>https://youngstats.github.io/post/2022/09/19/non-homogeneous-poisson-process-intensity-modeling-and-estimation-using-measure-transport/</link>
      <pubDate>Mon, 19 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/09/19/non-homogeneous-poisson-process-intensity-modeling-and-estimation-using-measure-transport/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;A NHPP defined on &lt;span class=&#34;math inline&#34;&gt;\({\cal S} \subset \mathbb{R}^{d}\)&lt;/span&gt; can be fully
characterized through its intensity function &lt;span class=&#34;math inline&#34;&gt;\(\lambda: {\cal S} \rightarrow [0, \infty)\)&lt;/span&gt;.
We present a general model for the intensity function of a non-homogeneous Poisson
process using measure transport. The model finds its roots in transportation of
probability measure &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-Marzouk2016&#34; role=&#34;doc-biblioref&#34;&gt;Marzouk et al. 2016&lt;/a&gt;)&lt;/span&gt;, an approach that has gained
popularity recently for its ability to model arbitrary probability
density functions. The basic idea of this approach is to construct a
“transport map” between the complex, unknown, intensity function of
interest, and a simpler, known, reference intensity function.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Measure Transport.&lt;/strong&gt; Consider two probability measures &lt;span class=&#34;math inline&#34;&gt;\(\mu_0(\cdot)\)&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(\mu_1(\cdot)\)&lt;/span&gt; defined on &lt;span class=&#34;math inline&#34;&gt;\({\cal X}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\({\cal Z}\)&lt;/span&gt;, respectively. A
transport map &lt;span class=&#34;math inline&#34;&gt;\(T: {\cal X} \rightarrow {\cal Z}\)&lt;/span&gt; is said to push forward
&lt;span class=&#34;math inline&#34;&gt;\(\mu_0(\cdot)\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\mu_1(\cdot)\)&lt;/span&gt; (written compactly as
&lt;span class=&#34;math inline&#34;&gt;\(T_{\#\mu_0}(\cdot) = \mu_1(\cdot)\)&lt;/span&gt;) if and only if &lt;span class=&#34;math display&#34;&gt;\[\begin{eqnarray}
\label{transport_map} \mu_1(B) = \mu_0(T^{-1}(B)), \quad \mbox{for any
Borel subset } B \subset {\cal Z}. \end{eqnarray}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\({\cal X}, {\cal Z} \subseteq \mathbb{R}^{d}\)&lt;/span&gt;, and that both
&lt;span class=&#34;math inline&#34;&gt;\(\mu_0(\cdot)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mu_1(\cdot)\)&lt;/span&gt; are absolutely continuous with respect
to the Lebesgue measure on &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^{d}\)&lt;/span&gt;, with densities
&lt;span class=&#34;math inline&#34;&gt;\(d\mu_0(x) = f_0(x) d x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(d\mu_1(z) = f_1(z) d z\)&lt;/span&gt;, respectively.
Furthermore, assume that the map &lt;span class=&#34;math inline&#34;&gt;\(T(\cdot)\)&lt;/span&gt; is bijective differentiable
with a differentiable inverse &lt;span class=&#34;math inline&#34;&gt;\(T^{-1}(\cdot)\)&lt;/span&gt; (i.e., assume that
&lt;span class=&#34;math inline&#34;&gt;\(T(\cdot)\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(C^{1}\)&lt;/span&gt; diffeomorphism), then we have &lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\label{eq:transport} f_0(x) = f_1(T(x)) \|\mbox{det}(\nabla T(x))\|,
\quad x \in {\cal X}. \end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Triangular Map.&lt;/strong&gt; One particular type of transport map is an
, that is, &lt;span class=&#34;math display&#34;&gt;\[\begin{eqnarray}
\label{triangular_map} T(x) = (T^{(1)}(x^{(1)}), T^{(2)}(x^{(1)},
x^{(2)}), \ldots, T^{(d)}(x^{(1)}, \ldots, x^{(d)}))&amp;#39;,\quad x \in {\cal
X}, \end{eqnarray}\]&lt;/span&gt; where, for &lt;span class=&#34;math inline&#34;&gt;\(k = 1,\dots, d,\)&lt;/span&gt; one has that
&lt;span class=&#34;math inline&#34;&gt;\(T^{(k)}(x^{(1)}, \ldots, x^{(k)})\)&lt;/span&gt; is monotonically increasing in
&lt;span class=&#34;math inline&#34;&gt;\(x^{(k)}\)&lt;/span&gt;. In particular, the Jacobian matrix of an increasing
triangular map, if it exists, is triangular with positive entries on its
diagonal.&lt;/p&gt;
&lt;p&gt;Various approaches to parameterize an increasing triangular map have been proposed (see, for example, &lt;span class=&#34;citation&#34;&gt;Germain et al. (&lt;a href=&#34;#ref-Germain2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;,
&lt;span class=&#34;citation&#34;&gt;Dinh, Krueger, and Bengio (&lt;a href=&#34;#ref-Dinh2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Dinh, Sohl-Dickstein, and Bengio (&lt;a href=&#34;#ref-Dinh2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;). One class of parameterizations is based on
the so-called “conditional networks” (&lt;span class=&#34;citation&#34;&gt;Papamakarios, Pavlakou, and Murray (&lt;a href=&#34;#ref-Papamakarios2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; and
&lt;span class=&#34;citation&#34;&gt;Huang et al. (&lt;a href=&#34;#ref-Huang2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;): &lt;span class=&#34;math display&#34;&gt;\[\begin{eqnarray} T_1^{(1)}(x^{(1)})
&amp;amp;=&amp;amp; S_1^{(1)}(x^{(1)}; \theta_{11}), \nonumber \\ T_1^{(k)}(x^{(1)},
\ldots, x^{(k)}) &amp;amp;=&amp;amp; S_1^{(k)}( x^{(k)}; \theta_{1k}(x^{(1)}, \ldots,
x^{(k-1)}; \vartheta_{1k}) ), \quad k = 2,\dots,d, \end{eqnarray}\]&lt;/span&gt; for
&lt;span class=&#34;math inline&#34;&gt;\(x \in {\cal X}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\theta_{1k}(x^{(1)}, \ldots, x^{(k-1)}; \vartheta_{1k}), k = 2,\dots,d\)&lt;/span&gt; is the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;th “conditional
network” that takes &lt;span class=&#34;math inline&#34;&gt;\(x^{(1)}, \ldots, x^{(k-1)}\)&lt;/span&gt; as inputs and is
parameterized by &lt;span class=&#34;math inline&#34;&gt;\(\vartheta_{1k}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(S_1^{(k)}(\cdot)\)&lt;/span&gt; is generally a
very simple univariate function of &lt;span class=&#34;math inline&#34;&gt;\(x^{(k)}\)&lt;/span&gt;, but with parameters that
depend in a relatively complex manner on &lt;span class=&#34;math inline&#34;&gt;\(x^{(1)}, \ldots, x^{(k-1)}\)&lt;/span&gt;
through the network. The class of neural autoregressive flows, proposed
by &lt;span class=&#34;citation&#34;&gt;Huang et al. (&lt;a href=&#34;#ref-Huang2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;, offers more flexibility where the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th component
of the map has the form &lt;span class=&#34;math display&#34;&gt;\[\begin{eqnarray} \label{uni_flow} S_1^{(k)}(
x^{(k)}; \theta_{1k} ) = \sigma^{-1} \Big( \sum_{i=1}^{M} w_{1ki}
\sigma( a_{1ki} x^{(k)} + b_{1ki}) \Big), \end{eqnarray}\]&lt;/span&gt; where
&lt;span class=&#34;math inline&#34;&gt;\(\sigma^{-1}(\cdot)\)&lt;/span&gt; is the logit function, and &lt;span class=&#34;math inline&#34;&gt;\(w_{1ki}\)&lt;/span&gt; is subject to
the constraint &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^{M} w_{1ki} = 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Composition of Increasing Triangular Maps.&lt;/strong&gt; Composition of Increasing
Triangular Mapsn does not break the required bijectivity of &lt;span class=&#34;math inline&#34;&gt;\(T(\cdot)\)&lt;/span&gt;,
since a bijective function of a bijective function is itself bijective.
Computations also remain tractable, since the determinant of the
gradient of the composition is simply the product of the determinants of
the individual gradients. Specifically, consider two increasing
triangular maps &lt;span class=&#34;math inline&#34;&gt;\(T_1(\cdot)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(T_2(\cdot)\)&lt;/span&gt;, each constructed using
the neural network approach described above. The composition
&lt;span class=&#34;math inline&#34;&gt;\(T_2 \circ T_1 (\cdot)\)&lt;/span&gt; is bijective, and its gradient at some
&lt;span class=&#34;math inline&#34;&gt;\(x \in \cal X\)&lt;/span&gt; has determinant,
&lt;span class=&#34;math display&#34;&gt;\[ \mbox{det}(\nabla T_2 \circ T_1(x)) = (\mbox{det} (\nabla T_1(x)))(\mbox{det} (\nabla T_2(T_1(x)))).\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;intensity-modeling-and-estimation-via-measure-transport&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Intensity Modeling and Estimation via Measure Transport&lt;/h2&gt;
&lt;p&gt; Consider a NHPP &lt;span class=&#34;math inline&#34;&gt;\({\cal P}\)&lt;/span&gt; defined on a bounded
domain &lt;span class=&#34;math inline&#34;&gt;\({\cal X} \subset \mathbb{R}^{d}\)&lt;/span&gt;. The density of a Poisson
process with respect to the density of the unit rate Poisson process is
given by &lt;span class=&#34;math display&#34;&gt;\[\begin{align} f_{\lambda}(X) &amp;amp;= \exp(\|{\cal X}\| -
\mu_{\lambda}({\cal X})) \prod_{x \in X} \lambda(x) \nonumber \\ &amp;amp;=
\exp \Big(\int_{{\cal X}} (\lambda(x) - 1)dx + \sum_{x \in X}
\log \lambda(x) \Big) \label{fdensity}. \end{align}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(|B|\)&lt;/span&gt; denote
the Lebesgue measure of a bounded set &lt;span class=&#34;math inline&#34;&gt;\(B \subset \mathbb{R}^{d}\)&lt;/span&gt;, and
let &lt;span class=&#34;math inline&#34;&gt;\(X \equiv \{x_1, \ldots, x_n\},\)&lt;/span&gt; where
&lt;span class=&#34;math inline&#34;&gt;\(x_i \in {\cal X}, i = 1,\ldots,n,\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n \ge 1,\)&lt;/span&gt; be a realization of
&lt;span class=&#34;math inline&#34;&gt;\({\cal P}\)&lt;/span&gt;. The unknown intensity function is estimated by a maximum
likelihood approach, which is equivalent to minimizing the
Kullback–Leibler (KL) divergence &lt;span class=&#34;math inline&#34;&gt;\(D_{KL}(p\|\|q) = \int p(x) \log (p(x)/ q(x))dx\)&lt;/span&gt; between the true density and the estimate:
&lt;span class=&#34;math display&#34;&gt;\[
\hat{\lambda}(\cdot) = \arg\min_{\rho(\cdot) \in {\cal A} }{{D_{KL}(f_{\lambda}\|f_{\rho})}}
\]&lt;/span&gt; where
&lt;span class=&#34;math inline&#34;&gt;\({\cal A}\)&lt;/span&gt; is some set of intensity functions, and &lt;span class=&#34;math inline&#34;&gt;\(f_\rho(\cdot)\)&lt;/span&gt; is
the density of a NHPP with intensity function &lt;span class=&#34;math inline&#34;&gt;\(\rho(\cdot)\)&lt;/span&gt; taken with
respect to the density of the unit rate Poisson process.&lt;/p&gt;
&lt;p&gt;In order to apply the measure transport approach to intensity function
estimation, we first define &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\rho}(\cdot) = \rho(\cdot) / \mu_{\rho}({\cal X})\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\lambda}(x) = \lambda(x) / \mu_{\lambda}({\cal X}),\)&lt;/span&gt; so that
&lt;span class=&#34;math inline&#34;&gt;\(\tilde{\rho}(\cdot)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\lambda}(\cdot)\)&lt;/span&gt; are valid density
functions with respect to Lebesgue measure. The KL divergence
&lt;span class=&#34;math inline&#34;&gt;\(D_{KL}(f_{\lambda}\|f_{\rho})\)&lt;/span&gt; can be written in terms of process
densities as follows,
&lt;span class=&#34;math display&#34;&gt;\[
D_{KL}(f_{\lambda}\|f_{\rho}) = \mu_{\rho}({\cal X)} -
\mu_{\lambda}({\cal X}) \int_{{\cal X}} \tilde{\lambda}(x) \log
\tilde{\rho}(x)dx - \mu_{\lambda}({\cal X}) \log
\mu_{\rho}({\cal X}) + \textrm{const.},
\]&lt;/span&gt; where “const.” captures other terms not dependent on
&lt;span class=&#34;math inline&#34;&gt;\(\mu_{\rho}({\cal X})\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\rho}(\cdot)\)&lt;/span&gt;. After further
approximations, we obtain the following optimization problem
&lt;span class=&#34;math display&#34;&gt;\[
\hat{\tilde{\lambda}}(\cdot) = \arg\min_{\tilde{\rho}(\cdot) \in {\cal \tilde{A}}} {-{\sum_{i=1}^{n} \log \tilde{\rho}(x_{i})}},
\]&lt;/span&gt; where
now &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\cal A}\)&lt;/span&gt; is some set of process densities.&lt;/p&gt;
&lt;p&gt;We model the process density &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\rho}(\cdot)\)&lt;/span&gt; using the transportation of
probability measure approach. Specifically, we seek a diffeomorphism
&lt;span class=&#34;math inline&#34;&gt;\(T: {\cal X} \rightarrow {\cal Z}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\({\cal Z}\)&lt;/span&gt; need not be the
same as &lt;span class=&#34;math inline&#34;&gt;\({\cal X}\)&lt;/span&gt;, such that
&lt;span class=&#34;math display&#34;&gt;\[ \tilde{\rho}(x) = \eta(T(x)) | \mbox{det}\nabla T(x)|, \quad x \in {\cal X},\]&lt;/span&gt;where
&lt;span class=&#34;math inline&#34;&gt;\(\eta(\cdot)\)&lt;/span&gt; is some simple reference density on &lt;span class=&#34;math inline&#34;&gt;\({\cal Z}\)&lt;/span&gt;, and
&lt;span class=&#34;math inline&#34;&gt;\(|\mbox{det}\nabla T(\cdot)| &amp;gt; 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We prove that the increasing triangular maps constructed using neural autoregressive flows satisfy a
universal property in the context of probability density approximation.&lt;/p&gt;
&lt;p&gt;Theorem 1. Let &lt;span class=&#34;math inline&#34;&gt;\(\cal P\)&lt;/span&gt; be a non-homogeneous Poisson process with positive continuous process density
&lt;span class=&#34;math inline&#34;&gt;\(\tilde{\lambda}(\cdot)\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\({\cal X} \subset \mathbb{R}^{d}\)&lt;/span&gt;. Suppose
further that the weak (Sobolev) partial derivatives of &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\lambda}\)&lt;/span&gt;
up to order &lt;span class=&#34;math inline&#34;&gt;\(d+1\)&lt;/span&gt; are integrable over &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^{d}\)&lt;/span&gt;. There exists a
sequence of triangular mappings &lt;span class=&#34;math inline&#34;&gt;\((T_i(\cdot))_{i}\)&lt;/span&gt; wherein the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;th
component of each map &lt;span class=&#34;math inline&#34;&gt;\(T^{(k)}_i(\cdot)\)&lt;/span&gt; has the form above, and wherein the corresponding conditional networks
are universal approximators (e.g., feedforward neural networks with
sigmoid activation functions), such that
&lt;span class=&#34;math display&#34;&gt;\[ \eta(T_i(\cdot)) \mbox{det} (\nabla T_i(\cdot)) \rightarrow \tilde{\lambda}(\cdot), \]&lt;/span&gt;
with respect to the sup norm on any compact subset of &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^{d}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;illustration&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Illustration&lt;/h2&gt;
&lt;p&gt;We apply our method for intensity function estimation to an earthquake
data set comprising 1000 seismic events of body-wave magnitude (MB) over
4.0. The data set is available from the
package. The events we analyze are those that occurred near Fiji from
1964 onwards. The left panel of Figure 1 shows a scatter plot of locations of the observed seismic events.&lt;/p&gt;
&lt;p&gt;We fitted our model using a composition of five triangular maps. The estimated
intensity surface and the standard error surface obtained using are
shown in the middle and right panels of Figure 1, respectively. The probability that the intensity function exceeds
various threshold can also be estimated using non-parametric bootstrap
resampling; some examples of these exceedance probability plots are
shown in Figure 2.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-09-19-non-homogeneous-poisson-process-intensity-modeling-and-estimation-using-measure-transport_files/quake_scatter-horz.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 1. Left panel: Scatter plot of earthquake events with
body-wave magnitude greater than 4.0 near Fiji since 1964. Middle
panel: Estimated intensity function obtained using measure transport.
Right panel: Estimated standard error of the intensity surface.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-09-19-non-homogeneous-poisson-process-intensity-modeling-and-estimation-using-measure-transport_files/quake_exceed_1-horz.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 2. Left panel: Estimated exceedance probability
&lt;span class=&#34;math inline&#34;&gt;\(P(\lambda(\cdot) &amp;gt; 1)\)&lt;/span&gt;. Middle panel: Estimated exceedance
probability &lt;span class=&#34;math inline&#34;&gt;\(P(\lambda(\cdot) &amp;gt; 5)\)&lt;/span&gt;. Right panel: Estimated exceedance
probability &lt;span class=&#34;math inline&#34;&gt;\(P(\lambda(\cdot) &amp;gt; 10)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-Dinh2015&#34; class=&#34;csl-entry&#34;&gt;
Dinh, Laurent, David Krueger, and Yoshua Bengio. 2015. &lt;span&gt;“&lt;span&gt;NICE:&lt;/span&gt; Non-Linear Independent Components Estimation.”&lt;/span&gt; In &lt;em&gt;Workshop Track Proceedings of the 3rd International Conference on Learning Representations&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-Dinh2017&#34; class=&#34;csl-entry&#34;&gt;
Dinh, Laurent, Jascha Sohl-Dickstein, and Samy Bengio. 2017. &lt;span&gt;“Density Estimation Using Real &lt;span&gt;NVP&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;Conference Track Proceedings of the 5th International Conference on Learning Representations&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-Germain2015&#34; class=&#34;csl-entry&#34;&gt;
Germain, Mathieu, Karol Gregor, Iain Murray, and Hugo Larochelle. 2015. &lt;span&gt;“&lt;span&gt;MADE&lt;/span&gt;: Masked Autoencoder for Distribution Estimation.”&lt;/span&gt; In &lt;em&gt;Proceedings of the 32nd International Conference on Machine Learning&lt;/em&gt;, 881–89.
&lt;/div&gt;
&lt;div id=&#34;ref-Huang2018&#34; class=&#34;csl-entry&#34;&gt;
Huang, Chin-Wei, David Krueger, Alexandre Lacoste, and Aaron Courville. 2018. &lt;span&gt;“Neural Autoregressive Flows.”&lt;/span&gt; In &lt;em&gt;Proceedings of the 35th International Conference on Machine Learning&lt;/em&gt;, 80:2078–87. Proceedings of Machine Learning Research.
&lt;/div&gt;
&lt;div id=&#34;ref-Marzouk2016&#34; class=&#34;csl-entry&#34;&gt;
Marzouk, Youssef, Tarek Moselhy, Matthew Parno, and Alessio Spantini. 2016. &lt;span&gt;“Sampling via Measure Transport: An Introduction.”&lt;/span&gt; In &lt;em&gt;Handbook of Uncertainty Quantification&lt;/em&gt;, 1–41.
&lt;/div&gt;
&lt;div id=&#34;ref-Papamakarios2017&#34; class=&#34;csl-entry&#34;&gt;
Papamakarios, George, Theo Pavlakou, and Iain Murray. 2017. &lt;span&gt;“Masked Autoregressive Flow for Density Estimation.”&lt;/span&gt; In &lt;em&gt;Advances in Neural Information Processing Systems 30&lt;/em&gt;, 2338–47. &lt;a href=&#34;http://papers.nips.cc/paper/6828-masked-autoregressive-flow-for-density-estimation.pdf&#34;&gt;http://papers.nips.cc/paper/6828-masked-autoregressive-flow-for-density-estimation.pdf&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Macroeconomic Determinants of Migration. A Comparative Analysis For Old vs New European Member States</title>
      <link>https://youngstats.github.io/post/2022/09/18/macroeconomic-determinants-of-migration-a-comparative-analysis-for-old-vs-new-european-member-states/</link>
      <pubDate>Sun, 18 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/09/18/macroeconomic-determinants-of-migration-a-comparative-analysis-for-old-vs-new-european-member-states/</guid>
      <description>


&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Migration is a multifaceted phenomenon including macro-, meso-, and
micro-triggers that, when combined, decide an individual’s final
decision to migrate. Apart from being an essential component of
population change, migration is also an important component of
population estimates and labor market force estimations. Jennissen
(2004) argues that the presence of migrant populations has a favorable
impact on natural population growth since migrants’ age characteristics
and fertility rates are often greater than those of the indigenous
population. Furthermore, the migration problem is far more significant
for population growth in European nations, which are generally seeing a
drop in natural population increase. In this environment, it’s critical
to identify and quantify the factors involved in the decision to
migrate. Income disparities or income inequality, economic development,
the tax system, the economic cycle, the availability of new job
opportunities, unemployment, and other variables have been highlighted
as factors affecting migration by several migration theories (Kumpikaite
and Zickute, 2012). People migrate for a variety of reasons, including
improved living conditions or an escape from adverse circumstances in
their native country. This is the foundation of Lee’s (1966) push and
pull hypothesis, which is one of the primary neo-classical migration
hypotheses. Individuals are influenced by supply-push forces to leave
their home country, while demand-pull variables draw migrants to the
destination country.&lt;/p&gt;
&lt;p&gt;Panel data analysis is the study of datasets in which entities are
observed through time and allows for the management of missing variables
without having to examine them. By examining changes in the dependent
variable across time, one may rule out the impact of neglected factors
that fluctuate between entities but remain constant over time. The basic
premise is that if unobserved factors (such as those peculiar to
nations) influence the dependent variable yet stay constant over time,
the changes in the dependent variable must come from other sources.&lt;/p&gt;
&lt;p&gt;The notation for panel data will be the following:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} (x_{1,it}, x_{2,it}, ..., x_{k,it}, y_{it}), i=1,2, ..., n; t=1,2, ..., T \end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;em&gt;i&lt;/em&gt; is the subscript for the entity being observed (in our case
study the country) and &lt;em&gt;t&lt;/em&gt; is the subscript for the date at which the
entity is observed (in our case study the year). Using these notations
we would have data for the variables
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} x_1, x_2, ..., x_k, y \end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The fixed effects approach&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The fixed effects model would be written as:&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} y_{it}=\beta_1 x_{1,it} + \beta_2 x_{2,it} + \dots + \beta_k x_{k,it} + \alpha_i + u_{it} \end{equation}\]&lt;/span&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_{j,it}\)&lt;/span&gt; represents the value of regressor &lt;em&gt;j&lt;/em&gt;, for entity &lt;em&gt;i&lt;/em&gt; and
time period &lt;em&gt;t&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_j\)&lt;/span&gt; represents the coefficients of the independent variables,
that do not vary across individuals.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha_i\)&lt;/span&gt; represents the entity specific intercepts.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;The random effects approach&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;the unobserved variable specific to the individual entity is encompassed
in the error term. The entities will have a common mean value for the
intercept (let’s denote this with &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;) and the specific differences
in the intercept values of each country would be reflected in an error
term (denoted with &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i\)&lt;/span&gt;).&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} y_{it}=\alpha + \sum_{j=1}^{k} \beta_j x_{j,it} + \epsilon_i + u_{it} \end{equation}\]&lt;/span&gt;
&lt;p&gt;We will obtain a composite error term, which is composed of
&lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i\)&lt;/span&gt;, the individual (cross section) specific error &lt;span class=&#34;math inline&#34;&gt;\(u_{it}\)&lt;/span&gt; and
a combined cross section and time series error:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} w_{it}=\epsilon_i + u_{it} \end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thus substituting both equations we obtain:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} y_{it}=\alpha + \sum_{j=1}^{k} \beta_j x_{j,it} + w_{it} \end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The main benefit of using panel data related techniques is that
individual heterogeneity of individual entities (countries) can be
explicitly taken into account in panel data estimation; additionally,
panel data offers “more informative data, more variability, less
collinearity, more degrees of freedom, and more efficiency” by combining
time series and cross-sectional observations (Baltagi, 2001).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The database involved&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Crude Rate of Net Migration plus adjustment is the dependent
variable in both estimated models. It is calculated as the annual ratio
of net migration to the average population. It is expressed in terms of
1000 people (of the average population). The difference between the
total number of immigrants and the total number of emigrants is referred
to as net migration; statistical adjustments refers to adjusting net
migration by taking the difference between total population change and
natural change; the indicator roughly covers the difference between
inward and outward migration. In the model, the variable is called “Net
Migration.”&lt;/p&gt;
&lt;p&gt;The independent variables in the models are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unemployment: The long-term unemployment rate is the percentage of
people who have been jobless for more than a year out of the total
number of people who are working.&lt;/li&gt;
&lt;li&gt;Earnings (adjusted): The adjusted gross disposable income of
households divided by the PPP (purchasing power parities) of the actual
individual consumption of households and by the entire resident
population (in purchasing power standard (PPS) per inhabitant) yields
gross disposable income of households per capita.&lt;/li&gt;
&lt;li&gt;Gini Coefficient (of equivalised disposable income): A relationship
between the cumulative shares of the population disposed based on
equivalised disposable income and the cumulative share of the
equivalised total disposable income received by the population is
defined as the Gini Coefficient (of equivalised disposable income).&lt;/li&gt;
&lt;li&gt;Poverty: The at-risk-of-poverty rate is the proportion of people whose
equivalised disposable income is less than the risk-of-poverty
threshold, which is set at 60% of the national median equivalised
disposable income.&lt;/li&gt;
&lt;li&gt;Economic Freedom: A 0–10 scale that assesses the degree of economic
liberty in five key areas: government size, legal system, sound money,
international trade freedom, and regulation.&lt;/li&gt;
&lt;li&gt;Hospital beds: the number of beds available in hospitals; the variable
is calculated per 100,000 people.&lt;/li&gt;
&lt;li&gt;Health spending: the overall amount spent on health as a proportion of
GDP ( percent ).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Eurostat is the data source for all variables except Economic Freedom.
The Fraser Institute is the source of data for the Economic Freedom
Index. Data was collected for 25 EU nations over a period of 18 years,
from 2000 to 2017.&lt;/p&gt;
&lt;p&gt;Figure 1 depicts the dispersion of average Net Migration rates for EU
nations for the period 2000–2017. Lithuania (-8.84), Latvia (-7.19),
and Romania (-7.19) are the nations with the lowest average Net
Migration between 2000 and 2017. (-5.48). Bulgaria, Estonia, Poland, and
Slovakia all had negative Net Migration averages over the time period
studied. This was to be expected, given that these economies in Central
and Eastern Europe are generally migration-sending countries (Jennissen,
2004). Luxembourg, Spain, and Sweden, on the other hand, have the
greatest average Net Migration rates, indicating that they are typically
migrant-receiving nations. The Net Migration Rate’s behavior proposes
that the group of nations be divided into two: typically sending and
traditionally receiving countries.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-09-18-macroeconomic-determinants-of-migration-a-comparative-analysis-for-old-vs-new-european-member-states_files/Figure%201.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 1 – Average Net Migration for the period 2000 – 2017 for the EU
countries; source: author’s processing in Tableau, based on Eurostat data.&lt;/p&gt;
&lt;p&gt;Figure 2 depicts the significant relationship between real GDP/capita,
chain linked volume (2010) in Euro/capita, and net migration rate
(averages from 2000 to 2017 were used). Nations with lower GDP per
capita also have negative or low Net Migration rates, indicating that
“poorer” countries are more likely to be the source of migrant flows
(sending countries). Nations with greater GDP/capita, on the other hand,
have higher migration rates, implying that they are primarily receiving
countries for migrants.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-09-18-macroeconomic-determinants-of-migration-a-comparative-analysis-for-old-vs-new-european-member-states_files/Figure%202.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 2 – Net Migration rate vs. GDP/capita – comparison between old
and new EU member states; source: author’s processing in Tableau, based
on Eurostat data.&lt;/p&gt;
&lt;p&gt;The tendency of Net Migration for old and new member states warrants the
dataset being separated into two categories. It is clear from this
research that the group of New Member States acts as Migrant Sending
Countries for the whole time under consideration, while the Old Member
States operate as Migrant Receiving Countries. As a result, the
empirical application will be split into two parts: one model for the
Old member nations and then another model (Table 1) for the New member
states (Table 2).&lt;/p&gt;
&lt;p&gt;Table 1. Results of estimation for fixed and random effects models for
Old Member States; source: author’s own results&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;23%&#34; /&gt;
&lt;col width=&#34;38%&#34; /&gt;
&lt;col width=&#34;38%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Fixed Effects&lt;/th&gt;
&lt;th&gt;Random Effects&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Unemployment&lt;/td&gt;
&lt;td&gt;-1.0468*** (0.0903)&lt;/td&gt;
&lt;td&gt;-1.0295*** (0.0881)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Ln_Income&lt;/td&gt;
&lt;td&gt;4.9042** (2.4037)&lt;/td&gt;
&lt;td&gt;8.1458*** (1.5631)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Gini&lt;/td&gt;
&lt;td&gt;-0.1777 (0.1691)&lt;/td&gt;
&lt;td&gt;-0.3282** (0.1391)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Poverty&lt;/td&gt;
&lt;td&gt;0.6876*** (0.1657)&lt;/td&gt;
&lt;td&gt;0.7331*** (0.1417)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Economic Freedom&lt;/td&gt;
&lt;td&gt;-4.5181*** (1.5230)&lt;/td&gt;
&lt;td&gt;-2.9796*** (1.0923)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Health Expenditure&lt;/td&gt;
&lt;td&gt;-1.4840*** (0.4175)&lt;/td&gt;
&lt;td&gt;-1.7844*** (0.2965)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Intercept&lt;/td&gt;
&lt;td&gt;-1.6247 (25.7912)&lt;/td&gt;
&lt;td&gt;-40.0461** (17.8813)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;R2&lt;/td&gt;
&lt;td&gt;Within 0.4184 Between 0.6589 Overall 0.5056&lt;/td&gt;
&lt;td&gt;Within 0.4109 Between 0.8405 Overall 0.5828&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;F&lt;/td&gt;
&lt;td&gt;29.85***&lt;/td&gt;
&lt;td&gt;229.47*** (Wald chi2)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Corr (u_i, xb)&lt;/td&gt;
&lt;td&gt;0.2190&lt;/td&gt;
&lt;td&gt;0 (assumed)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Sigma_u&lt;/td&gt;
&lt;td&gt;2.0795&lt;/td&gt;
&lt;td&gt;1.3274&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Sigma_e&lt;/td&gt;
&lt;td&gt;3.0081&lt;/td&gt;
&lt;td&gt;3.0081&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Rho&lt;/td&gt;
&lt;td&gt;0.3233&lt;/td&gt;
&lt;td&gt;0.1629&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;*** Significant at 0.01; ** Significant at 0.05&lt;/p&gt;
&lt;p&gt;(standard errors of the coefficients are reported in parenthesis)&lt;/p&gt;
&lt;p&gt;Table 2. Results of estimation for fixed and random effects models for
New Member States; source: author’s own results&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;23%&#34; /&gt;
&lt;col width=&#34;38%&#34; /&gt;
&lt;col width=&#34;38%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Fixed Effects&lt;/th&gt;
&lt;th&gt;Random Effects&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Unemployment&lt;/td&gt;
&lt;td&gt;-0.3900** (0.1547)&lt;/td&gt;
&lt;td&gt;-0.3893*** (0.1461)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Gini&lt;/td&gt;
&lt;td&gt;-0.3527** (0.1377)&lt;/td&gt;
&lt;td&gt;-0.5432*** (0.1226)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Economic Freedom&lt;/td&gt;
&lt;td&gt;4.3493*** (1.1280)&lt;/td&gt;
&lt;td&gt;4.3283*** (1.0755)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Health Expenditure&lt;/td&gt;
&lt;td&gt;-1.1240* (0.5841)&lt;/td&gt;
&lt;td&gt;-0.4513 (0.4765)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Intercept&lt;/td&gt;
&lt;td&gt;-15.6762 (8.0909)&lt;/td&gt;
&lt;td&gt;-13.3742** (7.5791)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;R2&lt;/td&gt;
&lt;td&gt;Within 0.1663 Between 0.0205 Overall 0.0855&lt;/td&gt;
&lt;td&gt;Within 0.1483 Between 0.5198 Overall 0.3147&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;F&lt;/td&gt;
&lt;td&gt;8.28***&lt;/td&gt;
&lt;td&gt;38.90*** (Wald chi2)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Corr (u_i, xb)&lt;/td&gt;
&lt;td&gt;-0.1069&lt;/td&gt;
&lt;td&gt;0 (assumed)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Sigma_u&lt;/td&gt;
&lt;td&gt;3.9181&lt;/td&gt;
&lt;td&gt;2.2254&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Sigma_e&lt;/td&gt;
&lt;td&gt;3.5999&lt;/td&gt;
&lt;td&gt;3.5999&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Rho&lt;/td&gt;
&lt;td&gt;0.5422&lt;/td&gt;
&lt;td&gt;0.2765&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;*** Significant at 0.01; ** Significant at 0.05; * Significant at
0.1&lt;/p&gt;
&lt;p&gt;(standard errors of the coefficients are reported in parenthesis)&lt;/p&gt;
&lt;p&gt;The influence of macroeconomic factors on the Crude Rate Net Migration
of European economies was quantified using panel data regression models.
A preliminary study of the dependent variable’s distribution reveals
that the net migration rate differs across Old Member States
(migrant-receiving nations) and New Member States (sending countries for
migrants). As a result, we opt to estimate two models for the two sets
of nations, following Mihi-Ramirez et al. (2017) method’s. Unemployment
rate, per capita income, Gini coefficient, poverty rate, Economic
Freedom Index, and two other characteristics relating to the health
system were included as independent variables (number of beds in
hospitals and health expenditure as percentage in GDP). The analysed
period was 2000 – 2017.&lt;/p&gt;
&lt;p&gt;The findings corroborated migratory economic theory. In terms of the
labor market, unemployment is a large and powerful supply push factor
for migration. Only for the Old Member states does income appear as a
key influence, validating the neo-classical economic theory of
migration, which claims that variations in earnings across nations are
one of the main factors driving labor movement (Massey et al, 1993).
Moving on to the social component, the Gini coefficient has been
established as a strong driving force behind migration in both Old and
New Member States. Poverty appears to be a factor with reduced
explanatory power, with a positive coefficient for the Old member states
and no significance for the New member states.&lt;/p&gt;
&lt;p&gt;In terms of Economic Freedom, the factor has a considerable beneficial
impact on net migration rates only in the New Member States.&lt;/p&gt;
&lt;p&gt;Furthermore, health-related macroeconomic factors were included in the
model, as well as the circular cumulative causation hypothesis, which
states that variations in standard of living across nations are the
primary cause of migration (Massey et al. 1993). The health system, on
the other hand, could not be proven to be a factor of migration.&lt;/p&gt;
&lt;p&gt;Because international migration has such a substantial impact on
European population dynamics, understanding and analyzing the factors
that influence international migration is critical. The findings of this
study might be utilized not just to produce migration forecasts, but
also to establish migration policies that would improve migrants’ labor
and social integration.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Smaranda Cimpoeru, PhD&lt;/p&gt;
&lt;p&gt;Department of Statistics and Econometrics, The Bucharest University of
Economic Studies&lt;/p&gt;
&lt;p&gt;E-mail:
&lt;a href=&#34;mailto:smaranda.cimpoeru@csie.ase.ro&#34; class=&#34;email&#34;&gt;smaranda.cimpoeru@csie.ase.ro&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Minimax Estimation and Identity Testing of Markov Chains</title>
      <link>https://youngstats.github.io/post/2022/09/18/minimax-estimation-and-identity-testing-of-markov-chains/</link>
      <pubDate>Sun, 18 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/09/18/minimax-estimation-and-identity-testing-of-markov-chains/</guid>
      <description>


&lt;p&gt;We briefly review the two classical problems of distribution estimation
and identity testing (in the context of property testing), then propose
to extend them to a Markovian setting. We will see that the sample
complexity depends not only on the number of states, but also on the
stationary and mixing properties of the chains.&lt;/p&gt;
&lt;div id=&#34;the-distribution-setting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The distribution setting&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Estimation/Learning.&lt;/strong&gt; A fundamental problem in statistics is to
estimate a probability distribution from independent samples. Consider
an alphabet &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}\)&lt;/span&gt; of size &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, and draw &lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots, X_n\)&lt;/span&gt;
from a distribution &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; over &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}\)&lt;/span&gt;. How large must &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; –the
sample size– be in order to obtain a good estimator
&lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}(X_1, \dots, X_n)\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;? In order to make the question
precise, we choose a notion of distance &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; between distributions,
pick two small numbers &lt;span class=&#34;math inline&#34;&gt;\(\delta, \varepsilon &amp;gt; 0\)&lt;/span&gt; and can for instance
say that an estimator is good, when with high probability &lt;span class=&#34;math inline&#34;&gt;\(1 - \delta\)&lt;/span&gt;
over the random choice of the sample, the estimator is &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;
close to the true distribution. Framed as a (probably approximately
correct) minimax problem, we define the sample complexity of the problem
to be &lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
    n_0(\varepsilon, \delta) = {\mathrm{\arg\min}} \{n \in \mathbb{N} \colon \min_{\hat{\mu}} \max_{\mu}  \mathbb{P}(\rho(\hat{\mu}(X_1, \dots, X_n), \mu) &amp;gt; \varepsilon) &amp;lt; \delta\},
\end{equation*}\]&lt;/span&gt; where the maximum is taken over all distributions over
&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; bins, and the minimum over all estimators. The problem of
determining &lt;span class=&#34;math inline&#34;&gt;\(n_0\)&lt;/span&gt; is then typically addressed by providing two distinct
answers. On one hand, we construct an estimator that would be good for
any distribution given that &lt;span class=&#34;math inline&#34;&gt;\(n &amp;gt; n_0^{UB}\)&lt;/span&gt;. Conversely, we set up a hard
problem such that no estimator can be good when &lt;span class=&#34;math inline&#34;&gt;\(n &amp;lt; n_0^{LB}\)&lt;/span&gt;. This
essentially leads to &lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
    n_0^{LB} \leq n_0 \leq n_0^{UB},
\end{equation*}\]&lt;/span&gt; and the smaller the gap between the upper and lower
bounds, the better we understand the sample complexity of the
statistical problem. With respect to the total variation distance
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
    \rho_{TV}(\mu, \mu&amp;#39;) = \frac{1}{2} \sum_{x \in \mathcal{X}} |\mu(x) - \mu&amp;#39;(x)|,
\end{equation*}\]&lt;/span&gt; it is folklore (see e.g. &lt;span class=&#34;citation&#34;&gt;Waggoner (&lt;a href=&#34;#ref-waggoner2015lp&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;) that
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
    n_0(\varepsilon, \delta) \asymp \frac{d + \log 1 /\delta}{\varepsilon^2},
\end{equation*}\]&lt;/span&gt;and that the empirical distribution achieves the minimax
rate ( &lt;span class=&#34;math inline&#34;&gt;\(f \asymp g\)&lt;/span&gt; stands for &lt;span class=&#34;math inline&#34;&gt;\(c g \leq f \leq C g\)&lt;/span&gt; for two universal
constants &lt;span class=&#34;math inline&#34;&gt;\(c,C \in \mathbb{R}\)&lt;/span&gt;). The take-away is that if we want to
estimate a distribution over &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; symbols w.r.t total variation, the
statistical hardness of the problem grows roughly linearly with the
support size.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Identity testing.&lt;/strong&gt; A different problem is to imagine that the
practitioner has access to independent data sampled from some unknown
distribution &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and the full description of a reference distribution
&lt;span class=&#34;math inline&#34;&gt;\(\bar{\mu}\)&lt;/span&gt;. They then have to make a determination as to whether the
data was sampled from &lt;span class=&#34;math inline&#34;&gt;\(\bar{\mu}\)&lt;/span&gt;, or from a distribution which is at
least &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; far from &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; (composite). To better compare with
the previous problem, we keep the total variation metric. We briefly
note that the unknown distribution being closer than &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;, but
not equal to &lt;span class=&#34;math inline&#34;&gt;\(\bar{\mu}\)&lt;/span&gt;, is not among the choices –we require
separation of the hypotheses.&lt;/p&gt;
&lt;p&gt;One can come up with a simple ``testing-by-learning” approach to solve
the problem. First estimate the unknown distribution down to precision
&lt;span class=&#34;math inline&#34;&gt;\(\varepsilon/2\)&lt;/span&gt; using the sample, and then verify whether the estimator
is close to &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}\)&lt;/span&gt;. However, identity testing corresponds to a
binary, seemingly easier question. As such, we would expect the sample
complexity to be less than that of the estimation problem. Among the
first to investigate the problem in the case where &lt;span class=&#34;math inline&#34;&gt;\(\bar{\mu}\)&lt;/span&gt; is the
uniform distribution was &lt;span class=&#34;citation&#34;&gt;Paninski (&lt;a href=&#34;#ref-paninski2008coincidence&#34; role=&#34;doc-biblioref&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt;, who showed
that the complexity of the problem grows roughly as the square root of
the alphabet size, i.e. much more economical than estimation. For this
uniformity testing problem, several procedures are known to achieve the
minimax rate (although not all work in all regimes of parameters). One
can for instance count the number of collisions in the sample, count the
number of bins appearing exactly once in the sample, or even compute a
chi-square statistic. In fact, the complexity for the worst-case
&lt;span class=&#34;math inline&#34;&gt;\(\bar{\mu}\)&lt;/span&gt; has been pinned-down to (see &lt;span class=&#34;citation&#34;&gt;Diakonikolas et al. (&lt;a href=&#34;#ref-diakonikolas2017optimal&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;)
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
    n_0(\varepsilon, \delta) \asymp \frac{\sqrt{d \log 1 /\delta} + \log 1 /\delta}{\varepsilon^2}
\end{equation*}\]&lt;/span&gt;
In recent years, the sub-field of distribution testing
has expanded beyond this basic question to investigate a vast collection
of properties of distributions –see the excellent survey of
&lt;span class=&#34;citation&#34;&gt;Canonne (&lt;a href=&#34;#ref-canonne2020survey&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; for an overview.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-markovian-setting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Markovian setting&lt;/h2&gt;
&lt;p&gt;We depart from independence and increase the richness of the process by
considering the Markovian setting. More specifically, we wish to perform
statistical inference from sampling a ``single trajectory” of a Markov
chain, i.e. a sequence of observations &lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots, X_m\)&lt;/span&gt; where
&lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; is drawn from an unknown, arbitrary initial distribution, and the
transition probabilities governing &lt;span class=&#34;math inline&#34;&gt;\(X_t \to X_{t+1}\)&lt;/span&gt; are collected in a
row-stochastic matrix &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;. It is a challenging but fair representation
of a process outside of the control of the scientist, that cannot for
example be restarted, or set to any particular state. For simplicity of
the exposition below, we will assume that &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is ergodic and time
reversible, but the approach and results can be extended to the periodic
or non-reversible cases (see the concluding remarks). To measure
distance between Markov chains, we here consider the infinity matrix
norm between their respective transition matrices, &lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
    \rho(P, P&amp;#39;) = \|P - P&amp;#39;\|_{\infty} = \max_{x \in \mathcal{X}} \sum_{x&amp;#39; \in \mathcal{X}} |P(x,x&amp;#39;) - P&amp;#39;(x,x&amp;#39;)|,
\end{equation*}\]&lt;/span&gt; and recognize that it corresponds to a uniform control
over the conditional distributions defined by each state w.r.t total
variation. The notion of sample size (we have only one sample) is
replaced with the trajectory length. Definitions of the minimax
estimation and identity testing problems follow from this sampling model
and choice of metric. Since the number of parameters in the model jumps
approximately from &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(d^2\)&lt;/span&gt;, a first guess would be to replace &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;
by &lt;span class=&#34;math inline&#34;&gt;\(d^2\)&lt;/span&gt; in the sample complexities obtained in the iid setting.
However, our ability to test or estimate the conditional distribution
defined by a state is restricted by the number of times we visit it. For
instance, if the stationary probability –the long run probability of
visit– of some states is small, or if the chain exhibits a sticky
behavior, we would not be able to make decisions unless the trajectory
length increases accordingly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estimation/Learning.&lt;/strong&gt; Finite sample analyses for the problem are
relatively recent, with the first treatments of &lt;span class=&#34;citation&#34;&gt;Hao, Orlitsky, and Pichapati (&lt;a href=&#34;#ref-hao2018learning&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;
in expectation and &lt;span class=&#34;citation&#34;&gt;Wolfer and Kontorovich (&lt;a href=&#34;#ref-pmlr-v98-wolfer19a&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; in the PAC framework.&lt;/p&gt;
&lt;p&gt;We estimate the unknown &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; with the empirical tally matrix, where
&lt;span class=&#34;math inline&#34;&gt;\(\widehat{P}(x,x&amp;#39;)\)&lt;/span&gt; is computed by counting transitions from &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(x&amp;#39;\)&lt;/span&gt;
and dividing by the number of visits to &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; (modulo some mild
smoothing). Tighter upper and lower bounds on the sample complexity were
later obtained (see &lt;span class=&#34;citation&#34;&gt;Wolfer and Kontorovich (&lt;a href=&#34;#ref-wolfer2021&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt;)
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
\begin{split}
    c \left( \frac{d}{\pi_{\star} \varepsilon^2} + \frac{d \log d}{{\gamma_\star}} \right) \leq m_0(\varepsilon, \delta) \leq C \left( \frac{d + \log 1/(\varepsilon \delta)}{\pi_{\star} \varepsilon^2} +  \frac{\log 1/(\pi_\star \delta)}{\pi_{\star} {\gamma_\star}} \right) \quad \\
\end{split}
\end{equation*}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(c, C &amp;gt; 0\)&lt;/span&gt; are universal constants,
&lt;span class=&#34;math inline&#34;&gt;\(\pi_\star = \min_{x \in \mathcal{X}} \pi(x)\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; the stationary
distribution of &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\gamma_\star\)&lt;/span&gt; is the absolute spectral gap
–the difference between the two largest eigenvalues of &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; in
magnitude– that largely controls the mixing time of the (reversible)
Markov chain. The bounds are independent of the starting state, but
require a priori knowledge of bounds on &lt;span class=&#34;math inline&#34;&gt;\(\pi_\star\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\gamma_\star\)&lt;/span&gt;
to be informative. Notably, procedures exist to efficiently estimate
&lt;span class=&#34;math inline&#34;&gt;\(\pi_\star\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\gamma_\star\)&lt;/span&gt; from the data
&lt;span class=&#34;citation&#34;&gt;Hsu et al. (&lt;a href=&#34;#ref-hsu2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Wolfer (&lt;a href=&#34;#ref-wolfer2022&#34; role=&#34;doc-biblioref&#34;&gt;2022&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We witness a stark difference between the iid and Markovian settings: a
phase transition appears in the sample complexity. Roughly speaking and
taking &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; as a constant, when the mixing time of the chain is
larger than the number of states, the second term becomes the dominant
one, i.e. the difficulty of estimation is chiefly controlled by how well
we can navigate among the states. When the chain mixes more quickly,
moving from state to state is not bottleneck, and the harder task is the
estimation of the respective conditional distributions. An additional
caveat is that the chain will generally not visit all states equally in
the long run. As a result, the sample complexity necessarily depends on
the proportion &lt;span class=&#34;math inline&#34;&gt;\(\pi_\star\)&lt;/span&gt; of the trajectory spent in the least visited
state. The astute reader may notice that when the mixing time is small
and the stationary distribution is uniform (&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is doubly stochastic),
we recover up to logarithmic factors the previously intuited complexity
of &lt;span class=&#34;math inline&#34;&gt;\(d^2/\varepsilon^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Identity testing.&lt;/strong&gt; For the Markov chain identity testing problem
(still w.r.t the matrix uniform norm), we rely on a two-stage testing
procedure. First, we verify that we visited all states about
&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{d}/\varepsilon^2\)&lt;/span&gt; times. As a second step, we perform identity
testing of all conditional distributions. The Markov property ensures
that our transition observations are drawn independently from the
conditional distribution, thus that we can rely on one of the known iid
testing procedures as a black-box. We obtain the below-listed upper and
lower bounds on the sample complexity (see &lt;span class=&#34;citation&#34;&gt;Wolfer and Kontorovich (&lt;a href=&#34;#ref-pmlr-v108-wolfer20a&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;)
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
\begin{split}
    c \left( \frac{\sqrt{d}}{\bar{\pi}_\star \varepsilon^2} + \frac{d}{\bar{\gamma}_\star} \right) \leq m_0(\varepsilon, \delta) \leq C \left( \frac{\sqrt{d}}{\bar{\pi}_\star \varepsilon^2} \log \frac{d}{\varepsilon \delta} + \frac{\log 1/(\bar{\pi}_\star \delta)}{\bar{\pi}_\star \bar{\gamma}_\star} \right) \qquad \\
\end{split}
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-09-18-minimax-estimation-and-identity-testing-of-markov-chains_files/Figure1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 1: Topology of class of Markov chains achieving the lower bounds
in &lt;span class=&#34;math inline&#34;&gt;\(d /\gamma_\star\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-09-18-minimax-estimation-and-identity-testing-of-markov-chains_files/Figure2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 2: Example of the class in Figure 1 with &lt;span class=&#34;math inline&#34;&gt;\(d = 9\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(\tau \in \{-1, 1\}^3\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; \eta \ll 1\)&lt;/span&gt;. The mixing time of the
chain is of the order of &lt;span class=&#34;math inline&#34;&gt;\(1/\eta\)&lt;/span&gt;, and is decoupled from the proximity
parameter &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Perhaps surprisingly, the bounds only depends on properties
&lt;span class=&#34;math inline&#34;&gt;\(\bar{\pi}_\star\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\bar{\gamma}_\star\)&lt;/span&gt; of the reference stochastic
matrix. As a matter of fact, the unknown chain need not even be
irreducible for the procedure to work. A quadratic speed-up also appears
in the bounds, which will affect the dominant term when the chain mixes
faster than &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{d}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;It is instructive to inspect the set of reversible Markov chains that
achieves the lower bounds in &lt;span class=&#34;math inline&#34;&gt;\(d / \gamma_\star\)&lt;/span&gt; (see Figure 1 and Figure
2). Every element consists of an “inner clique” and an “outer rim”. The
inner clique can be made arbitrarily sticky in the sense that the chain
will only move from one state to another within the clique with
underwhelming probability &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt;, which has an overall slowing effect on
the mixing time of the chain. On the other hand, being on one state of
the clique, the chain has at least constant –but parametrizable–
probability of reaching one of the two connected states in the outer
rim. In order to distinguish between two chains where one of the inner
nodes has &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;-different transition probabilities towards the
rim than the other, the trajectory will necessarily have to go through a
large fraction of the states.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related work and closing comments.&lt;/strong&gt; As mentioned earlier in this
note, the results are valid for a substantially larger class of chains.
In the non-reversible setting, the absolute spectral gap can readily be
replaced with the pseudo-spectral gap &lt;span class=&#34;citation&#34;&gt;Paulin (&lt;a href=&#34;#ref-paulin2015concentration&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;,
and if the chain is irreducible (but possibly periodic), one can instead
perform inference on the lazy process governed by &lt;span class=&#34;math inline&#34;&gt;\((P+I)/2\)&lt;/span&gt;, which can
be simulated with an additional fair coin. More recently,
&lt;span class=&#34;citation&#34;&gt;Chan, Ding, and Li (&lt;a href=&#34;#ref-chan2021learning&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; investigated the Markov chain minimax learning
and testing problems by considering cover times instead of mixing times.&lt;/p&gt;
&lt;p&gt;We end this post by stressing that the minimax rates pertain to the
choice of metric. We refer the interested reader to
&lt;span class=&#34;citation&#34;&gt;Daskalakis, Dikkala, and Gravin (&lt;a href=&#34;#ref-daskalakis2018testing&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Cherapanamjeri and Bartlett (&lt;a href=&#34;#ref-cherapanamjeri2019testing&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Fried and Wolfer (&lt;a href=&#34;#ref-pmlr-v151-fried22a&#34; role=&#34;doc-biblioref&#34;&gt;2022&lt;/a&gt;)&lt;/span&gt;,
who analyze the identity testing problem with respect to a different
notion of discrepancy between Markov chains. Although the last-mentioned
framework can so far only handle reversible chains, it advantageously
leads to minimax rates that are independent of mixing or hitting.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-canonne2020survey&#34; class=&#34;csl-entry&#34;&gt;
Canonne, Clément L. 2020. &lt;span&gt;“A Survey on Distribution Testing: Your Data Is Big. But Is It Blue?”&lt;/span&gt; &lt;em&gt;Theory of Computing&lt;/em&gt;, 1–100.
&lt;/div&gt;
&lt;div id=&#34;ref-chan2021learning&#34; class=&#34;csl-entry&#34;&gt;
Chan, Siu On, Qinghua Ding, and Sing Hei Li. 2021. &lt;span&gt;“Learning and Testing Irreducible &lt;span&gt;M&lt;/span&gt;arkov Chains via the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-Cover Time.”&lt;/span&gt; In &lt;em&gt;Algorithmic Learning Theory&lt;/em&gt;, 458–80. PMLR.
&lt;/div&gt;
&lt;div id=&#34;ref-cherapanamjeri2019testing&#34; class=&#34;csl-entry&#34;&gt;
Cherapanamjeri, Yeshwanth, and Peter L Bartlett. 2019. &lt;span&gt;“Testing Symmetric &lt;span&gt;M&lt;/span&gt;arkov Chains Without Hitting.”&lt;/span&gt; In &lt;em&gt;Conference on Learning Theory&lt;/em&gt;, 758–85. PMLR.
&lt;/div&gt;
&lt;div id=&#34;ref-daskalakis2018testing&#34; class=&#34;csl-entry&#34;&gt;
Daskalakis, Constantinos, Nishanth Dikkala, and Nick Gravin. 2018. &lt;span&gt;“Testing Symmetric &lt;span&gt;M&lt;/span&gt;arkov Chains from a Single Trajectory.”&lt;/span&gt; In &lt;em&gt;Conference on Learning Theory&lt;/em&gt;, 385–409. PMLR.
&lt;/div&gt;
&lt;div id=&#34;ref-diakonikolas2017optimal&#34; class=&#34;csl-entry&#34;&gt;
Diakonikolas, Ilias, Themis Gouleakis, John Peebles, and Eric Price. 2017. &lt;span&gt;“Optimal Identity Testing with High Probability.”&lt;/span&gt; &lt;em&gt;arXiv Preprint arXiv:1708.02728&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-pmlr-v151-fried22a&#34; class=&#34;csl-entry&#34;&gt;
Fried, Sela, and Geoffrey Wolfer. 2022. &lt;span&gt;“Identity Testing of Reversible &lt;span&gt;M&lt;/span&gt;arkov Chains.”&lt;/span&gt; In &lt;em&gt;Proceedings of the 25th International Conference on Artificial Intelligence and Statistics&lt;/em&gt;, 151:798–817. Proceedings of Machine Learning Research. PMLR.
&lt;/div&gt;
&lt;div id=&#34;ref-hao2018learning&#34; class=&#34;csl-entry&#34;&gt;
Hao, Yi, Alon Orlitsky, and Venkatadheeraj Pichapati. 2018. &lt;span&gt;“On Learning &lt;span&gt;M&lt;/span&gt;arkov Chains.”&lt;/span&gt; &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 31.
&lt;/div&gt;
&lt;div id=&#34;ref-hsu2019&#34; class=&#34;csl-entry&#34;&gt;
Hsu, Daniel, Aryeh Kontorovich, David A. Levin, Yuval Peres, Csaba Szepesvári, and Geoffrey Wolfer. 2019. &lt;span&gt;“Mixing Time Estimation in Reversible &lt;span&gt;M&lt;/span&gt;arkov Chains from a Single Sample Path.”&lt;/span&gt; &lt;em&gt;Ann. Appl. Probab.&lt;/em&gt; 29 (4): 2439–80. &lt;a href=&#34;https://doi.org/10.1214/18-AAP1457&#34;&gt;https://doi.org/10.1214/18-AAP1457&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-paninski2008coincidence&#34; class=&#34;csl-entry&#34;&gt;
Paninski, Liam. 2008. &lt;span&gt;“A Coincidence-Based Test for Uniformity Given Very Sparsely Sampled Discrete Data.”&lt;/span&gt; &lt;em&gt;IEEE Transactions on Information Theory&lt;/em&gt; 54 (10): 4750–55.
&lt;/div&gt;
&lt;div id=&#34;ref-paulin2015concentration&#34; class=&#34;csl-entry&#34;&gt;
Paulin, Daniel. 2015. &lt;span&gt;“Concentration Inequalities for &lt;span&gt;M&lt;/span&gt;arkov Chains by &lt;span&gt;M&lt;/span&gt;arton Couplings and Spectral Methods.”&lt;/span&gt; &lt;em&gt;Electronic Journal of Probability&lt;/em&gt; 20: 1–32.
&lt;/div&gt;
&lt;div id=&#34;ref-waggoner2015lp&#34; class=&#34;csl-entry&#34;&gt;
Waggoner, Bo. 2015. &lt;span&gt;“&lt;span class=&#34;math inline&#34;&gt;\(L_p\)&lt;/span&gt; Testing and Learning of Discrete Distributions.”&lt;/span&gt; In &lt;em&gt;Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science&lt;/em&gt;, 347–56.
&lt;/div&gt;
&lt;div id=&#34;ref-wolfer2022&#34; class=&#34;csl-entry&#34;&gt;
Wolfer, Geoffrey. 2022. &lt;span&gt;“Empirical and Instance-Dependent Estimation of &lt;span&gt;M&lt;/span&gt;arkov Chain and Mixing Time.”&lt;/span&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-pmlr-v98-wolfer19a&#34; class=&#34;csl-entry&#34;&gt;
Wolfer, Geoffrey, and Aryeh Kontorovich. 2019. &lt;span&gt;“Minimax Learning of Ergodic &lt;span&gt;M&lt;/span&gt;arkov Chains.”&lt;/span&gt; In &lt;em&gt;Proceedings of the 30th International Conference on Algorithmic Learning Theory&lt;/em&gt;, 98:904–30. Proceedings of Machine Learning Research. PMLR.
&lt;/div&gt;
&lt;div id=&#34;ref-pmlr-v108-wolfer20a&#34; class=&#34;csl-entry&#34;&gt;
———. 2020. &lt;span&gt;“Minimax Testing of Identity to a Reference Ergodic &lt;span&gt;M&lt;/span&gt;arkov Chain.”&lt;/span&gt; In &lt;em&gt;Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics&lt;/em&gt;, 108:191–201. Proceedings of Machine Learning Research. PMLR.
&lt;/div&gt;
&lt;div id=&#34;ref-wolfer2021&#34; class=&#34;csl-entry&#34;&gt;
———. 2021. &lt;span&gt;“Statistical Estimation of Ergodic &lt;span&gt;M&lt;/span&gt;arkov Chain Kernel over Discrete State Space.”&lt;/span&gt; &lt;em&gt;Bernoulli&lt;/em&gt; 27 (1): 532–53. &lt;a href=&#34;https://doi.org/10.3150/20-BEJ1248&#34;&gt;https://doi.org/10.3150/20-BEJ1248&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Regularization by Noise for Stochastic Differential and Stochastic Partial Differential Equations</title>
      <link>https://youngstats.github.io/post/2022/06/03/regularization-by-noise-for-stochastic-differential-and-stochastic-partial-differential-equations/</link>
      <pubDate>Fri, 03 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/06/03/regularization-by-noise-for-stochastic-differential-and-stochastic-partial-differential-equations/</guid>
      <description>


&lt;p&gt;&lt;strong&gt;Regularization by Noise for Stochastic Differential and Stochastic Partial Differential Equations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-06-03-regularization-by-noise-for-stochastic-differential-and-stochastic-partial-differential-equations_files/cover_image_reg_by_noise_spde.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The regularizing effects of noisy perturbations of differential equations is a central subject of stochastic analysis. Recent breakthroughs initiated a new wave of interest, particularly concerning non-Markovian, infinite dimensional, and rough-stochastic / Young-stochastic hybrid systems.&lt;/p&gt;
&lt;p&gt;On the webinar, selected younger scholars will present their recent works on the topic.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wednesday, September 21st, 7:00 PT / 10:00 EST / 16:00 CET.&lt;/li&gt;
&lt;li&gt;Online, via Zoom. The registration form is available &lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSdA_UmGknYO3Oo_yqVM80SqgnIZUgEyvINzdOMTI0BzhCMdPw/viewform?vc=0&amp;amp;c=0&amp;amp;w=1&amp;amp;flr=0&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.mi.fu-berlin.de/math/groups/stoch/members/Doktoranden/kremp.html&#34;&gt;Helena Kremp&lt;/a&gt;, Freie Universität Berlin, Germany: &lt;em&gt;Weak rough-path-type solutions for singular Lévy SDEs&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Abstract: Since the works by Delarue, Diel and Cannizzaro, Chouk (in the Brownian noise setting), and our previous work, the existence and uniqueness of solutions to the martingale problem associated to multidimensional SDEs with additive &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;-stable Lévy noise for &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; in (1, 2] and rough Besov drift of regularity &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; &amp;gt; (2-2&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;)/3 is known. Motivated by the equivalence of probabilistic weak solutions to SDEs with bounded, measurable drift and solutions to the martingale problem, we define a (non-canonical) weak solution concept for singular Lévy diffusions, proving moreover equivalence to the martingale solution in both the Young (&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; &amp;gt; (1-&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;)/2), as well as in the rough regime (&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; &amp;gt; (2-2&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;))/3). This turns out to be highly non-trivial in the rough case and forces us to define certain rough stochastic sewing integrals involved. In particular, we show that the canonical weak solution concept (introduced also by Athreya, Butkovsky, Mytnik), which is well-posed in the Young case, yields non-uniqueness of solutions in the rough case. If time permits, we apply our theory to the Brox diffusion with Lévy noise and prove a homogenization result for singular diffusions in the periodic Besov drift setting. The talk is based on joint work with Nicolas Perkowski.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.iam.uni-bonn.de/users/galeati/home&#34;&gt;Lucio Galeati&lt;/a&gt;, University of Bonn, Germany: &lt;em&gt;Advances on singular SDEs with fractional noise in subcritical regimes&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Abstract: In recent years, there has been a lot of interest in regulariaation by noise for SDEs driven by fractional Brownian motion of parameter &lt;span class=&#34;math inline&#34;&gt;\(H\in (0,1)\)&lt;/span&gt;, with first results going back to Nualart, Ouknine (2002) and Catellier, Gubinelli (2016). The main challenges when dealing with these equations are the presence of a non-Lipschitz (possibly singular) drift and the lack of any martingale or Markovian structure; in particular, classical stochastic analysis and PDE tools break down for &lt;span class=&#34;math inline&#34;&gt;\(H\neq 1/2\)&lt;/span&gt; and new techniques must be developed. Here I will present some advances on the topic, including generalizations to the regime &lt;span class=&#34;math inline&#34;&gt;\(H\in (1,\infty)\)&lt;/span&gt;, construction of the associated stochastic flow of solutions and stability estimates for SDEs driven by different drifts. The key tools in our analysis come from rough path theory and are given by Young integration and suitable versions of the stochastic sewing lemma. Based on a joint work (arXiv:2207.03475) with M. Gerencser (TU Wien).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.mics.centralesupelec.fr/en/teammics&#34;&gt;Lukas Anzeletti&lt;/a&gt;, Université Paris-Saclay, France: &lt;em&gt;Regularisation by noise for SDEs with (fractional) Brownian noise and a comparison of different notions of solutions&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Abstract: We study existence and uniqueness of solutions to the equation &lt;span class=&#34;math inline&#34;&gt;\(X_t=b(X_t)dt + dB_t\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; may be distributional and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; is a fractional Brownian motion with Hurst parameter &lt;span class=&#34;math inline&#34;&gt;\(H\leq 1/2\)&lt;/span&gt;. We follow two approaches, namely using the stochastic sewing lemma and nonlinear Young integrals in &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-variation. Furthermore, in the Brownian case &lt;span class=&#34;math inline&#34;&gt;\(H=1/2\)&lt;/span&gt; we present examples of drifts in which the classical notion of a solution and solutions in a path-by-path sense coincide, respectively not coincide. Partly based on joint work with Alexandre Richard and Etienne Tanré.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://math.univ-lyon1.fr/~honore/index.html&#34;&gt;Igor Honoré&lt;/a&gt;, Université Claude Bernard Lyon 1, France: &lt;em&gt;Selection by vanishing viscosity for the transport equation&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Abstract: We consider a transport equation whose the coefficients can be in a negative Besov space, we provide controls in Hölder spaces of some kind of vanishing viscosity solutions from Kolmogorov equation associated with a stochastic process. We perform a new analysis based on a proxy around the flow associated with regularize coefficients, and thanks to a cut locus technique we obtain a Hölder control which does not depend on the regularity of the coefficients. The regularity only matters for the considered type of solution. As a consequence, we can give a meaning of some product of distributions. Finally, by a fixed point theorem, we can match the coefficient with the solution itself. This is exactly the inviscid Burgers’ equation whose the selection principle allows to avoid the well-known time singularity.&lt;/p&gt;
&lt;p&gt;Discussant: &lt;a href=&#34;https://www.wias-berlin.de/people/butkovsky/&#34;&gt;Oleg Butkovsky&lt;/a&gt;, Weierstrass Institute for Applied Analysis and Stochastics, Germany&lt;/p&gt;
&lt;p&gt;The webinar is part of YoungStatS project of the Young Statisticians Europe initiative (FENStatS) supported by the Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics (IMS).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you missed this webinar, you can &lt;a href=&#34;https://www.youtube.com/watch?v=8jjlOsZ0cgs&#34;&gt;watch the recording on our YouTube channel&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Theory and Methods for Inference in Multi-armed Bandit Problems</title>
      <link>https://youngstats.github.io/post/2022/04/19/theory-and-methods-for-inference-in-multi-armed-bandit-problems/</link>
      <pubDate>Tue, 19 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/04/19/theory-and-methods-for-inference-in-multi-armed-bandit-problems/</guid>
      <description>


&lt;p&gt;&lt;strong&gt;Theory and Methods for Inference in Multi-armed Bandit Problems&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-04-19-theory-and-methods-for-inference-in-multi-armed-bandit-problems_files/Slika1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Multi-armed bandit (MAB) algorithms have been argued for decades as useful to conduct adaptively-randomized experiments. By skewing the allocation of the arms towards the more efficient or informative ones, they have the potential to enhance participants’ welfare, while resulting in a more flexible, efficient, and ethical alternative compared to traditional fixed studies. However, such allocation strategies complicate the problem of statistical inference. It is now recognized that traditional inference methods are typically not valid when used in MAB-collected data, leading to considerable biases in classical estimators and other relevant issues in hypothesis testing problems.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wednesday, May 11th, 7:00 PT / 10:00 EST / 16:00 CET.&lt;/li&gt;
&lt;li&gt;Online, via Zoom. The registration form is available &lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSexdudNXI46npEnpQZ0IwmcUNejJ8wSeZBjw1lNU4CftaFwUA/viewform?vc=0&amp;amp;c=0&amp;amp;w=1&amp;amp;flr=0&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.google.com/view/anandkalvit&#34;&gt;Anand Kalvit&lt;/a&gt;, Columbia University: “A Closer Look at the Worst-case Behavior of Multi-armed Bandit Algorithms”&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.stat.cmu.edu/~aramdas/&#34;&gt;Aaditya Ramdas&lt;/a&gt;, Carnegie Mellon University: “Safe, Anytime-Valid Inference in the face of 3 sources of bias in bandit data analysis”&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ruohanzhan.github.io/&#34;&gt;Ruohan Zhan&lt;/a&gt;, Stanford University: “Inference on Adaptively Collected Data”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prof. &lt;a href=&#34;https://www0.gsb.columbia.edu/faculty/azeevi/&#34;&gt;Assaf Zeevi&lt;/a&gt;, Columbia University&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The webinar is part of YoungStatS project of the Young Statisticians Europe initiative (FENStatS) supported by the Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics (IMS).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you missed this webinar, you can &lt;a href=&#34;https://www.youtube.com/watch?v=syoPMcLt3g4&#34;&gt;watch the recording on our YouTube channel&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Selection of Priors in Bayesian Structural Equation Modeling</title>
      <link>https://youngstats.github.io/post/2022/02/14/selection-of-priors-in-bayesian-structural-equation-modeling/</link>
      <pubDate>Mon, 14 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/02/14/selection-of-priors-in-bayesian-structural-equation-modeling/</guid>
      <description>


&lt;p&gt;&lt;strong&gt;Selection of Priors in Bayesian Structural Equation Modelling&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-02-14-selection-of-priors-in-bayesian-structural-equation-modeling_files/Cover2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Structural equation modeling (SEM) is an important framework within the social sciences that encompasses a wide variety of statistical models. Traditionally, estimation of SEMs has relied on maximum likelihood. Unfortunately, there also exist a variety of situations in which maximum likelihood performs subpar. This led researchers to turn to alternative estimation methods, in particular, Bayesian estimation of SEMs or BSEM. However, it is currently unclear how to specify the prior distribution in order to attain the advantages of Bayesian approaches.&lt;/p&gt;
&lt;p&gt;On the webinar, selected statisticians will present their recent works on the topic.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wednesday, April 20th, 7:00 PT / 10:00 ET / 16:00 Berlin/Amsterdam&lt;/li&gt;
&lt;li&gt;Online, via Zoom. The registration form is available
&lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSe-BAMdH2dhuON-kxKpVW3OgtnO2Qd6MWKlyUoaF_udbp5X2Q/viewform?vc=0&amp;amp;c=0&amp;amp;w=1&amp;amp;flr=0&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.mcgill.ca/psychology/milica-miocevic&#34;&gt;Milica Miočević&lt;/a&gt;, McGill University, Canada&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://winterstat.github.io/&#34;&gt;Sonja D. Winter&lt;/a&gt;, University of Missouri, USA&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://research.vu.nl/en/persons/mauricio-garnier-villarreal&#34;&gt;Mauricio Garnier-Villarreal&lt;/a&gt;, Vrije Universiteit Amsterdam, The Netherlands&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://saravanerp.com/&#34;&gt;Sara van Erp&lt;/a&gt;, Utrecht University, The Netherlands&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The webinar is part of YoungStatS project of the Young Statisticians Europe initiative (FENStatS) supported by the Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics (IMS).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you missed this webinar, you can &lt;a href=&#34;https://www.youtube.com/watch?v=Cuwhzd-8z4k&#34;&gt;watch the recording on our YouTube channel&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
