<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>YoungStatS</title>
    <link>https://youngstats.github.io/</link>
    <description>Recent content on YoungStatS</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 29 Apr 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://youngstats.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Recent Advances in Functional Data Analysis</title>
      <link>https://youngstats.github.io/post/2021/04/29/fda-webinar/</link>
      <pubDate>Thu, 29 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/04/29/fda-webinar/</guid>
      <description>&lt;p&gt;The fourth &amp;ldquo;&lt;em&gt;One World webinar&lt;/em&gt;&amp;rdquo; organized by YoungStatS will take place
on June 20th, 2021. The topic of this webinar is on Functional Data
Analysis. Selected young European researchers active in this area of
research will present their contributions on spherical functional
autoregressions, additive models, and clustering methods for functional
data, with the focus on both theoretical developments and applications.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Wednesday, June 30th, 16:30 CEST&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Online&lt;/em&gt;, via Zoom. The registration form will be provided on the YoungStatS website.
Further details and the Zoom link will be sent to the registered
addresses only.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Alessia Caponera (École polytechnique fédérale de Lausanne, Switzerland): &amp;ldquo;Asymptotics for spherical functional autoregressions&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Alexander Volkmann (Humboldt-Universität zu Berlin, Germany): &amp;ldquo;Multivariate Functional Additive Mixed Models&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fabio Centofanti (University of Naples Federico II, Italy): &amp;ldquo;Sparse and Smooth Functional Data Clustering&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Juhyun Park (École nationale supérieure d&#39;informatique pour
l&#39;industrie et l&#39;entreprise, France)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;The webinar is part of YoungStatS project of the Young Statisticians
Europe initiative (FENStatS) supported by the Bernoulli Society for
Mathematical Statistics and Probability.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For more information, please visit our
&lt;a href=&#34;https://youngstats.github.io/post/2020/09/10/webinars/&#34;&gt;website&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Composite-Based Structural Equation Modeling: Developments and Perspectives</title>
      <link>https://youngstats.github.io/post/2021/04/28/csem-webinar/</link>
      <pubDate>Wed, 28 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/04/28/csem-webinar/</guid>
      <description>&lt;p&gt;The third &amp;ldquo;&lt;em&gt;One World webinar&lt;/em&gt;&amp;rdquo; organized by YoungStatS will take place
on May 19th, 2021. The focus of this webinar will be on composite-based
structural equation modeling, particularly on partial least squares path
modeling (Wold, 1982; Lohmöller, 1989) and approaches to assess
composite models. The webinar will present some of the most interesting
and recent theoretical developments and applications from younger
scholars active in this area of research.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Wednesday, May 19th, 16:00 CEST&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Online&lt;/em&gt;, via Zoom. The registration form is available &lt;a href=&#34;https://docs.google.com/forms/d/19ozVQ71vZJi0_g_GYUC29w_xHnD9ub5HNC9sWbYhhvc/edit&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://econ.au.dk/research/researchcentres/creates/people/research-fellows/benjamin-d-liengaard/&#34;&gt;Benjamin Liengaard&lt;/a&gt; (Aarhus University, Denmark): &amp;lsquo;Measurement Invariance Testing with Latent Variable Scores using Partial Least Squares Path Modeling&amp;rsquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://nicholasdanks.com/&#34;&gt;Nicholas Danks&lt;/a&gt; (Trinity College Dublin, Ireland): &amp;lsquo;The Role of Prediction in Composite Modeling&amp;rsquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.florianschuberth.com/&#34;&gt;Florian Schuberth&lt;/a&gt; (University of Twente, The Netherlands): &amp;lsquo;Confirmatory Composite Analysis&amp;rsquo;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.henseler.com/&#34;&gt;Jörg Henseler&lt;/a&gt; (University of Twente, The Netherlands)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;The webinar is part of YoungStatS project of the Young Statisticians
Europe initiative (FENStatS) supported by the Bernoulli Society for
Mathematical Statistics and Probability.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For more information, please visit our
&lt;a href=&#34;https://youngstats.github.io/post/2020/09/10/webinars/&#34;&gt;website&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A small step to understand Generative Adversarial Networks</title>
      <link>https://youngstats.github.io/post/2021/04/27/a-small-step-to-understand-gda/</link>
      <pubDate>Tue, 27 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/04/27/a-small-step-to-understand-gda/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In the last decade, there have been spectacular advances on the practical side of machine learning.
One of the most impressive may be the success of Generative Adversarial Networks (GANs) for image generation &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-GoPoMiXuWaOzCoBe14&#34; role=&#34;doc-biblioref&#34;&gt;Goodfellow et al. 2014&lt;/a&gt;)&lt;/span&gt;.
State of the art models are capable of producing &lt;a href=&#34;https://www.youtube.com/watch?v=XOxxPcy5Gr4&#34;&gt;portraits of fake persons&lt;/a&gt; that look perfectly authentic to you and me (see e.g. &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-SaGoZaChRaCg16&#34; role=&#34;doc-biblioref&#34;&gt;Salimans et al. 2016&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-Karras2018&#34; role=&#34;doc-biblioref&#34;&gt;Karras et al. 2018&lt;/a&gt;)&lt;/span&gt;).
Other domains such as inpainting, text to image and speech are also concerned by outstanding results (see &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-Go16&#34; role=&#34;doc-biblioref&#34;&gt;Goodfellow 2016&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-JaLiBo20&#34; role=&#34;doc-biblioref&#34;&gt;Jabbar, Li, and Bourahla 2020&lt;/a&gt;)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Since their introduction by &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-GoPoMiXuWaOzCoBe14&#34; role=&#34;doc-biblioref&#34;&gt;Goodfellow et al. 2014&lt;/a&gt;)&lt;/span&gt;, GANs have unleashed passions in the community of machine learning, leading to a large volume of variants and possible applications, often referred to as &lt;a href=&#34;https://github.com/hindupuravinash/the-gan-zoo&#34;&gt;the GAN Zoo&lt;/a&gt;.
However, despite increasingly spectacular applications, little was known few years ago about the statistical properties of GANs.&lt;/p&gt;
&lt;p&gt;This post sketches the paper entitled ``Some Theoretical Properties of GANs’’ (G. Biau, B. Cadre, M. Sangnier and U. Tanielian, The Annals of Statistics, 2020),
which aims at building a statistical analysis of GANs in order to better understand their mathematical mechanism.
In particular, it proves a non-asymptotic bound on the excess of Jensen-Shannon error and the asymptotic normality of the parametric estimator.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathematical-framework&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mathematical framework&lt;/h2&gt;
&lt;div id=&#34;overview-of-the-method&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Overview of the method&lt;/h3&gt;
&lt;p&gt;The objective of GANs is to randomly generate artificial contents similar to some data.
Put another way, they are aimed at sampling according to an unknown distribution &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt;, based solely on i.i.d. observations &lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt; drawn according to &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt;.
Obviously, a naive approach would be to:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Estimate the distribution &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt; by some &lt;span class=&#34;math inline&#34;&gt;\(\hat P\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Sample according to &lt;span class=&#34;math inline&#34;&gt;\(\hat P\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;However, both tasks are difficult in themselves.
In particular, density estimation is made arduous by the complexity and high dimensionality of the data involved in the domain, relegating both standard parametric and nonparametric approaches unworkable.
Thus, GANs offer a completely different way to achieve our goal, often compared to the struggle between a police team, trying to distinguish true banknotes (the observed data &lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt;) from false ones (the generated data), and a counterfeiters team (the generator), slaving to produce banknotes as credible as possible and to mislead the police.&lt;/p&gt;
&lt;p&gt;To be a bit more specific, there are two brilliant ideas at the core of GANs:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Sample data in a very straightforward manner thanks to the transform method:
let &lt;span class=&#34;math inline&#34;&gt;\(\mathscr G=\left \{G_{\theta}: \mathbb R^\ell \to \mathbb R^d, \theta \in \Theta \right\}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\ell\)&lt;/span&gt; is the dimension of what is called the latent space and &lt;span class=&#34;math inline&#34;&gt;\(\Theta \subset \mathbb R^p\)&lt;/span&gt;, be a class of measurable functions, called generators (in practice &lt;span class=&#34;math inline&#34;&gt;\(\mathscr G\)&lt;/span&gt; is often a class of neural networks with &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; parameters).
Now, let us sample &lt;span class=&#34;math inline&#34;&gt;\(Z \sim \mathcal N(0, I_\ell)\)&lt;/span&gt; and compute &lt;span class=&#34;math inline&#34;&gt;\(U = G_\theta(Z)\)&lt;/span&gt;.
Then, &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is an observation drawn according to the distribution &lt;span class=&#34;math inline&#34;&gt;\(P_\theta = G_\theta \# N(0, I_\ell)\)&lt;/span&gt; (the push-forward measure of the latent distribution (N(0, I_)) according to (G_)).
In other words, the statistical model for the estimation of &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt; has the form &lt;span class=&#34;math inline&#34;&gt;\(\mathscr P = \left\{ P_\theta = G_\theta \# N(0, I_\ell), \theta \in \Theta \right\}\)&lt;/span&gt; and it is definitely straightforward to sample according to &lt;span class=&#34;math inline&#34;&gt;\(P_\theta\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Assessing the proximity between &lt;span class=&#34;math inline&#34;&gt;\(P_\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt; by comparing two samples &lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n \overset{i.i.d.}{\sim} P^\star\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(U_1, \dots, U_n \overset{i.i.d.}{\sim} P_\theta\)&lt;/span&gt;.
What does comparing mean?
Assume the group of &lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt; is very difficult to ``separate’’ from the group of &lt;span class=&#34;math inline&#34;&gt;\(U_1, \dots, U_n\)&lt;/span&gt;, or put another way,
it is very difficult to distinguish the class of &lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt; from the class of &lt;span class=&#34;math inline&#34;&gt;\(U_1, \dots, U_n\)&lt;/span&gt;.
Would you be convinced that the two distributions &lt;span class=&#34;math inline&#34;&gt;\(P_\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt; are very close (at least for large &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;)?
That is exactly the point.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-distributions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Comparing distributions&lt;/h3&gt;
&lt;p&gt;At this point, Task 2 is still a bit blurry and deserves further details about how to quantify the difficulty (or the ease) of separating the two classes &lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(U_1, \dots, U_n\)&lt;/span&gt;.
This problem is actually closely related to supervised learning, and in particular to classification:
assume that a classifier, let us say &lt;span class=&#34;math inline&#34;&gt;\(h : \mathbb R^d \to \{0, 1\}\)&lt;/span&gt;, manages to perfectly discriminate the two classes: &lt;span class=&#34;math inline&#34;&gt;\(\mathbb P(h(X_1)=1) = \mathbb P(h(U_1)=0) = 1\)&lt;/span&gt;, then we can say that the two distributions &lt;span class=&#34;math inline&#34;&gt;\(P_\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt; are different.
Conversely, if the classifier is fooled, that is &lt;span class=&#34;math inline&#34;&gt;\(\mathbb P(h(X_1)=1) = \mathbb P(h(U_1)=0) = \frac 12\)&lt;/span&gt;, we may accept that the two distributions are identical.&lt;/p&gt;
&lt;p&gt;This classification setting is formalized as following:
let &lt;span class=&#34;math inline&#34;&gt;\((X, Y)\)&lt;/span&gt; be a pair of random variables taking values in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb R^d \times \{0, 1\}\)&lt;/span&gt; such that:
&lt;span class=&#34;math display&#34;&gt;\[
    X|Y=1 \sim P^\star
    \quad \text{and} \quad
    X|Y=0 \sim P_\theta,
    %\tag{M}
\]&lt;/span&gt;
and let &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D = \left \{D_{\alpha} : \mathbb R^d \to [0, 1], \alpha \in \Lambda \right\}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\Lambda \subset \mathbb R^q\)&lt;/span&gt;, be a parametric model of discriminators such that &lt;span class=&#34;math inline&#34;&gt;\(D_\alpha(x)\)&lt;/span&gt; is aimed at estimating &lt;span class=&#34;math inline&#34;&gt;\(\mathbb P(Y=1 | X=x)\)&lt;/span&gt; (put another way, the distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y|X=x\)&lt;/span&gt; is estimated by &lt;span class=&#34;math inline&#34;&gt;\(\mathcal B(D_\alpha(x))\)&lt;/span&gt;).
For a given discriminator &lt;span class=&#34;math inline&#34;&gt;\(D_\alpha\)&lt;/span&gt;, the corresponding classifier is &lt;span class=&#34;math inline&#34;&gt;\(h : x \in \mathbb R^d \mapsto \mathbb 1_{D_\alpha(x) &amp;gt; \frac 12}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The sample ({ (X_1, 1), , (X_n, 1), (U_1, 0), , (U_n, 0) }), previously build by putting together observed and generated data, fits the classification model and can serve for estimating a classifier by maximizing the conditional log-likelihood:
&lt;span class=&#34;math display&#34;&gt;\[
  \hat \alpha \in \operatorname*{arg\,max}_{\alpha \in \Lambda} \hat L(\theta, \alpha),
  \quad \text{where} \quad
  \hat L(\theta, \alpha) = \frac 1n \sum_{i=1}^n \log(D_\alpha(X_i)) + \frac 1n \sum_{i=1}^n \log(1-D_\alpha(U_i)).
\]&lt;/span&gt;
In addition, the maximal log-likelihood &lt;span class=&#34;math inline&#34;&gt;\(\sup_{\alpha \in \Lambda} \hat L(\theta, \alpha)\)&lt;/span&gt; reflects exactly the ease of discrimination of the two classes &lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(U_1, \dots, U_n\)&lt;/span&gt;, that is the proximity between &lt;span class=&#34;math inline&#34;&gt;\(P_\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt;.
Task 2 is thus performed by introducing a class of discriminators (which are often neural networks) and maximizing a log-likelihood.
The latter quantity also helps in adjusting &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; such that the distribution &lt;span class=&#34;math inline&#34;&gt;\(P_{\theta}\)&lt;/span&gt; of the generated data &lt;span class=&#34;math inline&#34;&gt;\(G_\theta(Z)\)&lt;/span&gt; becomes closer and closer to &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In statistical terms, &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt; can be estimated by &lt;span class=&#34;math inline&#34;&gt;\(P_{\hat \theta}\)&lt;/span&gt;, where:
&lt;span class=&#34;math display&#34;&gt;\[
  \hat \theta \in \operatorname*{arg\,min}_{\theta \in \Theta} \sup_{\alpha \in \Lambda} \hat L(\theta, \alpha),
\]&lt;/span&gt;
where, as described previously, the generated data is &lt;span class=&#34;math inline&#34;&gt;\(U_i = G_\theta (Z_i)\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(Z_1, \dots, Z_n \overset{i.i.d.}{\sim} \mathcal N(0, I_\ell)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The story of GANs is not that gleaming since, in practice, we never have access to &lt;span class=&#34;math inline&#34;&gt;\(P_{\hat \theta}\)&lt;/span&gt;, which may be a very complicated object, but only to the generator &lt;span class=&#34;math inline&#34;&gt;\(G_{\hat \theta}\)&lt;/span&gt;.
Anyway, our aim is to sample according to &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt;, which can be achieved (up to the estimation error) thanks to &lt;span class=&#34;math inline&#34;&gt;\(G_{\hat \theta}(Z)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(Z \sim \mathcal N(0, I_\ell)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Actually, in this work, &lt;span class=&#34;math inline&#34;&gt;\(P_\theta\)&lt;/span&gt; is just as a mathematical object helping to understand GANs.
To go into details, the forthcoming results are based on the assumption that all distributions in play are absolutely continuous with respect to a known measure &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; (typically the Hausdorff measure on some submanifold of &lt;span class=&#34;math inline&#34;&gt;\(\mathbb R^d\)&lt;/span&gt;) and probability density functions are noted with lowercase letters (in particular &lt;span class=&#34;math inline&#34;&gt;\(P^\star = p^\star d\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P_\theta = p_\theta d\mu\)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;div id=&#34;concerning-the-comparison-of-distributions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Concerning the comparison of distributions&lt;/h3&gt;
&lt;p&gt;In order to give a mathematical foundation to our intuition in Task 2, it may be useful to analyze the big sample case, where
&lt;span class=&#34;math display&#34;&gt;\[\hat L(\theta, \alpha) \approx \mathbb E [\hat L(\theta, \alpha)] = \mathbb E [\log(D_\alpha(X_1))] + \mathbb E [\log(1-D_\alpha\circ G_\theta(Z_1))].\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If the class of discriminators &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D = \left\{ D_\alpha, \alpha \in \Lambda \right\}\)&lt;/span&gt; is rich enough to contain the ``optimal’’ discriminator &lt;span class=&#34;math inline&#34;&gt;\(D_\theta^\star = \frac{p^\star}{p_\theta + p^\star}\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(\theta \in \Theta\)&lt;/span&gt;,
then
&lt;span class=&#34;math display&#34;&gt;\[\sup_{\alpha \in \Lambda} \mathbb E [\hat L(\theta, \alpha)] = 2 D_{JS}(P^\star, P_\theta) - \log 4,\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(D_{JS}\)&lt;/span&gt; is the Jensen-Shannon divergence &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-EnSc03&#34; role=&#34;doc-biblioref&#34;&gt;Endres and Schindelin 2003&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Two things can be learned from this first result (still assuming that &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D\)&lt;/span&gt; contains ``optimal’’ discriminators):&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Up to the approximation capacity of &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\sup_{\alpha \in \Lambda} \hat L(\theta, \alpha)\)&lt;/span&gt; does reflect the proximity between &lt;span class=&#34;math inline&#34;&gt;\(P_\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt; (thanks to an approximated divergence).&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt; cannot be better than &lt;span class=&#34;math inline&#34;&gt;\(\theta^\star \in \operatorname*{arg\,min}_{\theta \in \Theta} \sup_{\alpha \in \Lambda} \mathbb E [\hat L(\theta, \alpha)] = \operatorname*{arg\,min}_{\theta \in \Theta} D_{JS}(P^\star, P_\theta)\)&lt;/span&gt;, which leads to the approximation &lt;span class=&#34;math inline&#34;&gt;\(P_{\theta^\star}\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt; obtained by minimizing the Jensen-Shannon divergence.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;non-asymptotic-bound-on-jensen-shannon-error&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Non-asymptotic bound on Jensen-Shannon error&lt;/h3&gt;
&lt;p&gt;Thus, GANs drive the world downhill in two directions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A limited approximation capacity for the class of discriminators &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D\)&lt;/span&gt; (which may not contain the ``optimal’’ discriminator &lt;span class=&#34;math inline&#34;&gt;\(D_\theta^\star = \frac{p^\star}{p_\theta + p^\star}\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(\theta \in \Theta\)&lt;/span&gt;): &lt;span class=&#34;math inline&#34;&gt;\(\sup_{\alpha \in \Lambda} \mathbb E [\hat L(\theta, \alpha)] &amp;lt; 2 D_{JS}(P^\star, P_\theta) - \log 4\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;A finite sample approximation: the criterion maximized is &lt;span class=&#34;math inline&#34;&gt;\(\hat L(\theta, \alpha)\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(\mathbb E [\hat L(\theta, \alpha)]\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These limitations introduce two kinds of error in the estimation procedure:
an approximation error (or bias), induced by the richness of &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D\)&lt;/span&gt;,
and an estimation error (or variance) occurring from the finiteness of the sample.&lt;/p&gt;
&lt;p&gt;This can be formalized in the following manner:
assume some regularity conditions of the first order on the models &lt;span class=&#34;math inline&#34;&gt;\(\mathscr P\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mathscr G\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D\)&lt;/span&gt;
and assume that optimal discriminators &lt;span class=&#34;math inline&#34;&gt;\(D_\theta^\star\)&lt;/span&gt; can be approximated by &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D\)&lt;/span&gt; up to an error &lt;span class=&#34;math inline&#34;&gt;\(\epsilon&amp;gt;0\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(L^2\)&lt;/span&gt;-norm.
Then:
&lt;span class=&#34;math display&#34;&gt;\[
  \mathbb E [D_{JS}(P^\star, P_{\hat \theta})] - D_{JS}(P^\star, P_{\theta^\star}) = O \left( \epsilon^2 + \frac{1}{\sqrt n} \right).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This result explains quantitatively that the discriminators in GANs have to be tuned carefully:
on the one hand, poor discriminators induce an uncontrolled gap between &lt;span class=&#34;math inline&#34;&gt;\(\sup_{\alpha \in \Lambda} \mathbb E [\hat L(\theta, \alpha)]\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(D_{JS}(P^\star, P_\theta)\)&lt;/span&gt;;
on the other hand, very flexible discriminators may lead to overfitting the finite sample.&lt;/p&gt;
&lt;p&gt;The first assertion is illustrated in the next figure.
The numerical experiment has been set up with classes of fully connected neural networks for the generators and the discriminators (respectively &lt;span class=&#34;math inline&#34;&gt;\(\mathscr G\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D\)&lt;/span&gt;) and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; sufficiently large.
The depth of the generators is either 2 (blue bars) or 3 (green bars) and the depth of the discriminator ranges from 2 to 5 (from left to right).
As expected, it appears clearly that the more flexible the discriminators are (from left to right), the smaller &lt;span class=&#34;math inline&#34;&gt;\(D_{JS}(P^\star, P_{\hat \theta})\)&lt;/span&gt; is.
Obviously, this is also inversely correlated with the richness of the class of generators &lt;span class=&#34;math inline&#34;&gt;\(\mathscr G\)&lt;/span&gt; (at least in a first regime).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-04-27-a-small-step-to-understand-GDA_files/divergences.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;asymptotic-normality&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Asymptotic normality&lt;/h3&gt;
&lt;p&gt;As a second important result, it can be shown that the estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt; is asymptotically normal with convergence rate &lt;span class=&#34;math inline&#34;&gt;\(\sqrt n\)&lt;/span&gt;.
More formally, let us assume &lt;span class=&#34;math inline&#34;&gt;\(\bar \theta \in \operatorname*{arg\,min}_{\theta\in\Theta} \sup_{\alpha \in \Lambda} \mathbb E [\hat L(\theta, \alpha)]\)&lt;/span&gt; exists and is unique.
Let also assume some regularity conditions of the second order on the models &lt;span class=&#34;math inline&#34;&gt;\(\mathscr P\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mathscr G\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D\)&lt;/span&gt;,
well definiteness and smoothness of &lt;span class=&#34;math inline&#34;&gt;\(\theta \mapsto \operatorname*{arg\,max}_{\alpha \in \Lambda} \mathbb E [\hat L(\theta, \alpha)]\)&lt;/span&gt; around &lt;span class=&#34;math inline&#34;&gt;\(\bar \theta\)&lt;/span&gt;.
Then, there exists a covariance matrice &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; such that:
&lt;span class=&#34;math display&#34;&gt;\[
  \sqrt n \left( \hat \theta - \bar \theta \right) \xrightarrow{dist} \mathcal N(0, \Sigma).
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;GANs have been statistically analyzed from the estimation point of view.
Even though some simplifications were made (known dominating measure &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, uniqueness of some quantities) compared to the empirical setting based on deep neural networks,
the theoretical results show the importance of tuning correctly the architecture of the discriminators,
and exhibit an asymptotic behavior similar to that of a standard M-estimator.&lt;/p&gt;
&lt;p&gt;It remains to study the impact of the architecture of neural nets on the performance of GANs, as well as their behavior in an overparametrized regime.
But that’s a different story.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;This post is based on&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;G. Biau, B. Cadre, M. Sangnier and U. Tanielian. 2020. ``Some Theoretical Properties of GANs.’’ The Annals of Statistics 48(3): 1539-1566.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-EnSc03&#34; class=&#34;csl-entry&#34;&gt;
Endres, D. M., and J. E. Schindelin. 2003. &lt;span&gt;“A New Metric for Probability Distributions.”&lt;/span&gt; &lt;em&gt;IEEE Transactions on Information Theory&lt;/em&gt; 49: 1858–60.
&lt;/div&gt;
&lt;div id=&#34;ref-Go16&#34; class=&#34;csl-entry&#34;&gt;
Goodfellow, I. 2016. &lt;em&gt;NIPS 2016 Tutorial: Generative Adversarial Networks&lt;/em&gt;. arXiv:1701.00160.
&lt;/div&gt;
&lt;div id=&#34;ref-GoPoMiXuWaOzCoBe14&#34; class=&#34;csl-entry&#34;&gt;
Goodfellow, I., J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and J. Bengio. 2014. &lt;span&gt;“Generative Adversarial Nets.”&lt;/span&gt; In &lt;em&gt;Advances in Neural Information Processing Systems 27&lt;/em&gt;, edited by Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, 2672–80. Red Hook: Curran Associates, Inc.
&lt;/div&gt;
&lt;div id=&#34;ref-JaLiBo20&#34; class=&#34;csl-entry&#34;&gt;
Jabbar, A., X. Li, and O. Bourahla. 2020. &lt;em&gt;A Survey on Generative Adversarial Networks: Variants, Applications, and Training&lt;/em&gt;. arXiv:2006.05132.
&lt;/div&gt;
&lt;div id=&#34;ref-Karras2018&#34; class=&#34;csl-entry&#34;&gt;
Karras, T., T. Aila, S. Laine, and J. Lehtinen. 2018. &lt;span&gt;“Progressive Growing of GANs for Improved Quality, Stability, and Variation.”&lt;/span&gt; In &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-SaGoZaChRaCg16&#34; class=&#34;csl-entry&#34;&gt;
Salimans, T., I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. 2016. &lt;span&gt;“Improved Techniques for Training &lt;span&gt;GAN&lt;/span&gt;s.”&lt;/span&gt; In &lt;em&gt;Advances in Neural Information Processing Systems 29&lt;/em&gt;, edited by D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, 2234–42. Red Hook: Curran Associates, Inc.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Generalizing the Neyman-Pearson Lemma for multiple hypothesis testing problems</title>
      <link>https://youngstats.github.io/post/2021/04/13/generalizing-the-neyman-pearson-lemma-for-multiple-hypothesis-testing-problems/</link>
      <pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/04/13/generalizing-the-neyman-pearson-lemma-for-multiple-hypothesis-testing-problems/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Let us start by considering the optimal rejection policy for a single hypothesis testing problem. There are three elements to the problem. The objective: to maximize the power to reject the null hypothesis; The constraint: to control the type I error probability, so that it is at most a predefined &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;; The decision policy: for every realized sample define the decision &lt;span class=&#34;math inline&#34;&gt;\(D\in \{0,1\}\)&lt;/span&gt;, where the null hypothesis is rejected if &lt;span class=&#34;math inline&#34;&gt;\(D=1\)&lt;/span&gt; and retained otherwise. For simplicity, let &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; be the realized test statistic on which the decision &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is based, and let &lt;span class=&#34;math inline&#34;&gt;\(g(z\mid h=1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g(z\mid h=0)\)&lt;/span&gt; be, respectively, the non-null and the null density of &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;. The optimization problem is therefore:
&lt;span class=&#34;math display&#34;&gt;\[\max_{D:\mathbb R \rightarrow \{0,1\}} \int D(z)g(z\mid h=1)dz\\ s.t.\int D(z)g(z\mid h=0)dz\leq \alpha.  \]&lt;/span&gt; This infinite dimensional integer problem happens to have a simple solution provided by the Neyman-Pearson (NP) Lemma.&lt;/p&gt;
&lt;p&gt;In this era of big data, conducting a study with a single hypothesis is rare. In many disciplines, hundreds or thousands of null hypotheses are tested in each study. In order to avoid an inflation of false positive findings, it is critical to use multiple testing procedures that guarantee that a meaningful error measure is controlled at a predefined level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. The most popular measures are the family wise error rate (FWER), i.e.,
the probability of at least one type I error, and the false discovery rate (FDR), i.e., the expected false discovery proportion.&lt;/p&gt;
&lt;p&gt;Arguably, it is just as critical that the multiple testing procedure will have high statistical power, thus facilitating scientific discoveries. As with the single hypothesis testing problem, we take an optimization approach to the problem with &lt;span class=&#34;math inline&#34;&gt;\(K&amp;gt;1\)&lt;/span&gt; null hypotheses: we seek to find the &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; decision functions, &lt;span class=&#34;math inline&#34;&gt;\(\vec D: {\mathbb R}^K\rightarrow \{0,1 \}^K\)&lt;/span&gt;,
that maximize some notion of power while guaranteeing that the error measure is at most &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. In this post, we shall consider the problem of finding the optimal decision functions when testing multiple hypotheses in a couple of settings of interest.&lt;/p&gt;
&lt;p&gt;This post is based on our three recent papers on this topic: Rosset et al. (2018), where a general theoretical framework is presented; Heller and Rosset (2021), where the two-group model is discussed; and Heller, Krieger and Rosset (2021), where optimal clinical trial design is discussed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;an-optimal-policy-for-the-two-group-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An optimal policy for the two-group model&lt;/h2&gt;
&lt;p&gt;The two-group model, which is widely used in large scale inference problems, assumes a Bayesian setting, where the observed test statistics are generated independently from the mixture model &lt;span class=&#34;math display&#34;&gt;\[(1-\pi)g(z\mid h=0)+\pi g(z\mid h=1),\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt; follows the &lt;span class=&#34;math inline&#34;&gt;\(Bernoulli(\pi)\)&lt;/span&gt; distribution, and indicates as before whether the null is true &lt;span class=&#34;math inline&#34;&gt;\((h=0)\)&lt;/span&gt; or false &lt;span class=&#34;math inline&#34;&gt;\((h=1)\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\vec z\)&lt;/span&gt; be a vector of length &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; generated according to the two group model, and consider the following optimization problem:
&lt;span class=&#34;math display&#34;&gt;\[\max_{\vec D:{\mathbb R}^K \rightarrow \{0,1\}^K} \mathbb E(\vec h^t \vec D) \\ s.t. \ FDR(\vec D) \leq \alpha.\]&lt;/span&gt; One of our main results is that the solution is to threshold the test statistic &lt;span class=&#34;math display&#34;&gt;\[\mathbb P(h=0\mid z) = \frac{(1-\pi)g(z\mid h=0)}{(1-\pi)g(z\mid h=0)+\pi g(z\mid h=1)}, \]&lt;/span&gt; and that the threshold depends on the entire vector of test statistics &lt;span class=&#34;math inline&#34;&gt;\(\vec z\)&lt;/span&gt;. Thus for a realized vector of test statistics &lt;span class=&#34;math inline&#34;&gt;\(\vec z = (z_1, \ldots, z_K)\)&lt;/span&gt;, the decision vector &lt;span class=&#34;math inline&#34;&gt;\(\vec D\)&lt;/span&gt; can be described in the following form: &lt;span class=&#34;math display&#34;&gt;\[\vec D(\vec z) = (\mathbb I[\mathbb P(h=0\mid z_1)\leq t(\vec z)], \ldots, \mathbb I[\mathbb P(h=0\mid z_k)\leq t(\vec z)]),\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\mathbb I[\cdot]\)&lt;/span&gt; is the indicator function. This leads to practical algorithms which improve theoretically and empirically on previous solutions for the two-group model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;an-optimal-policy-for-the-design-of-clinical-trials&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An optimal policy for the design of clinical trials&lt;/h2&gt;
&lt;p&gt;Assume that &lt;span class=&#34;math inline&#34;&gt;\(K=2\)&lt;/span&gt; hypothesis testing problems are examined in a clinical trial (e.g., treatment effectiveness in two distinct subgroups). The federal agencies that approve drugs typically require strong FWER control at level &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 0.05\)&lt;/span&gt;. At the design stage, it is necessary to decide on the number of subjects that will be allocated. For &lt;span class=&#34;math inline&#34;&gt;\(K=1\)&lt;/span&gt;, this is typically done by computing the minimal number of subjects for achieving minimal power with a type I error probability of &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. For &lt;span class=&#34;math inline&#34;&gt;\(K=2\)&lt;/span&gt;, with a relevant notion of required power &lt;span class=&#34;math inline&#34;&gt;\(\Pi(\vec D)\)&lt;/span&gt;, a smaller number of subjects will need to be allocated, if the multiple testing procedure is the optimal policy, rather than an off-the-shelf policy. However, finding the optimal policy may be difficult since
the policy has to guarantee that for every data generation with at least one null parameter value, the probability of rejecting the null hypothesis corresponding to the null parameter value should be at most &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; (i.e., there may be an infinite number of constraints for strong FWER control at level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;For simplicity, assume we have two independent test statistics that are normally distributed with variance one and expectation zero if the null hypothesis is true but negative if the null hypothesis is false. Then, the optimization problem can be formalized as follows:&lt;br /&gt;
&lt;span class=&#34;math display&#34;&gt;\[\max_{\vec D:{\mathbb R}^2 \rightarrow \{0,1\}^2} \Pi(\vec D) \\ s.t. \  \mathbb P_{(0, \theta)}(D_1=0) \leq \alpha \ \forall \ \theta &amp;lt;0; \ \mathbb P_{( \theta, 0 )}(D_2=0) \leq \alpha \ \forall \ \theta &amp;lt;0; \ \mathbb P_{(0, 0)}(\max(D_1,D_2)=0) \leq \alpha.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We also add the common-sense restriction that &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-values above &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; are not rejected (for details, see Heller, Krieger, and Rosset 2021).&lt;br /&gt;
The resulting optimal rejection policy is attractive for relevant objectives &lt;span class=&#34;math inline&#34;&gt;\(\Pi(\vec D)\)&lt;/span&gt; that can be useful for clinical trials.
Below we provide one example optimal policy: the policy maximizing the average power when both test statistics have expectation -2 (left panel). For comparison, we provide the off-the-shelf popular Hochberg procedure (which coincides with the Hommel procedure for &lt;span class=&#34;math inline&#34;&gt;\(K=2\)&lt;/span&gt;). Both policies satisfy the strong FWER control guarantee at level &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 0.05\)&lt;/span&gt;, but the average power of the optimal policy is 3% higher (63% versus 60%) if the test statistics are normally distributed with expectation -2. The color coding is as follows: in red, reject only the second hypothesis; in black, reject only the first hypothesis; in green, reject both hypotheses.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-04-13-generalizing-the-neyman-pearson-lemma-for-multiple-hypothesis-testing-problems_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;this-post-is-based-on&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;This post is based on&lt;/h2&gt;
&lt;p&gt;Heller, R. Krieger, A. and Rosset, S. (2021) &lt;em&gt;Optimal multiple testing and design in clinical trials&lt;/em&gt;. arXiv:2104.01346&lt;/p&gt;
&lt;p&gt;Heller, R. and Rosset, S. (2020) &lt;em&gt;Optimal control of false discovery criteria in the two-group model&lt;/em&gt;. Journal of the Royal Statistical Society, Series B,
&lt;a href=&#34;https://doi.org/10.1111/rssb.12403&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1111/rssb.12403&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Rosset, S. Heller, R. Painsky, A. and Aharoni, E. (2018) &lt;em&gt;Optimal and Maximin Procedures for Multiple Testing Problems&lt;/em&gt;. arXiv: 1804.10256&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;about-the-authors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;About the authors&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-04-13-generalizing-the-neyman-pearson-lemma-for-multiple-hypothesis-testing-problems_files/headRuth.jpg&#34; height=&#34;75&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ruth Heller&lt;/strong&gt; is an Associate Professor of Statistics at Tel-Aviv University, Israel, &lt;a href=&#34;mailto:ruheller@gmail.com&#34; class=&#34;email&#34;&gt;ruheller@gmail.com&lt;/a&gt;. Her research interests are in multiple comparisons, nonparametrics, and observational studies.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-04-13-generalizing-the-neyman-pearson-lemma-for-multiple-hypothesis-testing-problems_files/headSaharon.png&#34; height=&#34;75&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Saharon Rosset&lt;/strong&gt; is a Professor of Statistics at Tel-Aviv University, Israel, &lt;a href=&#34;mailto:saharon@tauex.tau.ac.il&#34; class=&#34;email&#34;&gt;saharon@tauex.tau.ac.il&lt;/a&gt;. His research interests are in Statistical Learning theory and methods
and in Statistical Genetics.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Developments in Bayesian Nonparametrics (updated with slides)</title>
      <link>https://youngstats.github.io/post/2021/04/06/bnp-webinar/</link>
      <pubDate>Tue, 06 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/04/06/bnp-webinar/</guid>
      <description>&lt;p&gt;The second &lt;em&gt;&amp;ldquo;One World webinar&amp;rdquo;&lt;/em&gt; organized by YoungStatS will take place on April 21st.
The focus of this webinar will be on illustrating modern advances in Bayesian Nonparametrics data analysis, discussing challenging theoretical problems and stimulating case-studies within this active area of research.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Wednesday, April 21st, 16:30 CEST&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Online&lt;/em&gt;, via Zoom. The registration form is available &lt;a href=&#34;https://forms.gle/vfinGjQJMeqhq6HQ7&#34;&gt;here&lt;/a&gt;. Further details and the Zoom link will be sent to the registered addresses only.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://francescapanero.github.io/&#34;&gt;Francesca Panero&lt;/a&gt; (University of Oxford, UK): &lt;em&gt;&amp;ldquo;Sparse Spatial Random Graphs&amp;rdquo;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://martacatalano.github.io/&#34;&gt;Marta Catalano&lt;/a&gt; (University of Torino, Italy). &lt;em&gt;&amp;ldquo;Measuring dependence in the Wasserstein distance for Bayesian nonparametric models&amp;rdquo;&lt;/em&gt; [&lt;a href=&#34;https://youngstats.github.io/post/2021/04/06/bnp-webinar/dBNP_Catalano.pdf&#34;&gt;talk slides&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://salleuska.github.io/&#34;&gt;Sally Paganin&lt;/a&gt; (Harvard School of Public Health, USA) &lt;em&gt;&amp;ldquo;Informative model-based clustering via Centered Partition Processes&amp;rdquo;&lt;/em&gt;  [&lt;a href=&#34;https://youngstats.github.io/post/2021/04/06/bnp-webinar/dBNP_Paganin.pdf&#34;&gt;talk slides&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.google.com/unimib.it/camerlenghi-federico/&#34;&gt;Federico Camerlenghi&lt;/a&gt; (University of Milano Bicocca, Italy) [&lt;a href=&#34;https://youngstats.github.io/post/2021/04/06/bnp-webinar/dBNP_Discussion_Camerlenghi.pdf&#34;&gt;discussion slides&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;The webinar is part of YoungStatS project of the Young Statisticians Europe initiative (FENStatS) supported by the Bernoulli Society for Mathematical Statistics and Probability.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For more information, please visit our &lt;a href=&#34;https://youngstats.github.io/post/2020/09/10/webinars/&#34;&gt;website&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Analysis of a Two-Layer Neural Network via Displacement Convexity</title>
      <link>https://youngstats.github.io/post/2021/03/14/analysis-of-a-two-layer-neural-network-via-displacement-convexity/</link>
      <pubDate>Sun, 14 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/03/14/analysis-of-a-two-layer-neural-network-via-displacement-convexity/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;abstract&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;We consider the problem of learning a function defined on a compact domain, using linear combinations
of a large number of “bump-like” components (neurons). This idea lies at the core of a variety of methods
from two-layer neural networks to kernel regression, to boosting. In general, the resulting risk minimization
problem is non-convex and is solved by gradient descent or its variants. Nonetheless, little is known about
global convergence properties of these approaches. In this work, we show that, as the number of neurons
diverges and the bump width tends to zero, the gradient flow has a limit which is a viscous porous medium
equation. By virtue of a property named “displacement convexity,” we show an exponential dimension-free
convergence rate for gradient descent.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-14-analysis-of-a-two-layer-neural-network-via-displacement-convexity_files/screenshot.82.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This post is based on the paper &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-javanmard2020analysis&#34; role=&#34;doc-biblioref&#34;&gt;Javanmard et al. 2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fitting a function with a linear combination of components.&lt;/strong&gt; In
supervised learning, we are given data points
&lt;span class=&#34;math inline&#34;&gt;\(\{(y_j,{\boldsymbol{x}}_j)\}_{j\le n}\)&lt;/span&gt;, which are often assumed to be
independent and identically distributed. Here
&lt;span class=&#34;math inline&#34;&gt;\({\boldsymbol{x}}_j\in {\mathbb R}^d\)&lt;/span&gt; is a feature vector, and
&lt;span class=&#34;math inline&#34;&gt;\(y_j\in{\mathbb R}\)&lt;/span&gt; is a label or response variable. We would like to
fit a model &lt;span class=&#34;math inline&#34;&gt;\(\widehat{f}:{\mathbb R}^d\to{\mathbb R}\)&lt;/span&gt; to predict the
labels at new points &lt;span class=&#34;math inline&#34;&gt;\({\boldsymbol{x}}\in{\mathbb R}^d\)&lt;/span&gt;. One of the most
fruitful ideas in this context is to use functions that are linear
combinations of simple components:
&lt;span class=&#34;math display&#34;&gt;\[\widehat{f}({\boldsymbol{x}};{\boldsymbol{w}}) = \frac{1}{N}\sum_{i=1}^N \sigma({\boldsymbol{x}};{\boldsymbol{w}}_i)\, ,\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\sigma:{\mathbb R}^d\times{\mathbb R}^D\to{\mathbb R}\)&lt;/span&gt; is a
component function (a ‘neuron’ or ‘unit’ in the neural network
parlance), and
&lt;span class=&#34;math inline&#34;&gt;\({\boldsymbol{w}}=({\boldsymbol{w}}_1,\dots,{\boldsymbol{w}}_N)\in{\mathbb R}^{D\times N}\)&lt;/span&gt;
are the parameters to be learnt from data. Specific instantiations of
this idea include, e.g., two-layer neural networks with radial
activations, sparse deconvolution, kernel ridge regression, random
feature methods, and boosting.&lt;/p&gt;
&lt;p&gt;A common approach towards fitting parametric models is by risk
minimization:
&lt;span class=&#34;math display&#34;&gt;\[R_N({\boldsymbol{w}}) = {\mathbb E}\Big\{\Big[y-\frac{1}{N}\sum_{i=1}^N\sigma({\boldsymbol{x}};{\boldsymbol{w}}_i)\Big]^2\Big\}\, .\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Despite the impressive practical success of these methods, the risk
function &lt;span class=&#34;math inline&#34;&gt;\(R_N({\boldsymbol{w}})\)&lt;/span&gt; is highly non-convex and little is
known about the global convergence of algorithms that try to minimize
it. The main objective of this work is to introduce a nonparametric
underlying regression model for which a global convergence result can be
proved for stochastic gradient descent (SGD), in the limit of a large
number of neurons.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Setting and main result.&lt;/strong&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(\Omega\subset{\mathbb R}^d\)&lt;/span&gt; be a
compact convex set with smooth boundary, and let
&lt;span class=&#34;math inline&#34;&gt;\(\{(y_j,{\boldsymbol{x}}_j)\}_{j\ge 1}\)&lt;/span&gt; be i.i.d. with
&lt;span class=&#34;math inline&#34;&gt;\({\boldsymbol{x}}_j\sim {\sf Unif}(\Omega)\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\({\mathbb E}(y_j|{\boldsymbol{x}}_j)=f({\boldsymbol{x}}_j)\)&lt;/span&gt;, where the
function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is smooth. We try to fit these data using a combination of
bumps, namely
&lt;span class=&#34;math display&#34;&gt;\[\widehat{f}({\boldsymbol{x}};{\boldsymbol{w}})= \frac{1}{N}\sum_{i=1}^NK^\delta({\boldsymbol{x}}-{\boldsymbol{w}}_i)\, ,\]&lt;/span&gt;
where
&lt;span class=&#34;math inline&#34;&gt;\(K^\delta({\boldsymbol{x}}) = \delta^{-d}K({\boldsymbol{x}}/\delta)\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(K:{\mathbb R}^d\to{\mathbb R}_{\ge 0}\)&lt;/span&gt; is a first order kernel with
compact support. The weights &lt;span class=&#34;math inline&#34;&gt;\({\boldsymbol{w}}_i\in{\mathbb R}^d\)&lt;/span&gt;
represent the centers of the bumps. Our model is general enough to
include a broad class of radial-basis function (RBF) networks which are
known to be universal function approximators. To the best of our
knowledge, there is no result on the global convergence of stochastic
gradient descent for learning RBF networks, and we establish the first
result of this type.&lt;/p&gt;
&lt;p&gt;We prove that, for sufficiently large &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; and small &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;, gradient
descent algorithms converge to weights &lt;span class=&#34;math inline&#34;&gt;\({\boldsymbol{w}}\)&lt;/span&gt; with nearly
optimum prediction error, provided &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is strongly concave. Let us
emphasize that the resulting population risk &lt;span class=&#34;math inline&#34;&gt;\(R_N({\boldsymbol{w}})\)&lt;/span&gt; is
non-convex regardless of the concavity properties of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;. Our proof
unveils a novel mechanism by which global convergence takes place.
Convergence results for non-convex empirical risk minimization are
generally proved by carefully ruling out local minima in the cost
function. Instead we prove that, as &lt;span class=&#34;math inline&#34;&gt;\(N\to\infty\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\delta\to 0\)&lt;/span&gt;, the
gradient descent dynamics converges to a gradient flow in Wasserstein
space, and that the corresponding cost function is ‘displacement
convex.’ Breakthrough results in optimal transport theory guarantee
dimension-free convergence rates for this limiting dynamics
&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-carrillo2003kinetic&#34; role=&#34;doc-biblioref&#34;&gt;Carrillo, McCann, and Villani 2003&lt;/a&gt;)&lt;/span&gt;. In particular, we expect the cost function
&lt;span class=&#34;math inline&#34;&gt;\(R_N({\boldsymbol{w}})\)&lt;/span&gt; to have many local minima, which are however
completely neglected by gradient descent.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof idea.&lt;/strong&gt; Let us start by providing some high-level insights about
our approach. Think about each model parameter as a particle moving
under the effect of other particles according to the SGD updates. Now,
instead of studying the microscopic dynamics of this system of
particles, we analyze the macroscopic dynamics of the medium when the
number of particles (i.e., the size of the hidden layer of the neural
network) goes to infinity. These dynamics are formulated through a
partial differential equation (more specifically, a viscous porous
medium equation) that describes the evolution of the mass density over
space and time. The nice feature of this approach is that, while the SGD
trajectory is a random object, it shows that in the large particle size
limit, it concentrates around the deterministic solution of this partial
differential equation (PDE).&lt;/p&gt;
&lt;p&gt;For a rigorous analysis and implementation of this idea, we use
propagation-of-chaos techniques. Specifically, we show that, in the
large &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; limit, the evolution of the weights
&lt;span class=&#34;math inline&#34;&gt;\(\{{\boldsymbol{w}}_i\}_{i=1}^N\)&lt;/span&gt; under gradient descent can be replaced
by the evolution of a probability distribution &lt;span class=&#34;math inline&#34;&gt;\(\rho^{\delta}_t\)&lt;/span&gt; which
satisfies the viscous porous medium PDE (with Neumann boundary
conditions). This PDE can also be described as the Wasserstein &lt;span class=&#34;math inline&#34;&gt;\(W_2\)&lt;/span&gt;
gradient flow for the following effective risk
&lt;span class=&#34;math display&#34;&gt;\[R^{\delta}(\rho) = \frac{1}{|\Omega|} \, \int_{\Omega} \big[f({\boldsymbol{x}}) - K^\delta\ast \rho({\boldsymbol{x}})\big]^2{\rm d}{\boldsymbol{x}}\, ,\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(|\Omega|\)&lt;/span&gt; is the volume of the set &lt;span class=&#34;math inline&#34;&gt;\(\Omega\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\ast\)&lt;/span&gt; is the
usual convolution. The use of &lt;span class=&#34;math inline&#34;&gt;\(W_2\)&lt;/span&gt; gradient flows to analyze two-layer
neural networks was recently developed in several papers
&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mei2018mean&#34; role=&#34;doc-biblioref&#34;&gt;Mei, Montanari, and Nguyen 2018&lt;/a&gt;; &lt;a href=&#34;#ref-rotskoff2018neural&#34; role=&#34;doc-biblioref&#34;&gt;Rotskoff and Vanden-Eijnden 2019&lt;/a&gt;; &lt;a href=&#34;#ref-chizat2018global&#34; role=&#34;doc-biblioref&#34;&gt;Chizat and Bach 2018&lt;/a&gt;; &lt;a href=&#34;#ref-sirignano2018mean&#34; role=&#34;doc-biblioref&#34;&gt;Sirignano and Spiliopoulos 2020&lt;/a&gt;)&lt;/span&gt;.
However, we cannot rely on earlier results because of the specific
boundary conditions in our problem.&lt;/p&gt;
&lt;p&gt;Note that even though the cost &lt;span class=&#34;math inline&#34;&gt;\(R^{\delta}(\rho)\)&lt;/span&gt; is quadratic and
convex in &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;, its &lt;span class=&#34;math inline&#34;&gt;\(W_2\)&lt;/span&gt; gradient flow can have multiple fixed
points, and hence global convergence cannot be guaranteed. Indeed, the
mathematical property that controls global convergence of &lt;span class=&#34;math inline&#34;&gt;\(W_2\)&lt;/span&gt; gradient
flows is not ordinary convexity but &lt;em&gt;displacement convexity&lt;/em&gt;. Roughly
speaking, displacement convexity is convexity along geodesics of the
&lt;span class=&#34;math inline&#34;&gt;\(W_2\)&lt;/span&gt; metric. Note that the risk function &lt;span class=&#34;math inline&#34;&gt;\(R^{\delta}(\rho)\)&lt;/span&gt; is &lt;em&gt;not&lt;/em&gt;
even displacement convex. However, for small &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;, we can formally
approximate &lt;span class=&#34;math inline&#34;&gt;\(K^\delta\ast \rho\approx \rho\)&lt;/span&gt;, and hence hope to replace
the risk function &lt;span class=&#34;math inline&#34;&gt;\(R^{\delta}(\rho)\)&lt;/span&gt; with the simpler one
&lt;span class=&#34;math display&#34;&gt;\[R(\rho) = \frac{1}{|\Omega|}\int_{\Omega} \big[f({\boldsymbol{x}}) - \rho({\boldsymbol{x}})\big]^2{\rm d}{\boldsymbol{x}}\, .\]&lt;/span&gt;
Most of our technical work is devoted to making rigorous this
&lt;span class=&#34;math inline&#34;&gt;\(\delta\to 0\)&lt;/span&gt; approximation.&lt;/p&gt;
&lt;p&gt;Remarkably, the risk function &lt;span class=&#34;math inline&#34;&gt;\(R(\rho)\)&lt;/span&gt; is strongly displacement convex
(provided &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is strongly concave). A long line of work in PDE and
optimal transport theory establishes dimension-free convergence rates
for its &lt;span class=&#34;math inline&#34;&gt;\(W_2\)&lt;/span&gt; gradient flow &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-carrillo2003kinetic&#34; role=&#34;doc-biblioref&#34;&gt;Carrillo, McCann, and Villani 2003&lt;/a&gt;)&lt;/span&gt;. By putting
everything together, we are able to show that SGD converges
exponentially fast to a near-global optimum with a rate that is
controlled by the convexity parameter of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A numerical illustration.&lt;/strong&gt; We demonstrate in a simple numerical
example that the convergence rate predicted by our asymptotic theory is
in excellent agreement with simulations. In the left column of the
figure below, we plot the true function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; together with the neural
network estimate obtained by running stochastic gradient descent (SGD)
with &lt;span class=&#34;math inline&#34;&gt;\(N=200\)&lt;/span&gt; neurons at several points in time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. Different plots
correspond to different values of &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; with
&lt;span class=&#34;math inline&#34;&gt;\(\delta\in\{1/5, 1/10, 1/20\}\)&lt;/span&gt;. We observe that the network estimates
converge to a limit curve which is an approximation of the true function
&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;. As expected, the quality of the approximation improves as &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;
gets smaller. In the right column, we report the evolution of the
population risk and we compare it to the risk predicted by the limit PDE
(corresponding to &lt;span class=&#34;math inline&#34;&gt;\(\delta=0\)&lt;/span&gt;). The PDE curve appears to capture well the
evolution of SGD towards optimality.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-14-analysis-of-a-two-layer-neural-network-via-displacement-convexity_files/screenshot.81.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;authors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Authors&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-14-analysis-of-a-two-layer-neural-network-via-displacement-convexity_files/javanmard.jpg&#34; width=&#34;75&#34; /&gt;
Adel Javanmard, Data Science and Operations Department, Marshall School of Business, University of Southern California&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-14-analysis-of-a-two-layer-neural-network-via-displacement-convexity_files/mondelli.jpg&#34; width=&#34;75&#34; /&gt;
Marco Mondelli, Institute of Science and Technology (IST) Austria&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-14-analysis-of-a-two-layer-neural-network-via-displacement-convexity_files/montanari.jpg&#34; width=&#34;75&#34; /&gt;
Andrea Montanari, Department of Electrical Engineering and Department of Statistics, Stanford University&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-carrillo2003kinetic&#34; class=&#34;csl-entry&#34;&gt;
Carrillo, José A., Robert J. McCann, and Cédric Villani. 2003. &lt;span&gt;“Kinetic Equilibration Rates for Granular Media and Related Equations: Entropy Dissipation and Mass Transportation Estimates.”&lt;/span&gt; &lt;em&gt;Revista Matematica Iberoamericana&lt;/em&gt; 19 (3): 971–1018.
&lt;/div&gt;
&lt;div id=&#34;ref-chizat2018global&#34; class=&#34;csl-entry&#34;&gt;
Chizat, Lenaic, and Francis Bach. 2018. &lt;span&gt;“On the Global Convergence of Gradient Descent for over-Parameterized Models Using Optimal Transport.”&lt;/span&gt; In &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt;, 3040–50.
&lt;/div&gt;
&lt;div id=&#34;ref-javanmard2020analysis&#34; class=&#34;csl-entry&#34;&gt;
Javanmard, Adel, Marco Mondelli, Andrea Montanari, and others. 2020. &lt;span&gt;“Analysis of a Two-Layer Neural Network via Displacement Convexity.”&lt;/span&gt; &lt;em&gt;Annals of Statistics&lt;/em&gt; 48 (6): 3619–42.
&lt;/div&gt;
&lt;div id=&#34;ref-mei2018mean&#34; class=&#34;csl-entry&#34;&gt;
Mei, Song, Andrea Montanari, and Phan-Minh Nguyen. 2018. &lt;span&gt;“A Mean Field View of the Landscape of Two-Layer Neural Networks.”&lt;/span&gt; &lt;em&gt;Proceedings of the National Academy of Sciences&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.1073/pnas.1806579115&#34;&gt;https://doi.org/10.1073/pnas.1806579115&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-rotskoff2018neural&#34; class=&#34;csl-entry&#34;&gt;
Rotskoff, Grant M., and Eric Vanden-Eijnden. 2019. &lt;span&gt;“Trainability and Accuracy of Neural Networks: An Interacting Particle System Approach.”&lt;/span&gt; &lt;em&gt;Communications on Pure and Applied Mathematics&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-sirignano2018mean&#34; class=&#34;csl-entry&#34;&gt;
Sirignano, Justin, and Konstantinos Spiliopoulos. 2020. &lt;span&gt;“Mean Field Analysis of Neural Networks: A Law of Large Numbers.”&lt;/span&gt; &lt;em&gt;SIAM Journal on Applied Mathematics&lt;/em&gt; 80 (2): 725–52.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Scalable Empirical Bayes Approach to Variable Selection in Generalized Linear Models</title>
      <link>https://youngstats.github.io/post/2021/03/13/a-scalable-empirical-bayes-approach-to-variable-selection-in-generalized-linear-models/</link>
      <pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/03/13/a-scalable-empirical-bayes-approach-to-variable-selection-in-generalized-linear-models/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In the toolbox of most scientists over the past century, there have been few methods as powerful and as versatile as linear regression. The introduction of the generalized linear model (GLM) framework in the 1970’s extended the inferential and predictive capabilities to binary or count data. While the effect of this ‘Swiss Army knife’ of scientific research cannot be overstated, rapid (and amazing) technological advances in other areas have pushed it beyond its theoretical capacity.&lt;/p&gt;
&lt;p&gt;Linear regression is obtained by applying Gauss’ ordinary least squares (OLS) method, which relies on having a sample size which is sufficiently larger than the number of predictors. However, in this age of ‘high throughput’ data, such as RNA sequencing, the number of predictors (say, the expression level of thousands of genes) greatly exceeds the number of observations. This is often called the ‘large P, small n’ problem. In such cases, the OLS procedure does not work, and the natural next step is to perform variable selection and choose a small number of predictors which are associated with the response variable.&lt;/p&gt;
&lt;p&gt;The question is, how to choose the right predictors? A good selection method should detect most (ideally, all) the true predictors, but not at the expense of getting too many false ones (in the ideal case, it would yield none.) This has been one of the greatest challenges in statistics in the past 25 years. The most widely used approach is based on adding a ‘penalty term’ to the optimization problem, so as to practically put a constraint on the number of predictors which can have a non-zero effect on the outcome. Among the methods which rely on this approach, the most well-known is the lasso. The approach we take in our paper is different. We assume that the effect predictors have on the outcome can be either positive, negative, or have no effect at all (which we call null predictors). Mathematically, we model it as a three-component mixture in which the effects of the non-null predictors are assumed to follow normal distributions. This approach is related to the Bayesian ‘spike-and-slab’ approach. The difference is that in the spike-and-slab model, the non-null predictors are assumed to follow a single normal distribution with mean 0. The difference between the two approaches is illustrated in Figure 1.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-14-a-scalable-empirical-bayes-approach-to-variable-selection-in-generalized-linear-models_files/fig1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 1 A graphical representation of the spike and slab model vs. our mixture model&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Real data, however, is noisy, which means that the measured effects of the null predictors is not a spike at zero. Rather, it is a Gaussian distribution which has a fair amount of overlap with the distributions of the true predictors. This is what makes the variable selection task so hard, regardless of which approach is used.&lt;/p&gt;
&lt;p&gt;The three-component mixture of normal distributions which we use in our model allows us, with a simple adaptation, to perform variable selection in the GLM framework. For example, we apply it to binary-type response in order to identify which gut bacteria is associated with obesity status. It can also be used to perform variable selection in survival analysis by modeling the number of new events at time t as a Poisson process. We identify five genes which are associated with survival probability of breast cancer patients.&lt;/p&gt;
&lt;p&gt;Our method (which is implemented in an R package called SEMMS) includes three important features. First, it allows the user to ‘lock in’ predictors, which means that it is possible to choose certain substantively important variables to always be included in the model, and not be subject to the selection procedure. Second, for the initialization of the algorithm we can use the variables selected by other methods (e.g., lasso, MCP) and because the algorithm will never result in a decrease in the log-likelihood from one iteration to the next, our final model will always be at least as good as the one obtained by the other variable selection method which was used in the initialization step. Third, the model accounts for correlations between pairs of predictors. This greatly alleviates the problem of identifying true predictors in the presence of multicollinearity. Methods based on penalization often struggle in the presence of multicollinearity. This is demonstrated in the following example, where the objective is to identify which of 4,088 genes is associated with the production rate of riboflavin. Out of 8,353,828 possible pairs of genes, 70,349 have correlation coefficient greater than 0.8 (in absolute value). This high degree of multicollinearity, combined with the relatively small sample size (N=71) makes this dataset especially challenging for variable selection methods. The model selected by SEMMS results in much smaller residuals than ones obtained from other variable selection approaches. It also provides much better prediction for low values of riboflavin.&lt;/p&gt;
&lt;p&gt;We conclude with a few words of caution. No matter which method is used, beware of declaring the selected model as the “best” because (i) the number of putative variables is so large that there is no way to evaluate all possible models, and (ii) some selected predictors can be part of a network of highly correlated variables. The riboflavin data provides an excellent example, which is illustrated in network diagram in Figure 2.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-14-a-scalable-empirical-bayes-approach-to-variable-selection-in-generalized-linear-models_files/fig2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 2 Riboflavin data – a graphical representation of the model found by SEMMS&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;SEMMS detects six strong predictors for riboflavin production rate (shown as red diamonds), but it also detects that four of them are highly correlated with other genes (orange dots). One gene (YURQ_at) is co-expressed with a large group of genes, each of which could be considered as a relevant predictor for the response. A network diagram like this helps to illustrate not only that there may be no single “best” model, but also that there may be complicated interactions between the predictors, and thus, the relationship between the predictors and the response may not be linear.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-14-a-scalable-empirical-bayes-approach-to-variable-selection-in-generalized-linear-models_files/HaimBar-280x400.jpg&#34; width=&#34;75&#34; /&gt;
Haim Bar is an Associate Professor in Statistics at the University of Connecticut. His professional interests include statistical modeling, shrinkage estimation, high throughput applications in genomics, Bayesian statistics, variable selection, and machine learning.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-14-a-scalable-empirical-bayes-approach-to-variable-selection-in-generalized-linear-models_files/Jim%20Booth_crop.jpg&#34; width=&#34;75&#34; /&gt;
James Booth is a Professor in the Department of Statistics and Data Science at Cornell University. He completed his PhD at the University of Kentucky in 1987 working with advisers Joseph Gani and Richard Kryscio, after which he was hired as an Assistant Professor at the University of Florida. He was hired at Cornell in 2004.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-14-a-scalable-empirical-bayes-approach-to-variable-selection-in-generalized-linear-models_files/marty-wells_crop.jpg&#34; width=&#34;75&#34; /&gt;
Martin T. Wells is the Charles A. Alexander Professor of Statistical Science and Chair of the Department of Statistics and Science at Cornell University. He also has joint appointments in the Cornell Law and Weill Medical Schools.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Compositional scalar-on-function regression as a tool (not only) for geological data</title>
      <link>https://youngstats.github.io/post/2021/03/10/compositional-scalar-on-function-regression-as-a-tool-not-only-for-geological-data/</link>
      <pubDate>Wed, 10 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/03/10/compositional-scalar-on-function-regression-as-a-tool-not-only-for-geological-data/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Compositional data are characterized by the fact that the relevant information is contained not necessarily in the absolute values but rather in the relative proportions between particular components. As an example, take household expenditures for different purposes (housing, groceries, travel etc.) or geochemical composition of a certain soil sample. In the latter case, the resulting composition of chemical elements is determined strongly by the particle size distribution (PSD, i.e., distribution of the size of soil grains). These distributions - although sampled in their discrete form as histogram data - show both relative and functional character and therefore can be described through probability density functions. A valid question to ask is how to modify the common multiple and/or functional regression model for the introduced case of relative (compositional) data.&lt;/p&gt;
&lt;div id=&#34;functional-regression-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Functional regression model&lt;/h2&gt;
&lt;p&gt;First, take a look at the standard functional data analysis (FDA) approach which was developed for functions from &lt;span class=&#34;math inline&#34;&gt;\(L^2\)&lt;/span&gt; space. A functional linear regression model with functional predictor is built as
&lt;span class=&#34;math display&#34;&gt;\[
y_{i} = \beta_{0} + \int_{I} \beta_{1}(t)f_{i}(t)dt + \epsilon_{i},\quad i=1,\dots,N,\quad t \in I
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\beta_{0}\)&lt;/span&gt; is the scalar intercept and &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt; represents the functional regression parameter. This model can be seen as an extension of the multiple regression - therefore, the estimators &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta_{0}}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta_{1}}\)&lt;/span&gt; minimize the following sum of squared errors (SSE)
&lt;span class=&#34;math display&#34;&gt;\[
\text{SSE} (\beta_{0},\beta_{1}) = \sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\int_{I}\beta_{1}(t)f_{i}(t)dt\right)^2.
\]&lt;/span&gt;
Unfortunately, it is not common for functional data to be available in its continuous form - we are usually left with dicrete observations. To represent the sparsely measured data as functions, a proper basis expansion for both the predictors and &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt; is necessary. This way, a reduction to a multivariate problem can be achieved. Furthermore, it is useful to apply the results of functional principal component analysis (FPCA) to project the data into a lower-dimensional space.&lt;/p&gt;
&lt;p&gt;But how can we use these ideas and adapt them for the situation where the covariate consists of density functions? As each PSD forms a probability density function on the considered support, specific properties of densities (scale invariance, relative scale, unit integral) prevent from using standard FDA methods directly to PSDs. Instead, we acknowledge the possibility to represent density functions in the Bayes space &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B^2}\)&lt;/span&gt; with square-integrable log-densities as they can be then adequately represented in the &lt;span class=&#34;math inline&#34;&gt;\(L^2\)&lt;/span&gt; space due to the isomorphism between &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{B^2}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(L^2\)&lt;/span&gt;. Frequently, the &lt;em&gt;centered log-ratio&lt;/em&gt; (clr) transformation
&lt;span class=&#34;math display&#34;&gt;\[
\text{clr}(f)(t):=f_{c}(t)= \text{ln} f(t) - \frac{1}{\eta}\int_{I}\text{ln}f(t) dt
\]&lt;/span&gt;
is used to the original densities with &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; representing the length of their common (bounded) support &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt;. It can be shown that the clr transformation of densities enforces the resulting functions to integrate on &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; to 0. To represent the original data in continuous form while fulfilling the zero integral constraint, the so-called &lt;em&gt;compositional splines&lt;/em&gt; were developed (more on this in Machalová et al. (2020)) and used throughout the regression modeling.&lt;/p&gt;
&lt;p&gt;In our geological example, 96 soil samples from loesses are examined and the task is to analyze how the geochemistry of the samples is influenced by their PSDs. The cubic polynomials were chosen for the spline basis of the PSDs together with 16 knots represented in the graphs by the grey dashed lines. The resulting clr densities are now ready to serve as predictor in our regression model. For the response, the clr-transformed geochemical compositions of the observed soil samples are taken into consideration. In this case, each composition is characterized by a real vector consisting of concentrations of 9 elements (Al, Si, K, Ca, Fe, As, Rb, Sr, Zr).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-10-compositional-scalar-on-function-regression-as-a-tool-not-only-for-geological-data_files/fig1.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;compositional-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Compositional regression&lt;/h2&gt;
&lt;p&gt;As mentioned above, the FPCA is a useful technique here to filter out noise which could distort the regression estimates - the FPCA allows us to represent the predictor using only a few functional principal components while explaining a substantial percentage of the variability of the original data. In this case, 3 principal components were used as they explained over 90% of the variability. The regression modeling is then performed on these functional principal components. The resulting functional parameters &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt; are shown in the plot below (in their clr form, of course).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-10-compositional-scalar-on-function-regression-as-a-tool-not-only-for-geological-data_files/fig2.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quality-of-the-model-interpretation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Quality of the model, interpretation&lt;/h2&gt;
&lt;p&gt;To assess the goodness-of-fit of the regression model, standard coefficient of determination can be computed with values close to one indicating a good fit of the model. Another possibility is to use a nonparametrical method of bootstrap confidence bands. The idea of bootstrap bands is based on resampling the residuals - for each part of the composition, the residuals can be estimated as
&lt;span class=&#34;math display&#34;&gt;\[
\hat{\epsilon}_{i}=y_i-\hat{y}_i.
\]&lt;/span&gt;
By resampling we are able to compute an arbitrary number of bootstrap samples
&lt;span class=&#34;math display&#34;&gt;\[
y_{i}^{boot}=\beta_{0}+\int_{I}\beta_{1}(t)\cdot f_{i}(t)dt + \epsilon_{i}^{boot},\quad i=1,\dots,N,
\]&lt;/span&gt;
and the resulting bootstrap estimates of the functional regression parameter then form a band “around” &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt;. Here, 100 bootstrap functions were plotted together with the estimate of &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt;. The bootstrap bands appear to be very useful for interpretation of the functional parameters &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt; as shown for Al and Ca bellow.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-10-compositional-scalar-on-function-regression-as-a-tool-not-only-for-geological-data_files/fig3.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While sticking with the clr form of &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt; (further as clr(&lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt;)) and their zero integral constraint, the functions have to cross the &lt;em&gt;x&lt;/em&gt;-axis meaning that we are able to split the original support &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; on subdomains where clr(&lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt;) is positive or negative, respectively. The same can be said about the clr transformation of the particle size distributions. For interpretation, we look at the positive and negative subdomains individually. For subdomain where the clr transformed PSDs are positive (&lt;span class=&#34;math inline&#34;&gt;\(I^{+}\)&lt;/span&gt;), three situations may occur:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the estimated parameter clr(&lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt;) is positive - in that case, we can expect an increasing relative presence of the given element within the geochemical composition (by considering intepretation of the clr representation of this element).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;clr(&lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt;) is negative, resulting in decreasing relative presence.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;clr(&lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt;) &lt;span class=&#34;math inline&#34;&gt;\(\approx 0\)&lt;/span&gt;, meaning that the relative presence of the given element within the geochemical composition is not influenced by the respective particle sizes of the PSDs. The bootstrap confidence bands can be used to define these subdomains.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Clearly, the opposite would apply for the subdomain with negative clr transformed PSDs (&lt;span class=&#34;math inline&#34;&gt;\(I^{-}\)&lt;/span&gt;). In case of Al it means that its relative presence in the composition is strongly (positively) influenced by the finest fractions and there is also a stronger negative effect of the fraction around 10 &lt;span class=&#34;math inline&#34;&gt;\(\mu m\)&lt;/span&gt;. For Ca completely opposite effects can be observed.&lt;/p&gt;
&lt;p&gt;To sum up, the specific properties of compositional data (as multivariate data) and probability density functions (as functional data) need a proper adaptation of standard statistical methods. Here, the linear regression was addressed by presenting a compositional scalar-on-function regression model with functional predictor and real response. Hopefully the presented example demonstrated that the compositional approach with the clr transformation not only provides an adequate platform for working with probability densities, but also leads to an easier and more straight-forward interpretation of the resulting parameters.&lt;/p&gt;
&lt;div id=&#34;based-on&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Based on:&lt;/h3&gt;
&lt;p&gt;Talská, R., Hron, K., Matys Grygar, T.: &lt;em&gt;Compositional scalar-on-function regression with application to sediment particle size distributions.&lt;/em&gt; Mathematical Geosciences, accepted for publication.&lt;/p&gt;
&lt;p&gt;Machalová, J., Talská, R., Hron, K., Gába, A.: &lt;em&gt;Compositional splines for representation of density functions.&lt;/em&gt; Computational Statistics (2020). &lt;a href=&#34;https://doi.org/10.1007/s00180-020-01042-7&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1007/s00180-020-01042-7&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;about-authors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;About authors&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-10-compositional-scalar-on-function-regression-as-a-tool-not-only-for-geological-data_files/ivana_pavlu_photo.png&#34; width=&#34;75&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ivana Pavlů&lt;/strong&gt; is a PhD student at Department of Mathematical Analysis and Applications of Mathematics, Palacký University in Olomouc, Czech Republic, &lt;a href=&#34;mailto:ivana.pavlu@upol.cz&#34; class=&#34;email&#34;&gt;ivana.pavlu@upol.cz&lt;/a&gt;. In her research she primarily focuses on functional data analysis of probability density functions using the Bayes spaces methodology.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-10-compositional-scalar-on-function-regression-as-a-tool-not-only-for-geological-data_files/Hron2017-small3.png&#34; width=&#34;75&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Karel Hron&lt;/strong&gt; is a Professor at Department of Mathematical Analysis and Applications of Mathematics, Palacký University in Olomouc, Czech Republic, &lt;a href=&#34;mailto:karel.hron@upol.cz&#34; class=&#34;email&#34;&gt;karel.hron@upol.cz&lt;/a&gt;. His research chiefly focuses on the statistical analysis of compositional data and its applications in a wide range of fields (geology, analytical chemistry, metabolomics, time-use epidemiology and others). He co-authored the book Applied Compositional Data Analysis, published in Springer Series in Statistics.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Higher Order Targeted Maximum Likelihood Estimation</title>
      <link>https://youngstats.github.io/post/2021/03/10/higher-order-targeted-maximum-likelihood-estimation/</link>
      <pubDate>Wed, 10 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/03/10/higher-order-targeted-maximum-likelihood-estimation/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We propose a higher order targeted maximum likelihood estimation (TMLE) that only relies on a sequentially and recursively defined set of data-adaptive fluctuations. Without the need to assume the often too stringent higher order pathwise differentiability, the method is practical for implementation and has the potential to be fully computerized.&lt;/p&gt;
&lt;div id=&#34;background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Background&lt;/h1&gt;
&lt;div id=&#34;targeted-maximum-likelihood-estimation-tmle&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Targeted Maximum Likelihood Estimation (TMLE)&lt;/h2&gt;
&lt;p&gt;It has been particularly of interest for semiparametric theories and real world practices to make efficient and substitution-based estimation for target quantities that are functions of data distribution. TMLE &lt;span class=&#34;citation&#34;&gt;(van der Laan and Rubin &lt;a href=&#34;#ref-van2006targeted&#34; role=&#34;doc-biblioref&#34;&gt;2006&lt;/a&gt;; van der Laan and Rose &lt;a href=&#34;#ref-van2011targeted&#34; role=&#34;doc-biblioref&#34;&gt;2011&lt;/a&gt;, &lt;a href=&#34;#ref-van2018targeted&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; provides a framework to construct such estimators and incorporates machine learning into efficient estimation and inference. Here we briefly review the regular first order TMLE.&lt;/p&gt;
&lt;p&gt;Suppose that the true distribution &lt;span class=&#34;math inline&#34;&gt;\(P_0\)&lt;/span&gt; lies in a statistical model &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{M}\)&lt;/span&gt;. Start with an initial distribution estimator &lt;span class=&#34;math inline&#34;&gt;\(P_n^0\)&lt;/span&gt;. Given pathwise differentiability of the target &lt;span class=&#34;math inline&#34;&gt;\(\Psi(P)\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; with a canonical gradient &lt;span class=&#34;math inline&#34;&gt;\(D^{(1)}_P\)&lt;/span&gt;, consider a least favorable path &lt;span class=&#34;math inline&#34;&gt;\(\{ \tilde P^{(1)}(P, \epsilon) \}\)&lt;/span&gt; through &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(\epsilon=0\)&lt;/span&gt;, where scores at &lt;span class=&#34;math inline&#34;&gt;\(\epsilon=0\)&lt;/span&gt; span the efficient influence curve (EIC) &lt;span class=&#34;math inline&#34;&gt;\(D_{P}^{(1)}\)&lt;/span&gt;. Define the TMLE update by maximizing the likelihood along the path, that is, &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_n^{(1)} = \mathrm{argmin}_\epsilon P_n L(\tilde P^{(1)}(P_n^0, \epsilon) )\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(L(P) = - \log p\)&lt;/span&gt;. The resulted TMLE update is &lt;span class=&#34;math inline&#34;&gt;\(P_n^* = \tilde P_n^{(1)} (P_n^0) = \tilde P_n^{(1)} (P_n^0, \epsilon_n^{(1)})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Define &lt;span class=&#34;math inline&#34;&gt;\(R^{(1)}(P, P_0) = \Psi(P) - \Psi(P_0) + P_0 D_P^{(1)}\)&lt;/span&gt; as the exact remainder. Then the TMLE satisfies &lt;span class=&#34;math inline&#34;&gt;\(P_n D_{P_n^*}^{(1)}\approx 0\)&lt;/span&gt; and the following exact expansion
&lt;span class=&#34;math display&#34;&gt;\[
\Psi(P_n^*) - \Psi(P_0) = R^{(1)}(P_n^*, P_0) - P_0 D_{P_n^*}^{(1)} = (P_n - P_0) D_{P_0}^{(1)} + (P_n - P_0) (D_{P_n^*}^{(1)} - D_{P_0}^{(1)}) - P_n D^{(1)}_{P_n^*} + R^{(1)}(P_n^*, P_0).
\]&lt;/span&gt;
Asymptotic efficiency for &lt;span class=&#34;math inline&#34;&gt;\(P_n^*\)&lt;/span&gt; requires:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\{D_{P}^{(1)}: P\in\mathcal{M}\}\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(P_0\)&lt;/span&gt;-Donsker class (often satisfied, or skipped with sample splitting),&lt;/li&gt;
&lt;li&gt;Solving the equation &lt;span class=&#34;math inline&#34;&gt;\(P_n D^{(1)}_{P_n^*}=0\)&lt;/span&gt; exactly or to an &lt;span class=&#34;math inline&#34;&gt;\(o_P(n^{-1/2})\)&lt;/span&gt; term,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(R^{(1)}(P_n^*, P_0)\)&lt;/span&gt; being exactly zero or up to an &lt;span class=&#34;math inline&#34;&gt;\(o_P(n^{-1/2})\)&lt;/span&gt; term.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(R^{(1)}(P, P_0)\)&lt;/span&gt; is often a second order difference in &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p_0\)&lt;/span&gt;. For example, when it consists of cross products, doubly or multiply robustness may exist.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;highly-adaptive-lasso-hal&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Highly Adaptive Lasso (HAL)&lt;/h2&gt;
&lt;p&gt;HAL &lt;span class=&#34;citation&#34;&gt;(van der Laan &lt;a href=&#34;#ref-van2015generally&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;, &lt;a href=&#34;#ref-van2017generally&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;; Benkeser and van ver Laan &lt;a href=&#34;#ref-benkeser2016highly&#34; role=&#34;doc-biblioref&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt; is a nonparametric maximum likelihood estimator that converges in Kullback-Leibler dissimilarity at a minimal rate of &lt;span class=&#34;math inline&#34;&gt;\(n^{-2/3}(\log~n)^d\)&lt;/span&gt;, even when the parameter space only assumes cadlag and finite variation norms.
This generally bounds the exact remainder, and immediately makes the TMLE that uses HAL as an initial asymptotically efficient. However, in finite samples, the second order remainder can still dominate the sampling distribution.&lt;/p&gt;
&lt;p&gt;Another important property of HAL is itself being a nonparametric MLE, so it can solve a large class of score equations to best approximates the desired score via increasing the &lt;span class=&#34;math inline&#34;&gt;\(L_1\)&lt;/span&gt;-norm of the HAL-MLE (called undersmoothing) &lt;span class=&#34;citation&#34;&gt;(M. J. van der Laan, Benkeser, and Cai &lt;a href=&#34;#ref-vanderLaan&amp;amp;Benkeser&amp;amp;Cai19&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;&lt;a href=&#34;#ref-vanderLaan&amp;amp;Benkeser&amp;amp;Cai19&#34; role=&#34;doc-biblioref&#34;&gt;a&lt;/a&gt;, &lt;a href=&#34;#ref-van2019efficient&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;&lt;a href=&#34;#ref-van2019efficient&#34; role=&#34;doc-biblioref&#34;&gt;b&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;higher-order-fluctuations-with-hal-mle&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Higher Order Fluctuations with HAL-MLE&lt;/h1&gt;
&lt;p&gt;Replace &lt;span class=&#34;math inline&#34;&gt;\(P_n^0\)&lt;/span&gt; in the first order TMLE by a TMLE &lt;span class=&#34;math inline&#34;&gt;\(\tilde P^{(2)}_n(P_n^0)\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\Psi_n^{(1)}(P_0) = \Psi(\tilde P^{(1)}_n(P_0)) = \Psi(\tilde P^{(1)}(P_0, \epsilon_n^{(1)}(P_0)))\)&lt;/span&gt;, which is a data-adaptive fluctuation of the original target parameter &lt;span class=&#34;math inline&#34;&gt;\(\Psi(P_0)\)&lt;/span&gt;. Then the final update of a second order TMLE, &lt;span class=&#34;math inline&#34;&gt;\(\tilde P^{(1)}_n \tilde P^{(2)}_n(P_n^0)\)&lt;/span&gt;, is just a first order TMLE that uses as the initial estimator a &lt;span class=&#34;math inline&#34;&gt;\(\tilde P^{(2)}(P_n^0)\)&lt;/span&gt; that is fully tailored for &lt;span class=&#34;math inline&#34;&gt;\(\Psi_n^{(1)}(P_0)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-10-higher-order-targeted-maximum-likelihood-estimation_files/figure_2nd_order.jpg&#34; /&gt;
&lt;em&gt;Figure 1: Left panel: regular TMLE. Right panel: second order TMLE. The horizontal axes represent the original target. The vertical axis represents the data-adaptive fluctuation. The second order TMLE searches for a better initial estimator for a regular TMLE.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Similarly if we iterate this process, and let &lt;span class=&#34;math inline&#34;&gt;\(\tilde P^{(k+1)}_n(P_n^0)\)&lt;/span&gt; be a regular TMLE tailored for a higher order fluctuation &lt;span class=&#34;math inline&#34;&gt;\(\Psi_n^{(k)}(P_0) = \Psi^{(k-1)}_n(\tilde P^{(k)}_n(P_0))=\Psi(\tilde P^{(1)}_n \tilde P^{(2)}_n\cdots \tilde P^{(k)}_n(P_n^0))\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(k=1, \ldots\)&lt;/span&gt;, then the final update of a &lt;span class=&#34;math inline&#34;&gt;\(k+1\)&lt;/span&gt;-th order TMLE is &lt;span class=&#34;math inline&#34;&gt;\(\tilde P^{(1)}_n \tilde P^{(2)}_n\cdots \tilde P^{(k+1)}_n(P_n^0)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The second order TMLE relies on pathwise differentiability of &lt;span class=&#34;math inline&#34;&gt;\(\Psi_n^{(1)}\)&lt;/span&gt;. However, &lt;span class=&#34;math inline&#34;&gt;\(\Psi_n^{(1)}(P) = \Psi(\tilde P^{(1)}_n(P)) = \Psi(\tilde P^{(1)}(P, \epsilon_n^{(1)}(P)))\)&lt;/span&gt;is smooth in &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; up till the dependence of &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_n^{(1)}(P) = \mathrm{argmax}_\epsilon P_n \log \tilde p_n^{(1)}(p, \epsilon)\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;, because &lt;span class=&#34;math inline&#34;&gt;\(P_n\)&lt;/span&gt; is not absolutely continuous w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; for most &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; that can occur as an initial or a higher order TMLE-update. This calls for the use of smooth distribution estimators such as HAL-MLE &lt;span class=&#34;math inline&#34;&gt;\(\tilde{P}_n\)&lt;/span&gt; in replacement of the empirical &lt;span class=&#34;math inline&#34;&gt;\(P_n\)&lt;/span&gt;, since &lt;span class=&#34;math inline&#34;&gt;\(d\tilde P_n/dP\)&lt;/span&gt; will exist for all &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; that can occur as an initial or higher order updates, which ensures pathwise differentiability of &lt;span class=&#34;math inline&#34;&gt;\(\Psi^{(1)}_n(P_0)\)&lt;/span&gt; and the existence of its canonical gradient &lt;span class=&#34;math inline&#34;&gt;\(D^{(2)}_{n, P}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In general, suppose that &lt;span class=&#34;math inline&#34;&gt;\(\{\tilde P^{(k)}_n(P, \epsilon)\}\)&lt;/span&gt; is a least favorable path through &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(\epsilon=0\)&lt;/span&gt;, whose scores at &lt;span class=&#34;math inline&#34;&gt;\(\epsilon=0\)&lt;/span&gt; span &lt;span class=&#34;math inline&#34;&gt;\(D^{(k)}_{n, P}\)&lt;/span&gt;. And the update step is also replaced by optimizing the &lt;span class=&#34;math inline&#34;&gt;\(\tilde P_n\)&lt;/span&gt;-regularized loss, that is, &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_n^{(k)} = \mathrm{argmin}_\epsilon \tilde P_n L(\tilde P^{(k)}(P_n^0, \epsilon) )\)&lt;/span&gt;, which solves &lt;span class=&#34;math inline&#34;&gt;\(\tilde P_n D^{(k)}_{n, P}=0\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(P = \tilde P^{(k)}_n(P_n^0)= \tilde P^{(k)}_n(P_n^0, \epsilon_n^{(k)})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A &lt;span class=&#34;math inline&#34;&gt;\(k+1\)&lt;/span&gt;-th order TMLE by its design searches for a better initial estimator given the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th order TMLE. Specifically, the &lt;span class=&#34;math inline&#34;&gt;\(k+1\)&lt;/span&gt;-th order TMLE moves in the same direction as the steepest descent algorithm for minimizing the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th exact total remainder that is the discrepancy between &lt;span class=&#34;math inline&#34;&gt;\(\Psi(\tilde P^{(1)}_n \tilde P^{(2)}_n\cdots \tilde P^{(k)}_n(P)) - \Psi(\tilde P^{(1)}_n \tilde P^{(2)}_n\cdots \tilde P^{(k)}_n(P_0))\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tilde{P}_n D^{(k)}_{n, P_0}\)&lt;/span&gt;. Moreover, compared to an oracle steepest descent algorithm, TMLE stops the moment the log-likelihood is not improving anymore, which corresponds exactly to when the TMLE cannot know in what direction a steepest descent algorithm would go. This avoids potential overfitting and ensures a local minimum in close neighborhood of the desired (but unknown) minimum &lt;span class=&#34;math inline&#34;&gt;\(P_0\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exact-expansions-of-higher-order-tmle&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exact Expansions of Higher Order TMLE&lt;/h1&gt;
&lt;p&gt;Denote the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th exact remainder as the exact remainder of &lt;span class=&#34;math inline&#34;&gt;\(\tilde P^{(k)}_n(P)\)&lt;/span&gt; for the fluctuation &lt;span class=&#34;math inline&#34;&gt;\(\Psi^{(k-1)}(P_0) = \Psi(\tilde P_n^{(1)}\cdots\tilde P_n^{(k-1)}(P_0))\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
R^{(k)}_n(\tilde P^{(k)}_n(P), P_0)
= &amp;amp; \Psi^{(k-1)}(\tilde P^{(k)}_n(P)) - \Psi^{(k-1)}(P_0) + P_0 D^{(k)}_{n, \tilde P^{(k)}_n(P)} \\
= &amp;amp; \Psi(\tilde P^{(1)}\cdots\tilde P^{(k)}_n(P)) - \Psi(\tilde P^{(1)}\cdots\tilde P^{(k-1)}(P_0)) + P_0 D^{(k)}_{n, \tilde P^{(k)}_n(P)}. 
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then we have the exact expansion for the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th order TMLE,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\Psi(\tilde P^{(1)}_n\cdots\tilde P^{(k)}_n(P)) - \Psi(P_0)
= &amp;amp; \sum_{j=1}^{k-1} (P_n-P_0)D^{(j)}_{n, \tilde P^{(j)}_n(P_0)} + R^{(j)}_n(\tilde P_n^{(j)}(P_0), P_0) \\
&amp;amp; + (P_n-P_0)D^{(k)}_{n, \tilde P^{(k)}_n}(P_n^0) + R^{(k)}_n(\tilde P_n^{(k)}(P_n^0), P_0) \\
&amp;amp; - \sum_{j=1}^{k-1} P_n D^{(j)}_{n, \tilde P^{(j)}_n(P_0)} - P_n D^{(k)}_{n, \tilde P^{(k)}_n(P_n^0)}, 
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which still holds if we replace &lt;span class=&#34;math inline&#34;&gt;\(P_n\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\tilde P_n\)&lt;/span&gt;. This can be further derived as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\Psi(\tilde P^{(1)}\cdots\tilde P^{(k)}_n(P)) - \Psi(P_0)
=&amp;amp;\sum_{j=1}^{k}\left\{ (\tilde{P}_n-P_0)D^{(j)}_{n,\tilde{P}_n^{(j)}(P_0)}+R_n^{(j)}(\tilde{P}_n^{(j)}(P_0),P_0)\right\} \\
%&amp;amp;&amp;amp;+(P_n-P_0)D^{(k)}_{n,P_n^{(k)}(P_0)}\\
&amp;amp; +R_n^{(k)}(\tilde{P}_n^{(k)}(P_n^0),\tilde{P}_n)-R_n^{(k)}(\tilde{P}_n^{(k)}(P_0),\tilde{P}_n)\\
&amp;amp;-\sum_{j=1}^{k}\tilde{P}_n D^{(j)}_{n,\tilde{P}_n^{(j)}(P_0)}.\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The followings can be shown:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((\tilde{P}_n-P_0)D^{(j)}_{n,\tilde{P}_n^{(j)}(P_0)}, j=1, \dots, k,\)&lt;/span&gt; are generalized &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th order difference in &lt;span class=&#34;math inline&#34;&gt;\(P_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tilde P_n\)&lt;/span&gt;, which resemble the performance of higher order &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;-statistics;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(R_n^{(j)}(\tilde{P}_n^{(j)}(P_0),P_0) = O_P(n^{-1})\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\((\tilde P_n - P_n) D_{n, P_0}^{(j)} = O_P(n^{-1/2})\)&lt;/span&gt;, which can be achieved by undersmothing HAL;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(R_n^{(k)}(\tilde{P}_n^{(k)}(P),\tilde P_n)\)&lt;/span&gt; is a generalized &lt;span class=&#34;math inline&#34;&gt;\(k+1\)&lt;/span&gt;-th order difference in &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tilde P_n\)&lt;/span&gt;, and hence &lt;span class=&#34;math inline&#34;&gt;\(R_n^{(k)}(\tilde{P}_n^{(k)}(P_n^0),\tilde P_n) - R_n^{(k)}(\tilde{P}_n^{(k)}(P_0),\tilde P_n)=o_P(n^{-1/2})\)&lt;/span&gt; so long as &lt;span class=&#34;math inline&#34;&gt;\(\lVert \tilde p_n - p_0 \rVert = o_P(n^{1/2(k+1)})\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\lVert p_n^0 - p_0 \rVert = o_P(n^{1/2(k+1)})\)&lt;/span&gt;;&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;The last term can be exactly &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; by defining &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_n^{(j)}(P)\)&lt;/span&gt; as a solution of the corresponding efficient score equation &lt;span class=&#34;math inline&#34;&gt;\(\tilde P_n D^{(j)}_{n, \tilde P^{(j)}_n(P, \epsilon)}=0\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;higher-order-inference&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Higher Order Inference&lt;/h1&gt;
&lt;p&gt;For the sake of statistical inference, we will need that &lt;span class=&#34;math inline&#34;&gt;\((\tilde P_n - P_n)D^{(1)}_{\tilde P_n^{(1)}(P_0)} = o_P(n^{-1/2})\)&lt;/span&gt;, and probably even &lt;span class=&#34;math inline&#34;&gt;\((\tilde P_n - P_n)D^{(j)}_{\tilde P_n^{(j)}(P_0)} = o_P(n^{-1/2})\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(j = 2, \dots, k\)&lt;/span&gt;. It can be shown that this essentially comes down to controlling &lt;span class=&#34;math inline&#34;&gt;\((\tilde P_n - P_n) D^{(1)}_{P_0}\)&lt;/span&gt;, which again can be achieved by undersmoothing HAL.&lt;/p&gt;
&lt;p&gt;Let
&lt;span class=&#34;math display&#34;&gt;\[
\bar D_n^k = \sum_{j=1}^k D^{(j)}_{n, \tilde P^{(j)}_n\cdots\tilde P^{(k)}_n(P_n^0)}
\]&lt;/span&gt;
which is an estimate of the influence curve &lt;span class=&#34;math inline&#34;&gt;\(\bar D_{n, P_0}^k = \sum_{j=1}^k D^{(j)}_{n, \tilde P^{(j)}_n(P_0)}\)&lt;/span&gt;. Note that for &lt;span class=&#34;math inline&#34;&gt;\(j&amp;gt;1\)&lt;/span&gt; the terms are higher order differences, so that &lt;span class=&#34;math inline&#34;&gt;\(\bar D_n^k\)&lt;/span&gt; will converge to the efficient influence curve &lt;span class=&#34;math inline&#34;&gt;\(D^{(1)}_{P_0}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let
&lt;span class=&#34;math display&#34;&gt;\[
\sigma_n^2 = \frac{1}{n}\sum_{i=1}^n \bar D_n^k(O_i)^2
\]&lt;/span&gt;
be the sample variance of this estimated influence curve. A corresponding 0.95 confidence interval is given by
&lt;span class=&#34;math display&#34;&gt;\[
\Psi(P^{(1)}_n\cdots\tilde P^{(k)}_n(P_n^0)) \pm 1.96 \sigma_n/n^{1/2}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simulation&lt;/h1&gt;
&lt;p&gt;The first example demonstrates the impact of second order TMLE steps during a process of estimating the average density. The exact total remainder &lt;span class=&#34;math inline&#34;&gt;\(\bar R^{(1)}(\tilde P_n^{(1)}(P), P_0)\)&lt;/span&gt; of first order TMLE is controlled due to the second order updates &lt;span class=&#34;math inline&#34;&gt;\(P = P_n^0 \mapsto \tilde P_n^{(2)}(P_n^0)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-10-higher-order-targeted-maximum-likelihood-estimation_files/middle_remainder_ZW-20210114.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Below it plots the simulated bias and bias/SD ratio at &lt;span class=&#34;math inline&#34;&gt;\(n=500\)&lt;/span&gt; when we increase the bias in the initial estimator &lt;span class=&#34;math inline&#34;&gt;\(P_n^0\)&lt;/span&gt; by adding a bias mass to each of the support points of the empirical pmf. Second order TMLE provides improved accuracy in both estimation and inference over first order TMLE following likelihood guidance.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-10-higher-order-targeted-maximum-likelihood-estimation_files/combine_500-20210115.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Lastly, we show an example of estimating average treatment effects (ATEs) while the initial estimator for propensity scores is &lt;span class=&#34;math inline&#34;&gt;\(n^{-1/4}\)&lt;/span&gt;-consistent while that for outcome models is not. The first order TMLE should have &lt;span class=&#34;math inline&#34;&gt;\(n^{1/2}\)&lt;/span&gt;-scaled bias that increases with &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; while the second order TMLE has a &lt;span class=&#34;math inline&#34;&gt;\(n^{1/2}\)&lt;/span&gt;-bias that should be constant in &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. The table below shows that the second order TMLE has a negligible bias and thereby still provides valid inference.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;n&lt;/th&gt;
&lt;th&gt;bias 1-st&lt;/th&gt;
&lt;th&gt;bias 2-nd&lt;/th&gt;
&lt;th&gt;se 1-st&lt;/th&gt;
&lt;th&gt;se 2-nd&lt;/th&gt;
&lt;th&gt;mse 1-st&lt;/th&gt;
&lt;th&gt;mse 2-nd&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;400&lt;/td&gt;
&lt;td&gt;-0.720&lt;/td&gt;
&lt;td&gt;0.078&lt;/td&gt;
&lt;td&gt;0.815&lt;/td&gt;
&lt;td&gt;1.175&lt;/td&gt;
&lt;td&gt;1.087&lt;/td&gt;
&lt;td&gt;1.178&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;750&lt;/td&gt;
&lt;td&gt;-0.996&lt;/td&gt;
&lt;td&gt;0.029&lt;/td&gt;
&lt;td&gt;0.800&lt;/td&gt;
&lt;td&gt;1.102&lt;/td&gt;
&lt;td&gt;1.278&lt;/td&gt;
&lt;td&gt;1.102&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;1000&lt;/td&gt;
&lt;td&gt;-1.258&lt;/td&gt;
&lt;td&gt;-0.062&lt;/td&gt;
&lt;td&gt;0.786&lt;/td&gt;
&lt;td&gt;1.066&lt;/td&gt;
&lt;td&gt;1.483&lt;/td&gt;
&lt;td&gt;1.068&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;1200&lt;/td&gt;
&lt;td&gt;-1.345&lt;/td&gt;
&lt;td&gt;0.022&lt;/td&gt;
&lt;td&gt;0.809&lt;/td&gt;
&lt;td&gt;1.028&lt;/td&gt;
&lt;td&gt;1.570&lt;/td&gt;
&lt;td&gt;1.028&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;1600&lt;/td&gt;
&lt;td&gt;-1.549&lt;/td&gt;
&lt;td&gt;-0.019&lt;/td&gt;
&lt;td&gt;0.818&lt;/td&gt;
&lt;td&gt;1.055&lt;/td&gt;
&lt;td&gt;1.752&lt;/td&gt;
&lt;td&gt;1.055&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;2500&lt;/td&gt;
&lt;td&gt;-2.066&lt;/td&gt;
&lt;td&gt;-0.094&lt;/td&gt;
&lt;td&gt;0.819&lt;/td&gt;
&lt;td&gt;0.999&lt;/td&gt;
&lt;td&gt;2.222&lt;/td&gt;
&lt;td&gt;1.003&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;discussions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Discussions&lt;/h1&gt;
&lt;p&gt;Although HAL-MLE-based fluctuations are fundamental to higher order TMLE, the update steps in practice can be based on empirical losses. Note that the &lt;span class=&#34;math inline&#34;&gt;\(j-1\)&lt;/span&gt;-th fluctuation &lt;span class=&#34;math inline&#34;&gt;\(\Psi(\tilde P_n^{(1)}\cdots\tilde P_n^{(j-1)}(P_0))\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(j = 0, \dots, k-1\)&lt;/span&gt;, is nothing but a pathwise differentiable parameter with a known canonical gradient, &lt;span class=&#34;math inline&#34;&gt;\(D^{(j)}_{n, P}\)&lt;/span&gt;. For jointly targeting this sequence of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; parameters, one can solve the empirical &lt;span class=&#34;math inline&#34;&gt;\(P_n\)&lt;/span&gt;-regularized efficient score equations (where the scores still involve HAL-MLEs). As we showed in the technical report, this preserves the exact expansion and even leads to an improved undersmoothing term, and therefore is the recommended implementation. At &lt;span class=&#34;math inline&#34;&gt;\(k=1\)&lt;/span&gt;, this exactly coincides with the regular first order TMLE.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-03-10-higher-order-targeted-maximum-likelihood-estimation_files/figure_targets.jpg&#34; /&gt;
&lt;em&gt;Figure 2: Jointly consider the sequence of data-adaptive fluctuations.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;An important next step is the (automated) computation of the first and higher order canonical gradients with least squares regression or symmetric matrix inversion &lt;span class=&#34;citation&#34;&gt;(van der Laan, Wang, and van der Laan &lt;a href=&#34;#ref-van2021higher&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt;, thereby opening up the computation of higher order TMLEs with standard machinery, avoiding delicate analytics needed to determine closed forms.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references hanging-indent&#34;&gt;
&lt;div id=&#34;ref-benkeser2016highly&#34;&gt;
&lt;p&gt;Benkeser, David, and Mark J van ver Laan. 2016. “The Highly Adaptive Lasso Estimator.” In &lt;em&gt;2016 Ieee International Conference on Data Science and Advanced Analytics (Dsaa)&lt;/em&gt;, 689–96. IEEE.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-van2015generally&#34;&gt;
&lt;p&gt;van der Laan, Mark J. 2015. “A Generally Efficient Targeted Minimum Loss Based Estimator.”&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-van2017generally&#34;&gt;
&lt;p&gt;———. 2017. “A Generally Efficient Targeted Minimum Loss Based Estimator Based on the Highly Adaptive Lasso.” &lt;em&gt;The International Journal of Biostatistics&lt;/em&gt; 13 (2).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-vanderLaan&amp;amp;Benkeser&amp;amp;Cai19&#34;&gt;
&lt;p&gt;van der Laan, Mark J, David Benkeser, and Weixin Cai. 2019a. “Causal Inference Based on Undersmoothing the Highly Adaptive Lasso.”&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-van2019efficient&#34;&gt;
&lt;p&gt;———. 2019b. “Efficient Estimation of Pathwise Differentiable Target Parameters with the Undersmoothed Highly Adaptive Lasso.” &lt;em&gt;arXiv Preprint arXiv:1908.05607&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-van2011targeted&#34;&gt;
&lt;p&gt;van der Laan, Mark J, and Sherri Rose. 2011. &lt;em&gt;Targeted Learning: Causal Inference for Observational and Experimental Data&lt;/em&gt;. Springer Science &amp;amp; Business Media.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-van2018targeted&#34;&gt;
&lt;p&gt;———. 2018. &lt;em&gt;Targeted Learning in Data Science&lt;/em&gt;. Springer.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-van2006targeted&#34;&gt;
&lt;p&gt;van der Laan, Mark J, and Daniel Rubin. 2006. “Targeted Maximum Likelihood Learning.” &lt;em&gt;The International Journal of Biostatistics&lt;/em&gt; 2 (1).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-van2021higher&#34;&gt;
&lt;p&gt;van der Laan, Mark J, Zeyi Wang, and Lars van der Laan. 2021. “Higher Order Targeted Maximum Likelihood Estimation.” &lt;em&gt;arXiv Preprint arXiv:2101.06290&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Give me an adequate correlation: assessing relationships in percentage (or proportional) data</title>
      <link>https://youngstats.github.io/post/2021/02/04/give-me-an-adequate-correlation-assessing-relationships-in-percentage-or-proportional-data/</link>
      <pubDate>Thu, 04 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/02/04/give-me-an-adequate-correlation-assessing-relationships-in-percentage-or-proportional-data/</guid>
      <description>


&lt;div id=&#34;correlations-and-negative-bias&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Correlations and negative bias&lt;/h2&gt;
&lt;p&gt;We assume that you are quite familiar with the following problem.
Consider a data set where the information is expressed in percentages or proportions.
An example are household expenditures, given as average amounts (in Euros) the households
are spending on food, housing, transportation, etc.
Since the expenditures would not be comparable among countries with very different economic
level, it makes sense to express the data as proportions (or percentages) of the single
categories on the total expenditures.
Now one can be interested in relationships between the different variables (expenditure
categories), and the common tools for this task would be to use the
Pearson or Spearman correlation coefficient to determine the strength of the associations.
However, is this appropriate to compute correlations from
percentages (or proportions, or any other constrained data), or more generally, from data
carrying relative information? Denote the underlying variables, expressed in proportions
or percentages, by &lt;span class=&#34;math inline&#34;&gt;\(x_1,\ldots,x_D\)&lt;/span&gt;.
When computing correlations, we
must count with a negative bias of the covariance, which leads to relations like
&lt;span class=&#34;math display&#34;&gt;\[\mathrm{cov}(x_1,x_2)+\mathrm{cov}(x_1,x_3)+\ldots+\mathrm{cov}(x_1,x_D)=-\mathrm{var}(x_1).\]&lt;/span&gt;
Consequently, the correlation coefficients cannot vary freely between &lt;span class=&#34;math inline&#34;&gt;\(-1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;;
they are forced towards negative values and thus do not produce reliable and interpretable results.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;initial-choice-variation-matrix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Initial choice: variation matrix&lt;/h2&gt;
&lt;p&gt;The way to obtain reasonable correlations with percentage data is to consider ratios,
or even mathematically easier, log-ratios as the source information. Log-ratios do not
change when the data are rescaled, thus the sum of components (100 for percentages) is
even irrelevant.
In our household expenditure data set we could thus even work with the raw data, expressed
in Euros, and the results of the analysis would be the same as for the
normalized data.
The scale invariant data are called &lt;em&gt;compositional&lt;/em&gt; in this context.
A natural way to assess strength of relationship between components of percentage
(compositional) data is thus to think in terms of their &lt;em&gt;proportionality&lt;/em&gt;. This leads
to the so called &lt;em&gt;variation matrix&lt;/em&gt;, which is defined as
&lt;span class=&#34;math display&#34;&gt;\[\mathbf{T}=\left(\mathrm{var}\left(\ln\frac{x_i}{x_j}\right)\right)_{i,j=1}^D.\]&lt;/span&gt;
If the components are proportional, the respective element of the variation matrix is
zero, and vice versa. Thus, bigger values of the variation matrix refer to deviations
from proportionality. Here, &lt;em&gt;var&lt;/em&gt; is denotes the variance, and practically one can
use classical or robust variance estimation, where the latter is preferable in presence
of outliers.
The variation matrix is thus definitely a reasonable choice to express relationship
between the variables, however, it cannot be interpreted in terms of correlations,
as possible negative relationships between the components are not captured.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;correlations-with-weighted-symmetric-pivot-coordinates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Correlations with (weighted) symmetric pivot coordinates&lt;/h2&gt;
&lt;p&gt;We can see that relating the original components has its clear limitations. A possible
alternative could be to consider &lt;em&gt;relative information&lt;/em&gt; carried by the original components
of a given composition, information contained in log-ratios to other components. The first
choice is to aggregate these logratios which leads, say for components &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;,
to the following variables (we refer to &lt;em&gt;symmetric pivot coordinates&lt;/em&gt;):
&lt;span class=&#34;math display&#34;&gt;\[\begin{eqnarray}
z_1^s&amp;amp;=&amp;amp;C\left(\frac{\sqrt{D-2}}{\sqrt{D-2}+\sqrt{D}}\ln\frac{x_1}{x_2}+\ln\frac{x_1}{x_3}+\ldots+\ln\frac{x_1}{x_D}\right),\\
z_2^s&amp;amp;=&amp;amp;C\left(\frac{\sqrt{D-2}}{\sqrt{D-2}+\sqrt{D}}\ln\frac{x_2}{x_1}+\ln\frac{x_2}{x_3}+\ldots+\ln\frac{x_2}{x_D}\right),
\end{eqnarray}\]&lt;/span&gt;
with a normalizing positive constant &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;. The first logratios in both variables are downscaled in order to remove a possible negative bias (from a geometrical perspective, we are talking about &lt;em&gt;orthonormality&lt;/em&gt; of the resulting variables/coordinates); interestingly, &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{D-2}/(\sqrt{D-2}+\sqrt{D})\rightarrow 1/2\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(D\rightarrow\infty\)&lt;/span&gt;. Even more, the effect of pairwise logratios which are aggregated into &lt;span class=&#34;math inline&#34;&gt;\(z_1^s\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z_2^s\)&lt;/span&gt; can be &lt;em&gt;weighted&lt;/em&gt;, e.g., according to their proportionality to &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;, respectively, by using the inverse values of the respective elements of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{T}\)&lt;/span&gt;. This guarantees that components not related to &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; have a limited impact to
the construction of both symmetric pivot coordinates.&lt;/p&gt;
&lt;p&gt;Now, standard (classical or robust) correlation coefficients between &lt;span class=&#34;math inline&#34;&gt;\(z_1^s\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z_2^s\)&lt;/span&gt; can be computed to estimate the relationship between &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; in the composition.
Its interpretation follows directly the construction of symmetric pivot coordinates. This means that the original components are replaced by their &lt;em&gt;dominance&lt;/em&gt; over
“averaged” contributions of the other components (ev. appropriately weighted) – a quite natural interpretation for data carrying relative information. Moreover, the correlation can be considered in both the positive and negative sense without any danger of the negative bias. We should be only aware that each correlation coefficient is coming from a set of coordinates which are constructed specifically for the given couple of components, so the resulting “correlation matrix” should not be simply treated in the multivariate sense.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;relative-structure-of-household-expenditures&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Relative structure of household expenditures&lt;/h2&gt;
&lt;p&gt;Let us come back to the initial example where the interest was in the relative structure of household expenditures, reported for several countries of the European Union.
Such a data set is contained as &lt;code&gt;expendituresEU&lt;/code&gt; in the library &lt;code&gt;robCompositions&lt;/code&gt;. With a
proportional representation of the expenditures the wealth status of countries is
suppressed and the focus is on the &lt;em&gt;relative&lt;/em&gt; correlation structure. In order to suppress the influence of outlying observations, the Spearman correlation coefficients between
the components are computed and the result is plotted in the heatmap below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-02-04-correlation/Raw_data_correlation.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are some strongly related groups of components visible, like &lt;em&gt;Foodstuff&lt;/em&gt;, &lt;em&gt;Alcohol&lt;/em&gt; and &lt;em&gt;Communications&lt;/em&gt;, which could be considered as those belonging to “basic” expenditures, and there is also another related group of components which could be connected rather to wealth of households (&lt;em&gt;Recreation&lt;/em&gt;, &lt;em&gt;Furnishing&lt;/em&gt;, &lt;em&gt;Transport&lt;/em&gt; and &lt;em&gt;Other&lt;/em&gt;). On the other hand, there are also some strongly negatively correlated couples of components, like &lt;em&gt;Food&lt;/em&gt; with &lt;em&gt;Recreation&lt;/em&gt;, &lt;em&gt;Other&lt;/em&gt; and &lt;em&gt;Furnishings&lt;/em&gt;, respectively, or &lt;em&gt;Recreation&lt;/em&gt; and &lt;em&gt;Health&lt;/em&gt;. The question is which of these negative correlation coefficients is a consequence of the negative bias, and which indeed reflects strong negative relationship between the &lt;em&gt;relative&lt;/em&gt; household expenditures.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-02-04-correlation/Variation_matrix.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As the next step we want to see whether these relationships can be observed also in the (robust) variation matrix, working with log-ratios instead of the original components. Indeed, the proportionality holds definitely for the first group of components (&lt;em&gt;Foodstuff&lt;/em&gt;, &lt;em&gt;Alcohol&lt;/em&gt; and &lt;em&gt;Communications&lt;/em&gt;), however, proportionality of supplementary expenditures is now structured differently (see, e.g., a strong relationship between the components &lt;em&gt;Recreation&lt;/em&gt; and &lt;em&gt;Other&lt;/em&gt;). In any case, any “negative proportionality” cannot be derived from the variation matrix.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-02-04-correlation/Sym_coordinates.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Therefore, we turn to symmetric pivot coordinates; they can be computed, similarly as the variation matrix, using the &lt;code&gt;robCompositions&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(robCompositions)
data(expendituresEU)
D &amp;lt;- ncol(expendituresEU)
Rs &amp;lt;- matrix(NA,nrow=D,ncol=D)
rownames(Rs) &amp;lt;- colnames(expendituresEU)
colnames(Rs) &amp;lt;- colnames(expendituresEU)
for(i in 1:D){
  for(j in 1:D){
    Z &amp;lt;- pivotCoord(expendituresEU[,c(i,j, (1:D)[-c(i,j)])],method=&amp;quot;symm&amp;quot;)
  Rs[i,j] &amp;lt;- cor(Z[,1:2],method=&amp;quot;spearman&amp;quot;)[1,2]
  }
}
round(Rs,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                 Food Alcohol Clothing Housing Furnishings Health Transport
## Food            1.00    0.58     0.31   -0.06       -0.55   0.44     -0.50
## Alcohol         0.58    1.00    -0.02   -0.09       -0.22   0.19     -0.35
## Clothing        0.31   -0.02     1.00   -0.16        0.24  -0.07      0.08
## Housing        -0.06   -0.09    -0.16    1.00        0.24   0.15      0.29
## Furnishings    -0.55   -0.22     0.24    0.24        1.00  -0.54      0.68
## Health          0.44    0.19    -0.07    0.15       -0.54   1.00     -0.29
## Transport      -0.50   -0.35     0.08    0.29        0.68  -0.29      1.00
## Communications  0.72    0.66     0.03   -0.08       -0.51   0.32     -0.47
## Recreation     -0.44   -0.03    -0.17    0.36        0.73  -0.51      0.78
## Education       0.16    0.09     0.16   -0.32       -0.24   0.13     -0.44
## Restaurants    -0.44   -0.42     0.18    0.13        0.39  -0.08      0.28
## Other          -0.64   -0.28    -0.16    0.32        0.68  -0.27      0.67
##                Communications Recreation Education Restaurants Other
## Food                     0.72      -0.44      0.16       -0.44 -0.64
## Alcohol                  0.66      -0.03      0.09       -0.42 -0.28
## Clothing                 0.03      -0.17      0.16        0.18 -0.16
## Housing                 -0.08       0.36     -0.32        0.13  0.32
## Furnishings             -0.51       0.73     -0.24        0.39  0.68
## Health                   0.32      -0.51      0.13       -0.08 -0.27
## Transport               -0.47       0.78     -0.44        0.28  0.67
## Communications           1.00      -0.24      0.15       -0.61 -0.35
## Recreation              -0.24       1.00     -0.44        0.11  0.75
## Education                0.15      -0.44      1.00        0.33 -0.42
## Restaurants             -0.61       0.11      0.33        1.00  0.21
## Other                   -0.35       0.75     -0.42        0.21  1.00&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the correlations are not computed from the original components and we have to refer to their dominance over averaged contributions of the other components instead, but the negative bias of correlations is eliminated now. Indeed, when looking at the heatmap, the main clusters of strongly positively correlated components remain unchanged. However, more substantial changes can be observed for negative correlations. From those mentioned previously only that one between &lt;em&gt;Food&lt;/em&gt; and &lt;em&gt;Other&lt;/em&gt; (in the symmetric pivot coordinates sense) remained. Nevertheless, we should be aware that for the computation of correlations we simply aggregated all log-ratios of the couple with the remaining components, and there are clearly those which could strongly influence the resulting symmetric pivot coordinates, although they are not related to any of components of interest. This is definitely the case of &lt;em&gt;Education&lt;/em&gt;, which is not proportional to the vast majority of components (see heatmap of the variation matrix), and whose influence should thus be rather suppressed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-02-04-correlation/Weighted_symm_coordinates.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This can be done by using weighted symmetric pivot coordinates, the grouping of positively correlated components is now suprisingly even more similar to the case of the initial proportional data. Of course, by considering a different interpretation of the “components” in both cases! However, now we are free again from any possible negative bias of correlations. The “tuned” symmetric pivot coordinates reveal three negative relationships, those between &lt;em&gt;Recreation&lt;/em&gt; and &lt;em&gt;Health&lt;/em&gt;, &lt;em&gt;Food&lt;/em&gt; and &lt;em&gt;Other&lt;/em&gt;, and also &lt;em&gt;Restaurants&lt;/em&gt; and &lt;em&gt;Communications&lt;/em&gt;. All of them have a quite intuitive interpretation and support the &lt;strong&gt;take-home message&lt;/strong&gt; that (weighted) symmetric pivot coordinates are a reasonable alternative to correlations of proportional or percentage data.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;This article is based on&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Filzmoser, P. Hron, K. and Templ, M. (2018) &lt;em&gt;Applied Compositional Data Analysis. With Worked Examples in R&lt;/em&gt;. Springer Series in Statistics. Springer, Cham, Switzerland, 2018,
ISBN: 978-3-319-96422-5.&lt;/p&gt;
&lt;p&gt;Kynčlová, P., Hron, K., Filzmoser, P. (2017) &lt;em&gt;Correlation between compositional parts based on symmetric balances&lt;/em&gt;. Mathematical Geosciences, 49 (6), 777-796.&lt;/p&gt;
&lt;p&gt;Hron, K., Engle, M., Filzmoser, P., Fišerová, E. (2021) &lt;em&gt;Weighted symmetric pivot coordinates for compositional data with geochemical applications&lt;/em&gt;. Mathematical Geosciences,
&lt;a href=&#34;https://doi.org/10.1007/s11004-020-09862-5&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1007/s11004-020-09862-5&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;authors-biography&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Authors’ biography&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-02-04-correlation/Hron2017-small3.png&#34; alt=&#34;drawing&#34; width=&#34;75&#34;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Karel Hron&lt;/strong&gt; is a Professor at Department of Mathematical Analysis and Applications of Mathematics, Palacký University in Olomouc, Czech Republic, &lt;a href=&#34;mailto:karel.hron@upol.cz&#34; class=&#34;email&#34;&gt;karel.hron@upol.cz&lt;/a&gt;. His research chiefly focuses on the statistical analysis of compositional data and its applications in a wide range of fields (geology, analytical chemistry, metabolomics, time-use epidemiology and others). He co-authored the book &lt;em&gt;Applied Compositional Data Analysis&lt;/em&gt;, published in Springer Series in Statistics.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-02-04-correlation/pf2018small.jpg&#34; alt=&#34;drawing&#34; width=&#34;100&#34;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Peter Filzmoser&lt;/strong&gt; is a Professor of Statistics at the Vienna University of Technology
(TU Wien), Austria, &lt;a href=&#34;mailto:Peter.Filzmoser@tuwien.ac.at&#34; class=&#34;email&#34;&gt;Peter.Filzmoser@tuwien.ac.at&lt;/a&gt;. He has authored more than 200 research
articles and several R packages and has co-authored books on compositional data analysis
(Springer, 2018), on multivariate methods in chemometrics (CRC Press, 2009) and on analyzing
environmental data (Wiley, 2008).&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
