<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>YoungStatS</title>
    <link>https://youngstats.github.io/</link>
    <description>Recent content on YoungStatS</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 06 Dec 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://youngstats.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Heterogeneous Treatment Effects with Instrumental Variables: A Causal Machine Learning Approach</title>
      <link>https://youngstats.github.io/post/2021/12/06/heterogeneous-treatment-effects-with-instrumental-variables-a-causal-machine-learning-approach/</link>
      <pubDate>Mon, 06 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/12/06/heterogeneous-treatment-effects-with-instrumental-variables-a-causal-machine-learning-approach/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;problem-setting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Problem Setting&lt;/h2&gt;
&lt;p&gt;In our forthcoming &lt;a href=&#34;https://arxiv.org/pdf/1905.12707.pdf&#34;&gt;paper&lt;/a&gt; on Annals of Applied Statistics, we propose a new method – which we call &lt;strong&gt;Bayesian Causal Forest with Instrumental Variable&lt;/strong&gt; (BCF-IV) – to interpretably discover the subgroups with the largest or smallest causal effects in an instrumental variable setting.&lt;/p&gt;
&lt;p&gt;These are many situations, ranging in complexity and importance, where one would like to estimate the causal effect of a defined intervention on a specific outcome. When the intervention is not randomized, researchers can recur to an instrumental variable (IV) to assess the causal effects. A valid instrument, &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;, is a variable that affects the receipt of the treatment, &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;, without directly affecting the outcome, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. Using an IV enables researchers to effectively control for potential confounding factors and estimate the local effect of the treatment on individuals who would take a treatment if assigned to it, and not take it if not assigned (the so-called &lt;em&gt;compliers&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;If the classical four IV assumptions (&lt;em&gt;monotonicity, exclusion restriction, unconfoundedness of the instrument, existence of compliers&lt;/em&gt;) hold, one can identify the causal effect of the treatment on the sub-population of compliers, the so-called Complier Average Causal Effect (CACE), that is:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} 
   \tau^{cace} = \frac{\mathbb{E}\left[Y_i\mid Z_i = 1\right]-\mathbb{E}\left[Y_i\mid Z_i = 0\right]}{\mathbb{E}\left[W_i\mid Z_i =
        1\right]-\mathbb{E}\left[W_i\mid Z_i = 0\right]}={ITT\over\pi_C},
    \end{equation}\]&lt;/span&gt;
where the numerator represents the average effect of the instrument, also referred to as &lt;em&gt;Intention-To-Treat&lt;/em&gt; (&lt;span class=&#34;math inline&#34;&gt;\(ITT\)&lt;/span&gt;) effect, and the denominator represents the overall proportion of units that comply with the treatment assignment, also referred to as &lt;em&gt;proportion of compliers&lt;/em&gt; (&lt;span class=&#34;math inline&#34;&gt;\(\pi_C\)&lt;/span&gt;). For example, researchers can make use of an IV – such as being eligible for additional school funding – to isolate the causal effects of the primary treatment – i.e., receiving the funding – on the outcome of interest – i.e., the performance of students.&lt;/p&gt;
&lt;p&gt;In IV settings, it may be of interest to disentangle the heterogeneity in the causal effects by estimating the causal effects within different subgroups. In our paper, we introduce and consider the following conditional version of CACE:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} 
   \tau^{cace}(x) = \frac{\mathbb{E}\left[Y_i\mid Z_i = 1, X_i=x\right]-\mathbb{E}\left[Y_i\mid Z_i = 0, X_i=x\right]}{\mathbb{E}\left[W_i\mid Z_i =
        1, X_i=x\right]-\mathbb{E}\left[W_i\mid Z_i = 0, X_i=x\right]}= {ITT_Y(x)\over\pi_C(x)}.
    \end{equation}\]&lt;/span&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\tau^{cace}(x)\)&lt;/span&gt; is critical as it enables researchers to investigate the heterogeneity in causal effects within different subgroups defined by partitions &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; of the features’ space.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;methodology&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Methodology&lt;/h2&gt;
&lt;p&gt;Various causal machine learning methods have been proposed to estimate conditional causal effects. However, few methods have been developed to discover and estimate heterogeneity in IVs scenarios. To account for this shortcoming, we propose the BCF-IV method. BCF-IV is a &lt;strong&gt;three steps algorithm&lt;/strong&gt; that can be used to interpretably discover the subgroups with the largest or smallest effects.&lt;/p&gt;
&lt;p&gt;In &lt;strong&gt;step one&lt;/strong&gt;, we divide the data into two subsamples: one to build the tree for the discovery of the heterogeneous effects (&lt;em&gt;discover subsample&lt;/em&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{I}^{dis}\)&lt;/span&gt;) and another for making inference (&lt;em&gt;inference subsample&lt;/em&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{I}^{inf}\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;In &lt;strong&gt;step two&lt;/strong&gt;, we discover the heterogeneity in the conditional CACE on &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{I}^{dis}\)&lt;/span&gt; by modeling separately the conditional ITT (&lt;span class=&#34;math inline&#34;&gt;\(ITT_Y(x)\)&lt;/span&gt;) and the conditional proportion of compliers (&lt;span class=&#34;math inline&#34;&gt;\(\pi_C(x)\)&lt;/span&gt;). To do so, we adapt the Bayesian Causal Forest (BCF) method – proposed by Hanh et al. (2020), and recently featured &lt;a href=&#34;https://youngstats.github.io/post/2021/01/26/machine-learning-for-causal-inference-that-works/&#34;&gt;on the YoungStats blog&lt;/a&gt; – for the estimation of &lt;span class=&#34;math inline&#34;&gt;\(ITT_Y(x)\)&lt;/span&gt;, by including the IV, &lt;span class=&#34;math inline&#34;&gt;\(Z_i\)&lt;/span&gt;, in functional form for the conditional expected value of the outcome:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} 
    \mathbb{E}[Y_i\mid Z_i=z, X_i=x] = \mu(\pi(x),x) + ITT_{Y}(x)  z
\end{equation}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\pi(x)\)&lt;/span&gt; is the propensity score for the IV:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
    \pi(x) =  \mathbb{E}[Z_i=1\mid X_i=x].
\end{equation}\]&lt;/span&gt;
Both functions &lt;span class=&#34;math inline&#34;&gt;\(\mu(\cdot)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(ITT_Y(\cdot)\)&lt;/span&gt; are Bayesian Additive Regression Trees (Chipman, 2010) and are given independent priors to model differently the contributions of the covariates and the treatment on &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. The conditional proportion of compliers can be expressed:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} 
    \mathbb{E}\left[W_i\mid Z_i = 1, X_i=x\right]-\mathbb{E}\left[W_i\mid Z_i = 0, X_i=x\right]=\delta(1,x)-\delta(0,x),
\end{equation}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\delta(z,x)\)&lt;/span&gt; can be estimated using the Bayesian machine learning methodology for causal effects estimation proposed by Hill (2011). The conditional CACE can be computed as the ratio between conditional ITT and conditional proportion of compliers:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
   \hat{\tau}^{cace}(x) =\frac{\mu(\hat{\pi}(x), x) + \hat{ITT}_{Y}(x)  z}{\hat{\delta}(1,x)-\hat{\delta}(0,x)}.
\end{equation}\]&lt;/span&gt;
One can then regress &lt;span class=&#34;math inline&#34;&gt;\(\hat{\tau}^{cace}(x)\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; via a binary decision tree to discover, in an interpretable manner, the drivers of the heterogeneity (see, e.g., Lee et al., 2020).&lt;/p&gt;
&lt;p&gt;In &lt;strong&gt;step three&lt;/strong&gt;, once the heterogeneous subgroups are learned, one can estimate the conditional CACE, &lt;span class=&#34;math inline&#34;&gt;\(\hat{\tau}^{cace}(x)\)&lt;/span&gt; on the inference subsample &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{I}^{inf}\)&lt;/span&gt;. To do so, one can use the method of moments IV estimator from Angrist et al. (1996) within all the different sub-populations that were detected in the previous step. Alternative estimation strategies, such as Two-Stages-Least-Squares, can be employed as well. Finally, multiple hypotheses tests adjustments are performed to control for familywise error rate or – less stringently – for the false discovery rate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;application&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Application&lt;/h2&gt;
&lt;p&gt;In our motivating application, implemented via the &lt;a href=&#34;https://github.com/fbargaglistoffi/BCF-IV&#34;&gt;BCF-IV package&lt;/a&gt;, we evaluate the effects of the Equal Educational Opportunity program, promoted in Flanders (Belgium) to provide additional funding for secondary schools with a significant share of disadvantaged students. We use the quasi-randomized assignment of the funding as an IV to assess the effect of additional financial resources on students’ performance in compliant schools. The Flemish Ministry of Education provided us with data on student level characteristics and school level characteristics for the universe of pupils in the first stage of education in the school year 2010/2011 (135,682 students).&lt;/p&gt;
&lt;p&gt;While the overall effects are negative but not significant (consistently with the findings of previous literature), there are significant differences among different sub-populations of students. Indeed, for students in schools with younger and less senior principals (i.e., principals younger than 55 years old and with less than 30 years of experience) the effects of the policy are larger (see Figure 1).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-12-06-heterogeneous-treatment-effects-with-instrumental-variables-a-causal-machine-learning-approach_files/tree.png&#34; style=&#34;width:70.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 1. Visualization of the heterogeneous Complier Average Causal Effects (CACE) of additional funding on student performance. The tree was discovered and estimated using the proposed BCF-IV model.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;By investigating the heterogeneity in the causal effects, BCF-IV expedites targeted policies. In fact, BCF-IV can shed light on the heterogeneity of causal effects in IVs scenarios and, in turn, provides a relevant knowledge for designing targeted interventions. Furthermore, in a Monte Carlo simulation study, we manifested that the BCF-IV technique outperforms other machine learning techniques tailored for causal inference in precisely estimating the causal effects and converges to an optimal large sample performance in identifying the subgroups with heterogeneous effects.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;essential-bibliography&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Essential bibliography&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;This article is based on:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Angrist, J. D., Imbens, G. W., &amp;amp; Rubin, D. B. (1996). &lt;em&gt;Identification of causal effects using instrumental variables.&lt;/em&gt; Journal of the American statistical Association, 91(434), 444-455.&lt;/p&gt;
&lt;p&gt;Bargagli-Stoffi, F. J., De-Witte, K. and Gnecco, G. (2021+) &lt;em&gt;Heterogeneous causal effects with imperfect compliance: a Bayesian machine learning approach.&lt;/em&gt; The Annals of Applied Statistics, forthcoming.&lt;/p&gt;
&lt;p&gt;Chipman, H. A., George, E. I., &amp;amp; McCulloch, R. E. (2010). &lt;em&gt;BART: Bayesian additive regression trees.&lt;/em&gt; The Annals of Applied Statistics, 4(1), 266-298.&lt;/p&gt;
&lt;p&gt;Lee, K., Bargagli-Stoffi, F. J., &amp;amp; Dominici, F. (2020). &lt;em&gt;Causal rule ensemble: Interpretable inference of heterogeneous treatment effects&lt;/em&gt;. arXiv preprint arXiv:2009.09036.&lt;/p&gt;
&lt;p&gt;Hahn, P. R., Murray, J. S., &amp;amp; Carvalho, C. M. (2020). &lt;em&gt;Bayesian regression tree models for causal inference: Regularization, confounding, and heterogeneous effects&lt;/em&gt;. Bayesian Analysis, 15(3), 965-1056.&lt;/p&gt;
&lt;p&gt;Hill, J. L. (2011). &lt;em&gt;Bayesian nonparametric modeling for causal inference.&lt;/em&gt; Journal of Computational and Graphical Statistics, 20(1), 217-240.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;authors-biography&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Authors’ biography&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-12-06-heterogeneous-treatment-effects-with-instrumental-variables-a-causal-machine-learning-approach_files/PhotoHandler%20(Custom).jpeg&#34; height=&#34;70&#34; /&gt;
&lt;a href=&#34;https://www.falcobargaglistoffi.com/&#34;&gt;&lt;strong&gt;Falco J. Bargagli Stoffi&lt;/strong&gt;&lt;/a&gt; is a Postdoctoral Research Fellow at the Harvard T.H. Chan School of Public Health.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Frozen percolation on the binary tree is nonendogenous</title>
      <link>https://youngstats.github.io/post/2021/11/25/frozen-percolation-on-the-binary-tree-is-nonendogenous/</link>
      <pubDate>Thu, 25 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/11/25/frozen-percolation-on-the-binary-tree-is-nonendogenous/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In frozen percolation on a graph, there is a barrier located on each
edge. Initially, the barriers are closed and they are assigned
i.i.d. uniformly distributed activation times. At its activation time,
a barrier opens, provided it is not frozen. At a fixed set &lt;span class=&#34;math inline&#34;&gt;\(\Xi\)&lt;/span&gt; of
freezing times, all barriers that percolate are frozen. In particular,
if &lt;span class=&#34;math inline&#34;&gt;\(\Xi\)&lt;/span&gt; is the whole unit interval, this means that clusters stop
growing as soon as they reach infinite size. Aldous (2000) showed that
such a process can be constructed on the infinite 3-regular tree. He
also proved uniqueness in law under natural conditions and asked
whether almost sure uniqueness holds. We answer this question
negatively. For general sets of freezing times, the answer turns out
to depend subtly on &lt;span class=&#34;math inline&#34;&gt;\(\Xi\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;definition-of-the-model&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;Definition of the model&lt;/h2&gt;
&lt;p&gt;To construct frozen percolation on a directed graph, we place a
&lt;em&gt;barrier&lt;/em&gt; on each directed edge. These barriers can be in three states:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-11-25-frozen-percolation-on-the-binary-tree-is-nonendogenous_files/screenshot-66.png&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We assign i.i.d. Unif&lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt; &lt;em&gt;activation times&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\((\tau_\mathbf{i})_{\mathbf{i}\in{\mathbb B}}\)&lt;/span&gt; to the barriers
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{i}\in{\mathbb B}\)&lt;/span&gt;, and we fix a closed set &lt;span class=&#34;math inline&#34;&gt;\(\Xi\subset[0,1]\)&lt;/span&gt; of
&lt;em&gt;freezing times&lt;/em&gt;. The states of barriers evolve according to the
following informal description:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Initially, all barriers are closed.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;At its activation time, a barrier opens, provided it is not frozen.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;At each freezing time &lt;span class=&#34;math inline&#34;&gt;\(t\in\Xi\)&lt;/span&gt;, all closed barriers that percolate
are frozen.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here, we say that a barrier percolates if there starts an infinite open
path just above it:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-11-25-frozen-percolation-on-the-binary-tree-is-nonendogenous_files/screenshot-71.png&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;More formally, we set &lt;span class=&#34;math display&#34;&gt;\[\begin{array}{r@{\,}c@{\,}l}
\displaystyle{\mathbb A}_t&amp;amp;:=&amp;amp;\displaystyle\big\{\mathbf{i}\in{\mathbb B}:\tau_\mathbf{i}\leq t\big\},\\[5pt]
\displaystyle{\mathbb F}&amp;amp;:=&amp;amp;\displaystyle\big\{\mathbf{i}\in{\mathbb B}:\mathbf{i}\mbox{ is frozen at the final time }1\big\}.
\end{array}\]&lt;/span&gt; Then &lt;span class=&#34;math inline&#34;&gt;\({\mathbb A}_t\backslash{\mathbb F}\)&lt;/span&gt; is the set of
barriers that are open at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. We write
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{i}\overset{{{\mathbb A}_t\backslash{\mathbb F}}}{\longrightarrow}\infty\)&lt;/span&gt;
if &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{i}\)&lt;/span&gt; percolates at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. Then the &lt;em&gt;Frozen Percolation
Equation&lt;/em&gt; reads:
&lt;span class=&#34;math display&#34;&gt;\[{\mathbb F}=\big\{\mathbf{i}\in{\mathbb B}:\mathbf{i}\overset{{{\mathbb A}_t\backslash{\mathbb F}}}{\longrightarrow}\infty
\mbox{ for some }t\in(0,\tau_\mathbf{i}]\cap\Xi\big\}\qquad({\rm FPE}).\]&lt;/span&gt;
It is clear from our informal description that if &lt;span class=&#34;math inline&#34;&gt;\(\Xi\)&lt;/span&gt; is finite, then
(FPE) has a solution &lt;span class=&#34;math inline&#34;&gt;\({\mathbb F}\)&lt;/span&gt;, which is a.s. unique given the
activation times &lt;span class=&#34;math inline&#34;&gt;\((\tau_\mathbf{i})_{\mathbf{i}\in{\mathbb B}}\)&lt;/span&gt;. For
general &lt;span class=&#34;math inline&#34;&gt;\(\Xi\)&lt;/span&gt; and general directed graphs, one can ask whether solutions
exist and whether they are unique in distribution or perhaps even almost
surely.&lt;/p&gt;
&lt;p&gt;Frozen precolation can also be defined on undirected graphs. Just
replace each undirected edge by two directed edges whose barriers are
activated at the same time. The case &lt;span class=&#34;math inline&#34;&gt;\(\Xi=[0,1]\)&lt;/span&gt; is of special interest,
since in this case clusters freeze as soon as they reach infinite size,
which leads to self-organised criticality. For this reason, in the year
2000 David Aldous introduced frozen percolation (Aldous, 2000). He proved that
the frozen percolation equation (FPE) has a solution on the undriected
3-regular tree, and that under natural assumptions this solution is
unique in law. On the other hand, Itai Benjamini and Oded Schramm
observed that (FPE) does not have a solution on the two-dimensional
integer lattice. A sketch of their proof can be found in Section 3 of
(van den Berg and Tóth, 2001).&lt;/p&gt;
&lt;p&gt;If from the directed binary tree &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;, we remove a finite subtree &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;
containing the root, then &lt;span class=&#34;math inline&#34;&gt;\(T\backslash U\)&lt;/span&gt; consists of finitely many
disjoint subtrees &lt;span class=&#34;math inline&#34;&gt;\(T_1,\ldots,T_n\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-11-25-frozen-percolation-on-the-binary-tree-is-nonendogenous_files/screenshot-70.png&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We say that a solution &lt;span class=&#34;math inline&#34;&gt;\({\mathbb F}\)&lt;/span&gt; satisfies the &lt;em&gt;natural conditions&lt;/em&gt;
if its restrictions to &lt;span class=&#34;math inline&#34;&gt;\(T_1,\ldots,T_n\)&lt;/span&gt; are i.i.d., equally distributed
with the process on the whole tree &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;, and independent of the
activation times in &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;. We label barriers in the obvious way and let
&lt;span class=&#34;math inline&#34;&gt;\([\mathbf{i}]\)&lt;/span&gt; denote the vertex just below &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{i}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-11-25-frozen-percolation-on-the-binary-tree-is-nonendogenous_files/screenshot-68.png&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is easy to see that the freezing time of the root
&lt;span class=&#34;math display&#34;&gt;\[Y_{[\varnothing]}:=\inf\big\{t\in\Xi:[\varnothing]\overset{{{\mathbb A}_t\backslash{\mathbb F}}}{\longrightarrow}\infty\big\}\]&lt;/span&gt;
solves the &lt;em&gt;Recursive Distributional Equation&lt;/em&gt;
&lt;span class=&#34;math display&#34;&gt;\[Y_{[\varnothing]}\stackrel{\scriptstyle{\rm d}}{=}\gamma(\tau_\varnothing,Y_{[1]},Y_{[2]}):=\left\{\begin{array}{ll}
Y_{[1]}\wedge Y_{[2]}\quad&amp;amp;\mbox{if }\tau_\varnothing&amp;lt;Y_{[1]}\wedge Y_{[2]},\\[5pt]
\infty\quad&amp;amp;\mbox{otherwise,}
\end{array}\right.\quad({\rm RDE}),\]&lt;/span&gt; where, because the solution to
(FPE) saties the natural conditions, &lt;span class=&#34;math inline&#34;&gt;\(Y_{[1]},Y_{[2]}\)&lt;/span&gt; are independent
of each other and equally distributed with &lt;span class=&#34;math inline&#34;&gt;\(Y_{[\varnothing]}\)&lt;/span&gt;, and
&lt;span class=&#34;math inline&#34;&gt;\(\tau_\varnothing\)&lt;/span&gt; is an independent Unif&lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt; distributed random
variable.&lt;/p&gt;
&lt;p&gt;The following theorem is a free formulation of of Theorem 1 and Lemma 5
from (Ráth, Swart and Szőke, 2021), which using Proposition 42 of (Ráth, Swart and Terpai, 2021) are translated
into the present setting.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; For each closed &lt;span class=&#34;math inline&#34;&gt;\(\Xi\subset[0,1]\)&lt;/span&gt;, there exists a unique
solution to (RDE) that yields a solution &lt;span class=&#34;math inline&#34;&gt;\({\mathbb F}\)&lt;/span&gt; to (FPE). In
particular, for each closed &lt;span class=&#34;math inline&#34;&gt;\(\Xi\subset[0,1]\)&lt;/span&gt;, (FPE) has a solution
that satisfies the natural conditions, and the joint law of
&lt;span class=&#34;math inline&#34;&gt;\(\big(\tau_\mathbf{i})_{\mathbf{i}\in{\mathbb B}},{\mathbb F}\big)\)&lt;/span&gt; is
uniquely determined.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For the case &lt;span class=&#34;math inline&#34;&gt;\(\Xi=[0,1]\)&lt;/span&gt;, this theorem was (basically) already proved in
(Aldous, 2000) by Aldous, who showed that in this case the freezing time of
the root has the law
&lt;span class=&#34;math display&#34;&gt;\[{\mathbb P}\big[Y_{[\varnothing]}&amp;gt;y\big]=1\wedge\frac{1}{2y}\qquad\big(y\in[0,1]\big),\]&lt;/span&gt;
with
&lt;span class=&#34;math inline&#34;&gt;\({\mathbb P}\big[Y_{[\varnothing]}=\infty\big]={\textstyle\frac{{1}}{{2}}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To study almost sure uniqueness in this and related problems, David
Aldous and Antar Bandyopadhyay (Aldous and Bandyopadhyay, 2005) developed a general theory of
&lt;em&gt;Recursive Tree Processes&lt;/em&gt;. In the general setting, almost sure
uniqueness is called &lt;em&gt;endogeny&lt;/em&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\({\mathbb F},{\mathbb F}&amp;#39;\)&lt;/span&gt; be
solutions to (FPE) that satisfy the natural conditions and are
conditionally independent given
&lt;span class=&#34;math inline&#34;&gt;\((\tau_\mathbf{i})_{\mathbf{i}\in{\mathbb B}}\)&lt;/span&gt;. Then
&lt;span class=&#34;math inline&#34;&gt;\((Y_{[\varnothing]},Y&amp;#39;_{[\varnothing]})\)&lt;/span&gt; solves the &lt;em&gt;bivariate RDE&lt;/em&gt;
&lt;span class=&#34;math display&#34;&gt;\[(Y_{[\varnothing]},Y&amp;#39;_{[\varnothing]})\stackrel{\scriptstyle{\rm d}}{=}
\big(\gamma(\tau_\varnothing,Y_{[1]},Y_{[2]}),\gamma(\tau_\varnothing,Y&amp;#39;_{[1]},Y&amp;#39;_{[2]})\big),\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; is the function from (RDE). In Theorem 11 of (Aldous and Bandyopadhyay, 2005), it
is proved that &lt;span class=&#34;math inline&#34;&gt;\({\mathbb F}={\mathbb F}&amp;#39;\)&lt;/span&gt; a.s. if and only if each
solution to the bivariate RDE with the right marginals is concentrated
on the diagonal. After a failed proof of endogeny that turned out to
contain an error (Bandyopadhyay, 2004), Antar Bandyopadhyay ran numerical
calculations that strongly suggested almost sure uniqueness does not
hold, but for 15 years nobody was able to prove this.&lt;/p&gt;
&lt;p&gt;We discovered the problem becomes easier if we place a geometrically
distributed number of barriers with mean one on each edge.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-11-25-frozen-percolation-on-the-binary-tree-is-nonendogenous_files/screenshot-67.png&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We call this the &lt;em&gt;Marked Binary Branching Tree&lt;/em&gt; (MBBT). The MBBT
naturally arises as the near-critical scaling limit of percolation on
regular trees. Since it is itself a scaling limit, the MBBT has a
natural scaling property. For fixed &lt;span class=&#34;math inline&#34;&gt;\(r\in[0,1]\)&lt;/span&gt;, the &lt;em&gt;pruned&lt;/em&gt; MBBT is
obtained by removing all parts of the tree that do not percolate at time
&lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; even in the absence of freezing. For the MBBT,
&lt;span class=&#34;math inline&#34;&gt;\({\mathbb P}\big[[\varnothing]\overset{{{\mathbb A}_r}}{\longrightarrow}\infty\big]=r\)&lt;/span&gt;
and conditional on this event, the pruned tree is equally distributed
with a scaled version of the original tree.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-11-25-frozen-percolation-on-the-binary-tree-is-nonendogenous_files/screenshot-69.png&#34; style=&#34;width:80.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The bivariate RDE gives an integral equation for
&lt;span class=&#34;math display&#34;&gt;\[F(s,t):={\mathbb P}\big[Y_{[\varnothing]}\leq s,\ Y&amp;#39;_{[\varnothing]}\leq t\big].\]&lt;/span&gt;
For the MBBT, scaling gives &lt;span class=&#34;math inline&#34;&gt;\(F(rs,rt)=rF(s,t)\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\((r,s,t\in[0,1])\)&lt;/span&gt;. The
bivariate RDE now reduces to an integral equation for a function of one
variable, which is easier to solve than the original bivariate RDE for
the binary tree. However, as demonstrated in Proposition 42 of (Ráth, Swart and Terpai, 2021),
frozen percolation on the binary tree and on the MBBT can be translated
into each other. The basic idea is that several barriers on one oriented
edge can be replaced by a single effective barrier, whose activation
time is the maximum of the activation times of the individual barriers.
As a result, we were able to obtain Theorem 3 of (Ráth, Swart and Terpai, 2021), which says:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; For frozen percolation on the binary tree with
&lt;span class=&#34;math inline&#34;&gt;\(\Xi=[0,1]\)&lt;/span&gt;, solutions to (FPE) are not almost surely unique.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In (Ráth, Swart and Szőke, 2021) we investigated almost sure uniqueness for frozen
percolation on the MBBT for sets of freezing times of the form
&lt;span class=&#34;math inline&#34;&gt;\(\Xi_\theta:=\{\theta^n:n\geq 0\}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(0&amp;lt;\theta&amp;lt;1\)&lt;/span&gt;. These sets behave
well under scaling, so we were able to use the same techniques as in
(Ráth, Swart and Terpai, 2021). To translate this back from the MBBT into the setting of the
binary tree, one has to replace &lt;span class=&#34;math inline&#34;&gt;\(\Xi_\theta\)&lt;/span&gt; by its image under the map
&lt;span class=&#34;math inline&#34;&gt;\(H(t):=1/(2-t)\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\((t\in[0,1])\)&lt;/span&gt;. Theorem 9 of (Ráth, Swart and Szőke, 2021) says:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; For frozen percolation on the MBBT and sets of freezing
times of the form &lt;span class=&#34;math inline&#34;&gt;\(\Xi_\theta\)&lt;/span&gt;, there exists a parameter
&lt;span class=&#34;math inline&#34;&gt;\(\theta^\ast=0.636\ldots\)&lt;/span&gt; such that, if &lt;span class=&#34;math inline&#34;&gt;\({\mathbb F},{\mathbb F}&amp;#39;\)&lt;/span&gt; are
solutions to (FPE) for &lt;span class=&#34;math inline&#34;&gt;\(\Xi_\theta\)&lt;/span&gt;, that satisfy the natural
conditions and are conditionally independent given
&lt;span class=&#34;math inline&#34;&gt;\((\tau_\mathbf{i})_{\mathbf{i}\in{\mathbb B}}\)&lt;/span&gt;, then
&lt;span class=&#34;math inline&#34;&gt;\({\mathbb F}={\mathbb F}&amp;#39;\)&lt;/span&gt; a.s. if and only if
&lt;span class=&#34;math inline&#34;&gt;\(\theta\leq\theta^\ast\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We have an explicit formula for the parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta^\ast\)&lt;/span&gt; (see Lemma 8
in (Ráth, Swart and Szőke, 2021)), which is rather complicated. The result shows that in
general, one cannot hope to settle the question of almost sure
uniqueness by “soft” arguments. Instead, any proof will always require
some hard calculations.&lt;/p&gt;
&lt;p&gt;A number of open problems remain, notably:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We don’t know if distributional or almost sure uniqueness still hold
if we drop the natural conditions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Almost sure (non-)uniqueness is open for &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-regular directed trees
with &lt;span class=&#34;math inline&#34;&gt;\(n\geq 3\)&lt;/span&gt; and for more general sets of freezing times &lt;span class=&#34;math inline&#34;&gt;\(\Xi\)&lt;/span&gt;.
The technical challenge is to replace perfect scale invariance in
the proofs by approximate scale invariance near the critical point.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Existence and/or uniqueness of solutions to (FPE) on more general
graphs, including &lt;span class=&#34;math inline&#34;&gt;\({\mathbb Z}^d\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(d\geq 3\)&lt;/span&gt;, is wide open.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 thebibliography&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;D.J. Aldous and A. Bandyopadhyay. A survey of max-type recursive
distributional equations. &lt;em&gt;Ann. Appl. Probab.&lt;/em&gt; 15(2) (2005), 1047–1110.&lt;/p&gt;
&lt;p&gt;D.J. Aldous. The percolation process on a tree where infinite clusters
are frozen. &lt;em&gt;Math. Proc. Cambridge Philos. Soc.&lt;/em&gt; 128 (2000), 465–477.&lt;/p&gt;
&lt;p&gt;A. Bandyopadhyay. Bivariate uniqueness and endogeny for recursive
distributional equations: two examples. Preprint (2004),
arXiv:math/0407175.&lt;/p&gt;
&lt;p&gt;J. van den Berg, B. Tóth. A signal-recovery system: asymptotic
properties, and construction of an infinite-volume process. &lt;em&gt;Stochastic
Process. Appl.&lt;/em&gt; 96(2) (2001), 177–190.&lt;/p&gt;
&lt;p&gt;B. Ráth, J.M. Swart, and M. Szőke. A phase transition between endogeny
and nonendogeny. Preprint (2021), arXiv:2103.14408.&lt;/p&gt;
&lt;p&gt;B. Ráth, J.M. Swart, and T. Terpai. Frozen percolation on the binary
tree is nonendogenous. &lt;em&gt;Ann. Probab.&lt;/em&gt; 49(5) (2021), 2272–2316.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Novel Algebraic Approaches to Maximum Likelihood Estimation</title>
      <link>https://youngstats.github.io/post/2021/10/04/novel-algebraic-approaches-to-maximum-likelihood-estimation/</link>
      <pubDate>Mon, 04 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/10/04/novel-algebraic-approaches-to-maximum-likelihood-estimation/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;Novel Algebraic Approaches to Maximum Likelihood Estimation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-10-04-novel-algebraic-approaches-to-maximum-likelihood-estimation_files/nihat_ay_v2.png&#34; height=&#34;300&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The seventh “One World webinar” organized by YoungStatS will take place
on November 17th, 2021. Maximum likelihood estimation (MLE) is a tool in
data analysis to estimate a probability distribution or density in a
statistical model for given data. In recent decades, algebraic and
combinatorial tools have proved useful for computing MLEs and
understanding the geometry of the MLE problem which in recent years led
to new and interesting results in combinatorics and algebraic geometry.
Selected young researchers active in the area of algebraic statistics
will present their recent contributions on this topic.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;p&gt;Wednesday, November 17th, 18:00 Central European Time&lt;/p&gt;
&lt;p&gt;Online, via Zoom. The registration form is available
&lt;a href=&#34;https://docs.google.com/forms/d/1rfiEa9RWDBdT_UEHspBuOCIEwsq5UzDUqKbtGhV38Oo&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://janeivycoons.wordpress.ncsu.edu/&#34;&gt;Jane Ivy Coons&lt;/a&gt;,
St John’s College, University of Oxford, United Kingdom&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.google.com/view/aidamaraj/&#34;&gt;Aida Maraj&lt;/a&gt;, University
of Michigan – Ann Arbor, USA&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.maths.ox.ac.uk/people/anna.seigal&#34;&gt;Anna Seigal&lt;/a&gt;,
Harvard University, USA&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.luke-amendola.appspot.com/&#34;&gt;Carlos Améndola&lt;/a&gt;, Technical University of Munich, Germany&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The webinar is part of YoungStatS project of the Young Statisticians
Europe initiative (FENStatS) supported by the Bernoulli Society for
Mathematical Statistics and Probability and the Institute of
Mathematical Statistics (IMS).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Advancements in Symbolic Data Analysis</title>
      <link>https://youngstats.github.io/post/2021/09/30/advancements-in-symbolic-data-analysis/</link>
      <pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/09/30/advancements-in-symbolic-data-analysis/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;Advancements in Symbolic Data Analysis&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-09-30-advancements-in-symbolic-data-analysis_files/cover-sda.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The sixth “One World webinar” organized by YoungStatS will take place on
November 8th, 2021. With the development of digital systems, very large
datasets have become routine. However, standard statistical approaches
do not have the power or flexibility to analyse these efficiently, and
extract the required knowledge. Symbolic Data Analysis provides a
framework allowing for the representation of data with intrinsic
variability, where the observed “values” are not just single real values
or categories, but finite sets, intervals or distributions over a given
domain. Methods for the (multivariate) analysis of such symbolic data
have been developed, following different approaches, and using distinct
criteria, which allow taking data variability into account.&lt;/p&gt;
&lt;p&gt;Selected young researchers active in the area will present their recent
contributions on this developing topic.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;p&gt;Monday, November 8th, 12:00 Central European Time&lt;/p&gt;
&lt;p&gt;Online, via Zoom. The registration form is available &lt;a href=&#34;https://docs.google.com/forms/d/1WfJ6NG4hWq3-6C0q6gs9axaDR8THX9uP1UPS-5iB-dU/edit&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.researchgate.net/scientific-contributions/Yuying-Sun-2145350022&#34;&gt;Yuying Sun&lt;/a&gt; (Chinese Academy of Sciences, Beijing, China): “&lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3846927&#34;&gt;Model Averaging for Interval-valued Data&lt;/a&gt;”&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.borisberanger.com/&#34;&gt;Boris Beranger&lt;/a&gt; (University of New South Wales, Australia): “&lt;a href=&#34;https://arxiv.org/abs/1809.03659&#34;&gt;Using symbolic data to understand underlying data behaviour&lt;/a&gt;”&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=--uDCpYAAAAJ&amp;amp;hl=en&#34;&gt;Bruno Pimentel&lt;/a&gt; (Universidade Federal de Alagoas, Brazil): “&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0950705121003543&#34;&gt;Kohonen Map-Wise Regression Applied to Interval Data&lt;/a&gt;”&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.estg.ipvc.pt/~sdias/&#34;&gt;Sónia Dias&lt;/a&gt; (Polytechnic Institute of Viana do Castelo, Portugal): “&lt;a href=&#34;https://doi.org/10.1016/j.ejor.2021.01.025&#34;&gt;Discriminant analysis of distributional data&lt;/a&gt;”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prof. &lt;a href=&#34;https://www.fep.up.pt/docentes/mpbrito/&#34;&gt;Paula Brito&lt;/a&gt;, University of Porto, Portugal&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The webinar is part of YoungStatS project of the Young Statisticians
Europe initiative (FENStatS) supported by the Bernoulli Society for
Mathematical Statistics and Probability and the Institute of
Mathematical Statistics (IMS).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Advances in Difference-in-Differences in Econometrics</title>
      <link>https://youngstats.github.io/post/2021/09/30/advances-in-difference-in-differences-in-econometrics/</link>
      <pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/09/30/advances-in-difference-in-differences-in-econometrics/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;Advances in Difference-in-Differences in Econometrics&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-09-30-advances-in-difference-in-differences-in-econometrics_files/cover_did_roth_image.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The eighth “One World webinar” organized by YoungStatS will take place
on December 15th, 2021. The difference-in-differences design is a
quasi-experimental identification strategy for estimating causal effects
which has become the single most popular research design in the
quantitative social sciences, and as such, it merits careful study by
researchers everywhere. It is also a flourishing field of present
research in econometrics. Selected younger researchers active in the
area will present their recent contributions on this topic.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;p&gt;Wednesday, December 15th, 9:00 PT / 12:00 EST / 18:00 CEST&lt;/p&gt;
&lt;p&gt;Online, via Zoom. The registration form is available
&lt;a href=&#34;https://docs.google.com/forms/d/1cdQT9YwjzIEjuNo_OMVjnPgKpO_6M2U6tBq9E471Df0/edit&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.google.com/site/clementdechaisemartin/&#34;&gt;Clément de
Chaisemartin&lt;/a&gt;, Sciences Po, Paris: “Two-Way Fixed Effects and Differences-in-Differences with Heterogeneous Treatment Effects: A Survey” (joint work with Xavier d’Haultfoeuille)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jonathandroth.github.io/&#34;&gt;Jonathan Roth&lt;/a&gt;, Brown University: “Difference-in-Differences When Parallel Trends Might Be Violated” (based on joint work with Ashesh Rambachan)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://bcallaway11.github.io/&#34;&gt;Brantly Callaway&lt;/a&gt;, University of Georgia: “Difference-in-Differences with a Continuous Treatment” (joint work with Andrew Goodman-Bacon and Pedro H. C. Sant’Anna)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lihualei71.github.io/&#34;&gt;Lihua Lei&lt;/a&gt;, Stanford University: “Double-Robust Two-Way-Fixed-Effects Regression For Panel Data” (joint work with Dmitry Arkhangelsky, Guido W. Imbens and Xiaoman Luo)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prof. &lt;a href=&#34;https://pedrohcgs.github.io/&#34;&gt;Pedro H.C. Sant&#39;Anna&lt;/a&gt;, Microsoft and Vanderbilt University,
USA&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The webinar is part of YoungStatS project of the Young Statisticians
Europe initiative (FENStatS) supported by the Bernoulli Society for
Mathematical Statistics and Probability and the Institute of
Mathematical Statistics (IMS).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Optimal disclosure risk assessment</title>
      <link>https://youngstats.github.io/post/2021/09/30/optimal-disclosure-risk-assessment/</link>
      <pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/09/30/optimal-disclosure-risk-assessment/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;disclosure-risk-for-microdata&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Disclosure risk for microdata&lt;/h1&gt;
&lt;p&gt;Protection against disclosure is a legal and ethical obligation for
agencies releasing microdata files for public use. Consider a microdata
sample &lt;span class=&#34;math inline&#34;&gt;\({X}_n=(X_{1},\ldots,X_{n})\)&lt;/span&gt; of size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; from a finite
population of size &lt;span class=&#34;math inline&#34;&gt;\(\bar{n}=n+\lambda n\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(\lambda&amp;gt;0\)&lt;/span&gt;, such that
each sample record &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; contains two disjoint types of information:
identifying categorical information and sensitive information.
Identifying information consists of a set of categorical variables which
might be matchable to known units of the population. A risk of
disclosure results from the possibility that an intruder might succeed
in identifying a microdata unit through such a matching, and hence be
able to disclose sensitive information on this unit. To quantify the
risk of disclosure, sample records &lt;span class=&#34;math inline&#34;&gt;\({X}_n\)&lt;/span&gt; are typically
cross-classified according to identifying variables. That is, &lt;span class=&#34;math inline&#34;&gt;\({X}_n\)&lt;/span&gt;
is partitioned in &lt;span class=&#34;math inline&#34;&gt;\(K_{n}\leq n\)&lt;/span&gt; cells, with &lt;span class=&#34;math inline&#34;&gt;\(Y_{j,n}\)&lt;/span&gt; being the number
of &lt;span class=&#34;math inline&#34;&gt;\(X_{i}\)&lt;/span&gt;’s belonging to cell &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(j=1,\ldots, K_{n}\)&lt;/span&gt;, such that
&lt;span class=&#34;math inline&#34;&gt;\(\sum_{1\leq j\leq K_{n}}Y_{j,n}=n\)&lt;/span&gt;; we refer to the number of
occurrences &lt;span class=&#34;math inline&#34;&gt;\(Y_{j,n}\)&lt;/span&gt; as the sample frequency of cell &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. We also
indicate by &lt;span class=&#34;math inline&#34;&gt;\(Y_{j,\bar{n}}\)&lt;/span&gt; the same quantities referring to the entire
population of size &lt;span class=&#34;math inline&#34;&gt;\(\bar{n}\)&lt;/span&gt;. Then, a risk of disclosure arises from
cells in which both sample frequencies and population frequencies are
small. Of special interest are cells with frequency &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; (singletons or
uniques) since, assuming no errors in the matching process or data
sources, for these cells the match is guaranteed to be correct. This has
motivated inferences on measures of disclosure risk that are suitable
functionals of the number of uniques, the most common being the number
&lt;span class=&#34;math inline&#34;&gt;\(\tau_{1}\)&lt;/span&gt; of sample uniques which are also population uniques, namely
the following functional:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\tau_{1}=\sum_{j\geq 1}\mathbf{1}_{\{Y_{j,n}=1\}}\mathbf{1}_{\{Y_{j,\bar{n}}=1\}},\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{1}\)&lt;/span&gt; denotes the indicator function.&lt;/p&gt;
&lt;p&gt;We first introduce a class of nonparametric estimators of &lt;span class=&#34;math inline&#34;&gt;\(\tau_{1}\)&lt;/span&gt;, we
then show that they provably estimate &lt;span class=&#34;math inline&#34;&gt;\(\tau_{1}\)&lt;/span&gt; all of the way up to
the sampling fraction &lt;span class=&#34;math inline&#34;&gt;\((\lambda+1)^{-1}\propto (\log n)^{-1}\)&lt;/span&gt;, with
vanishing normalized mean squared error (NMSE) for large sample size
&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. More importantly we prove that
&lt;span class=&#34;math inline&#34;&gt;\((\lambda+1)^{-1}\propto (\log n)^{-1}\)&lt;/span&gt; is the smallest possible
sampling fraction for consistently estimating &lt;span class=&#34;math inline&#34;&gt;\(\tau_{1}\)&lt;/span&gt;, thus the
estimators’ NMSE is near optimal. Our paper also provides a rigorous
answer to an open question raised by Skinner and Elliot (2002) about the feasibility of nonparametric estimation of &lt;span class=&#34;math inline&#34;&gt;\(\tau_{1}\)&lt;/span&gt; and for a sampling fraction
&lt;span class=&#34;math inline&#34;&gt;\((\lambda+1)^{-1}&amp;lt;1/2\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;nonparametric-estimation-of-tau_1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Nonparametric estimation of &lt;span class=&#34;math inline&#34;&gt;\(\tau_{1}\)&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;A nonparametric estimator for &lt;span class=&#34;math inline&#34;&gt;\(\tau_1\)&lt;/span&gt; may be simply deduced by
comparing expectations. Indeed, under a suitable Poisson abundance model
for the cells’ proportions, it easy to see that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\label{eq:comparingE}
\mathbb{E} [\tau_1] =  \sum_{i \geq 0}(-1)^i \lambda^i (i+1) \mathbb{E} [Z_{i+1, n}],\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Z_{i,n}\)&lt;/span&gt; denotes the number of cells with frequency &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; out of
the sample &lt;span class=&#34;math inline&#34;&gt;\({X}_n\)&lt;/span&gt;. Thus, according to identity , we can define the
following estimator of &lt;span class=&#34;math inline&#34;&gt;\(\tau_1\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\label{eq:estimator&amp;lt;1}
\hat{\tau}_{1}=\sum_{i \geq 0} (-1)^{i} (i+1)\lambda^{i}Z_{i+1}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;By construction, the estimator is unbiased and it admits a natural
interpretation as a nonparametric empirical Bayes estimator in the sense
of Robbins(1956). The use of estimator is legitimated under the assumption
&lt;span class=&#34;math inline&#34;&gt;\(\lambda&amp;lt;1\)&lt;/span&gt;. For &lt;span class=&#34;math inline&#34;&gt;\(\lambda \geq 1\)&lt;/span&gt; it becomes useless, because of its
high variance due to the exponential growth of the coefficients
&lt;span class=&#34;math inline&#34;&gt;\(\lambda^i\)&lt;/span&gt;. Unfortunately, the assumption &lt;span class=&#34;math inline&#34;&gt;\(\lambda &amp;lt; 1\)&lt;/span&gt; is
unrealistic in the context of disclosure risk assessment, where the size
&lt;span class=&#34;math inline&#34;&gt;\(\lambda n\)&lt;/span&gt; of the unobserved population is typically much bigger than
the size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; of the observed sample. Thus the estimator requires an
adjustment via suitable smoothing techniques, along similar lines as
Orlitsky et al. (2016) in the context of the nonparametric estimation of the number of unseen species. We propose a smoothed version of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\tau}_{1}\)&lt;/span&gt; by
truncating the series at an independent random location &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;, and then
averaging over the distribution of &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;, i.e.,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\label{eq:estimator&amp;gt;1}
\hat{\tau}_{1}^L&amp;amp;= {\mathbb{E}}_L \left[ \sum_{i =1}^L (-1)^{i} (i+1) \lambda^i Z_{i+1,n}\right],\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; is supposed to be a Poisson or a Binomial random variable, but
other choices are possible.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;main-results&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Main results&lt;/h1&gt;
&lt;p&gt;We have evaluated the performance of the estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat{\tau}_{1}^L\)&lt;/span&gt; in
terms of the normalized mean squared error (NMSE). The NMSE is the mean
squared error (MSE) of the estimator normalized by the maximum value of
&lt;span class=&#34;math inline&#34;&gt;\(\tau_{1}\)&lt;/span&gt; (which is exactly &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;). Thus, the performance of an estimator
is evaluated in terms of the rate of convergence to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; of the NMSE as
&lt;span class=&#34;math inline&#34;&gt;\(n \to +\infty\)&lt;/span&gt;. See also Orlitsky et al. (2016) for a definition of NMSE. In our
paper we have proved that &lt;span class=&#34;math inline&#34;&gt;\(\hat{\tau}_{1}^L\)&lt;/span&gt; provably estimate
&lt;span class=&#34;math inline&#34;&gt;\(\tau_{1}\)&lt;/span&gt; all of the way up to the sampling fraction
&lt;span class=&#34;math inline&#34;&gt;\((\lambda+1)^{-1}\propto (\log n)^{-1}\)&lt;/span&gt; of the population, with
vanishing normalized mean-square error (NMSE) as &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; becomes large.
Then, by relying on recent techniques developed in Wu and Yang (2019) in
the context of nonparametric estimation of the support size of discrete
distributions, we are also able to provide us with a lower bound for the
NMSE of any estimator of the disclosure risk &lt;span class=&#34;math inline&#34;&gt;\(\tau_1\)&lt;/span&gt;. The lower bound
we find has an important implication: without imposing any parametric
assumption on the model, one can estimate &lt;span class=&#34;math inline&#34;&gt;\(\tau_1\)&lt;/span&gt; with vanishing NMSE
all the way up to &lt;span class=&#34;math inline&#34;&gt;\(\lambda \propto \log n\)&lt;/span&gt;. It is then impossible to
determine an estimator having provable guarantees, in terms of vanishing
NMSE, when &lt;span class=&#34;math inline&#34;&gt;\(\lambda = \lambda (n)\)&lt;/span&gt; goes to &lt;span class=&#34;math inline&#34;&gt;\(+\infty\)&lt;/span&gt; much faster than
&lt;span class=&#34;math inline&#34;&gt;\(\log (n)\)&lt;/span&gt;, as a function of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. Moreover it follows that the ``limit
of predictability&#34; of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\tau}_{1}^L\)&lt;/span&gt; in is near optimal, because it
matches (asymptotically) with its maximum possible value
&lt;span class=&#34;math inline&#34;&gt;\(\lambda \propto \log (n)\)&lt;/span&gt;, under suitable choices of the smoothing
distribution &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-09-30-optimal-disclosure-risk-assessment_files/unif.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 1: The normalized mean squared error as a function of the sampling
fraction &lt;span class=&#34;math inline&#34;&gt;\((1+\lambda)^{-1}\)&lt;/span&gt; when the cell’s probabilities are uniform
distributed. Each curve corresponds to a different estimator of
&lt;span class=&#34;math inline&#34;&gt;\(\tau_1\)&lt;/span&gt;: i) the nonparametric estimator with Binomial smoothing
&lt;span class=&#34;math inline&#34;&gt;\(\hat{\tau}_1^{L_b}\)&lt;/span&gt;; ii) the nonparametric estimator with Poisson
smoothing &lt;span class=&#34;math inline&#34;&gt;\(\hat{\tau}_1^{L_p}\)&lt;/span&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The performance of our nonparametric approach is shown in Figure 1.&lt;/p&gt;
&lt;p&gt;In order to do that, we generated a collection of synthetic
tables with &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; cells, where &lt;span class=&#34;math inline&#34;&gt;\(C=3 \cdot 10^6\)&lt;/span&gt;. The population size is
fixed to &lt;span class=&#34;math inline&#34;&gt;\(\bar{n}=10^{6}\)&lt;/span&gt;, and we evaluated the NMSE for different
values of the sample size &lt;span class=&#34;math inline&#34;&gt;\(n=\bar{n}(\lambda+1)^{-1}\)&lt;/span&gt;. The underlying
true cells’ probabilities are generated according to a uniform
distribution over the total number of cells. The figure shows how the
NMSE varies as a function of the sampling fraction &lt;span class=&#34;math inline&#34;&gt;\((1+\lambda)^{-1}\)&lt;/span&gt;,
for the estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat{\tau}_1^L\)&lt;/span&gt;, under a Poisson and a Binomial
smoothing. All the estimates are averaged over 100 iterations. The
sampling fractions considered in our simulation study are above the
limiting threshold &lt;span class=&#34;math inline&#34;&gt;\((\log n)^{-1}\)&lt;/span&gt; and the better performance seems to
be achieved under the Binomial smoothing.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;authors&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Authors&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-09-30-optimal-disclosure-risk-assessment_files/federico.jpg&#34; height=&#34;100&#34; /&gt;
&lt;strong&gt;Federico Camerlenghi&lt;/strong&gt; is an Assistant Professor of Statistics at the
University of Milano-Bicocca (Italy).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-09-30-optimal-disclosure-risk-assessment_files/favaro.jpg&#34; height=&#34;100&#34; /&gt;
&lt;strong&gt;Stefano Favaro&lt;/strong&gt; is a Full Professor of Statistics at the University
of Torino (Italy), and he is also a Carlo Alberto Chair at Collegio
Carlo Alberto (Torino, Italy).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-09-30-optimal-disclosure-risk-assessment_files/zacharie.png&#34; height=&#34;100&#34; /&gt;
&lt;strong&gt;Zacharie Naulet&lt;/strong&gt; is a Maître de Conférence at the Department of
Mathematics of Université Paris-Sud (France).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-09-30-optimal-disclosure-risk-assessment_files/panero.jpeg&#34; height=&#34;100&#34; /&gt;
&lt;strong&gt;Francesca Panero&lt;/strong&gt; is finishing her Ph.D. in Statistics at the
University of Oxford (UK).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p&gt;Good, I.J. and Toulmin, G.H. (1956). The number of new species, and the
increase in population coverage, when a sample is increased.
&lt;em&gt;Biometrika&lt;/em&gt; &lt;strong&gt;43&lt;/strong&gt;, 45–63.&lt;/p&gt;
&lt;p&gt;Orlitsky, A., Suresh, A.T. and Wu, Y. (2017). Optimal prediction of the
number of unseen species. &lt;em&gt;Proc. Natl. Acad. Sci. USA&lt;/em&gt; &lt;strong&gt;113&lt;/strong&gt;,
13283–13288.&lt;/p&gt;
&lt;p&gt;Robbins, H. (1956). An empirical Bayes approach to statistics. &lt;em&gt;Proc.
3rd Berkeley Symp.&lt;/em&gt;,&lt;strong&gt;1&lt;/strong&gt;, 157–163.&lt;/p&gt;
&lt;p&gt;Skinner, C.J. and Elliot, M.J. (2002). A measure of disclosure risk for
microdata. &lt;em&gt;J. Roy. Statist. Soc. B&lt;/em&gt; &lt;strong&gt;64&lt;/strong&gt;, 855–867.&lt;/p&gt;
&lt;p&gt;Skinner, C., Marsh, C., Openshaw, S. and Wymer, C. (1994). Disclosure
control for census microdata. &lt;em&gt;J. Off. Stat.&lt;/em&gt; &lt;strong&gt;10&lt;/strong&gt;, 31–51.&lt;/p&gt;
&lt;p&gt;Skinner, and Shlomo, N. (2008). Assessing identification risk in survey
microdata using log-linear models. &lt;em&gt;J. Amer. Statist. Assoc.&lt;/em&gt; &lt;strong&gt;103&lt;/strong&gt;,
989–1001.&lt;/p&gt;
&lt;p&gt;Wu, Y. and Yang, P. (2019). Chebyshev polynomials, moment matching, and
optimal estimation of the unseen. &lt;em&gt;Ann. Statist.&lt;/em&gt;, &lt;strong&gt;47&lt;/strong&gt;, 857–883.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Depth Quantile Functions</title>
      <link>https://youngstats.github.io/post/2021/07/01/depth-quantile-functions/</link>
      <pubDate>Thu, 01 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/07/01/depth-quantile-functions/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-07-01-depth-quantile-functions_files/f1.png&#34; /&gt;
&lt;em&gt;Figure 1: Depth quantile functions for the wine data (d=13), class 2 vs class 3. Blue curves correspond to between class comparisons, red/pink correspond to within class comparisons.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A common technique in modern statistics is the so-called kernel trick, where data is mapped into a (usually) infinite-dimensional feature space, where various statistical tasks can be carried out. Relatedly, we introduce the &lt;strong&gt;depth quantile function&lt;/strong&gt; (DQF), &lt;span class=&#34;math inline&#34;&gt;\(q_{ij}(\alpha)\)&lt;/span&gt; which similarly maps observations into an infinite dimensional space (the double index will become clear below), though in this case, these new representations of the data are functions of a one-dimensional variable &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; which allows plotting. By construction, described below, these functions encode geometric information about the underlying data set. Consequently, we obtain a tool that permits an interpretable visualization of point cloud geometry regardless of dimension of the data. Additionally, tools from functional data analysis can now be used to solve problems (classification, anomaly detection, etc).&lt;/p&gt;
&lt;p&gt;The primary tool used is that of Tukey’s half space depth (HSD), which provides a higher dimensional analog to the order statistics as a measure of centrality of an observation in a data set (where, for instance, the median is the most central or “deepest” point). In fact, the one dimensional version of HSD (&lt;span class=&#34;math inline&#34;&gt;\(D(x) = \min\{F_n(x), 1-F_n(x)\}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(F_n\)&lt;/span&gt; the EDF) is all we need, as we consider projections of our data onto lines before computing centrality, see below.&lt;/p&gt;
&lt;p&gt;It is known that for HSD in any dimension, the level sets &lt;span class=&#34;math inline&#34;&gt;\(\{x:D(x)\geq\lambda\}\)&lt;/span&gt; are necessarily convex, thus not conforming to the shape of the underlying density. Additionally, in high dimensions, it’s likely that most points live near the boundary of the point cloud (the convex hull), i.e. we expect almost all points to be “non-central”. To get around this second problem, we instead consider, for every pair of points (&lt;span class=&#34;math inline&#34;&gt;\(x_i, x_j\)&lt;/span&gt;), the midpoint &lt;span class=&#34;math inline&#34;&gt;\(m_{ij} = \frac{x_i + x_j}{2}\)&lt;/span&gt; as the base point in the construction of our feature functions &lt;span class=&#34;math inline&#34;&gt;\(q_{ij}(\alpha)\)&lt;/span&gt;. Thus, we construct a &lt;em&gt;matrix&lt;/em&gt; of feature functions, with each observation corresponding to &lt;span class=&#34;math inline&#34;&gt;\(n-1\)&lt;/span&gt; feature functions, one for every other observation in the data set (though current work uses only the average of these functions or appropriate subsets of them). The choice of the midpoint as base point is motivated as follows. Consider a 2-class modal classification problem, where each class is represented by a component of a mixture of two normal distributions for which the corresponding cluster centers (the means) are sufficiently separated. When considering a pair of observations from different classes, their midpoint is likely to live in a region between the two point clouds with few nearby observations, in other words, a low density region with a high measure of centrality. The opposite can be expected for within class comparisons, i.e. two observations from the same class.&lt;/p&gt;
&lt;p&gt;To addresses the convexity issue alluded to above, we use “local” versions of the HSD. This is done by taking random subsets of the data space containing &lt;span class=&#34;math inline&#34;&gt;\(m_{ij}\)&lt;/span&gt; and computing the HSD for this point after projection of the subset onto the line &lt;span class=&#34;math inline&#34;&gt;\(\ell_{ij}\)&lt;/span&gt; that is determined by the two points &lt;span class=&#34;math inline&#34;&gt;\(x_i,x_j\)&lt;/span&gt;. Specifically, the subsets are given by the data residing in randomly selected spherical cones of a fixed angle (which is a tuning parameter) with axis of symmetry &lt;span class=&#34;math inline&#34;&gt;\(\ell_{ij}\)&lt;/span&gt; (see figure 2.) We define a distribution of cone tips along &lt;span class=&#34;math inline&#34;&gt;\(\ell_{ij}\)&lt;/span&gt;, which induces a distribution of “local” depths (HSD), and define the DQF as the corresponding quantile function of this distribution. Using directions determined by pairs of points (i.e. the lines &lt;span class=&#34;math inline&#34;&gt;\(\ell_{ij}\)&lt;/span&gt;) addresses a challenge with high dimensional data: which direction should one look to capture interesting feature of the data? It also results in this method being automatically adaptive to sparseness (data living in a lower dimensional subspace of our data space).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-07-01-depth-quantile-functions_files/f2.png&#34; /&gt;
&lt;em&gt;Figure 2: A local depth for midpoint in red. Depth value will be 2 for this cone tip.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As a result of this construction, we end up with a multi-scale method, a function defined on [0,1], that is non-degenerate at both boundaries (in contrast to most multi-scale methods). One can show that the derivative of &lt;span class=&#34;math inline&#34;&gt;\(q_{ij}(\alpha)\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(\alpha\to 0\)&lt;/span&gt; yields information about the density at &lt;span class=&#34;math inline&#34;&gt;\(m_{ij}\)&lt;/span&gt;, while &lt;span class=&#34;math inline&#34;&gt;\(q_{ij}(1)\)&lt;/span&gt; is related to its centrality in the entire point cloud. The manner in which &lt;span class=&#34;math inline&#34;&gt;\(q_{ij}\)&lt;/span&gt; grows with increasing &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, while less interpretable, yields valuable information about the observations it corresponds to.&lt;/p&gt;
&lt;p&gt;As an example of how this information might be used, we again consider the 2-class classification problem. Each observation is described by two functions, the average function &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{|C_1|}\sum_{j\in C_1}q_{ij}(\alpha)\)&lt;/span&gt; for comparisons with class 1 (&lt;span class=&#34;math inline&#34;&gt;\(C_1\)&lt;/span&gt;), and similarly the average function for comparisons with class 2. In line with the heuristic laid out above, it can be seen in figure 1 that for an observation from class 1, comparisons with class two tends to yield functions that have low density (so are slow to grow for small quantile levels) and large value for &lt;span class=&#34;math inline&#34;&gt;\(\alpha=1\)&lt;/span&gt; corresponding to high centrality. In-class comparisons have the opposite properties. A simple heuristic for solving this classification problem might be to use the first few loadings from an fPCA and your favorite classifier for Euclidean data. Results on several data sets using an untuned SVM were competitive with existing methods with extensive tuning.&lt;/p&gt;
&lt;p&gt;Finally, the construction only depends on the data via inner products, meaning that DQFs can be constructing on any data type for which a kernel is defined, for instance persistence diagrams in topological data analysis, allowing for a visualization of non-Euclidean data in addition to high (including infinite) dimensional Euclidean data.&lt;/p&gt;
&lt;p&gt;Reference: Chandler, G. and Polonik, W. “Multiscale geometric feature extraction for high-dimensional and non-Euclidean data with applications.” Ann. Statist. 49 (2) 988 - 1010, April 2021. (&lt;a href=&#34;https://arxiv.org/abs/1811.10178&#34; class=&#34;uri&#34;&gt;https://arxiv.org/abs/1811.10178&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Concentration Inequalities in Machine Learning</title>
      <link>https://youngstats.github.io/post/2021/06/30/concentration-inequalities-in-machine-learning/</link>
      <pubDate>Wed, 30 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/06/30/concentration-inequalities-in-machine-learning/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;The fifth “One World webinar” organized by YoungStatS will take place on September 15th, 2021. Selected young European researchers active in the areas of probability and machine learning will present their recent contributions. The webinar is joint cooperation between the Young Researchers Committee of the Bernoulli Society and the YoungStatS project.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wednesday, September 15th, 17:00 CEST&lt;/li&gt;
&lt;li&gt;Online, via Zoom. The registration form is available &lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSe3S-SK35qjIG2bNUs1lBNQsq1uXWBYkO3VMqpvxywM9LdXpQ/viewform?vc=0&amp;amp;c=0&amp;amp;w=1&amp;amp;flr=0&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://amarchina.perso.math.cnrs.fr/index_en.html&#34;&gt;Antoine Marchina&lt;/a&gt; (Université de Paris): »Concentration inequalities for suprema of unbounded empirical processes« (Based on the &lt;a href=&#34;https://hal.archives-ouvertes.fr/hal-01545101/&#34;&gt;preprint&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.google.com/view/geoffreychinot&#34;&gt;Geoffrey Chinot&lt;/a&gt; (ETH Zurich): »Adaboost and robust one-bit compressed sensing« (based on the &lt;a href=&#34;https://arxiv.org/pdf/2105.02083.pdf&#34;&gt;preprint&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prof. Gábor Lugosi, ICREA Research Professor at Pompeu Fabra University and Barcelona GSE Research Professor, Blackwell Lecture speaker at the Bernoulli-IMS 2021 World Congress in Probability and Statistics&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The webinar is part of YoungStatS project of the Young Statisticians Europe initiative (FENStatS) supported by the Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics (IMS).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-06-30-concentration-inequality-in-machine-learning_files/webinar1.jpg&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Optional stopping with Bayes factors: possibilities and limitations</title>
      <link>https://youngstats.github.io/post/2021/06/10/optional-stopping-with-bayes-factors-possibilities-and-limitations/</link>
      <pubDate>Thu, 10 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/06/10/optional-stopping-with-bayes-factors-possibilities-and-limitations/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In recent years, a surprising number of scientific results have failed
to hold up to continued scrutiny. Part of this ‘replicability crisis’
may be caused by practices that ignore the assumptions of traditional
(frequentist) statistical methods &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-john-2012-measur-preval&#34; role=&#34;doc-biblioref&#34;&gt;John, Loewenstein, and Prelec 2012&lt;/a&gt;)&lt;/span&gt;. One of
these assumptions is that the experimental protocol should be completely
determined upfront. In practice, researchers often adjust the protocol
due to unforeseen circumstances or collect data until a point has been
proven. This practice, which is referred to as &lt;em&gt;optional stopping&lt;/em&gt;, can
cause true hypotheses to be wrongly rejected much more often than these
statistical methods promise.&lt;br /&gt;
Bayes factor hypothesis testing has long been advocated as an
alternative to traditional testing that can resolve several of its
problems; in particular, it was claimed early on that Bayesian methods
continue to be valid under optional stopping
&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-lindley-1957-statis-parad&#34; role=&#34;doc-biblioref&#34;&gt;Lindley 1957&lt;/a&gt;; &lt;a href=&#34;#ref-RaiffaS61&#34; role=&#34;doc-biblioref&#34;&gt;Raiffa and Schlaifer 1961&lt;/a&gt;; &lt;a href=&#34;#ref-edwards-1963-bayes-statis&#34; role=&#34;doc-biblioref&#34;&gt;Edwards, Lindman, and Savage 1963&lt;/a&gt;)&lt;/span&gt;. In
light of the replicability crisis, such claims have received much
renewed interest
&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-wagenmakers-2007-pract-solut&#34; role=&#34;doc-biblioref&#34;&gt;Wagenmakers 2007&lt;/a&gt;; &lt;a href=&#34;#ref-rouder-2014-option&#34; role=&#34;doc-biblioref&#34;&gt;Jeffrey N. Rouder 2014&lt;/a&gt;; &lt;a href=&#34;#ref-schonbrodt-2017-sequen-hypot&#34; role=&#34;doc-biblioref&#34;&gt;Schönbrodt et al. 2017&lt;/a&gt;; &lt;a href=&#34;#ref-yu-2013-when-decis&#34; role=&#34;doc-biblioref&#34;&gt;Yu et al. 2014&lt;/a&gt;; &lt;a href=&#34;#ref-sanborn-2013-frequen-implic&#34; role=&#34;doc-biblioref&#34;&gt;Sanborn and Hills 2014&lt;/a&gt;)&lt;/span&gt;.
But what do they mean mathematically? It turns out that different
authors mean quite different things by ‘Bayesian methods handle optional
stopping’; moreover, such claims are often shown to hold only in an
informal sense, or in restricted contexts. In the paper
&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hendriksen2020optional&#34; role=&#34;doc-biblioref&#34;&gt;Hendriksen, Heide, and Grünwald 2020&lt;/a&gt;)&lt;/span&gt; we give a systematic overview and
formalization of such claims, and explain their relevance for practice:
can we effectively rely on Bayes factor testing to do a good job under
optional stopping or not? As we shall see, the answer is subtle.
Secondly, we extend the reach of such claims to more general settings,
for which they have never been formally verified and for which
verification is not always trivial. In the paper &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-heide2020optional&#34; role=&#34;doc-biblioref&#34;&gt; Heide and Grünwald 2020&lt;/a&gt;)&lt;/span&gt;,
we explain claims about optional stopping for an audience of
methodologists and applied statisticians with the help of computer
simulations.&lt;br /&gt;
&lt;/p&gt;
&lt;div id=&#34;bayesian-inference&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;Bayesian inference&lt;/h1&gt;
&lt;p&gt;Bayesianism is about a certain interpretation of the concept
probability: as &lt;em&gt;degrees of belief&lt;/em&gt;. A Bayesian first expresses this
belief as a probability function. We call this the prior distribution,
and we denote it by &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{P}(\theta)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is the
parameter (or several parameters) of the model. After the specification
of the prior, we obtain the data &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; and the likelihood
&lt;span class=&#34;math inline&#34;&gt;\(\mathbb{P}(D | \theta)\)&lt;/span&gt;. Now we can compute the &lt;em&gt;posterior
distribution&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{P}(\theta | D)\)&lt;/span&gt; with the help of &lt;em&gt;Bayes’
theorem&lt;/em&gt;: &lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\mathbb{P}(\theta | D) = \frac{\mathbb{P}(D | \theta) \mathbb{P}(\theta)}{\mathbb{P}(D)}.\end{aligned}\]&lt;/span&gt;
Suppose we want to test a null hypothesis &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_0\)&lt;/span&gt; against an
alternative hypothesis &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_1\)&lt;/span&gt;. We can do this in a Bayesian way
with &lt;em&gt;Bayes factors&lt;/em&gt;: we start with the &lt;em&gt;prior odds&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\mathbb{P}(\mathcal{H}_1) / \mathbb{P}(\mathcal{H}_0)\)&lt;/span&gt;, our belief
before seeing the data. Often we believe that both hypotheses are
equally probable, then our prior odds are 1-to-1. Next we gather our
data &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;, and update our odds with the new knowledge, using Bayes’
theorem: &lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\text{posterior odds}(\mathcal{H}_1 \text{ vs. } \mathcal{H}_0) = \frac{\mathbb{P}(\mathcal{H}_1 | D)}{\mathbb{P}(\mathcal{H}_0 | D)} = \frac{\mathbb{P}(\mathcal{H}_1)}{\mathbb{P}(\mathcal{H}_0)} \frac{\mathbb{P}(D | \mathcal{H}_1)}{\mathbb{P}(D | \mathcal{H}_0)}.\end{aligned}\]&lt;/span&gt;
The posterior odds is our updated belief about which hypothesis is more
likely.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;three-notions-of-optional-stopping&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;Three notions of optional stopping&lt;/h1&gt;
&lt;p&gt;Validity under optional stopping is a desirable property of hypothesis
testing: we gather some data, look at the results, and decide whether we
stop of gather some additional data. Informally, we call ‘peeking at the
results to decide whether to collect more data’ &lt;em&gt;optional stopping&lt;/em&gt;, but
if we want to make more precise what it means if we say that a test can
handle optional stopping, it turns out that different approaches
(frequentist, subjective Bayesian and objective Bayesian) lead to
different interpretations and definitions. It tuns out that we can
discern three main mathematical concepts of handling optional stopping,
which we identify and formally define in the paper
&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hendriksen2020optional&#34; role=&#34;doc-biblioref&#34;&gt;Hendriksen, Heide, and Grünwald 2020&lt;/a&gt;)&lt;/span&gt;.&lt;br /&gt;
The first concept we call &lt;em&gt;subjective Bayesian optional stopping&lt;/em&gt; or
&lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;-independence. If one considers a purely subjective Bayesian
setting, appropriate if one truly believes one’s prior, then Bayesian
updating from prior to posterior is not affected by the employed
stopping rule: one ends up with the same posterior if one had decided
the sample size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; in advance, or if it had been determined, for
example, because one was satisfied with the result at this &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. In this
sense a subjective Bayesian procedure does not depend on the stopping
rule.&lt;br /&gt;
The second sense of optional stopping we call &lt;em&gt;calibration&lt;/em&gt;. As
&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-rouder-2014-option&#34; role=&#34;doc-biblioref&#34;&gt;Jeffrey N. Rouder 2014&lt;/a&gt;)&lt;/span&gt; writes: ‘If a replicate experiment yielded a
posterior odds of 3.5-to-1 in favor of the null, then we expect that the
null was 3.5 times as probable as the alternative to have produced the
data.’ In more mathematical language, this can be expressed as
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\text{post-odds} (\mathcal{H}_1 \text{ vs. } \mathcal{H}_0 | ``\text{post-odds} (\mathcal{H}_1 \text{ vs. } \mathcal{H}_0 | D ) = a&amp;quot;) = a.\end{aligned}\]&lt;/span&gt;
We say this equation expresses &lt;em&gt;calibration of the posterior odds&lt;/em&gt;. It
turns out that this calibration fails to hold if one does not adhere to
a purely subjective Bayesian view, in particular, it does not hold for
the &lt;em&gt;default&lt;/em&gt; priors the Bayesian psychology community is advocating
&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-wagenmakers-2007-pract-solut&#34; role=&#34;doc-biblioref&#34;&gt;Wagenmakers 2007&lt;/a&gt;; &lt;a href=&#34;#ref-rouder-2012-default-bayes&#34; role=&#34;doc-biblioref&#34;&gt;J. N. Rouder et al. 2012&lt;/a&gt;)&lt;/span&gt;. To get a
first idea of one of the issues: default priors sometimes depend on the
data. Then it is unclear what &lt;em&gt;optional stopping&lt;/em&gt; really means, because
if, using prior &lt;span class=&#34;math inline&#34;&gt;\(P_1(\theta)\)&lt;/span&gt; based on a sample of size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, one had
stopped at sample size &lt;span class=&#34;math inline&#34;&gt;\(n&amp;#39;&amp;lt; n\)&lt;/span&gt;, one should have really used prior
&lt;span class=&#34;math inline&#34;&gt;\(P&amp;#39;_1(\theta)\)&lt;/span&gt; based on sample of size &lt;span class=&#34;math inline&#34;&gt;\(n&amp;#39;\)&lt;/span&gt;...but then one would have
stopped at yet another sample size &lt;span class=&#34;math inline&#34;&gt;\(n&amp;#39;&amp;#39;\)&lt;/span&gt;, and so on. See our paper
&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-heide2020optional&#34; role=&#34;doc-biblioref&#34;&gt; Heide and Grünwald 2020&lt;/a&gt;)&lt;/span&gt; for an extensive discussion and many examples.&lt;br /&gt;
&lt;/p&gt;
&lt;p&gt;The third sense is a frequentist interpretation of handling optional
stopping, which is about controlling the Type I error of an experiment.
A Type I error occurs when we reject the null hypothesis when it is
true, also called &lt;em&gt;false positive&lt;/em&gt;. The frequentist interpretation of
handling optional stopping is that the Type I error guarantee holds if
we do not determine the sampling plan — and thus the stopping rule —
in advance, but we may stop when we see significant results. In the case
&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_0\)&lt;/span&gt; is &lt;em&gt;simple&lt;/em&gt; (containing just one hypothesis), there is a
well-known intriguing connection between Bayes factors and Type I error
probabilities: if we reject &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_0\)&lt;/span&gt; iff the posterior odds in
favor of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_0\)&lt;/span&gt; are smaller than some fixed level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;,
then we are guaranteed a Type I error of at most &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. And
interestingly, this holds not just for fixed sample sizes but even under
optional stopping. However, for &lt;em&gt;composite&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_0\)&lt;/span&gt; this does
not continue to hold. Except for the special case where &lt;em&gt;all&lt;/em&gt; free
parameters in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_0\)&lt;/span&gt; are nuisance parameters observing a group
structure and equipped with the corresponding right-Haar prior, and are
shared with &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_1\)&lt;/span&gt;, as we prove in &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hendriksen2020optional&#34; role=&#34;doc-biblioref&#34;&gt;Hendriksen, Heide, and Grünwald 2020&lt;/a&gt;)&lt;/span&gt;.
But for general priors and composite &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_0\)&lt;/span&gt;, this is typically
not the case.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-06-09-optional-stopping-with-bayes-factors-possibilities-and-limitations_files/screenshot.138.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 1: Posterior odds in an experiment of testing whether the mean of a normal distribution is 0
(&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_0\)&lt;/span&gt;), versus non-zero (&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_1\)&lt;/span&gt;), from 20; 000 replicate experiments. (a) The empirical sampling distribution
of the posterior odds as a histogram under &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_0\)&lt;/span&gt; (blue) and &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_1\)&lt;/span&gt; (pink). (b) Calibration plot: the observed
posterior odds as a function of the nominal posterior odds.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;One can give three distinct mathematical meanings to the notion of
&lt;em&gt;optional stopping&lt;/em&gt;. Whether or not we can say that ‘the Bayes factor
method can handle optional stopping’ in practice is a subtle matter,
depending on the specifics of the given situation: what models are used,
what priors, and what is the goal of the analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;authors&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Authors&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-06-09-optional-stopping-with-bayes-factors-possibilities-and-limitations_files/Allard.jpg&#34; height=&#34;70&#34; /&gt;
Allard Hendriksen, CWI Amsterdam&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-06-09-optional-stopping-with-bayes-factors-possibilities-and-limitations_files/Rianne.jpg&#34; height=&#34;70&#34; /&gt;
dr. Rianne de Heide, Leiden University &amp;amp; CWI Amsterdam&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-06-09-optional-stopping-with-bayes-factors-possibilities-and-limitations_files/Peter.jpg&#34; height=&#34;70&#34; /&gt;
prof. Peter Grünwald, Leiden University &amp;amp; CWI Amsterdam&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-edwards-1963-bayes-statis&#34; class=&#34;csl-entry&#34;&gt;
Edwards, Ward, Harold Lindman, and Leonard J. Savage. 1963. &lt;span&gt;“Bayesian Statistical Inference for Psychological Research.”&lt;/span&gt; &lt;em&gt;Psychological Review&lt;/em&gt; 70 (3): 193–242. &lt;a href=&#34;https://doi.org/10.1037/h0044139&#34;&gt;https://doi.org/10.1037/h0044139&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-heide2020optional&#34; class=&#34;csl-entry&#34;&gt;
 Heide, Rianne, and Peter D. Grünwald. 2020. &lt;span&gt;“Why Optional Stopping Can Be a Problem for Bayesians.”&lt;/span&gt; &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, Advance Publication&lt;/em&gt;, 1–18.
&lt;/div&gt;
&lt;div id=&#34;ref-hendriksen2020optional&#34; class=&#34;csl-entry&#34;&gt;
Hendriksen, Allard, Rianne Heide, and Peter Grünwald. 2020. &lt;span&gt;“Optional Stopping with Bayes Factors: A Categorization and Extension of Folklore Results, with an Application to Invariant Situations.”&lt;/span&gt; &lt;em&gt;Bayesian Analysis, Advance Publication&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-john-2012-measur-preval&#34; class=&#34;csl-entry&#34;&gt;
John, Leslie K, George Loewenstein, and Drazen Prelec. 2012. &lt;span&gt;“Measuring the Prevalence of Questionable Research Practices with Incentives for Truth Telling.”&lt;/span&gt; &lt;em&gt;Psychological Science&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-lindley-1957-statis-parad&#34; class=&#34;csl-entry&#34;&gt;
Lindley, D. V. 1957. &lt;span&gt;“A Statistical Paradox.”&lt;/span&gt; &lt;em&gt;Biometrika&lt;/em&gt; 44 (1/2): 187–92. &lt;a href=&#34;https://doi.org/10.2307/2333251&#34;&gt;https://doi.org/10.2307/2333251&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-RaiffaS61&#34; class=&#34;csl-entry&#34;&gt;
Raiffa, H., and R. Schlaifer. 1961. &lt;em&gt;Applied Statistical Decision Theory&lt;/em&gt;. Cambridge, MA: Harvard University Press.
&lt;/div&gt;
&lt;div id=&#34;ref-rouder-2012-default-bayes&#34; class=&#34;csl-entry&#34;&gt;
Rouder, J N, R D Morey, P L Speckman, and J M Province. 2012. &lt;span&gt;“Default &lt;span&gt;B&lt;/span&gt;ayes Factors for &lt;span&gt;ANOVA&lt;/span&gt; Designs.”&lt;/span&gt; &lt;em&gt;Journal of Mathematical Psychology&lt;/em&gt; 56 (5): 356–74.
&lt;/div&gt;
&lt;div id=&#34;ref-rouder-2014-option&#34; class=&#34;csl-entry&#34;&gt;
Rouder, Jeffrey N. 2014. &lt;span&gt;“Optional Stopping: No Problem for &lt;span&gt;B&lt;/span&gt;ayesians.”&lt;/span&gt; &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt; 21 (2): 301–8. &lt;a href=&#34;https://doi.org/10.3758/s13423-014-0595-4&#34;&gt;https://doi.org/10.3758/s13423-014-0595-4&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-sanborn-2013-frequen-implic&#34; class=&#34;csl-entry&#34;&gt;
Sanborn, Adam N., and Thomas T. Hills. 2014. &lt;span&gt;“The Frequentist Implications of Optional Stopping on &lt;span&gt;B&lt;/span&gt;ayesian Hypothesis Tests.”&lt;/span&gt; &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt; 21 (2): 283–300. &lt;a href=&#34;https://doi.org/10.3758/s13423-013-0518-9&#34;&gt;https://doi.org/10.3758/s13423-013-0518-9&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-schonbrodt-2017-sequen-hypot&#34; class=&#34;csl-entry&#34;&gt;
Schönbrodt, Felix D., Eric-Jan Wagenmakers, Michael Zehetleitner, and Marco Perugini. 2017. &lt;span&gt;“Sequential Hypothesis Testing with &lt;span&gt;B&lt;/span&gt;ayes Factors: Efficiently Testing Mean Differences.”&lt;/span&gt; &lt;em&gt;Psychological Methods&lt;/em&gt; 22 (2): 322–39. &lt;a href=&#34;https://doi.org/10.1037/met0000061&#34;&gt;https://doi.org/10.1037/met0000061&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-wagenmakers-2007-pract-solut&#34; class=&#34;csl-entry&#34;&gt;
Wagenmakers, Eric-Jan. 2007. &lt;span&gt;“A Practical Solution to the Pervasive Problems of p Values.”&lt;/span&gt; &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt; 14 (5): 779–804. &lt;a href=&#34;https://doi.org/10.3758/bf03194105&#34;&gt;https://doi.org/10.3758/bf03194105&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-yu-2013-when-decis&#34; class=&#34;csl-entry&#34;&gt;
Yu, Erica C., Amber M. Sprenger, Rick P. Thomas, and Michael R. Dougherty. 2014. &lt;span&gt;“When Decision Heuristics and Science Collide.”&lt;/span&gt; &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt; 21 (2): 268–82. &lt;a href=&#34;https://doi.org/10.3758/s13423-013-0495-z&#34;&gt;https://doi.org/10.3758/s13423-013-0495-z&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Spatiotemporal modeling and real-time prediction of origin-destination traffic demand</title>
      <link>https://youngstats.github.io/post/2021/06/09/spatiotemporal-modeling-and-real-time-prediction-of-origin/</link>
      <pubDate>Wed, 09 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/06/09/spatiotemporal-modeling-and-real-time-prediction-of-origin/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the past decades, intelligent transportation system (ITS) has brought advanced technology that enables a data-rich environment and unprecedented opportunities for traffic prediction, which is considered as one of the most prevalent issues facing ITS (Li et al., 2015). We discuss the online prediction of the origin-destination (OD) demand count in traffic networks, which represents the number of trips between certain combinations of an origin and a destination. The study of OD demand prediction based on count data has a growing impact on many traffic control and management policies (Ashok, 1996, Ashok and Ben-Akiva, 2002, Li, 2005, Hazelton, 2008, Shao et al., 2014). For example, dynamic OD demand prediction is critical in planning for the charging services of the electrical vehicles (EV; Zhang et al., 2017). A well-designed charging facility network is necessary to extend the vehicle range and popularize the use of EVs. In particular, the dynamic demand between nodes of the traffic network plays a key role in determining the availability of the charging facilities, planning the multi-period charging schedules, and meeting the customer needs at the maximum extent (Zhang et al., 2017, Brandstatter et al., 2017). The objective of this study is to appropriately model the stochastic OD traffic demand counts considering the spatiotemporal correlations between different routes and epochs, while incorporating physical knowledge of the traffic network in the estimation. The estimation results are expected to enhance the prediction accuracy and robustness of the online traffic demand prediction for future epochs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Model and method&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We investigate a multivariate Poisson log-normal model with a
block-diagonal covariance matrix and incorporate domain knowledge of the
traffic network features to account for spatial correlations. Let
&lt;span class=&#34;math inline&#34;&gt;\(N_{\text{ijt}}\)&lt;/span&gt; denote the observed traffic demand (i.e., the count of
vehicles) for route &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; on day &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, at epoch &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. Based on the natural
characteristics of the demand counts, it is reasonable to model each
observation &lt;span class=&#34;math inline&#34;&gt;\(N_{\text{ijt}}\)&lt;/span&gt; with a Poisson log-linear model (Perrakis
et al., 2014, Xian et al. 2018) such that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[N_{\text{ijt}}\sim\text{Poisson}\left( \lambda_{\text{ijt}} \right),u_{\text{ijt}} = \log\lambda_{\text{ijt}}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{\text{ijt}}\)&lt;/span&gt; is the intensity of the Poisson process, and
&lt;span class=&#34;math inline&#34;&gt;\(u_{\text{ijt}}\)&lt;/span&gt; is the log transformation of the intensity. To
characterize the spatiotemporal correlations across different routes and
time points, we model &lt;span class=&#34;math inline&#34;&gt;\(u_{\text{ijt}}\)&lt;/span&gt; as a mixed-effect Gaussian
process based on &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; basis functions &lt;span class=&#34;math inline&#34;&gt;\(B_{k}(t)\)&lt;/span&gt; that&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math display&#34;&gt;\[u_{\text{ijt}} = \mu_{\text{jt}} + \sum_{k = 1}^{K}{\gamma_{\text{jk}}B_{k}(t)} + Z_{\text{ijt}}.\]&lt;/span&gt; (1)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\mu =}\left\lbrack \mu_{11},\mu_{12},\ \cdots,\ \mu_{\text{JT}} \right\rbrack\mathbf{&amp;#39;}\)&lt;/span&gt;
is the fixed effect coefficient that models the common characteristics
of the whole traffic network, and
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\gamma}_{k}\mathbf{=}\left\lbrack \gamma_{1k},\ \gamma_{2k},\ \cdots,\ \gamma_{\text{Jk}} \right\rbrack\)&lt;/span&gt;
is the random effect coefficient with prior distribution
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\gamma}_{k}\sim N(0,\ \mathbf{R}_{\theta_{y}})\)&lt;/span&gt; that
characterizes the uniqueness of different routes. Here
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{R}_{\theta_{y}}\)&lt;/span&gt; is the correlation matrix which takes into
consideration of the traffic network information, where
&lt;span class=&#34;math inline&#34;&gt;\(\left\lbrack \mathbf{R}_{\theta_{y}} \right\rbrack_{j_{1},j_{2}} = \sigma_{j_{1},\ j_{2}}\exp\left\{ - \theta_{y}\left| \mathbf{y}_{j_{1}} - \mathbf{y}_{j_{2}} \right|^{2} \right\}\)&lt;/span&gt;.
In this expression, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{y}_{j}\)&lt;/span&gt; denotes the unique features of
route &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, such as information about the origin and destination, the
maximum speed limit on a route, and the travel distance. The term
&lt;span class=&#34;math inline&#34;&gt;\(Z_{\text{ijt}}\)&lt;/span&gt; in model (1) is the random error that follows a
Gaussian distribution which has the covariance structure&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{cov}\left( Z_{ij_{1}t_{1}},Z_{ij_{2}t_{2}} \right) = \sigma_{j_{1},\ j_{2}}\exp\left\{ - \theta_{y}\left| \mathbf{y}_{j_{1}} - \mathbf{y}_{j_{2}} \right|^{2} \right\} \cdot \tau^{2}\exp\left\{ - \theta_{t}\left| t_{1} - t_{2} \right| \right\}.\]&lt;/span&gt; (2)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Which depends on both the features of routes &lt;span class=&#34;math inline&#34;&gt;\(j_{1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j_{2}\)&lt;/span&gt;, and
the time points &lt;span class=&#34;math inline&#34;&gt;\(t_{1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(t_{2}\)&lt;/span&gt;, which we refer to as the spatial
and temporal covariance structures, respectively.&lt;/p&gt;
&lt;p&gt;Denote the log-transformed intensity of the OD traffic demand on day &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;
as
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{u}_{i} = \left( u_{i11},u_{i12},\ldots,u_{\text{iJT}} \right)^{&amp;#39;}\)&lt;/span&gt;.
We can further derive that conditioning on parameters
&lt;span class=&#34;math inline&#34;&gt;\(\left( \mathbf{\mu},\ \theta_{y},\theta_{t},\mathbf{\sigma,\ }\tau^{2} \right)\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{u}_{i}\)&lt;/span&gt; follows normal distribution
&lt;span class=&#34;math inline&#34;&gt;\(N\left( \mathbf{\mu},\ \mathbf{\Sigma} \right)\)&lt;/span&gt;, where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathbf{\mu} = \left( \mu_{11},\mu_{12},\ \cdots,\ \mu_{\text{JT}} \right)^{&amp;#39;},\]&lt;/span&gt;&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathbf{\Sigma} = \mathbf{R}_{\theta_{y}}\bigotimes\left\lbrack \mathbf{R}_{B} + \tau^{2}\mathbf{R}_{\theta_{t}} \right\rbrack.\]&lt;/span&gt; (3)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here, the symbol &lt;span class=&#34;math inline&#34;&gt;\(\bigotimes\)&lt;/span&gt; denotes the Kronecker product,
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{R}_{B}\)&lt;/span&gt; is a fixed &lt;span class=&#34;math inline&#34;&gt;\(T \times T\)&lt;/span&gt; matrix with the &lt;span class=&#34;math inline&#34;&gt;\((t_{1},t_{2})\)&lt;/span&gt;
element equal to &lt;span class=&#34;math inline&#34;&gt;\(\sum_{k = 1}^{K}{B_{k}(t_{1})B_{k}(t_{2})}\)&lt;/span&gt;, and
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{R}_{\theta_{t}}\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(T \times T\)&lt;/span&gt; matrix with the
&lt;span class=&#34;math inline&#34;&gt;\((t_{1},t_{2})\)&lt;/span&gt; element equal to
&lt;span class=&#34;math inline&#34;&gt;\(\exp\left\{ - \theta_{t}|t_{1} - t_{2}| \right\}\)&lt;/span&gt;. Therefore, the large
covariance matrix is parametrized based on only the parameters
&lt;span class=&#34;math inline&#34;&gt;\(\theta_{y},\theta_{t},\mathbf{\sigma}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\tau^{2}\)&lt;/span&gt;. This
parsimonious model has several advantages, such as high interpretability
tailored to the traffic demand count data, increased stability of the
estimation results, and reduced computational burden for parameter
estimation. We treat &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{u}\)&lt;/span&gt; as a latent variable and further
employ the EM algorithm to obtain the maximum likelihood estimation
(MLE) for the parameters.&lt;/p&gt;
&lt;p&gt;In this way, we can fully explore the complicated spatiotemporal
correlation structure of the traffic network demand and automatically
cluster the routes with high correlations, without introducing a large
number of parameters that impact the estimation accuracy. Besides
transportation systems, the proposed method can be easily extended to
other network applications with count data through few modifications,
such as communication systems, supply chain management, smart grid, or
even three-dimensional networks (Wang et al., 2018).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case study&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We apply the proposed method to a real New York yellow taxi dataset
which is collected from June 1&lt;sup&gt;st&lt;/sup&gt; to July 31&lt;sup&gt;st&lt;/sup&gt; in 2017 (NYC taxi,
2017). The dataset records all yellow taxi trips during the
aforementioned time period including the pick-up and drop-off dates and
times, pick-up and drop-off locations, trip distances, and payment
information about the trips. We focus on the trips between the four
busiest zones in Manhattan and investigate the structure of the travel
demand counts on these zones as OD pairs. The details of the four taxi
zones are shown in Figure 1.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-06-10-spatiotemporal-modeling-and-real-time-prediction-of-origin_files/f1.jpg&#34; /&gt;&lt;/p&gt;
&lt;table style=&#34;width:97%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;14%&#34; /&gt;
&lt;col width=&#34;24%&#34; /&gt;
&lt;col width=&#34;58%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Index&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Borough&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Zones&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Manhattan&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Lincoln Square East&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Manhattan&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Times Square/Theatre District&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Manhattan&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Upper East Side North&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Manhattan&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Upper East Side South&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;Figure 1. Illustration of the taxi zones in the case study&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Figure 2 further shows the specific taxi demand prediction results of
two routes &lt;span class=&#34;math inline&#34;&gt;\((4,\ 3)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\((4,\ 4)\)&lt;/span&gt; for four test days. The solid black
line in this figure represents the true dynamic traffic demand counts,
where it can be observed that the true taxi demand indeed exhibits high
spatial and temporal variation and strong correlations for observations
between the routes and across different epochs. The solid red line in is
the predicted demand using the proposed method, and the dashed error
bars show the 90% confidence interval of the prediction based on the
variance derivation in equation (7), which significantly outperforms the
existing method shown in black dotted line.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-06-10-spatiotemporal-modeling-and-real-time-prediction-of-origin_files/f2.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 2. Taxi demand prediction results for routes &lt;span class=&#34;math inline&#34;&gt;\((4,\ 3)\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\((4,\ 4)\)&lt;/span&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;This post is based on&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Xian, X., Ye, H., Wang, X., and Liu, K. (2021). Spatiotemporal modeling and real-time prediction of origin-destination traffic demand. &lt;em&gt;Technometrics&lt;/em&gt;, 63(1), 77-89.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Li, L., Su, X., Zhang, Y., Lin, Y., and Li, Z. (2015). Trend modeling for traffic time series analysis: An integrated study. &lt;em&gt;IEEE Transactions on Intelligent Transportation Systems&lt;/em&gt;, 16(6), 3430-3439.&lt;/li&gt;
&lt;li&gt;Ashok, K. (1996). Estimation and prediction of time-dependent origin-destination flows (Doctoral dissertation, Massachusetts Institute of Technology).&lt;/li&gt;
&lt;li&gt;Ashok, K., and Ben-Akiva, M. E. (2002). Estimation and prediction of time-dependent origin-destination flows with a stochastic mapping to path flows and link flows. &lt;em&gt;Transportation Science&lt;/em&gt;, 36(2), 184-198.&lt;/li&gt;
&lt;li&gt;Brandstatter, G., Kahr, M., and Leitner, M. (2017). Determining optimal locations for charging stations of electric car-sharing systems under stochastic demand. &lt;em&gt;Transportation Research Part B: Methodological&lt;/em&gt;, 104, 17-35.&lt;/li&gt;
&lt;li&gt;Li, B. (2005). Bayesian inference for origin-destination matrices of transport networks using the EM algorithm. &lt;em&gt;Technometrics&lt;/em&gt;, 47(4), 399-408.&lt;/li&gt;
&lt;li&gt;Hazelton, M. L. (2008). Statistical inference for time varying origin–destination matrices. &lt;em&gt;Transportation Research Part B: Methodological&lt;/em&gt;, 42(6), 542-552.
NYC taxi, (2017). Retrieved from &lt;a href=&#34;http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml&#34; class=&#34;uri&#34;&gt;http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Perrakis, K., Karlis, D., Cools, M., Janssens, D., Vanhoof, K., and Wets, G. (2012). A Bayesian approach for modeling origin-destination matrices. &lt;em&gt;Transportation Research Part A: Policy and Practice&lt;/em&gt;, 46(1), 200-212.&lt;/li&gt;
&lt;li&gt;Shao, H., Lam, W. H., Sumalee, A., Chen, A., and Hazelton, M. L. (2014). Estimation of mean and covariance of peak hour origin-destination demands from day-to-day traffic counts. &lt;em&gt;Transportation Research Part B: Methodological&lt;/em&gt;, 68, 52-75.&lt;/li&gt;
&lt;li&gt;Wang, D., Liu, K., and Zhang, X. (2018), “Modeling of a three-dimensional dynamic thermal field under grid-based sensor networks in grain storage”, in press, &lt;em&gt;IISE Transactions&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Zhang, A., Kang, J. E., and Kwon, C. (2017). Incorporating demand dynamics in multi-period capacitated fast-charging location planning for electric vehicles. &lt;em&gt;Transportation Research Part B: Methodological&lt;/em&gt;, 103, 5-29.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;About the authors&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-06-10-spatiotemporal-modeling-and-real-time-prediction-of-origin_files/xiaochen.jpg&#34; height=&#34;75&#34; /&gt;
Dr. Xiaochen Xian is an assistant professor from the Department of Industrial and Systems Engineering at the University of Florida.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-06-10-spatiotemporal-modeling-and-real-time-prediction-of-origin_files/ye.jpg&#34; height=&#34;75&#34; /&gt;
Honghan Ye is a Ph.D. candidate from the Department of Industrial and Systems Engineering at UW-Madison.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-06-10-spatiotemporal-modeling-and-real-time-prediction-of-origin_files/wang.jpg&#34; height=&#34;75&#34; /&gt;
Dr. Xin Wang is an assistant professor from the Department of Industrial and Systems Engineering at UW-Madison.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-06-10-spatiotemporal-modeling-and-real-time-prediction-of-origin_files/liu.png&#34; height=&#34;75&#34; /&gt;
Dr. Kaibo Liu is an associate professor from the Department of Industrial and Systems Engineering at UW-Madison.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
