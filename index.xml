<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>YoungStatS</title>
    <link>https://youngstats.github.io/</link>
    <description>Recent content on YoungStatS</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 19 Jan 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://youngstats.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Fitting robust non-Gaussian models in Stan and R-INLA</title>
      <link>https://youngstats.github.io/post/2023/01/19/fitting-robust-non-gaussian-models-in-stan-and-r-inla/</link>
      <pubDate>Thu, 19 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2023/01/19/fitting-robust-non-gaussian-models-in-stan-and-r-inla/</guid>
      <description>


&lt;div id=&#34;overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;Traditionally the excitation noise of spatial and temporal models is Gaussian. Take, for instance, an AR1 (autoregressive of order 1) process, where the increments &lt;span class=&#34;math inline&#34;&gt;\(x_{i+1}-\rho x_i, \ \ |\rho|&amp;lt;1\)&lt;/span&gt; are assumed to follow a Gaussian distribution. However, it is easy to find datasets that contain inherently non-Gaussian features, such as sudden jumps or spikes, that adversely affect the inferences and predictions made from Gaussian models. In this post, we introduce a specific class of non-Gaussian models, their advantages over Gaussian models, and their Bayesian implementation in Stan and R-INLA, two well-established platforms for statistical modeling.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-go-beyond-gaussian-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why go beyond Gaussian models?&lt;/h2&gt;
&lt;p&gt;Often, these Gaussian and non-Gaussian models are used as latent components in hierarchical models leading to latent Gaussian models (LGMs) and latent non-Gaussian models (LnGMs), respectively. We highlight next the benefits of using a (latent) non-Gaussian model:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;More accurate predictions&lt;/strong&gt;. &lt;span class=&#34;citation&#34;&gt;Asar et al. (&lt;a href=&#34;#ref-asar2020linear&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; considered measurements related to the kidney function of several patients recorded over time. The LGM did not adapt well to sudden drops in measurements, as shown in the following figure, which was problematic since these drops are an example of “acute kidney injury”, which should prompt an immediate medical intervention. The red curve shows a prediction based on an LnGM, which clearly is more accurate.&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;100%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-01-19-fitting-robust-non-gaussian-models-in-stan-and-r-inla_files/Cabral_Image1_time_series.png&#34; /&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;b&gt; Observations of a quantity related to a patient’s kidney function and predictions using an LGM (green) and an LnGM (red). &lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Robustness&lt;/strong&gt;. Non-Gaussian models provide a means for both accommodating possible outliers in the data and reduce their impact on the inferences (&lt;span class=&#34;citation&#34;&gt;West (&lt;a href=&#34;#ref-west1984outlier&#34; role=&#34;doc-biblioref&#34;&gt;1984&lt;/a&gt;)&lt;/span&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Model checking&lt;/strong&gt;. We often elicit a Gaussian model for simplicity and convenience in a somewhat casual fashion. However, we can fit a non-Gaussian model, and if the relevant inferences do not change significantly, this can confirm the reasonableness of the Gaussian assumption.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;what-type-of-non-gaussian-models-are-you-considering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What type of non-Gaussian models are you considering?&lt;/h2&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}^G\)&lt;/span&gt; follows a multivariate Gaussian with mean &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{0}\)&lt;/span&gt; and precision matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Q}= \mathbf{\mathbf{D}}^T\mathbf{D}\)&lt;/span&gt;, it can be expressed through&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbf{D}\mathbf{x}^G\overset{d}{=} \sigma\mathbf{Z},
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Z}\)&lt;/span&gt; is a vector of i.i.d. standard Gaussian variables. The non-Gaussian extension for &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}^G\)&lt;/span&gt; consists in replacing the driving noise distribution:
&lt;span class=&#34;math display&#34;&gt;\[
\mathbf{D}\mathbf{x}\overset{d}{=} \sigma \mathbf{\Lambda}(\eta,\zeta),
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Lambda}\)&lt;/span&gt; is a vector of independent and standardized normal-inverse Gaussian (NIG) random variables that depend on the parameter &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\zeta\)&lt;/span&gt;, which controls the leptokurtosis and skewness of the driving noise. &lt;span class=&#34;citation&#34;&gt;Cabral, Bolin, and Rue (&lt;a href=&#34;#ref-cabral2022controlling&#34; role=&#34;doc-biblioref&#34;&gt;2022a&lt;/a&gt;)&lt;/span&gt; presented these models as flexible extensions of Gaussian models because they contain the Gaussian model as a special case (when &lt;span class=&#34;math inline&#34;&gt;\(\eta=0\)&lt;/span&gt;), and deviations from the Gaussian model are quantified by the parameters &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\zeta\)&lt;/span&gt;. These models admit a useful variance-mean mixture representation. Considering the mixing variables &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{V}= [V_1, \dotsc, V_n]^T\)&lt;/span&gt; we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}   \label{eq:model}
\tag{1}
\mathbf{x}|\mathbf{V} \sim \text{N}\left(\frac{\sigma}{\sqrt{1+\zeta^2\eta}} \zeta \mathbf{D}^{-1}(\mathbf{V}-\mathbf{1}),\frac{\sigma^2}{1+\zeta^2\eta} \mathbf{D}^{-1}\text{diag}(\mathbf{V})\mathbf{D}^{-T} \right), \ \ \  V_{i}|\eta \overset{ind.}{\sim} \mathrm{IG}(1,\eta^{-1} h_i^2),
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{IG}\)&lt;/span&gt; stands for the inverse-Gaussian distribution. We condition the Gaussian latent field &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}\)&lt;/span&gt; on a vector of mixing variables &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{V}\)&lt;/span&gt;, which enter the covariance matrix, adding more flexibility to the model. We note that the previous parameterization preserves the mean and covariance structure of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}\)&lt;/span&gt; while allowing for non-zero skewness and excess kurtosis on the marginal distributions.&lt;/p&gt;
&lt;p&gt;This non-Gaussian extension can be used in: random walk (RW) and autoregressive processes for time series; simultaneous and conditional autoregressive processes for graphical models and areal data; Matérn processes, which can be used in a variety of applications, such as in geostatistics and spatial point processes. The corresponding dependency matrices &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{D}\)&lt;/span&gt; that specify each model can be found in &lt;span class=&#34;citation&#34;&gt;Cabral, Bolin, and Rue (&lt;a href=&#34;#ref-cabral2022fitting&#34; role=&#34;doc-biblioref&#34;&gt;2022b&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-sample-paths-do-these-models-produce&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What sample paths do these models produce?&lt;/h2&gt;
&lt;p&gt;The first row of the following figure shows Gaussian (left) and NIG (right) white noise processes. The rows below show several processes built from these driving noises, including RW1, RW2, Ornstein–Uhlenbeck (OU), and Matérn (&lt;span class=&#34;math inline&#34;&gt;\(\alpha=2\)&lt;/span&gt;) processes. Whenever the NIG noise takes an extreme value (for instance, near location 0.8), the CRW1 and OU processes will exhibit a distinct jump, and the RW2 and Matérn processes will exhibit a kink (discontinuity in the first derivative).&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;100%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-01-19-fitting-robust-non-gaussian-models-in-stan-and-r-inla_files/Cabral_Image2_simulation.png&#34; /&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;b&gt; Noise and sample paths of several models for &lt;span class=&#34;math inline&#34;&gt;\(\eta=10^{-6}\)&lt;/span&gt; (left) and &lt;span class=&#34;math inline&#34;&gt;\(\eta=1\)&lt;/span&gt; (right), for &lt;span class=&#34;math inline&#34;&gt;\(\sigma=1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\zeta=0\)&lt;/span&gt;. &lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;stan-implementation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stan implementation&lt;/h2&gt;
&lt;p&gt;The Gaussian model (&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{D}\mathbf{x}^G = \mathbf{Z}\)&lt;/span&gt;) can be declared in Stan as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;x ~ multi_normal_prec(rep_vector(0,N), D&amp;#39;*D)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The non-Gaussian model (&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{D}\mathbf{x} = \mathbf{\Lambda}\)&lt;/span&gt;) declaration is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;x ~ nig_model(D, eta, zeta, h, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The argument &lt;code&gt;h&lt;/code&gt; should be a vector of ones for discrete-space models. For models defined in continuous space, for instance, a continuous random walk of order 1, &lt;code&gt;h&lt;/code&gt; contains the distance between locations. The last argument is an integer with value 1 if the log-determinant of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{D}\)&lt;/span&gt; should be computed (if &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{D}\)&lt;/span&gt; depends on parameters), or 0 otherwise. The &lt;code&gt;nig_model&lt;/code&gt; and other Stan functions can be found in &lt;a href=&#34;https://github.com/rafaelcabral96/nigstan&#34;&gt;github.com/rafaelcabral96/nigstan&lt;/a&gt;, along with a more comprehensive theoretical background and several code examples for time series, geostatistical and areal data application. The function &lt;code&gt;nig_model&lt;/code&gt; uses a collapsed representation of (1) and leverages within-chain parallelization to improve speed and scalability.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-a-non-gaussian-model-in-stan-columbus-dataset&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fitting a non-Gaussian model in Stan (Columbus dataset)&lt;/h2&gt;
&lt;p&gt;The Columbus dataset consists of crime rates in thousands (&lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt;) in 49 counties of Columbus, Ohio, and can be found in the &lt;code&gt;spdep&lt;/code&gt; R package. The next Leaflet widget shows the crime rates.&lt;/p&gt;
&lt;div class=&#34;line-block&#34;&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-01-19-fitting-robust-non-gaussian-models-in-stan-and-r-inla_files/Cabral_Image_3_Columbus.png&#34; /&gt;|&lt;/div&gt;
&lt;p&gt;We consider the following model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
y_i= \beta_0 + \beta_1 \text{HV}_i + \beta_2 \text{HI}_i +  \sigma\mathbf{x}_i,
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\text{HV}_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\text{HI}_i\)&lt;/span&gt; are the average household value and household income for county &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}\)&lt;/span&gt; is a simultaneous autoregressive (SAR) model to account for spatially structured random effects. We consider a Gaussian SAR model built from the following relationship:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbf{x} = \mathbf{B}\mathbf{x} + \sigma\mathbf{Z},
\]&lt;/span&gt;
where each element of the random vector &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}\)&lt;/span&gt; corresponds to a county and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Z}\)&lt;/span&gt; is a vector of i.i.d. standard Gaussian noise. The matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{B}\)&lt;/span&gt; causes simultaneous autoregressions of each county on its neighbors, where two counties are considered to be neighbors if they share a common border. The diagonal elements of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{B}\)&lt;/span&gt; are 0 so each node does not depend on itself. For simplicity, we assume &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{B}=\rho\mathbf{W}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{W}\)&lt;/span&gt; is a row standardized adjacency matrix and &lt;span class=&#34;math inline&#34;&gt;\(-1&amp;lt;\rho&amp;lt;1\)&lt;/span&gt; so that the resulting precision matrix is valid. We end up with the system &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{D}_{SAR}\mathbf{x} = \sigma\mathbf{Z}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{D}_{SAR}=\mathbf{I}-\rho\mathbf{W}\)&lt;/span&gt;. The equivalent model driven by NIG noise is then &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{D}_{SAR}\mathbf{x} = \sigma\mathbf{\Lambda}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\Lambda}\)&lt;/span&gt; is i.i.d. standardized NIG noise with parameters &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\zeta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The code for the Stan implementation can be found in this &lt;a href=&#34;https://github.com/stan-dev/connect22-space-time/tree/main/resources/Speaker%203%20-%20Rafael%20Cabral&#34;&gt;link&lt;/a&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{B}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\beta}=[\beta_0,\beta_1,\beta_2]^T\)&lt;/span&gt; stand for the design matrix and the regression coefficients, respectively. Then, the Stan declaration for the Gaussian and non-Gaussian models is as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;transformed parameters{
  vector[N] X = (y - B*beta)/sigma;                       // Spatial effects
}

model{

  matrix[N,N] D = add_diag(-rho*W, 1);                    // D = I - rho W;

  //Gaussian model
  //X ~ multi_normal_prec(rep_vector(0,N), D&amp;#39;*D);   
  
  //NIG model
  X ~ nig_model(D, etas, zetas, h, 1);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The posterior mean and standard deviation of the spatial effects can be found on the Leaflet widget. The widget also shows the posterior mean of the mixing variables &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{V}\)&lt;/span&gt;, and from it, we can identify two outlier counties. The posterior distributions of &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\zeta\)&lt;/span&gt; suggest heavy-tailedness, although no asymmetry, as shown in the following table.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;variable&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;median&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;sd&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mad&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;q5&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;q95&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;rhat&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;ess_bulk&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;ess_tail&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;sigma&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;27.41&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;27.32&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.34&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.34&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;22.10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;33.10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4578.12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5636.57&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;rho&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.51&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.52&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.19&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.80&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3505.46&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3361.02&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;eta&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.51&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.26&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.45&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.38&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.95&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10.94&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5081.12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5061.76&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;zeta&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.01&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.76&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.23&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.53&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.27&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1895.06&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2635.10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;beta[0]&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;59.30&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;60.42&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16.32&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.13&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;33.02&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;81.45&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2091.19&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2467.55&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;beta[1]&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.13&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.39&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.07&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3542.21&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3231.01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;beta[2]&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.45&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.45&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.57&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.57&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2.38&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2892.18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4458.28&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;r-inla-implementation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;R-INLA implementation&lt;/h2&gt;
&lt;p&gt;To account for measurement error, we can consider the following hierarchical model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
y_i|\mu_i \sim N(\mu_i, \sigma_\epsilon^2), \\
\mu_i = \beta_0 + \beta_1 \text{HV}_i + \beta_2 \text{HI}_i +  \sigma\mathbf{x}_i,
\end{equation*}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\epsilon^2\)&lt;/span&gt; is the measurement error variance, and now the non-Gaussian SAR model &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}\)&lt;/span&gt; is a latent component. When fitting latent models, such as the previous one, the posterior distribution often induces a geometry that frustrates sampling algorithms, such as Stan’s Hamiltonian Monte Carlo (&lt;span class=&#34;citation&#34;&gt;Margossian et al. (&lt;a href=&#34;#ref-margossian2020approximate&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Recent work on fitting latent non-Gaussian models (LnGMs) using variational Bayes and Laplace approximations permits fast and scalable estimation of these models. The approximation uses R-INLA (&lt;span class=&#34;citation&#34;&gt;Rue, Martino, and Chopin (&lt;a href=&#34;#ref-rue2009approximate&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt;), and the methods are implemented in the &lt;em&gt;ngvb&lt;/em&gt; package (&lt;span class=&#34;citation&#34;&gt;Cabral, Bolin, and Rue (&lt;a href=&#34;#ref-cabral2022fitting&#34; role=&#34;doc-biblioref&#34;&gt;2022b&lt;/a&gt;)&lt;/span&gt;). The previous latent SAR model is implemented in the &lt;em&gt;ngvb&lt;/em&gt; package &lt;a href=&#34;https://rafaelcabral96.github.io/ngvb/articles/ngvb.html&#34;&gt;vignette&lt;/a&gt;. There was significant evidence for non-Gaussianity since the Bayes factor between the fitted LGM and LnGM was around 80.000.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;about-the-authors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;About the Authors&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Rafael Cabral&lt;/strong&gt; (corresponding author) PhD student in Statistics at KAUST. Academic &lt;a href=&#34;https://rafaelcabral96.github.io/&#34;&gt;Website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bolin&lt;/strong&gt; Associate Professor of Statistics at KAUST. Principal investigator of the &lt;a href=&#34;https://cemse.kaust.edu.sa/stochproc&#34;&gt;Stochastic Processes and Applied Statistics group&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Haavard Rue&lt;/strong&gt; Professor of Statistics at KAUST. Principal investigator of the &lt;a href=&#34;https://cemse.kaust.edu.sa/bayescomp&#34;&gt;Bayesian Computational Statistics &amp;amp; Modeling group&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bibliography&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;Bibliography&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-asar2020linear&#34; class=&#34;csl-entry&#34;&gt;
Asar, Özgür, David Bolin, Peter J Diggle, and Jonas Wallin. 2020. &lt;span&gt;“Linear Mixed Effects Models for Non-Gaussian Continuous Repeated Measurement Data.”&lt;/span&gt; &lt;em&gt;Journal of the Royal Statistical Society: Series C (Applied Statistics)&lt;/em&gt; 69 (5): 1015–65.
&lt;/div&gt;
&lt;div id=&#34;ref-cabral2022controlling&#34; class=&#34;csl-entry&#34;&gt;
Cabral, Rafael, David Bolin, and Håvard Rue. 2022a. &lt;span&gt;“Controlling the Flexibility of Non-Gaussian Processes Through Shrinkage Priors.”&lt;/span&gt; &lt;em&gt;arXiv Preprint arXiv:2203.05510&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-cabral2022fitting&#34; class=&#34;csl-entry&#34;&gt;
———. 2022b. &lt;span&gt;“Fitting Latent Non-Gaussian Models Using Variational Bayes and Laplace Approximations.”&lt;/span&gt; &lt;em&gt;arXiv Preprint arXiv:2211.11050&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-margossian2020approximate&#34; class=&#34;csl-entry&#34;&gt;
Margossian, Charles C, Aki Vehtari, Daniel Simpson, and Raj Agrawal. 2020. &lt;span&gt;“Approximate Bayesian Inference for Latent Gaussian Models in Stan.”&lt;/span&gt; &lt;em&gt;Stan Con 2020&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-rue2009approximate&#34; class=&#34;csl-entry&#34;&gt;
Rue, Håvard, Sara Martino, and Nicolas Chopin. 2009. &lt;span&gt;“Approximate Bayesian Inference for Latent Gaussian Models by Using Integrated Nested Laplace Approximations.”&lt;/span&gt; &lt;em&gt;Journal of the Royal Statistical Society: Series b (Statistical Methodology)&lt;/em&gt; 71 (2): 319–92.
&lt;/div&gt;
&lt;div id=&#34;ref-west1984outlier&#34; class=&#34;csl-entry&#34;&gt;
West, Mike. 1984. &lt;span&gt;“Outlier Models and Prior Distributions in Bayesian Linear Regression.”&lt;/span&gt; &lt;em&gt;Journal of the Royal Statistical Society: Series B (Methodological)&lt;/em&gt; 46 (3): 431–39.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Merry Christmas and Happy New Year 2023!</title>
      <link>https://youngstats.github.io/post/2022/12/21/merry-christmas-and-happy-new-year-2023/</link>
      <pubDate>Wed, 21 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/12/21/merry-christmas-and-happy-new-year-2023/</guid>
      <description>


&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-12-21-merry-christmas-and-happy-new-year-2023_files/Final_Image.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Dear Followers of the YoungStatS project, Dear All!&lt;/p&gt;
&lt;p&gt;It has been an exciting year for our project, including 7 One World YoungStatS webinars and blogposts from leading authors in various areas of statistics, probability and econometrics.&lt;/p&gt;
&lt;p&gt;In particular, we wish to thank our supporters: The Federation of European National Statistical Societies (FENStatS), Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics (IMS).&lt;/p&gt;
&lt;p&gt;And we thank You, readers of our postings and participants at our webinars for your support and feedback thus far!&lt;/p&gt;
&lt;p&gt;YoungStatS project will resume after the New Year including webinars on random matrix theory, causal AI inference, topological data analysis, copula models and extreme value theory, and regular blogposts. You would also be able to read shorter interviews with some of our webinar speakers and blogpost authors soon in the IMS Bulletin.&lt;/p&gt;
&lt;p&gt;We wish you to have a great holiday time - Merry Christmas and a Happy New Year 2023!&lt;/p&gt;
&lt;p&gt;Editorial Members of the YoungStatS project.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Weighted residual empirical processes in semi-parametric copula adjusted for regression</title>
      <link>https://youngstats.github.io/post/2022/12/07/weighted-residual-empirical-processes-in-semi-parametric-copula-adjusted-for-regression/</link>
      <pubDate>Wed, 07 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/12/07/weighted-residual-empirical-processes-in-semi-parametric-copula-adjusted-for-regression/</guid>
      <description>


&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;p&gt;In this post we first review the concept of semi-parametric copula and
the accompanying estimation procedure of pseudo-likelihood estimation
(PLE). We then generalize the estimation problem to the setting where
the copula signal is hidden in a semi- or non-parametric regression
model. Under this setting we have to base the PLE on the residuals. The
particular challenge of the diverging score function is handled via the
technique of the weighted residual empirical processes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-semi-parametric-copula-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The semi-parametric copula model&lt;/h1&gt;
&lt;p&gt;Copula has been a popular method to model multivariate dependence
structure since its introduction in &lt;span class=&#34;citation&#34;&gt;Sklar (&lt;a href=&#34;#ref-Sklar1959&#34; role=&#34;doc-biblioref&#34;&gt;1959&lt;/a&gt;)&lt;/span&gt;. Consider a random vector
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{E}=(E_1,\dots,E_p)^\top\in\mathbb{R}^p\)&lt;/span&gt; with joint distribution
function &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt;; we assume throughout that &lt;span class=&#34;math inline&#34;&gt;\(E_k\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(k\in\{1,\dots,p\}\)&lt;/span&gt; has
absolutely continuous marginal distribution function &lt;span class=&#34;math inline&#34;&gt;\(F_k\)&lt;/span&gt;. Then the
&lt;em&gt;copula&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; associated with &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{E}\)&lt;/span&gt; is the joint distribution
function of the marginally transformed random vector
&lt;span class=&#34;math inline&#34;&gt;\((F_1(E_1),\dots,F_p(E_p))^\top\)&lt;/span&gt;. It is clear from this definition that
&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; itself is always a distribution supported on the unit hypercube
&lt;span class=&#34;math inline&#34;&gt;\([0,1]^p\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; always has &lt;em&gt;uniform marginals&lt;/em&gt; supported on &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt;
whatever the marginals of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{E}\)&lt;/span&gt; may be. (The explicit form of &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;
follows from the Sklar’s theorem, for instance Corollary 2.10.10 in
&lt;span class=&#34;citation&#34;&gt;Nelsen (&lt;a href=&#34;#ref-Nelsen1999&#34; role=&#34;doc-biblioref&#34;&gt;2006&lt;/a&gt;)&lt;/span&gt;:
&lt;span class=&#34;math inline&#34;&gt;\(C(\mathbf{u}) = H(F_1^{\leftarrow}(u_1),\dots,F_p^{\leftarrow}(u_p))\)&lt;/span&gt;
for &lt;span class=&#34;math inline&#34;&gt;\(F_k^{\leftarrow}\)&lt;/span&gt; the left-continuous inverse of &lt;span class=&#34;math inline&#34;&gt;\(F_k\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{u} = (u_1,\dots,u_p)^\top\in[0,1]^p\)&lt;/span&gt;.) Furthermore, by the
&lt;em&gt;invariance property&lt;/em&gt;, if &lt;span class=&#34;math inline&#34;&gt;\(g_1,\dots,g_p\)&lt;/span&gt; are univariate strictly
increasing functions, then &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{E}\)&lt;/span&gt; and its marginally transformed
version &lt;span class=&#34;math inline&#34;&gt;\((g_1(E_1),\dots,g_p(E_p))^\top\)&lt;/span&gt; will admit the copula.&lt;/p&gt;
&lt;p&gt;Thus, copula is a &lt;em&gt;margin-free&lt;/em&gt; measure of multivariate dependence.
Applied in the opposite direction, one could also start from a copula
and couple the copula with arbitrary marginals to create multivariate
distributions in a flexible manner. For instance, beyond the usual
applications in finance and economy, copulas could be used in the latter
manner to model the dependence among the repeated observations in
longitudinal data (&lt;span class=&#34;citation&#34;&gt;Sun, Frees, and Rosenberg (&lt;a href=&#34;#ref-SunFreesRosenberg2008&#34; role=&#34;doc-biblioref&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;In this post we will focus on the &lt;em&gt;semi-parametric&lt;/em&gt; copula model that
serves as a middle ground between a totally non-parametric approach to
copula modelling (via the so called &lt;em&gt;empirical copula&lt;/em&gt;, see for instance
&lt;span class=&#34;citation&#34;&gt;Fermanian, Radulović, and Wegkamp (&lt;a href=&#34;#ref-FermanianRadulovicWegkamp2004&#34; role=&#34;doc-biblioref&#34;&gt;2004&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Berghaus, Bücher, and Volgushev (&lt;a href=&#34;#ref-BerghausBucherVolgushev2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;) and a
totally parametric modelling of the random vector &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{E}\)&lt;/span&gt;. In the
semi-parametric copula model, we consider a collection of possible
distributions of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{E}\)&lt;/span&gt; where the copulas
&lt;span class=&#34;math inline&#34;&gt;\(C=C(\mathbf{\cdot};\mathbf{\theta})\)&lt;/span&gt; are constrained to be parametrized
by an Euclidean &lt;em&gt;copula parameter&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\theta}=(\theta_1,\dots,\theta_d)^\top\)&lt;/span&gt;, but where the
marginals &lt;span class=&#34;math inline&#34;&gt;\(F_1,\dots,F_p\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{E}\)&lt;/span&gt; could range over all
&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-tuples of absolutely continuous univariate distribution functions.&lt;/p&gt;
&lt;!--------------------------------------------------------------------------------&gt;
&lt;/div&gt;
&lt;div id=&#34;the-pseudo-likelihood-method&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The pseudo-likelihood method&lt;/h1&gt;
&lt;p&gt;In the semi-parametric copula model, the primary interest is often the
true value &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\theta}^*\)&lt;/span&gt; of the copula parameter that determines
the multivariate dependence. An obvious challenge in estimating
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\theta}^*\)&lt;/span&gt; in the copula setting is how to handle the unknown
marginals &lt;span class=&#34;math inline&#34;&gt;\(F_1,\dots,F_p\)&lt;/span&gt;. The canonical solution is the
pseudo-likelihood estimation (PLE) introduced in &lt;span class=&#34;citation&#34;&gt;Oakes (&lt;a href=&#34;#ref-Oakes1994&#34; role=&#34;doc-biblioref&#34;&gt;1994&lt;/a&gt;)&lt;/span&gt; and
&lt;span class=&#34;citation&#34;&gt;Genest, Ghoudi‬, and Rivest (&lt;a href=&#34;#ref-GenestGhoudiRivest1995&#34; role=&#34;doc-biblioref&#34;&gt;1995&lt;/a&gt;)&lt;/span&gt; that we now describe.&lt;/p&gt;
&lt;p&gt;Let
&lt;span class=&#34;math inline&#34;&gt;\(g_1(\mathbf{\cdot};\mathbf{\theta}),\dots,g_d(\mathbf{\cdot};\mathbf{\theta})\)&lt;/span&gt;
be a collection of appropriate &lt;em&gt;score functions&lt;/em&gt; such that the
population &lt;em&gt;estimating equation&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\mathbb{E} g_m(F_1(E_1),\dots,F_p(E_p);\mathbf{\theta})=0\)&lt;/span&gt; holds only
when &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\theta}=\mathbf{\theta}^*\)&lt;/span&gt;, for all &lt;span class=&#34;math inline&#34;&gt;\(m\in\{1,\dots,d\}\)&lt;/span&gt;.
In principle one can always choose the score functions to be the ones in
the maximum likelihood estimation, namely
&lt;span class=&#34;math inline&#34;&gt;\(g_m(\cdot;\mathbf{\theta})=\frac{\partial}{\partial \theta_m} \log c(\mathbf{\cdot};\mathbf{\theta})\)&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(c(\mathbf{\cdot};\mathbf{\theta})\)&lt;/span&gt; is the density of the copula
&lt;span class=&#34;math inline&#34;&gt;\(C(\mathbf{\cdot};\mathbf{\theta})\)&lt;/span&gt;. Thus, if &lt;span class=&#34;math inline&#34;&gt;\(F_1,\dots,F_p\)&lt;/span&gt; were
known, to estimate &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\theta}^*\)&lt;/span&gt; empirically based on a sample
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{E}_i=(E_{i,1},\dots,E_{i,p})^\top\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(i\in\{1,\dots,n\}\)&lt;/span&gt; of
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{E}\)&lt;/span&gt;, one could simply “find the zero” of the empirical version
of the estimating equation, that is to estimate &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\theta}^*\)&lt;/span&gt; by
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\hat{\theta}}^{\text{parametric}}\)&lt;/span&gt; that solves, for all
&lt;span class=&#34;math inline&#34;&gt;\(m\in\{1,\dots,d\}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
    \frac{1}{n} \sum_{i=1}^n g_m(F_1(E_{i,1}),\dots,F_p(E_{i,p});\hat{\theta}^{\text{parametric}}) =0 .
\end{align*}\]&lt;/span&gt; (The superscript “parametric” points to the fact that when
&lt;span class=&#34;math inline&#34;&gt;\(F_1,\dots,F_p\)&lt;/span&gt; are known, we are basically solving a parametric
problem.) However, in semi-parametric copula modelling we commonly avoid
setting &lt;span class=&#34;math inline&#34;&gt;\(F_1,\dots,F_p\)&lt;/span&gt; to some particular form. The PLE method solves
this problem by replacing the unknown &lt;span class=&#34;math inline&#34;&gt;\(F_k\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(k\in\{1,\dots,p\}\)&lt;/span&gt; by its
empirical counterpart, namely the empirical distribution function
&lt;span class=&#34;math inline&#34;&gt;\(F_{n,k}(t) = \frac{1}{n+1} \sum_{i=1}^n \mathbf{1}\{E_{i,k} \le t \}\)&lt;/span&gt;.
The &lt;em&gt;oracle&lt;/em&gt; PLE estimator &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\hat{\theta}}^{\text{oracle}}\)&lt;/span&gt; of
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\theta}^*\)&lt;/span&gt; is then the one that solves the following, revised
estimating equation: for all &lt;span class=&#34;math inline&#34;&gt;\(m\in\{1,\dots,d\}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[\begin{align}
    \frac{1}{n} \sum_{i=1}^n g_m(F_{n,1}(E_{i,1}),\dots,F_{n,p}(E_{i,p});\mathbf{\hat{\theta}}^{\text{oracle}}) = \int g_m(\mathbf{u};\mathbf{\hat{\theta}}^{\text{oracle}})\mathrm{d}C_n(\mathbf{u}) = 0.
    \label{eq1}\tag{1}
\end{align}\]&lt;/span&gt; All integrals in this post are over &lt;span class=&#34;math inline&#34;&gt;\([0,1]^p\)&lt;/span&gt;. To simplify
our expression above and later, we have introduced the empirical copula
&lt;span class=&#34;math inline&#34;&gt;\(C_n\)&lt;/span&gt; that is a multivariate distribution function on &lt;span class=&#34;math inline&#34;&gt;\([0,1]^p\)&lt;/span&gt; with a
mass of &lt;span class=&#34;math inline&#34;&gt;\(1/n\)&lt;/span&gt; at each of
&lt;span class=&#34;math inline&#34;&gt;\((F_{n,1}(E_{i,1}),\dots,F_{n,p}(E_{i,p}))^\top\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(i\in\{1,\dots,n\}\)&lt;/span&gt;
(precisely,
&lt;span class=&#34;math inline&#34;&gt;\(C_n(\mathbf{u}) = \frac{1}{n} \sum_{i=1}^n \mathbf{1}\left\{ F_{n,1}(E_{i,1})\le u_1, \dots, F_{n,p}(E_{i,p})\le u_p \right\}\)&lt;/span&gt;).
The qualifier “oracle” in &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\hat{\theta}}^{\text{oracle}}\)&lt;/span&gt; is
used to distinguish the current case when we can still directly observe
the copula sample &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{E}_i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(i\in\{1,\dots,n\}\)&lt;/span&gt; (albeit without
knowing &lt;span class=&#34;math inline&#34;&gt;\(F_1,\dots,F_p\)&lt;/span&gt;), from the case when even that sample will be
subject to perturbation which we now turn to.&lt;/p&gt;
&lt;!--------------------------------------------------------------------------------&gt;
&lt;/div&gt;
&lt;div id=&#34;residual-based-pseudo-likelihood-for-semi-parametric-copula-adjusted-for-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Residual-based pseudo-likelihood for semi-parametric copula adjusted for regression&lt;/h1&gt;
&lt;p&gt;From now on we suppose that the copula signal
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{E}=\mathbf{E}_{\mathbf{\theta}^*}\)&lt;/span&gt; is “hidden” in a
multivariate response semi- or non-parametric regression model, a
setting considered in our recent work (&lt;span class=&#34;citation&#34;&gt;Zhao, Gijbels, and Van Keilegom (&lt;a href=&#34;#ref-ZhaoGijbelsKeilegom2022&#34; role=&#34;doc-biblioref&#34;&gt;2022&lt;/a&gt;)&lt;/span&gt;): for a
covariate &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}\in\mathbb{R}^q\)&lt;/span&gt; (independent of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{E}\)&lt;/span&gt;) and
a response &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y}=(Y_1,\dots,Y_p)^\top\in\mathbb{R}^p\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
    Y_1 &amp;amp;= m_1(\mathbf{X}) + E_1 , \\
    &amp;amp;\quad \vdots \\
    Y_p &amp;amp;= m_p(\mathbf{X}) + E_p .
\end{align*}\]&lt;/span&gt; In its raw form, the model above is a purely non-parametric
regression model; by specifying particular forms of the regression
function &lt;span class=&#34;math inline&#34;&gt;\(m_k\)&lt;/span&gt;, the above will accommodate a wide range of popular non-
and semi-parametric regression variants such as the partly linear
regression model and the additive model. (It’s not much more difficult
to consider a more flexible, heteroscedastic model
&lt;span class=&#34;math inline&#34;&gt;\(Y_k = m_k(\mathbf{X}) + \sigma_k(\mathbf{X})E_k\)&lt;/span&gt;, though we refrain
from doing so in this post.) &lt;span class=&#34;citation&#34;&gt;Gijbels, Omelka, and Veraverbeke (&lt;a href=&#34;#ref-GijbelsOmelkaVeraverbeke2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; considered a
similar model and studied the resulting &lt;em&gt;empirical copula process&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Under this regression model, we can observe an i.i.d. sample
&lt;span class=&#34;math inline&#34;&gt;\((\mathbf{Y}_1,\mathbf{X}_1),\dots,(\mathbf{Y}_n,\mathbf{X}_n)\)&lt;/span&gt; of
&lt;span class=&#34;math inline&#34;&gt;\((\mathbf{Y},\mathbf{X})\)&lt;/span&gt;, but crucially &lt;em&gt;not&lt;/em&gt; the copula sample
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{E}_1, \dots, \mathbf{E}_n\)&lt;/span&gt;. To eventually arrive at our
estimator for &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\theta}^*\)&lt;/span&gt; in this setting, we will first form
our empirical copula &lt;span class=&#34;math inline&#34;&gt;\(\hat{C}_n\)&lt;/span&gt; based on the residuals of the
regression as follows. Let &lt;span class=&#34;math inline&#34;&gt;\(\hat{m}_k\)&lt;/span&gt; be some estimator for &lt;span class=&#34;math inline&#34;&gt;\(m_k\)&lt;/span&gt;.
Let’s estimate the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;th component of &lt;span class=&#34;math inline&#34;&gt;\(E_i=(E_{i,1},\dots,E_{i,p})^\top\)&lt;/span&gt;
by the residual &lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
    \hat{E}_{i,k} = Y_{i,k} - \hat{m}_k(X_i) .
\end{align*}\]&lt;/span&gt; Then, we form the residual-based empirical distribution and
copula: &lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
    \hat{F}_{n,k}(t) = \dfrac{1}{n+1} \sum_{i=1}^n \mathbf{1}\{ \hat{E}_{i,k} \le t \},\quad \hat{C}_n(\mathbf{u}) = \frac{1}{n} \sum_{i=1}^n \prod_{k=1}^p \mathbf{1}\{\hat{F}_{n,k}(\hat{E}_{i,k})\le u_k \}.
\end{align*}\]&lt;/span&gt; Finally, to estimate &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\theta}^*\)&lt;/span&gt;, we settle for the
estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}^{\text{residual}}\)&lt;/span&gt; that solves &lt;span class=&#34;math display&#34;&gt;\[\begin{align}
    \frac{1}{n} \sum_{i=1}^n g_m( \hat{F}_{n,1}(\hat{E}_{i,1}),\dots,\hat{F}_{n,p}(\hat{E}_{i,p}) ;\hat{\theta}^{\text{residual}}) = \int g_m(\mathbf{u};\hat{\theta}^{\text{residual}})\mathrm{d}\hat{C}_n(\mathbf{u}) = 0 .
    \label{eq2}\tag{2}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Comparing Eq. (&lt;a href=&#34;#mjx-eqn-eq2&#34;&gt;2&lt;/a&gt;) to Eq. (&lt;a href=&#34;#mjx-eqn-eq1&#34;&gt;1&lt;/a&gt;), we would
expect that when the residual-based empirical copula &lt;span class=&#34;math inline&#34;&gt;\(\hat{C}_n\)&lt;/span&gt; is
asymptotically indistinguishable from the oracle empirical copula &lt;span class=&#34;math inline&#34;&gt;\(C_n\)&lt;/span&gt;,
the residual-based copula parameter estimator
&lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}^{\text{residual}}\)&lt;/span&gt; should be asymptotically
indistinguishable from &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}^{\text{oracle}}\)&lt;/span&gt; as well. To
formally reach this conclusion, standard estimating equation theory
requires (among other conditions that we will ignore in this post) that
the estimating equations at the truth should become indistinguishable,
namely &lt;span class=&#34;math display&#34;&gt;\[\begin{align}
    \int g_m(\mathbf{u};\mathbf{\theta}^*)\mathrm{d}\hat{C}_n(\mathbf{u}) - \int g_m(\mathbf{u};\mathbf{\theta}^*)\mathrm{d}C_n(\mathbf{u}) = o_p(n^{-1/2}) .
    \label{eq3}\tag{3}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;One typical, although ultimately restrictive, approach to establish Eq.
(&lt;a href=&#34;#mjx-eqn-eq3&#34;&gt;3&lt;/a&gt;) is to invoke integration by parts
(&lt;span class=&#34;citation&#34;&gt;Neumeyer, Omelka, and Hudecová (&lt;a href=&#34;#ref-NeumeyerOmelkaHudecova2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Chen, Huang, and Yi (&lt;a href=&#34;#ref-ChenHuangYi2021&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt;): ideally, this would
yield &lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
    \int g_m(\mathbf{u};\mathbf{\theta}^*)\mathrm{d}\hat{C}_n(\mathbf{u}) \sim &amp;amp;\int \hat{C}_n(\mathbf{u})\mathrm{d}g_m(\mathbf{u};\mathbf{\theta}^*) \\
    \stackrel{\text{up to}~o_p(n^{-1/2})}{\approx} &amp;amp; C_n(\mathbf{u})\mathrm{d}g_m(\mathbf{u};\mathbf{\theta}^*) \sim \int g_m(\mathbf{u};\mathbf{\theta}^*)\mathrm{d}C_n(\mathbf{u}) .
\end{align*}\]&lt;/span&gt; In the above “&lt;span class=&#34;math inline&#34;&gt;\(\sim\)&lt;/span&gt;” is only meant to give a drastically
simplified and hence not-quite-correct representation of integration by
parts (we refer the readers to Appendix A in &lt;span class=&#34;citation&#34;&gt;Radulović, Wegkamp, and Zhao (&lt;a href=&#34;#ref-RWZ2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; for a precise
multivariate integration by parts formula particularly useful for
copulas), but it already conveys the underlying idea: the aim is to
convert &lt;span class=&#34;math inline&#34;&gt;\(\hat{C}_n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(C_n\)&lt;/span&gt; in the integrals from measures to
integrands so that proving the closeness between the two integrals is
clearly reduced to proving the closeness between &lt;span class=&#34;math inline&#34;&gt;\(\hat{C}_n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(C_n\)&lt;/span&gt;.
However, the integration by parts trick, although popular, often
requires &lt;em&gt;bounded&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(g_m\)&lt;/span&gt; to properly define the measure &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{d}g_m\)&lt;/span&gt;,
but this is often &lt;em&gt;not satisfied for even the most common copulas&lt;/em&gt;. For
instance, in Gaussian copula, the score functions are quadratic forms of
the &lt;span class=&#34;math inline&#34;&gt;\(\Phi^{\leftarrow}(\mathbf{u}_k)\)&lt;/span&gt;’s where &lt;span class=&#34;math inline&#34;&gt;\(\Phi^{\leftarrow}\)&lt;/span&gt; is the
standard normal quantile function (see Eq. (2.2) in &lt;span class=&#34;citation&#34;&gt;Segers, Akker, and Werker (&lt;a href=&#34;#ref-SVW2014&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;), so are
clearly divergent as &lt;span class=&#34;math inline&#34;&gt;\(u_k\)&lt;/span&gt; approaches 0 or 1.&lt;/p&gt;
&lt;p&gt;In &lt;span class=&#34;citation&#34;&gt;Zhao, Gijbels, and Van Keilegom (&lt;a href=&#34;#ref-ZhaoGijbelsKeilegom2022&#34; role=&#34;doc-biblioref&#34;&gt;2022&lt;/a&gt;)&lt;/span&gt; we instead adopted a more direct approach.
Let
&lt;span class=&#34;math inline&#34;&gt;\(g_m^{[k]}(\mathbf{u};\mathbf{\theta}^*)=\frac{\partial}{\partial u_k} g_m(\mathbf{u};\mathbf{\theta}^*)\)&lt;/span&gt;.
Then Taylor-expanding Eq. (&lt;a href=&#34;#mjx-eqn-eq3&#34;&gt;3&lt;/a&gt;) we see that we will need
(among other ingredients) an &lt;span class=&#34;math inline&#34;&gt;\(o_p(n^{-1/2})\)&lt;/span&gt; rate for the terms on the
right-hand side of &lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\int g_m(\mathbf{u};\mathbf{\theta}^*)\mathrm{d}({\hat{C}_n-C_n})(\mathbf{u}) \approx \sum_{k=1}^p \left[ \frac{1}{n} \sum_{i=1}^n g_m^{[k]}(F_k(E_{i,k});\mathbf{\theta}^*) \left\{ \hat{F}_{n,k}(\hat{E}_{i,k})-F_{n,k}(E_{i,k}) \right\}  \right] .
\label{eq4}\tag{4}
\end{align}\]&lt;/span&gt; It is &lt;em&gt;not&lt;/em&gt; enough that the terms
&lt;span class=&#34;math inline&#34;&gt;\(\hat{F}_{n,k}(\hat{E}_{i,k})-F_{n,k}(E_{i,k})\)&lt;/span&gt; on the right-hand side
are &lt;span class=&#34;math inline&#34;&gt;\(o_p(n^{-1/2})\)&lt;/span&gt; (in fact they are not), due to the divergence of
&lt;span class=&#34;math inline&#34;&gt;\(g_m^{[k]}\)&lt;/span&gt;. We need to take a more careful look at
&lt;span class=&#34;math inline&#34;&gt;\(\hat{F}_{n,k}(\hat{E}_{i,k})-F_{n,k}(E_{i,k})\)&lt;/span&gt;, whose analysis belongs
to &lt;em&gt;residual empirical processes&lt;/em&gt;. To demonstrate the benefits of
considering the &lt;em&gt;weighted&lt;/em&gt; version of such processes, we first review
some basic literature on the weighted (non-residual) empirical processes
in the simplified setting of the real line.&lt;/p&gt;
&lt;!--------------------------------------------------------------------------------&gt;
&lt;/div&gt;
&lt;div id=&#34;weighted-empirical-processes-on-the-real-line&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Weighted empirical processes on the real line&lt;/h1&gt;
&lt;p&gt;In this section we consider estimating a distribution function &lt;span class=&#34;math inline&#34;&gt;\(F=F_U\)&lt;/span&gt;
of a random variable &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;. We rely on the empirical distribution function
&lt;span class=&#34;math inline&#34;&gt;\(F_n\)&lt;/span&gt; constructed from the i.i.d. observations &lt;span class=&#34;math inline&#34;&gt;\(U_1, \dots, U_n\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;:
&lt;span class=&#34;math inline&#34;&gt;\(F_n(t) = \frac{1}{n+1} \sum_{i=1}^n \mathbf{1}\left\{U_i\le t\right\}\)&lt;/span&gt;.
The resulting classical empirical process on the real line
&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{n} (F_n -F)(t)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(t\in \mathbb{R}\)&lt;/span&gt; must be one of the most
extensively studied objects in all of probability; for illustration we
will just quote a form of the associated law of the iterated logarithm
(LIL): &lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
    \limsup_{n\rightarrow\infty} \sup_{t} \left|\dfrac{1}{\sqrt{\log\log(n)}} \sqrt{n} (F_n -F)(t) \right| = \dfrac{1}{\sqrt{2}} .
\end{align*}\]&lt;/span&gt; Clearly, the LIL treats all points &lt;span class=&#34;math inline&#34;&gt;\(t\in \mathbb{R}\)&lt;/span&gt;
&lt;em&gt;equally&lt;/em&gt;. However, in reality the &lt;span class=&#34;math inline&#34;&gt;\(F(t)\)&lt;/span&gt; at some &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is easier to
estimate than others. This is essentially because the variability
&lt;span class=&#34;math inline&#34;&gt;\(\text{var}\left\{F_n(t)\right\}=F(t)\left\{1-F(t)\right\}/n\)&lt;/span&gt; approaches
&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(F(t)\)&lt;/span&gt; approaches &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, for any &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. We can clearly
observe this feature in the small simulation study represented by the
following figure, where the “band” enclosing the deviations (based on
100 Monte-Carlo simulations) gets narrower toward the boundaries of the
support of &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-12-07-weighted-residual-empirical-processes-in-semi-parametric-copula-adjusted-for-regression_files/weighted_Fn_02.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Above: Plot of the deviation &lt;span class=&#34;math inline&#34;&gt;\(F_n-F\)&lt;/span&gt;, for sample size &lt;span class=&#34;math inline&#34;&gt;\(n=50\)&lt;/span&gt;, based on
100 Monte-Carlo simulations. For simplicity we assumed
&lt;span class=&#34;math inline&#34;&gt;\(U\sim{Unif}(0,1)\)&lt;/span&gt;, so &lt;span class=&#34;math inline&#34;&gt;\(F(t)=t\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(t\in[0,1]\)&lt;/span&gt;. The deviation from each
simulation is represented by a single red dashed line. The 10% and 90%
quantiles of the deviations at each &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; value are indicated by the two
blue lines. Clearly, the “band” enclosing the deviations gets narrower
toward the boundaries of the support of &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This feature can also be characterized theoretically. For instance, we
can find the LIL for the weighted process &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{n}(F_n -F)/\sqrt{F}\)&lt;/span&gt;,
that is the classical process but now scaled by an additional standard
deviation factor &lt;span class=&#34;math inline&#34;&gt;\(1/\sqrt{F}\)&lt;/span&gt;, from &lt;span class=&#34;citation&#34;&gt;Csáki (&lt;a href=&#34;#ref-Csaki1977&#34; role=&#34;doc-biblioref&#34;&gt;1977&lt;/a&gt;)&lt;/span&gt;: for some &lt;span class=&#34;math inline&#34;&gt;\(0&amp;lt;C&amp;lt;\infty\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
    \limsup_{n\rightarrow\infty} \sup_{F(t)\in(\frac{1}{n},\frac{1}{2}]} \left|\dfrac{\log\log\log(n)}{\log\log(n)} \sqrt{n}\dfrac{(F_n -F)(t)}{\sqrt{F(t)}}\right| = C.
\end{align*}\]&lt;/span&gt; Compared to the LIL for the unweighted process earlier, we
can see that the weighted process is just slightly more difficult to
bound, but now &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{n}(F_n -F)(t)\)&lt;/span&gt; clearly enjoys a tighter bound
toward the boundaries of the support of &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; due to the vanishing
&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{F(t)}\)&lt;/span&gt; there.&lt;/p&gt;
&lt;p&gt;Such results on the weighted empirical processes can be generalized to
settings beyond the real line, for instance to sets in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^p\)&lt;/span&gt;
(&lt;span class=&#34;citation&#34;&gt;Alexander (&lt;a href=&#34;#ref-Alexander1987&#34; role=&#34;doc-biblioref&#34;&gt;1987&lt;/a&gt;)&lt;/span&gt;) and sets of functions
(&lt;span class=&#34;citation&#34;&gt;Giné, Koltchinskii, and Wellner (&lt;a href=&#34;#ref-GineKoltchinskiiJonWellner2003&#34; role=&#34;doc-biblioref&#34;&gt;2003&lt;/a&gt;)&lt;/span&gt;).&lt;/p&gt;
&lt;!--------------------------------------------------------------------------------&gt;
&lt;/div&gt;
&lt;div id=&#34;results-for-residual-based-estimators&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Results for residual-based estimators&lt;/h1&gt;
&lt;p&gt;For us, the idea of the weighted empirical processes will be applied to
the residual empirical processes, which will further culminate in our
eventual result on the residual-based estimator
&lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}^{\text{residual}}\)&lt;/span&gt; for the copula parameter. We will first
consider the weighted residual empirical processes.&lt;/p&gt;
&lt;div id=&#34;results-on-weighted-residual-empirical-process&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Results on weighted residual empirical process&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(f_k\)&lt;/span&gt; be the density of &lt;span class=&#34;math inline&#34;&gt;\(E_k\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\cal{T}_n\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;-field
generated by the &lt;span class=&#34;math inline&#34;&gt;\((\mathbf{X}_i,\mathbf{Y}_i)\)&lt;/span&gt;’s, &lt;span class=&#34;math inline&#34;&gt;\(i\in\{1,\dots,n\}\)&lt;/span&gt;.
The usual decomposition of a residual empirical process is
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
    \hat{F}_{n,k}(t) - F_{n,k}(t) = f_k(t) \cdot \mathbb{E}\left[ (\hat{m}_k-m_k)(\mathbf{X}) | \cal{T}_n \right] + r_{1,k}(t)
\end{align*}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(r_{1,k}\)&lt;/span&gt; is a remainder term that could be
controlled as follows:&lt;/p&gt;
&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Suppose that we can embed &lt;span class=&#34;math inline&#34;&gt;\(\hat{m}_k-m_k\)&lt;/span&gt; into a function
class &lt;span class=&#34;math inline&#34;&gt;\(\cal{D}\)&lt;/span&gt; with &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-1-4757-2545-2_19&#34;&gt;bracketing
number&lt;/a&gt;
&lt;span class=&#34;math inline&#34;&gt;\(N_{[]}(\tau,\cal{D}) \lesssim (1/\tau)^\beta \exp(K (1/\tau)^{1/\nu})\)&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; are constants. Suppose that
&lt;span class=&#34;math inline&#34;&gt;\(\|\hat{m}_k-m_k\|_\infty=O_p(a_n)\)&lt;/span&gt; (where &lt;span class=&#34;math inline&#34;&gt;\(\|\cdot\|_\infty\)&lt;/span&gt; is the
supremum norm over the support of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}\)&lt;/span&gt;). Then under mild
regularity conditions, &lt;span class=&#34;math display&#34;&gt;\[\begin{gather*}
        \sup_{t\in\ \mathbb{R}} \dfrac{|r_{1,k}(t)|}{ n^{-\frac{1}{2}} \left\{ f_k(t) \cdot a_n + a_n^2\right\}^{\frac{1}{2}(1-1/\nu)} + n^{-\frac{1}{1+1/\nu}} + a_n^2 } = O_p( 1 ) .
    \end{gather*}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Clearly, the convergence rate of the remainder term &lt;span class=&#34;math inline&#34;&gt;\(r_{1,k}\)&lt;/span&gt; is
improved by both the rate of &lt;span class=&#34;math inline&#34;&gt;\(\|\hat{m}_k-m_k\|_\infty\)&lt;/span&gt; and the weight
&lt;span class=&#34;math inline&#34;&gt;\(f_k\)&lt;/span&gt;. The latter point is especially beneficial when &lt;span class=&#34;math inline&#34;&gt;\(f_k\)&lt;/span&gt; is a density
“of the usual shape” that decays at its tails, which will allow the rate
of &lt;span class=&#34;math inline&#34;&gt;\(r_{1,k}\)&lt;/span&gt; to be tightened accordingly (exactly similar to how the
rate of &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{n}(F_n -F)(t)\)&lt;/span&gt; is tightened by &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{F(t)}\)&lt;/span&gt; in the
weighted empirical processes on the real line that we reviewed earlier).
These features will tame the divergence of &lt;span class=&#34;math inline&#34;&gt;\(g_m^{[k]}\)&lt;/span&gt; in Eq.
(&lt;a href=&#34;#mjx-eqn-eq4&#34;&gt;4&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Because what eventually form the ingredients of the residual-based
copula &lt;span class=&#34;math inline&#34;&gt;\(\hat{C}_n\)&lt;/span&gt; are the &lt;em&gt;residual ranks&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\hat{F}_{n,k}( \hat{E}_{i,k} )\)&lt;/span&gt;’s, we need to go one step further and
consider the analogous results for them:&lt;/p&gt;
&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: For all &lt;span class=&#34;math inline&#34;&gt;\(n\ge 1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(k\in\{1,\dots,p\}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(i\in\{1,\dots,n\}\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
        \label{eq5}\tag{5}
        \hat{F}_{n,k}( \hat{E}_{i,k} ) - F_{n,k}( E_{i,k} ) &amp;amp;= -f_k(E_{i,k}) \left\{ (\hat{m}_k-m_k)(\mathbf{X}_i) - \mathbb{E}\left[ (\hat{m}_k-m_k)(\mathbf{X}) | \cal{T}_n \right] \right\} + r_{1,k}( \hat{E}_{i,k} ) + r_{2,k,i}
    \end{align}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(r_{1,k}\)&lt;/span&gt; is as in the last theorem, and
&lt;span class=&#34;math inline&#34;&gt;\(r_{2,k,i}\)&lt;/span&gt; is another remainder term that satisfies &lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
        \max_{i\in\{1,\dots,n\}} \dfrac{ |r_{2,k,i}| }{ \log^{\frac{1}{2}}(n) n^{-\frac{1}{2}} \left\{ f_k(E_{i,k})\cdot a_n \right\}^{\frac{1}{2}} + a_n^2 } = O_p(1) .
    \end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Note that similar to &lt;span class=&#34;math inline&#34;&gt;\(r_{1,k}\)&lt;/span&gt; earlier, the rate of &lt;span class=&#34;math inline&#34;&gt;\(r_{2,k,i}\)&lt;/span&gt; also
enjoys the dependence on &lt;span class=&#34;math inline&#34;&gt;\(\|\hat{m}_k-m_k\|_\infty\)&lt;/span&gt; and the weighing by
&lt;span class=&#34;math inline&#34;&gt;\(f_k\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;results-on-residual-based-estimator-for-the-copula-parameter&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Results on residual-based estimator for the copula parameter&lt;/h2&gt;
&lt;p&gt;We are now ready to plug in the decomposition of
&lt;span class=&#34;math inline&#34;&gt;\(\hat{F}_{n,k}( \hat{E}_{i,k} ) - F_{n,k}( E_{i,k} )\)&lt;/span&gt; in Eq.
(&lt;a href=&#34;#mjx-eqn-eq5&#34;&gt;5&lt;/a&gt;) in the last theorem into (&lt;a href=&#34;#mjx-eqn-eq4&#34;&gt;4&lt;/a&gt;). The
leading term in Eq. (&lt;a href=&#34;#mjx-eqn-eq5&#34;&gt;5&lt;/a&gt;) (the one proportional to &lt;span class=&#34;math inline&#34;&gt;\(f_k\)&lt;/span&gt;),
which is centered, is now summed over &lt;span class=&#34;math inline&#34;&gt;\(i\in\{1,\dots,n\}\)&lt;/span&gt; in
(&lt;a href=&#34;#mjx-eqn-eq4&#34;&gt;4&lt;/a&gt;) and so enjoys an additional &lt;span class=&#34;math inline&#34;&gt;\(n^{-1/2}\)&lt;/span&gt;-scaling. The
remainder terms are weighted and so tame the divergence of the scores
&lt;span class=&#34;math inline&#34;&gt;\(g_m^{[k]}\)&lt;/span&gt;. Eventually, we arrive at the asymptotic equivalence between
the residual-based PLE and the oracle PLE:&lt;/p&gt;
&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Under the conditions of the last two theorems, and some
additional regularity conditions which in particular require, for
&lt;span class=&#34;math inline&#34;&gt;\(m\in\{1,\dots,d\}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(k\in\{1,\dots,p\}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[\begin{align}
        \int \left\{g_m^{[k]}(\mathbf{u};\mathbf{\theta}^*) f_k(F_k^{\leftarrow}(u_k)) \right\}^2\mathrm{d}C(\mathbf{u}) &amp;lt; \infty,
        \label{eq6}\tag{6}
    \end{align}\]&lt;/span&gt; the asymptotic equivalence in Eq. (&lt;a href=&#34;#mjx-eqn-eq3&#34;&gt;3&lt;/a&gt;)
holds. Furthermore,
&lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}^{\text{residual}} - \hat{\theta}^{\text{oracle}} = o_p(n^{-1/2})\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Condition (&lt;a href=&#34;#mjx-eqn-eq6&#34;&gt;6&lt;/a&gt;) in fact allows for quite non-trivial
divergence of the score functions &lt;span class=&#34;math inline&#34;&gt;\(g_m\)&lt;/span&gt; (it certainly accommodates the
Gaussian and the &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-copulas). To apply the theorem above, one still
needs to verify the correct upper bound on the bracketing number for
embedding &lt;span class=&#34;math inline&#34;&gt;\(\hat{m}_k-m_k\)&lt;/span&gt;, which again turns out to be non-restrictive.
For instance, for partly linear regression
&lt;span class=&#34;math inline&#34;&gt;\(Y_k = \tilde{m}_k(\mathbf{x}) + \mathbf{\theta}_k^\top \mathbf{w} + E_k\)&lt;/span&gt;
with &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{w}\in\mathbb{R}^{q_{\mathbf{L},n}}\)&lt;/span&gt;, we can allow the
dimension &lt;span class=&#34;math inline&#34;&gt;\(q_{\mathbf{L},n}\)&lt;/span&gt; of the linear covariate to grow up to
&lt;span class=&#34;math inline&#34;&gt;\(q_{\mathbf{L},n}=o(n^{1/4})\)&lt;/span&gt;.&lt;/p&gt;
&lt;!--------------------------------------------------------------------------------&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;bibliography&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;Bibliography&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-Alexander1987&#34; class=&#34;csl-entry&#34;&gt;
Alexander, Kenneth S. 1987. &lt;span&gt;“Rates of Growth and Sample Moduli for Weighted Empirical Processes Indexed by Sets.”&lt;/span&gt; &lt;em&gt;Probability Theory and Related Fields&lt;/em&gt; 75 (3): 379–423.
&lt;/div&gt;
&lt;div id=&#34;ref-BerghausBucherVolgushev2017&#34; class=&#34;csl-entry&#34;&gt;
Berghaus, Betina, Axel Bücher, and Stanislav Volgushev. 2017. &lt;span&gt;“Weak Convergence of the Empirical Copula Process with Respect to Weighted Metrics.”&lt;/span&gt; &lt;em&gt;Bernoulli&lt;/em&gt; 23 (1): 743–72.
&lt;/div&gt;
&lt;div id=&#34;ref-ChenHuangYi2021&#34; class=&#34;csl-entry&#34;&gt;
Chen, Xiaohong, Zhuo Huang, and Yanping Yi. 2021. &lt;span&gt;“Efficient Estimation of Multivariate Semi-Nonparametric &lt;span&gt;GARCH&lt;/span&gt; Filtered Copula Models.”&lt;/span&gt; &lt;em&gt;Journal of Econometrics&lt;/em&gt; 222 (1): 484–501.
&lt;/div&gt;
&lt;div id=&#34;ref-Csaki1977&#34; class=&#34;csl-entry&#34;&gt;
Csáki, E. 1977. &lt;span&gt;“The Law of the Iterated Logarithm for Normalized Empirical Distribution Function.”&lt;/span&gt; &lt;em&gt;Zeitschrift Für Wahrscheinlichkeitstheorie Und Verwandte Gebiete&lt;/em&gt; 238 (2): 147–67.
&lt;/div&gt;
&lt;div id=&#34;ref-FermanianRadulovicWegkamp2004&#34; class=&#34;csl-entry&#34;&gt;
Fermanian, Jean-David, Dragan Radulović, and Marten Wegkamp. 2004. &lt;span&gt;“Weak Convergence of Empirical Copula Processes.”&lt;/span&gt; &lt;em&gt;Bernoulli&lt;/em&gt; 10 (5): 847–60.
&lt;/div&gt;
&lt;div id=&#34;ref-GenestGhoudiRivest1995&#34; class=&#34;csl-entry&#34;&gt;
Genest, Christian, ‪Kilani Ghoudi‬, and ‪Louis-Paul Rivest. 1995. &lt;span&gt;“A Semiparametric Estimation Procedure of Dependence Parameters in Multivariate Families of Distributions.”&lt;/span&gt; &lt;em&gt;Biometrika&lt;/em&gt; 82: 543–52.
&lt;/div&gt;
&lt;div id=&#34;ref-GijbelsOmelkaVeraverbeke2015&#34; class=&#34;csl-entry&#34;&gt;
Gijbels, Irène, Marek Omelka, and Noël Veraverbeke. 2015. &lt;span&gt;“Estimation of a Copula When a Covariate Affects Only Marginal Distributions.”&lt;/span&gt; &lt;em&gt;Scandinavian Journal of Statistics&lt;/em&gt; 42 (4): 1109–26.
&lt;/div&gt;
&lt;div id=&#34;ref-GineKoltchinskiiJonWellner2003&#34; class=&#34;csl-entry&#34;&gt;
Giné, Evarist, Vladimir Koltchinskii, and Jon. A. Wellner. 2003. &lt;span&gt;“Ratio Limit Theorems for Empirical Processes.”&lt;/span&gt; In &lt;em&gt;Stochastic Inequalities and Applications&lt;/em&gt;, edited by Evarist Giné, Christian Houdré, and David Nualart, 249–78. Birkhäuser.
&lt;/div&gt;
&lt;div id=&#34;ref-Nelsen1999&#34; class=&#34;csl-entry&#34;&gt;
Nelsen, Roger B. 2006. &lt;em&gt;An &lt;span&gt;I&lt;/span&gt;ntroduction to &lt;span&gt;C&lt;/span&gt;opulas&lt;/em&gt;. 2nd ed. New York: Springer.
&lt;/div&gt;
&lt;div id=&#34;ref-NeumeyerOmelkaHudecova2019&#34; class=&#34;csl-entry&#34;&gt;
Neumeyer, Natalie, Marek Omelka, and Šárka Hudecová. 2019. &lt;span&gt;“A Copula Approach for Dependence Modeling in Multivariate Nonparametric Time Series.”&lt;/span&gt; &lt;em&gt;Journal of Multivariate Analysis&lt;/em&gt; 171: 139–62.
&lt;/div&gt;
&lt;div id=&#34;ref-Oakes1994&#34; class=&#34;csl-entry&#34;&gt;
Oakes, David. 1994. &lt;span&gt;“Multivariate Survival Distributions.”&lt;/span&gt; &lt;em&gt;Journal of Nonparametric Statistics&lt;/em&gt; 3 (3–4): 343–54.
&lt;/div&gt;
&lt;div id=&#34;ref-RWZ2017&#34; class=&#34;csl-entry&#34;&gt;
Radulović, Dragan, Marten Wegkamp, and Yue Zhao. 2017. &lt;span&gt;“Weak Convergence of Empirical Copula Processes Indexed by Functions.”&lt;/span&gt; &lt;em&gt;Bernoulli&lt;/em&gt; 23 (4): 3346–84.
&lt;/div&gt;
&lt;div id=&#34;ref-SVW2014&#34; class=&#34;csl-entry&#34;&gt;
Segers, Johan, Ramon van den Akker, and Bas J. M. Werker. 2014. &lt;span&gt;“Semiparametric &lt;span&gt;G&lt;/span&gt;aussian Copula Models: Geometry and Efficient Rank-Based Estimation.”&lt;/span&gt; &lt;em&gt;The Annals of Statistics&lt;/em&gt; 42 (5): 1911–40.
&lt;/div&gt;
&lt;div id=&#34;ref-Sklar1959&#34; class=&#34;csl-entry&#34;&gt;
Sklar, Abe. 1959. &lt;span&gt;“Fonctions de Répartition à n Dimensions Et Leurs Marges.”&lt;/span&gt; &lt;em&gt;Publications de l’Institut de Statistique de L’Université de Paris&lt;/em&gt; 8: 229–31.
&lt;/div&gt;
&lt;div id=&#34;ref-SunFreesRosenberg2008&#34; class=&#34;csl-entry&#34;&gt;
Sun, Jiafeng, Edward W. Frees, and Marjorie A. Rosenberg. 2008. &lt;span&gt;“Heavy-Tailed Longitudinal Data Modeling Using Copulas.”&lt;/span&gt; &lt;em&gt;Insurance: Mathematics and Economics&lt;/em&gt; 42 (2): 817–30.
&lt;/div&gt;
&lt;div id=&#34;ref-ZhaoGijbelsKeilegom2022&#34; class=&#34;csl-entry&#34;&gt;
Zhao, Yue, Irène Gijbels, and Ingrid Van Keilegom. 2022. &lt;span&gt;“&lt;span class=&#34;nocase&#34;&gt;Parametric copula adjusted for non- and semiparametric regression&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;The Annals of Statistics&lt;/em&gt; 50 (2): 754–80.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Some Recent Developments in Mixture Cure Model Methodology for Survival Analysis</title>
      <link>https://youngstats.github.io/post/2022/10/30/some-recent-developments-in-mixture-cure-model-methodology-for-survival-analysis/</link>
      <pubDate>Sun, 30 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/10/30/some-recent-developments-in-mixture-cure-model-methodology-for-survival-analysis/</guid>
      <description>


&lt;div id=&#34;abstract&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;The mixture cure model in survival analysis has received large and growing attention in the last few decades. Here we present an overview drawing together early results and some recent new developments, and pointing out areas where further work is needed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction-the-mixture-cure-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction: the Mixture Cure Model&lt;/h2&gt;
&lt;p&gt;In certain clinical trials or observational studies, either prospective or based on historically accumulated data, individuals are or have been &lt;em&gt;followed up&lt;/em&gt; for a period of time and their status at some endpoint reported. See for example the &lt;span class=&#34;citation&#34;&gt;National Cancer Institute (&lt;a href=&#34;#ref-SEER2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; (Surveillance, Epidemiology, and End Results, US National Cancer Institute) data base, which contains a massive amount of data with extended followup on a wide range of cancers — an important source for historical data. In another context, in &lt;span class=&#34;citation&#34;&gt;Liu et al. (&lt;a href=&#34;#ref-Liu2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;, the times of occurrence of four endpoints (overall survival, disease-specific survival, disease-free interval, or progression-free interval) for 11,160 patients across 33 cancer types were obtained from follow-up data files, with a view to making recommendations to clinicians regarding their patient’s status.&lt;/p&gt;
&lt;p&gt;The data confronting the statistician consists of observations like this, on the time to the occurrence of some event such as death, or the recurrence of a disease, etc. For definiteness, suppose we are analysing overall survival, and the measurement is the life-lengths of a sample of individuals. A particular characteristic of this kind of data is that it is commonly &lt;em&gt;right-censored&lt;/em&gt;. This happens when an individual’s complete lifetime is not observed, either because s/he left the study early for some reason, or was still alive at the end of the study (and all real-life studies must be terminated at some finite time). The censored observations must be taken into account in any analysis; to ignore them would introduce bias, in that, typically, some of the longer lifetimes would have been ignored.&lt;/p&gt;
&lt;p&gt;Methods for the analysis of such survival data have long been known. See for example &lt;span class=&#34;citation&#34;&gt;Kalbfleisch and Prentice (&lt;a href=&#34;#ref-kalbfleischprentice1981&#34; role=&#34;doc-biblioref&#34;&gt;1981&lt;/a&gt;)&lt;/span&gt;. A good place to start is simply to look at the data, literally, in the form of the &lt;em&gt;Kaplan-Meier Estimator&lt;/em&gt; (KME, &lt;span class=&#34;citation&#34;&gt;Kaplan and Meier (&lt;a href=&#34;#ref-kaplanmeier1958&#34; role=&#34;doc-biblioref&#34;&gt;1958&lt;/a&gt;)&lt;/span&gt;), which is a nonparametric estimator of the survival function (the tail, or complement, of the distribution describing the lifetimes) which takes into account the censoring. An example KME plot is the first thing we see when looking at the current Wikipedia entry for the KME.&lt;/p&gt;
&lt;p&gt;Notable about this example and many others we can see in the literature is that the survivor function is &lt;em&gt;improper&lt;/em&gt;; it does not reach zero at its right endpoint. Equivalently, in such cases, the KME as a cdf has total mass less than 1. This is so for all of the data sets in &lt;span class=&#34;citation&#34;&gt;Liu et al. (&lt;a href=&#34;#ref-Liu2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;, and we give other examples below.&lt;/p&gt;
&lt;p&gt;A KME which levels off or “plateaus” at its right hand end because the largest or perhaps a number of the largest lifetimes are censored may indicate the presence of a proportion of individuals in the population who will not suffer the event, no matter how long they are followed up. We refer to them as “cured of” or “immune to” the event, and methods are now well developed to deal with this kind of data, generally known as &lt;em&gt;cure model&lt;/em&gt; methods. As well as providing significant extra information beyond that of a standard survival analysis, ignoring the presence of cures in an analysis can lead to biased and misleading conclusions, sometimes with profound consequences for diagnostic prognostications and evaluations. Various versions of cure models have been formulated over the years but here we concentrate on a version which seems easiest to us to formulate, analyse and interpret: the &lt;em&gt;mixture cure models&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;boags-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Boag’s Data&lt;/h2&gt;
&lt;p&gt;The first recognition of the need for and implementation of a cure model seems to have been by &lt;span class=&#34;citation&#34;&gt;Boag (&lt;a href=&#34;#ref-boag&#34; role=&#34;doc-biblioref&#34;&gt;1949&lt;/a&gt;)&lt;/span&gt;. He collected data from a number of centres in England, for various sites of the disease and treatment methods, and noticed that, while the distributions of life-lengths (measured from the beginning of treatment) of those dying appeared to follow quite well a lognormal distribution, “&lt;em&gt;a large group of patients … were still alive and symptom-free&lt;/em&gt; (at the limits of his followup). &lt;em&gt;In this instance we should conclude that a proportion of the patients was permanently cured by the treatment&lt;/em&gt;”.&lt;/p&gt;
&lt;p&gt;Accordingly, he proposed a model in which “&lt;em&gt;A proportion, &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;, of all patients treated is permanently cured. Patients in the remaining fraction &lt;span class=&#34;math inline&#34;&gt;\((1- c)\)&lt;/span&gt; are liable to die of cancer if they do not previously die from other causes.&lt;/em&gt;” He went on to fit by maximum likelihood a lognormal distribution with mass at infinity – a mixture cure model – to followup data on 121 women with breast cancer, finding a significant “cured” proportion in the data.&lt;/p&gt;
&lt;p&gt;Fig. 2.1 shows the KME of the survival distribution with 95% confidence intervals, and a Weibull mixture distribution fitted, for Boag’s 121 breast cancer patients. The KME jumps only at uncensored (death) times, remaining constant at censored times. For this data it clearly levels off at a value less than 1, consistent with Boag’s observation of a possible cured component, with a tendency to remain constant at lifetimes greater than 90 months, except for one late death at 120.6 months. The length of the level stretch at the righthand end of the KME is indicative of the amount of followup in the data.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-10-30-some-recent-developments-in-mixture-cure-methodology-for-survival-analysis_files/Figure1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 2.1: &lt;em&gt;KME for Boag Breast Cancer Data with Fitted Weibull Mixture Distribution&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The KME in Fig. 2.1 is very typical of the kind that can be seen in much of the medical literature. It displays clearly the main issues we want to address:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bullet\)&lt;/span&gt; has the KME levelled off at a value &lt;em&gt;significantly&lt;/em&gt; less than 1 thereby indicating the possible presence of immunes in the population? and&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bullet\)&lt;/span&gt; has the KME levelled off &lt;em&gt;sufficiently&lt;/em&gt; for us to be confident of this?&lt;/p&gt;
&lt;p&gt;Since the prospect of a cure is surely the hope of many or most medical procedures, the importance of Boag’s insight can hardly be overstated. Following his groundbreaking paper a number of researchers followed up with various aspects and analyses of the model, but the first systematic treatment of what is now called the long term survivor or cure mixture model seems to have been in &lt;span class=&#34;citation&#34;&gt;Maller and Zhou (&lt;a href=&#34;#ref-mallerzhoubook1996&#34; role=&#34;doc-biblioref&#34;&gt;1996&lt;/a&gt;)&lt;/span&gt;. That book combines nonparametric and parametric theoretical formulations and proofs with many practical applications and examples of the model.&lt;/p&gt;
&lt;p&gt;There has been an upsurge in interest in the model since the 1990s, with many applications areas explored, especially in medical statistics, and some substantial theoretical advances made. Correspondingly, computational facilities have improved tremendously, and with modern capabilities a wide variety of parametric models of censored data with long term survivors can now be fitted routinely with the statistical package R.&lt;/p&gt;
&lt;p&gt;More recent work of the present authors concerns some aspects left unresolved in &lt;span class=&#34;citation&#34;&gt;Maller and Zhou (&lt;a href=&#34;#ref-mallerzhoubook1996&#34; role=&#34;doc-biblioref&#34;&gt;1996&lt;/a&gt;)&lt;/span&gt;, as well as some quite new points of view, which we discuss below.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;notation-assumptions-and-distributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Notation, Assumptions and Distributions&lt;/h2&gt;
&lt;p&gt;A tractable and reasonably realistic model for the data is an independent and identically distributed (iid) censoring model with right censoring. In it, a sample of size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; consists of observations on the sequence of iid 2-vectors &lt;span class=&#34;math inline&#34;&gt;\(\big(T_i=T_i^*\wedge U_i, C_i={\bf 1}(T_i^*\le U_i);\, 1\le i\le n\big)\)&lt;/span&gt; where the &lt;span class=&#34;math inline&#34;&gt;\(T_i\)&lt;/span&gt; represent the censored survival times and the &lt;span class=&#34;math inline&#34;&gt;\(C_i\)&lt;/span&gt; are the censor indicators. The &lt;span class=&#34;math inline&#34;&gt;\(T_i^*\)&lt;/span&gt; with continuous cumulative distribution function (cdf) &lt;span class=&#34;math inline&#34;&gt;\(F^*\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\([0,\infty)\)&lt;/span&gt; represent the times of occurrence of the event under study. The &lt;span class=&#34;math inline&#34;&gt;\(U_i\)&lt;/span&gt;, iid with continuous cdf &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\([0,\infty)\)&lt;/span&gt;, are censoring random variables, independent of the &lt;span class=&#34;math inline&#34;&gt;\(T_i^*\)&lt;/span&gt;. In a sample from a population containing long-term survivors we observe the censored random variables &lt;span class=&#34;math inline&#34;&gt;\(T_i=T_i^*\wedge U_i\)&lt;/span&gt;, these being potential lifetimes censored at a limit of follow-up represented for individual &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; by the random variable &lt;span class=&#34;math inline&#34;&gt;\(U_i\)&lt;/span&gt; with censor indicators &lt;span class=&#34;math inline&#34;&gt;\(C_i={\bf 1}(T_i^*\le U_i)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In the general mixture cure model, the censoring distribution &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; of the &lt;span class=&#34;math inline&#34;&gt;\(U_i\)&lt;/span&gt; is always assumed proper (total mass 1), but the distribution &lt;span class=&#34;math inline&#34;&gt;\(F^*\)&lt;/span&gt; of the &lt;span class=&#34;math inline&#34;&gt;\(T_i^*\)&lt;/span&gt; may be improper, with mass &lt;span class=&#34;math inline&#34;&gt;\(1-p\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(0\le p&amp;lt;1\)&lt;/span&gt;, at infinity. We assume &lt;span class=&#34;math inline&#34;&gt;\(F^*\)&lt;/span&gt;to be of the form &lt;span class=&#34;math display&#34;&gt;\[\begin{equation}    \label{FandF0}
    F^*(t)=pF(t),
\end{equation}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(0&amp;lt;p\le 1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; is a proper distribution. &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; is the distribution of the lifetimes of susceptible individuals in the population; only these can experience the event of interest and have a potentially uncensored failure time. The remainder are immune to the event of interest or cured of it. The presence of cured subjects is signalled by a value of &lt;span class=&#34;math inline&#34;&gt;\(p&amp;lt;1\)&lt;/span&gt;, in which case the distribution &lt;span class=&#34;math inline&#34;&gt;\(F^*\)&lt;/span&gt; is improper, with total mass &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. Then &lt;span class=&#34;math inline&#34;&gt;\(1-p\)&lt;/span&gt; is the proportion of immune or cured individuals in the population.&lt;/p&gt;
&lt;p&gt;We do not know whether a particular censored lifetime in the sample is from a cured or immune individual (uncensored lifetimes are obviously not from immunes); but observations on cured or immune individuals are always censored; those on susceptibles may or may not be according as the corresponding &lt;span class=&#34;math inline&#34;&gt;\(T_i^*&amp;gt;U_i\)&lt;/span&gt; or not.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-display-the-kaplan-meier-estimator&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Display: the Kaplan-Meier Estimator&lt;/h2&gt;
&lt;p&gt;The KME is a highly informative data display which shows clearly in visual form the features we want to investigate. To define it, denote the ordered sample lifetimes as &lt;span class=&#34;math inline&#34;&gt;\(T_n^{(1)}&amp;lt; T_n^{(2)}&amp;lt; \cdots &amp;lt;T_n^{(n)}\)&lt;/span&gt;, with associated censor indicators &lt;span class=&#34;math inline&#34;&gt;\(C_n^{(1)}, C_n^{(2)}, \ldots, C_n^{(n)}\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(M(n)=T_n^{(n)}=\max_{1\leq i\leq n}T_i\)&lt;/span&gt; be the largest survival time and let &lt;span class=&#34;math inline&#34;&gt;\(M_u(n)\)&lt;/span&gt; be the largest observed &lt;em&gt;uncensored&lt;/em&gt; survival time. An explicit definition of the KME is &lt;span class=&#34;math display&#34;&gt;\[
F_n(t):= 1-\prod_{1\le i\le n: \, T_n^{(i)} \le t}^n \big(1- \frac{C_n^{(i)}}{n-i+1} \big),
\ {\rm  for}\ 0&amp;lt;t\le M(n),
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(\hat F_n(0):=0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat F_n(t):=\hat F_n(M(n))\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(t&amp;gt; M(n)\)&lt;/span&gt;. In (4.1), &lt;span class=&#34;math inline&#34;&gt;\(n-i+1\)&lt;/span&gt; is the number of subjects “at risk” at times just prior to &lt;span class=&#34;math inline&#34;&gt;\(T_n^{(i)}\)&lt;/span&gt;. Recall we assume &lt;span class=&#34;math inline&#34;&gt;\(F^*\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; are continuous so there are no tied survival times in the data with probability 1. Let &lt;span class=&#34;math inline&#34;&gt;\(\hat p_n:= \hat F_n(M(n))\)&lt;/span&gt; be the value of the KME at its right extreme.&lt;/p&gt;
&lt;p&gt;In a sample we observe data values &lt;span class=&#34;math inline&#34;&gt;\((t_i, c_i)_{1\le i\le n}\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\((T_i,C_i)_{1\le i\le n}\)&lt;/span&gt;, order them as &lt;span class=&#34;math inline&#34;&gt;\(t_n^{(1)}&amp;lt; t_n^{(2)}&amp;lt; \cdots &amp;lt;t_n^{(n)}\)&lt;/span&gt;, and define associated censor indicators &lt;span class=&#34;math inline&#34;&gt;\(c_n^{(1)}, c_n^{(2)}, \ldots, c_n^{(n)}\)&lt;/span&gt;. Then &lt;span class=&#34;math inline&#34;&gt;\(t_n^{(n)}=\max_{1\leq i\leq n}t_i\)&lt;/span&gt; is the largest observed survival time. The sample KME is the same function with observed data values substituted for the random quantities, and we obtain a sample estimate of &lt;span class=&#34;math inline&#34;&gt;\(\hat p_n\)&lt;/span&gt; by substituting &lt;span class=&#34;math inline&#34;&gt;\(t_n^{(n)}\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(M(n)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A nonparametric estimate of the population proportion dying is given by the maximum value of the KME, that is &lt;span class=&#34;math inline&#34;&gt;\(\hat p_n\)&lt;/span&gt;, and its complement is the estimated cure proportion, which as can be seen in Fig. 2.1 for Boag’s data is 0.30 with a 95% confidence interval (CI) of &lt;span class=&#34;math inline&#34;&gt;\([0.19, 0.48]\)&lt;/span&gt;. This interval excludes 0, in general agreement with Boag’s observation of a possible cured component. This confidence interval assessment though indicative is not strictly correct usage, however, as the restriction of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt; should be taken into account, as should the fact that &lt;span class=&#34;math inline&#34;&gt;\(\hat p_n\)&lt;/span&gt; is calculated from the KME at a random (not deterministic) time.&lt;/p&gt;
&lt;p&gt;When a parametric mixture model such as the Weibull is fitted, a rigorous test for &lt;span class=&#34;math inline&#34;&gt;\(H_0: p=1\)&lt;/span&gt; (no immunes present) is available (see Section 5.3, p.109, of &lt;span class=&#34;citation&#34;&gt;Maller and Zhou (&lt;a href=&#34;#ref-mallerzhoubook1996&#34; role=&#34;doc-biblioref&#34;&gt;1996&lt;/a&gt;)&lt;/span&gt;), and a nonparametric test using &lt;span class=&#34;math inline&#34;&gt;\(\hat p_n\)&lt;/span&gt; is outlined in Section 4.2, p.76, p.109, of &lt;span class=&#34;citation&#34;&gt;Maller and Zhou (&lt;a href=&#34;#ref-mallerzhoubook1996&#34; role=&#34;doc-biblioref&#34;&gt;1996&lt;/a&gt;)&lt;/span&gt; (with percentage points in Table A.1 of the book), but we still do not have complete understanding of the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\hat p_n\)&lt;/span&gt; under the null hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The KME contains further evidence about the existence of a cured component. We see in Fig.2.1 a tendency for the KME to remain constant at lifetimes greater than 90 months, except for one late death at 120.6 months. The length of the level stretch at the righthand end of the KME is indicative of the amount of followup in the data. A statistic &lt;span class=&#34;math inline&#34;&gt;\(Q_n\)&lt;/span&gt; is suggested in &lt;span class=&#34;citation&#34;&gt;Maller and Zhou (&lt;a href=&#34;#ref-mallerzhoubook1996&#34; role=&#34;doc-biblioref&#34;&gt;1996&lt;/a&gt;)&lt;/span&gt; for assessing “sufficient followup”.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;recent-research&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Recent Research&lt;/h2&gt;
&lt;p&gt;These ideas are related to the magnitudes of the largest survival time observed, and the largest &lt;em&gt;uncensored&lt;/em&gt; survival time observed, and the numbers of observations in the two time intervals defined by these. A key structural result obtained in &lt;span class=&#34;citation&#34;&gt;Maller, Resnick, and Shemehsavar (&lt;a href=&#34;#ref-MRS2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; is that, conditional on the value of the largest uncensored survival time, and knowing the number of censored observations exceeding this time, the sample partitions into two independent subsamples, each subsample having the distribution of an iid sample of censored survival times, of reduced size, from truncated random variables. This result provides valuable insight and intuition into the construction of samples of censored survival data, and facilitates the calculation of explicit finite sample formulae, for example, for the joint distribution of the largest and the largest &lt;em&gt;uncensored&lt;/em&gt; survival time observed, and for &lt;span class=&#34;math inline&#34;&gt;\(Q_n\)&lt;/span&gt;. Further, the asymptotic distributions of these statistics can then be worked out under conditions related to those familiar from extreme value theory. Our recent research is very much in this line. See &lt;span class=&#34;citation&#34;&gt;Escobar-Bach et al. (&lt;a href=&#34;#ref-emvz2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; (adjusting for insufficient follow-up), &lt;span class=&#34;citation&#34;&gt;Maller and Resnick (&lt;a href=&#34;#ref-MR2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; (extremes of censored and uncensored lifetimes), &lt;span class=&#34;citation&#34;&gt;Maller, Resnick, and Shemehsavar (&lt;a href=&#34;#ref-MRS2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; (splitting the sample at the largest uncensored observation, testing for sufficient followup, estimating the probability of being cured).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion-take-away-points&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion: Take-Away Points&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bullet\)&lt;/span&gt; It’s very common in survival analysis to encounter a KME which has levelled off at a value less than 1. This may indicate the presence of immune or cured individuals in the population — but not always — even in the absence of cures, it’s possible for the right extreme of the KME to be less than 1 just by chance.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bullet\)&lt;/span&gt; A significance test is available for the hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H_0: p=1\)&lt;/span&gt; when a well-fitting parametric model has been found for the data. A wide variety of models can be fitted routinely with R. These cover a class of generalised F models and, as a submodel, an extended generalised gamma model, which between them include as submodels most of the usual survival distributions such as the exponential, Weibull, lognormal, Gumbel, log-logistic, Burr, etc.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bullet\)&lt;/span&gt; A nonparametric test for &lt;span class=&#34;math inline&#34;&gt;\(H_0: p=1\)&lt;/span&gt; is available too, but at present we have to rely on simulated, tabulated, percentage points for the distribution.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bullet\)&lt;/span&gt; An important point is whether the KME has levelled off &lt;em&gt;sufficiently&lt;/em&gt; at its right endpoint. The &lt;span class=&#34;math inline&#34;&gt;\(Q_n\)&lt;/span&gt; statistic has been developed to measure and test for this.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bullet\)&lt;/span&gt; We’ve confined our discussion to the one-sample case. In practice, we usually have one or more groups (treatment groups, or otherwise), and/or covariates, and want to examine the effects of these. Much of &lt;span class=&#34;citation&#34;&gt;Maller and Zhou (&lt;a href=&#34;#ref-mallerzhoubook1996&#34; role=&#34;doc-biblioref&#34;&gt;1996&lt;/a&gt;)&lt;/span&gt; is concerned with methods for handling this.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bullet\)&lt;/span&gt; We’ve also confined our discussion to medical data and survival analysis. But the methodology applies to many other kinds of time-to-event data. A wide variety of examples can be found in a web search. &lt;span class=&#34;citation&#34;&gt;Maller and Zhou (&lt;a href=&#34;#ref-mallerzhoubook1996&#34; role=&#34;doc-biblioref&#34;&gt;1996&lt;/a&gt;)&lt;/span&gt; use much criminological data (time to re-arrest of a released prisoner, etc.) to illustrate the methods.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bullet\)&lt;/span&gt; Ignoring the possible presence of cured, immune or long-term survivors in a population not only risks losing valuable information but can result in bias and misleading conclusions. An important point is that including the possibility of long-term survivors in &lt;em&gt;any&lt;/em&gt; survival analysis can do no damage; if their presence is allowed for but found not to be significant, no harm is done (but keeping in mind the risks of overfitting).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bullet\)&lt;/span&gt; The mixture cure model can be regarded as a special case of a competing risks analysis where death or failure of an individual may be due to a number of possible causes. The issue of sufficient followup is clearly relevant in this context, but has not been addressed at all, to our knowledge.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;about-the-authors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;About the authors&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Ross Maller&lt;/strong&gt;, Research School of Finance, Actuarial Studies &amp;amp; Statistics, Australian National University&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sidney Resnick&lt;/strong&gt;, School of Operations Research and Information Engineering, Cornell University&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Soudabeh Shemehsavar&lt;/strong&gt; (corresponding author), College of Science, Health, Engineering and Education, Murdoch University, Perth, Western Australia and School of Mathematics, Statistics &amp;amp; Computer Sciences, University of Tehran, Soudabeh.Shemehsavar(at)murdoch.edu.au&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Muzhi Zhao&lt;/strong&gt;, Research School of Finance, Actuarial Studies &amp;amp; Statistics, Australian National University&lt;/p&gt;
&lt;div id=&#34;bibliography&#34; class=&#34;section level3 unnumbered&#34;&gt;
&lt;h3&gt;Bibliography&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-boag&#34; class=&#34;csl-entry&#34;&gt;
Boag, J. W. 1949. &lt;span&gt;“Maximum Likelihood Estimates of the Proportion of Patients Cured by Cancer Therapy.”&lt;/span&gt; &lt;em&gt;J. Roy. Stat. Soc. (B)&lt;/em&gt; 11: 15.
&lt;/div&gt;
&lt;div id=&#34;ref-emvz2020&#34; class=&#34;csl-entry&#34;&gt;
Escobar-Bach, M., R. Maller, I. Van Keilegom, and M. Zhao. 2020. &lt;span&gt;“Estimation of the Cure Rate for Distributions in the &lt;span&gt;G&lt;/span&gt;umbel Maximum Domain of Attraction Under Insufficient Follow-up.”&lt;/span&gt; &lt;em&gt;Biometrika&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-kalbfleischprentice1981&#34; class=&#34;csl-entry&#34;&gt;
Kalbfleisch, J. D., and R. L. Prentice. 1981. &lt;span&gt;“Estimation of the Average Hazard Ratio.”&lt;/span&gt; &lt;em&gt;Biometrika&lt;/em&gt; 68(1): 105–12.
&lt;/div&gt;
&lt;div id=&#34;ref-kaplanmeier1958&#34; class=&#34;csl-entry&#34;&gt;
Kaplan, E. L., and P. Meier. 1958. &lt;span&gt;“Nonparametric Estimation from Incomplete Observations.”&lt;/span&gt; &lt;em&gt;J. Amer. Statist. Assoc.&lt;/em&gt; 53: 457–81.
&lt;/div&gt;
&lt;div id=&#34;ref-Liu2018&#34; class=&#34;csl-entry&#34;&gt;
Liu, J., T. Lichtenberg, K. A. Hoadley, L. M. Poisson, A. J. Lazar, A. D. Cherniack, A. J. Kovatich, et al. 2018. &lt;span&gt;“An Integrated TCGA Pan-Cancer Clinical Data Resource to Drive High-Quality Survival Outcome Analytics.”&lt;/span&gt; &lt;em&gt;Cell&lt;/em&gt; 173: 400–416.
&lt;/div&gt;
&lt;div id=&#34;ref-MR2020&#34; class=&#34;csl-entry&#34;&gt;
Maller, R., and S. Resnick. 2020. &lt;span&gt;“Extremes of Censored and Uncensored Lifetimes in Survival Data.”&lt;/span&gt; &lt;em&gt;Extremes&lt;/em&gt; to appear.
&lt;/div&gt;
&lt;div id=&#34;ref-MRS2020&#34; class=&#34;csl-entry&#34;&gt;
Maller, R., S. Resnick, and S. Shemehsavar. 2020. &lt;span&gt;“Splitting the Sample at the Largest Uncensored Observation.”&lt;/span&gt; &lt;em&gt;Submitted&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-mallerzhoubook1996&#34; class=&#34;csl-entry&#34;&gt;
Maller, R., and X. Zhou. 1996. &lt;span&gt;“Survival Analysis with Long Term Survivors.”&lt;/span&gt; &lt;em&gt;Wiley, Chichester&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-SEER2019&#34; class=&#34;csl-entry&#34;&gt;
National Cancer Institute, DCCPS. 2019. &lt;span&gt;“Surveillance, Epidemiology, and End Results (SEER) Program Research Data (1975-2016).”&lt;/span&gt; &lt;em&gt;Surveillance, Epidemiology, and End Results (SEER) Program (Www.seer.cancer.gov) Research Data (1975-2016)&lt;/em&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Graphical modeling of stochastic processes driven by correlated noise</title>
      <link>https://youngstats.github.io/post/2022/10/27/graphical-modeling-of-stochastic-processes-driven-by-correlated-noise/</link>
      <pubDate>Thu, 27 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/10/27/graphical-modeling-of-stochastic-processes-driven-by-correlated-noise/</guid>
      <description>


&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-10-28-graphical-modeling-of-stochastic-processes-driven-by-correlated-noise_files/image1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Complex systems are difficult to understand. We need good tools to study the interactions that define such systems. In this blog post, we describe how the framework in &lt;span class=&#34;citation&#34;&gt;Mogensen and Hansen (&lt;a href=&#34;#ref-mogensen2022graphical&#34; role=&#34;doc-biblioref&#34;&gt;2022&lt;/a&gt;)&lt;/span&gt; provides such a tool. This summary of the paper is meant to be accessible for readers with some background in statistics or other quantitative sciences. We couldn’t help but include some more specific comments intended for those readers that are familiar with graphical models in statistics. These comments are enclosed in double parentheses: ((comment)).&lt;/p&gt;
&lt;p&gt;We will use the following example as an illustration of the main ideas. Assume that we observe a herd of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; animals moving together on a plane, that is, in a two-dimensional space (or rather on a plane). We assume that there exists an abstract center of the herd which at each time point is at &lt;span class=&#34;math inline&#34;&gt;\((0,0)\)&lt;/span&gt;. This means that all individual movement is relative to this reference point. &lt;span class=&#34;citation&#34;&gt;Niu, Blackwell, and Skarin (&lt;a href=&#34;#ref-niu2016modeling&#34; role=&#34;doc-biblioref&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt; describe a similar, though possibly more realistic, model of animal movement. See the gif for an example of animal movement of a small herd, &lt;span class=&#34;math inline&#34;&gt;\(n = 3\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-10-28-graphical-modeling-of-stochastic-processes-driven-by-correlated-noise_files/herd.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Our paper studies the use of graphs as means of representing dependence in how a set of processes evolve. In the example, how the animal movements interact. We essentially assume that there are two different ways that the animals and their movements interact: an asymmetrical dependence and a symmetric dependence. The asymmetric dependence may be thought of as one animal reacting to the movement of another animal, e.g., a young animal following a parent. The symmetric dependence may be thought of as two animals responding in similar ways to external stimulus, e.g., an obstacle on the ground. We can model the location of each animal as a two-dimensional stochastic process such that the joint locations of the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; animals are described by a &lt;span class=&#34;math inline&#34;&gt;\(2n\)&lt;/span&gt;-dimensional process, e.g., using a stochastic differential equation,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathrm{d}Z_t = MZ_t \mathrm{d}t + D \mathrm{d} W_t
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Z_t = (Z_t^{1},Z_t^{2},\ldots, Z_t^{n})^T\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Z_t^i = (X_t^{i},Y_t^{i})\)&lt;/span&gt; is the location in two-dimensional space of animal &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; (recall that this is the location relative to the center of the herd). The matrix &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; encodes the asymmetric dependence and the matrix &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; encodes the symmetric dependence in the system.&lt;/p&gt;
&lt;p&gt;Let’s say that &lt;span class=&#34;math inline&#34;&gt;\(n = 3\)&lt;/span&gt; and that &lt;span class=&#34;math display&#34;&gt;\[
    M = \left(\begin{array}{ccc} M_{11} &amp;amp; M_{12} &amp;amp; M_{13} \\ 0 &amp;amp; M_{22} &amp;amp; 0 \\
    0 &amp;amp; 0
    &amp;amp;
    M_{33}
    \end{array}\right), \ \ \ \ \   D = \left(\begin{array}{ccc} D_{11} &amp;amp; 0 &amp;amp; 0
    \\ 0 &amp;amp; D_{22} &amp;amp; D_{23} \\0 &amp;amp; D_{33}
    &amp;amp;
    D_{33}
    \end{array}\right).
    \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The submatrices of &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(M_{ij}\)&lt;/span&gt;, and of &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(D_{ij}\)&lt;/span&gt;, are &lt;span class=&#34;math inline&#34;&gt;\(2\times 2\)&lt;/span&gt;-matrices. We define a graph in terms of the matrices M and D. It has nodes &lt;span class=&#34;math inline&#34;&gt;\(1, \ldots, n\)&lt;/span&gt;, it has a directed edge &lt;span class=&#34;math inline&#34;&gt;\(i\rightarrow j\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(M_{ji}\)&lt;/span&gt; may be nonzero, and it has a blunt edge if &lt;span class=&#34;math inline&#34;&gt;\((DD^T)_{ij}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(i\neq j\)&lt;/span&gt;, may be nonzero. The graph encodes the sparsity patterns of the matrices, and we see that the sparsity of the above matrices is encoded by the graph &lt;strong&gt;G1&lt;/strong&gt;. The simulated movement data in the gif was generated from a system with the above sparsity in matrices &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We say that this kind of graph is a &lt;em&gt;directed correlation graph&lt;/em&gt; (cDG). The blunt edges represent symmetric dependence and the directed edges represent asymmetric dependence. ((We note that the blunt edges do not represent marginalization, but specifically correlation in the noise processes driving the model – for stochastic processes this is not the same as partial observation)). To aid our understanding of this kind of graph, we can construct an &lt;em&gt;unrolled version&lt;/em&gt; (graph &lt;strong&gt;G2&lt;/strong&gt;). In graph &lt;strong&gt;G1&lt;/strong&gt;, each node (circle) represents an entire two-dimensional process, i.e., the movement of one animal. In graph &lt;strong&gt;G2&lt;/strong&gt;, each node represents the location of an animal at a single point in time, that is, &lt;strong&gt;G2&lt;/strong&gt; represents a discretized version of the movement process. The &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;-variables represent external stimulus creating dependence between noise processes.&lt;/p&gt;
&lt;p&gt;A cDG is more useful than it may look at first glance! If we want to predict the immediate movement of Animal &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, say, in the herd, the cDG tells us that we only need to know the position of all animals with an arrow pointing into Animal &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. Moreover, the immediate movement correlates only with the immediate movements of all animals connected to Animal &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; via a blunt edge. Suppose now that there is no arrow from &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, but that we also do not observe the entire herd. Will Animal &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; then be predictive of the immediate movements of Animal &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; given the position of all other observed animals? We show a key result, known as the global Markov property, to help answer this question. From the cDG one can use a purely graphical algorithm (just computations using the graph) to check conditions that are sufficient for Animal &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; not to be predictive given the other observed animals. ((this graphical criterion is known as &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;- or &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;-separation and is an adaptation of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;-separation)). In summary, we can learn, from the graph alone, probabilistic facts about which groups of animals can be ignored when predicting immediate movement of other animals. This is quite useful.&lt;/p&gt;
&lt;p&gt;Having established a useful link between the evolution of the stochastic differential equation model above and its associated graph, the paper investigates the properties of the graphs. When we have two graphs on the same node set (i.e., only the edges differ), a natural question is whether they represent the same dependence structure. In the animal movement example, we may think of each possible graph on three nodes as encoding a certain set of movement (in)dependences. If we are to learn graphs from observation of animal movement, then we need to understand which graphs encode the same set of movement (in)dependences. The paper characterizes this kind of equivalence. Finally, the paper shows that determining Markov equivalence of two directed correlation graphs is computationally hard (it is coNP-complete).&lt;/p&gt;
&lt;p&gt;This work was supported by VILLUM FONDEN (research grant 13358).&lt;/p&gt;
&lt;div id=&#34;about-the-authors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;About the authors&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Søren Wengel Mogensen&lt;/strong&gt;, Department of Automatic Control, Lund University, Lund, Sweden&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Niels Richard Hansen&lt;/strong&gt;, Department of Mathematical Sciences, University of Copenhagen, Copenhagen, Denmark&lt;/p&gt;
&lt;div id=&#34;bibliography&#34; class=&#34;section level3 unnumbered&#34;&gt;
&lt;h3&gt;Bibliography&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-mogensen2022graphical&#34; class=&#34;csl-entry&#34;&gt;
Mogensen, Søren Wengel, and Niels Richard Hansen. 2022. &lt;span&gt;“Graphical Modeling of Stochastic Processes Driven by Correlated Noise.”&lt;/span&gt; &lt;em&gt;Bernoulli&lt;/em&gt; 28 (4): 3023–50.
&lt;/div&gt;
&lt;div id=&#34;ref-niu2016modeling&#34; class=&#34;csl-entry&#34;&gt;
Niu, Mu, Paul G Blackwell, and Anna Skarin. 2016. &lt;span&gt;“Modeling Interdependent Animal Movement in Continuous Time.”&lt;/span&gt; &lt;em&gt;Biometrics&lt;/em&gt; 72 (2): 315–24.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Inference on Adaptively Collected Data</title>
      <link>https://youngstats.github.io/post/2022/10/11/inference-on-adaptively-collected-data/</link>
      <pubDate>Tue, 11 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/10/11/inference-on-adaptively-collected-data/</guid>
      <description>


&lt;div id=&#34;abstract&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;It is increasingly common for data to be collected adaptively, where
experimental costs are reduced progressively by assigning promising
treatments more frequently. However, adaptivity also poses great
challenges on post-experiment inference, since observations are
dependent, and standard estimates can be skewed and heavy-tailed. We
propose a treatment-effect estimator that is consistent and
asymptotically normal, allowing for constructing frequentist confidence
intervals and testing hypotheses.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-10-11-inference-on-adaptively-collected-data_files/eye_catching.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Adaptive data collection can optimize sample efficiency during the
course of the experiment for particular objectives, such as identifying
the best treatment &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-russo2020simple&#34; role=&#34;doc-biblioref&#34;&gt;D. Russo 2020&lt;/a&gt;)&lt;/span&gt; or improving operational
performance &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-agrawal2013thompson&#34; role=&#34;doc-biblioref&#34;&gt;Agrawal and Goyal 2013&lt;/a&gt;)&lt;/span&gt; . To achieve these efficiency gains,
the experimenter—rather than staying with a fixed randomization
rule—updates the data-collection policy (which maps individual
characteristics/contexts to treatments/actions) in response to observed
outcomes over the course of the experiment. In this way, the
experimenter can resolve uncertainty about some aspects of the data
generating process at the expense of learning little about others
&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-murphy2005experimental&#34; role=&#34;doc-biblioref&#34;&gt;Murphy 2005&lt;/a&gt;)&lt;/span&gt;. A common family of the design algorithms are
bandit algorithms &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-lai1985asymptotically&#34; role=&#34;doc-biblioref&#34;&gt;Lai and Robbins 1985&lt;/a&gt;)&lt;/span&gt;, where treatment assignments
are selected to trade off exploration and exploitation to maximize the
cumulative performance over time.&lt;/p&gt;
&lt;p&gt;The increasing popularity of adaptive experiments results in the growing
availability of data collected from such designs. A natural query
arises: can we reuse the data to answer a variety of questions that may
not be originally targeted by the experiments? However, adaptivity also
poses great statistical challenges if the post-experiment objective
differs significantly from the original, and standard approaches used to
analyze independently collected data can be plagued by bias, excessive
variance, or both. This post seeks to address the problem of offline
policy evaluation, which is to estimate the expected benefit of one
treatment assignment policy—often termed as &lt;em&gt;policy value&lt;/em&gt;—with data
that was collected using another potentially different policy. For
example, in personalized healthcare, doctors may use electronic medical
records to evaluate how particular groups of patients will respond to
heterogenenous treatments (e.g., different types of drugs/therapies or
different dosage levels of the same drug) &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-bertsimas2017personalized&#34; role=&#34;doc-biblioref&#34;&gt;Bertsimas et al. 2017&lt;/a&gt;)&lt;/span&gt;,
whereas in targeted advertising, retailers may want to understand how
alternative product promotions (either in mail or online) affect
different consumer segments &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-schnabel2016recommendations&#34; role=&#34;doc-biblioref&#34;&gt;Schnabel et al. 2016&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-formulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Problem formulation&lt;/h2&gt;
&lt;p&gt;Consider that samples are collected by a multi-armed bandit algorithm,
where each observation is represented by a tuple &lt;span class=&#34;math inline&#34;&gt;\((W_t, Y_t)\)&lt;/span&gt;. The
random variables &lt;span class=&#34;math inline&#34;&gt;\(W_t \in \{1, 2, \dots, K\}\)&lt;/span&gt; are called the arms,
treatments or interventions. The reward or outcome &lt;span class=&#34;math inline&#34;&gt;\(Y_t\)&lt;/span&gt; represents the
individual’s response to the treatment, for which we use the potential
outcome framework and denote &lt;span class=&#34;math inline&#34;&gt;\(Y_t(w)\)&lt;/span&gt; as the random variable
representing the outcome that would be observed if individual &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; were
assigned to a treatment &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;. We here consider a stationary setting of
the potential outcomes, where &lt;span class=&#34;math inline&#34;&gt;\((Y_t(1),Y_t(2),\dots, Y_t(K))\)&lt;/span&gt; is sampled
from a fixed distribution. The set of observations up to a certain time
&lt;span class=&#34;math inline&#34;&gt;\(H_t := \{(W_s, Y_s) \}_{s=1}^T\)&lt;/span&gt; is called a history. The treatment
assignment probabilities (also known as &lt;em&gt;propensities&lt;/em&gt;)
&lt;span class=&#34;math inline&#34;&gt;\(e_t(w) := \mathbb{P}[W_t = w | H_{t-1}]\)&lt;/span&gt; are updated over time by the
experimenter, in response to the observations &lt;span class=&#34;math inline&#34;&gt;\(H_{t-1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Our goal is to estimate the value of an arm &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;, denoted by
&lt;span class=&#34;math inline&#34;&gt;\(Q(w):=\mathbb{E}[Y_t(w)]\)&lt;/span&gt;. We will provide consistent and
asymptotically normal test statistics for &lt;span class=&#34;math inline&#34;&gt;\(Q(w)\)&lt;/span&gt;, so that we can
construct confidence intervals around the estimations to test
hypotheses. We would like to do that even in data-poor situations in
which the experimenter did not collect many samples around the arm &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;our-approach&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Our approach&lt;/h2&gt;
&lt;p&gt;The main challenging in evaluating an arm &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; with observational data is
known as the &lt;em&gt;overlap&lt;/em&gt; issue between the target arm and the
data-collection mechanism, when the arm assignments made during data
collection differ substantially from the target arm &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;. This issue
becomes more severe when data is collected adaptively, since overlap
with the target arm can deteriorate as the experimenter shifts the
data-collection mechanism in response to what they observe. As a result,
estimates from standard estimators can be skewed and heavy-tailed.&lt;/p&gt;
&lt;p&gt;Our approach to recover the asymptotic normality is done in three steps.
First, we construct an unbiased arm evaluation score of each sample,
which is a transformation of the observed outcome. Second, we average
these scores with non-uniform and data-adaptive weights, obtaining a new
estimator with controlled variance. Finally, by dividing the estimator
by its estimated standard error we obtain a test statistic that is
consistent and asymptotically normal.&lt;/p&gt;
&lt;div id=&#34;step-1-constructing-an-unbiased-arm-evaluation-score-for-each-observation&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Step 1: Constructing an unbiased arm evaluation score for each observation&lt;/h4&gt;
&lt;p&gt;The arm evaluation scoring rule should address the sampling bias issue,
which is notorious in adaptively collected data &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-nie2018adaptively&#34; role=&#34;doc-biblioref&#34;&gt;Nie et al. 2018&lt;/a&gt;)&lt;/span&gt;.
One natural method is to re-weight observed outcomes based on importance
sampling, which results in an inverse propensity score weighted (IPW)
estimator: &lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
  \label{eq:ipw}
   \widehat{Q}_T^{IPW}(w) := \frac{1}{T}\sum_{t=1}^T\widehat{\Gamma}_t^{IPW}(w), \ \mbox{where} \  \widehat{\Gamma}_t^{IPW}(w) := \frac{\mathbb{I}\{W_t=w\}}{e_t(w)} Y_t.
\end{equation}\]&lt;/span&gt; With the observation that
&lt;span class=&#34;math inline&#34;&gt;\(\mathbb{P}({W_t = w | H_{t-1}, \, Y_t(w)} = \mathbb{P}({W_t = w | H_{t-1}} = e(w;H_{t-1})\)&lt;/span&gt;,
one can immediately see the unbiasedness of
&lt;span class=&#34;math inline&#34;&gt;\(\widehat{\Gamma}_t^{IPW}(w)\)&lt;/span&gt; that has
&lt;span class=&#34;math inline&#34;&gt;\(\mathbb{E}[\widehat{\Gamma}_t^{IPW}(w)|H_{t-1}]=Q(w)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The augmented inverse propensity weighted (AIPW) estimator generalizes
the above by incorporating regression adjustment
: &lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
  \label{eq:aipw}
  \begin{split}
  &amp;amp;\widehat{Q}_T^{AIPW}(w) := \frac{1}{T}\sum_{t=1}^T \widehat{\Gamma}_t^{AIPW}(w), \ \mbox{where}\ \widehat{\Gamma}_t^{AIPW}(w) :=  \hat{\mu}_t(w) + \frac{\mathbb{I}\{W_t=w\}}{e_t(w)} \left(  Y_t - \hat{\mu}_t(w)\right).
  \end{split}
\end{equation}\]&lt;/span&gt; The symbol &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}_t(w)\)&lt;/span&gt; denotes an estimator of the
conditional mean function &lt;span class=&#34;math inline&#34;&gt;\(\mu(w) = \mathbb{E}[Y_t(w)]\)&lt;/span&gt; based on the
history &lt;span class=&#34;math inline&#34;&gt;\(H_{t-1}\)&lt;/span&gt;, but it need not be a good one—it could be biased or
even inconsistent. The second term of &lt;span class=&#34;math inline&#34;&gt;\(Y_t - \hat{\mu}_t(w)\)&lt;/span&gt; acts as a
bias-correction term: adding it preserves unbiasedness but also reduces
the variance, since the residual—when &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}_t(w)\)&lt;/span&gt; is a reasonable
estimator of &lt;span class=&#34;math inline&#34;&gt;\(\mu(w)\)&lt;/span&gt;—potentially has a smaller absolute mean as
compared to the raw outcome &lt;span class=&#34;math inline&#34;&gt;\(Y_t\)&lt;/span&gt;. We will hereby use the AIPW score
&lt;span class=&#34;math inline&#34;&gt;\(\widehat{\Gamma}_t^{AIPW}(w)\)&lt;/span&gt; for each observation &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-averaging-arm-evaluations-with-adaptive-weights&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Step 2: Averaging arm evaluations with adaptive weights&lt;/h4&gt;
&lt;p&gt;When data is collected non-adaptively but by a fixed randomization rule,
AIPW estimator is semiparametrically efficient &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hahn1998role&#34; role=&#34;doc-biblioref&#34;&gt;Hahn 1998&lt;/a&gt;)&lt;/span&gt;. However,
adaptivity makes the sampling distribution non-normal and heavy-tailed,
and the variance of the AIPW scores &lt;span class=&#34;math inline&#34;&gt;\(\widehat{\Gamma}_t^{AIPW}(w)\)&lt;/span&gt; can
vary hugely over the course of experiment. In fact, the conditional
variances &lt;span class=&#34;math inline&#34;&gt;\(\mbox{Var}(\widehat{\Gamma}_t^{AIPW}(w)|H_{t-1})\)&lt;/span&gt; depend
primarily on the behavior of the inverse propensities &lt;span class=&#34;math inline&#34;&gt;\(1/e_t(w)\)&lt;/span&gt;, which
may explode to infinity or fail to converge.&lt;/p&gt;
&lt;p&gt;To address this difficulty, we consider a generalization of the AIPW
estimator, which non-uniformly averages the unbiased scores
&lt;span class=&#34;math inline&#34;&gt;\(\widehat{\Gamma}^{AIPW}_t(w)\)&lt;/span&gt; using a sequence of &lt;em&gt;adaptive weights&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(h_t(w)\)&lt;/span&gt;. The resulting estimator is the &lt;em&gt;adaptively-weighted AIPW
estimator&lt;/em&gt;: &lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  \label{eq:aw}
  \widehat{Q}^{h}_T(w) = \frac{\sum_{t=1}^T h_t(w) \widehat{\Gamma}_t^{AIPW}(w)}{\sum_{t=1}^T h_t(w)}.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Adaptive weights &lt;span class=&#34;math inline&#34;&gt;\(h_t(w)\)&lt;/span&gt; provide an additional degree of flexibility in
controlling the variance and the tail of the sampling distribution. When
chosen appropriately, these weights compensate for erratic trajectories
of the assignment probabilities &lt;span class=&#34;math inline&#34;&gt;\(e_t(w)\)&lt;/span&gt;, stabilizing the variance of
the estimator. A natural choice of adaptive weights is to approximate
the inverse standard deviation of the &lt;span class=&#34;math inline&#34;&gt;\(\widehat{\Gamma}_t^{AIPW}(w)\)&lt;/span&gt;. In
this way each re-weighted term &lt;span class=&#34;math inline&#34;&gt;\(h_t\widehat{\Gamma}_t^{AIPW}(w)\)&lt;/span&gt; has
comparable variance, such that averaging these object may lead to a
normal limiting distribution. We shall refer it to
&lt;strong&gt;constant-allocation&lt;/strong&gt; weighting scheme since each weighted element
&lt;span class=&#34;math inline&#34;&gt;\(h_t\widehat{\Gamma}_t^{AIPW}(w)\)&lt;/span&gt; contributes to roughly the same share
of variance in the final estimator. Weights of this type were proposed
by &lt;span class=&#34;citation&#34;&gt;Luedtke and Laan (&lt;a href=&#34;#ref-luedtke2016statistical&#34; role=&#34;doc-biblioref&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt; for the purpose of estimating the expected
value of non-unique optimal policies that possibly depend on observable
covariates.&lt;/p&gt;
&lt;p&gt;More generally, to get an adaptively-weighted AIPW estimator
&lt;span class=&#34;math inline&#34;&gt;\(\widehat{Q}^{h}_T(w)\)&lt;/span&gt; that is consistent and asymptotically normal, we
require the following assumptions on our weighting schemes stated below.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1&lt;/strong&gt; (Infinite sampling).
&lt;span class=&#34;math inline&#34;&gt;\(\big(\sum_{t=1}^T h_t(w)]\big)^2 \,\big/\, \mathbb{E}\big[ \sum_{t=1}^T \frac{h^2_t(w)}{e_t(w)} \big] \xrightarrow[T \to \infty]{p} \infty.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2&lt;/strong&gt; (Variance convergence).
&lt;span class=&#34;math inline&#34;&gt;\(\sum_{t=1}^T \frac{h^2_t(w)}{e_t(w) } \,\bigg/\, \mathbb{E}\big[ \sum_{t=1}^T \frac{h^2_t(w)}{e_t(w)} \big]\xrightarrow[T \to \infty]{L_p} 1\)&lt;/span&gt;,
for some &lt;span class=&#34;math inline&#34;&gt;\(p&amp;gt;0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption 3&lt;/strong&gt; (Bounded moments).
&lt;span class=&#34;math inline&#34;&gt;\(\sum_{t=1}^T \frac{h^{2 + \delta}(w)}{e^{1 + \delta}(w) } \,\Big/\, \mathbb{E}\Big[ \Big(\sum_{t=1}^T \frac{h^2_t(w)}{e_t(w)} \Big)^{1 +\delta/2}\Big]\xrightarrow[T \to \infty]{p} 0\)&lt;/span&gt;,
for some &lt;span class=&#34;math inline&#34;&gt;\(\delta&amp;gt;0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Above, Assumption 1 requires that the effective sample size after
adaptive weighting goes to infinity, which implies that the estimator
converges. Assumption 3 is a Lyapunov-type regularity condition on the
weights, which controls higher moments of the distribution. Assumption 2
is the more subtle condition that standard estimators such as AIPW
estimator (i.e., &lt;span class=&#34;math inline&#34;&gt;\(h_t(w) \equiv 1\)&lt;/span&gt;) often fail to satisfy. We refer
interesting readers to our paper &lt;span class=&#34;citation&#34;&gt;Hadad et al. (&lt;a href=&#34;#ref-hadad2021confidence&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; for a recipe on
building weights that satisfy Assumption 2. In particular, we note a
&lt;strong&gt;two-point allocation&lt;/strong&gt; weighting scheme when the assignment
probabilities &lt;span class=&#34;math inline&#34;&gt;\(e_t\)&lt;/span&gt; reflect the experimenter’s belief on arm optimality
(as is the case for Thompson sampling). This weighting scheme allows us
to downweight samples with small propensities more boldly but still
preserve the asymptotic normality, and therefore often merits smaller
variance and tighter confidence intervals as compared to the
constant-allocation scheme.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3-estimating-standard-error-and-constructing-a-test-statistic&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Step 3: Estimating standard error and constructing a test statistic&lt;/h4&gt;
&lt;p&gt;With the evaluation weights discussed in step 2, when normalized by an
estimate of its standard deviation, the adaptively-weighted AIPW
estimator has a centered and normal asymptotic distribution. Similar
``self-normalization’’ schemes are often key to martingale central
limit theorems &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-pena2008self&#34; role=&#34;doc-biblioref&#34;&gt;Peña, Lai, and Shao 2008&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;. Suppose that we observe arms &lt;span class=&#34;math inline&#34;&gt;\(W_t\)&lt;/span&gt; and rewards
&lt;span class=&#34;math inline&#34;&gt;\(Y_t=Y_t(W_t)\)&lt;/span&gt;, and that the underlying potential outcomes
&lt;span class=&#34;math inline&#34;&gt;\((Y_t(w))_{w \in \mathcal{W}}\)&lt;/span&gt; are independent and identically
distributed with nonzero variance, and satisfy
&lt;span class=&#34;math inline&#34;&gt;\(\mathbb{E}|Y_{t}(w)|^{2+\delta} &amp;lt; \infty\)&lt;/span&gt; for some &lt;span class=&#34;math inline&#34;&gt;\(\delta &amp;gt; 0\)&lt;/span&gt; and all
&lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;. Suppose that the assignment probabilities &lt;span class=&#34;math inline&#34;&gt;\(e_t(w)\)&lt;/span&gt; are strictly
positive and let &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}_t(w)\)&lt;/span&gt; be any history-adapted estimator of
&lt;span class=&#34;math inline&#34;&gt;\(Q(w)\)&lt;/span&gt; that is bounded and that converges almost-surely to some constant
&lt;span class=&#34;math inline&#34;&gt;\(\mu_{\infty}(w)\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(h_t(w)\)&lt;/span&gt; be non-negative history-adapted weights
satisfying Assumptions 1-3. Suppose that either &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}_t(w)\)&lt;/span&gt; is
consistent or &lt;span class=&#34;math inline&#34;&gt;\(e_t(w)\)&lt;/span&gt; has a limit &lt;span class=&#34;math inline&#34;&gt;\(e_{\infty}(w) \in [0, \, 1]\)&lt;/span&gt;, i.e.,
either &lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
     \label{eq:e_mu_alternative}
      \hat{\mu}_t(w) \xrightarrow[t \to \infty]{a.s.} Q(w)  \quad \text{or}
      \quad e_t(w)  \xrightarrow[t \to \infty]{a.s.} e_{\infty}(w)
\end{equation}\]&lt;/span&gt; Then, for any arm &lt;span class=&#34;math inline&#34;&gt;\(w \in \mathcal{W}\)&lt;/span&gt;, the
adaptively-weighted estimator &lt;span class=&#34;math inline&#34;&gt;\(\widehat{Q}_T^{h}(w)\)&lt;/span&gt; is consistent for
the arm value &lt;span class=&#34;math inline&#34;&gt;\(Q(w)\)&lt;/span&gt;, and the following studentized statistic is
asymptotically normal: &lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
  \begin{aligned}
    \label{eq:clt}
    &amp;amp;\frac{\widehat{Q}_T^{h}(w) - Q(w)}{\widehat{V}_T^{h}(w)^{\frac{1}{2}}} \xrightarrow{d} \mathcal{N}(0, 1),
    \ \ \ \text{where} \ \widehat{V}_T^{h}(w) := \frac{\sum_{t=1}^T h^2_t(w) \left( \widehat{\Gamma}_t(w) - \widehat{Q}_T(w) \right)^2}{\left( \sum_{t=1}^T h_t(w) \right)^2}.
  \end{aligned}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;simulations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulations&lt;/h2&gt;
&lt;p&gt;We consider three simulation settings, each with &lt;span class=&#34;math inline&#34;&gt;\(K = 3\)&lt;/span&gt; arms that yield
rewards observed with additive &lt;span class=&#34;math inline&#34;&gt;\(\text{uniform}[-1, 1]\)&lt;/span&gt; noise. The
settings vary in the difference between the arm values. In the
&lt;em&gt;no-signal&lt;/em&gt; case, we set arm values to &lt;span class=&#34;math inline&#34;&gt;\(Q(w) = 1\)&lt;/span&gt; for all
&lt;span class=&#34;math inline&#34;&gt;\(w \in \{1, 2, 3\}\)&lt;/span&gt;; in the &lt;em&gt;low signal&lt;/em&gt; case, we set
&lt;span class=&#34;math inline&#34;&gt;\(Q(w) = 0.9+ 0.1w\)&lt;/span&gt;; and &lt;em&gt;high signal&lt;/em&gt; case we set &lt;span class=&#34;math inline&#34;&gt;\(Q(w) = 0.5 + 0.5w\)&lt;/span&gt;.
During the experiment, treatment is assigned by a modified Thompson
sampling method: first, tentative assignment probabilities are computed
via Thompson sampling with normal likelihood and normal
prior&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-russo2018tutorial&#34; role=&#34;doc-biblioref&#34;&gt;D. J. Russo et al. 2018&lt;/a&gt;)&lt;/span&gt;; they are then adjusted to impose the lower
bound &lt;span class=&#34;math inline&#34;&gt;\(e_t(w) \geq (1/K)t^{-0.7}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We show comparison among four point estimators of arm values &lt;span class=&#34;math inline&#34;&gt;\(Q(w)\)&lt;/span&gt;: the
sample mean, the AIPW estimator with uniform weights (labeled as
“unweighted AIPW”), and the adaptively-weighted AIPW estimator with
constant and two-point allocation schemes. For the AIPW-based
estimators, we use the same formula given in our theorem to construct
confidence intervals. For the sample mean we use the usual variance
estimate
&lt;span class=&#34;math inline&#34;&gt;\(\smash{\widehat{V}^{AVG}(w) := T_w^{-2} \sum_{t: W_t = w}^{T} (Y_t - \widehat{Q}^{AVG}_T(w))^2}\)&lt;/span&gt;.
Approximate normality is not theoretically justified for the unweighted
AIPW estimator or for the sample mean. We also consider non-asymptotic
confidence intervals for the sample mean, based on the method of
time-uniform confidence sequences described in &lt;span class=&#34;citation&#34;&gt;Howard et al. (&lt;a href=&#34;#ref-howard2021uniform&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt;. We
refer interesting readers to our paper for more results on estimating
other arms as well as the contrast between arms &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hadad2021confidence&#34; role=&#34;doc-biblioref&#34;&gt;Hadad et al. 2021&lt;/a&gt; )&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The below panel demonstrates that we are able to estimate the “good” arm
value &lt;span class=&#34;math inline&#34;&gt;\(Q(3)\)&lt;/span&gt; with considerable ease in high- and low-signal settings, in
that all methods produce point estimates with negligible bias and small
root mean-squared error, and moreover attain roughly correct coverage
for large &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;. Conversely, estimating the “bad” arm value &lt;span class=&#34;math inline&#34;&gt;\(Q(1)\)&lt;/span&gt; is
challenging. Although the AIPW estimator is unbiased, it performs very
poorly in terms of RMSE and confidence interval width. In the low and
high signal case, its problem is that it does not take into account the
fact that the bad arm is undersampled, so its variance is high. Of our
two adaptively-weighted AIPW estimators, two-point allocation better
controls the variance of bad arm estimates by more aggressively
downweighting `unlikely’ observations associated with large inverse
propensity weights; this results in smaller RMSE and tighter confidence
intervals. For the sample-mean estimator, naive confidence intervals
based on the normal approximation are invalid, with severe
under-coverage when there’s little or no signal. On the other hand, the
non-asymptotic confidence sequences of &lt;span class=&#34;citation&#34;&gt;Howard et al. (&lt;a href=&#34;#ref-howard2021uniform&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; are
conservative and often wide.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-10-11-inference-on-adaptively-collected-data_files/arm_values.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;extensions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Extensions&lt;/h2&gt;
&lt;p&gt;One direct extension is to apply the above adaptive weighting technique
to evaluating policies on populations with hetegeneous outcomes, using
data that is collected adaptively via contextual bandit algorithms. In
&lt;span class=&#34;citation&#34;&gt;Zhan, Hadad, et al. (&lt;a href=&#34;#ref-zhan2021off&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt;, we carefully choose adaptive weights to accommodate the
variances of AIPW scores that may differ not only over time but also
across the context space. The resulting estimator further reduces
estimation variance.&lt;/p&gt;
&lt;p&gt;Aside from policy evaluation, learning the optimal treatment assignment
policies using adaptive data is also desirable, which enables
personalized decision making in a wide variety of domains. In
&lt;span class=&#34;citation&#34;&gt;Zhan, Ren, et al. (&lt;a href=&#34;#ref-zhan2021policy&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt;, we establish the first-of-the-kind regret lower bound
that characterizes the fundamental difficulty of policy learning with
adaptive data. We then propose an algorithm based on re-weighted AIPW
estimators and show that it is minimax optimal with the best weighting
scheme.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;about-the-author-and-article&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;About the author and article&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ruohanzhan.github.io/&#34;&gt;Ruohan Zhan&lt;/a&gt;: postdoctoral fellow at
the Stanford Graduate School of Business, incoming Assistant
Professor at the Hong Kong University of Science and Technology.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Based on:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Vitor Hadad, David A. Hirschberg, Ruohan Zhan, Stefan Wager and Susan
Athey. Confidence intervals for policy evaluation in adaptive
experiments. Proceedings of the National Academy of Sciences, April 5,
2021, 118 (15) e2014602118, &lt;a href=&#34;https://doi.org/10.1073/pnas.2014602118&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1073/pnas.2014602118&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;bibliography&#34; class=&#34;section level3 unnumbered&#34;&gt;
&lt;h3&gt;Bibliography&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-agrawal2013thompson&#34; class=&#34;csl-entry&#34;&gt;
Agrawal, Shipra, and Navin Goyal. 2013. &lt;span&gt;“Thompson Sampling for Contextual Bandits with Linear Payoffs.”&lt;/span&gt; In &lt;em&gt;International Conference on Machine Learning&lt;/em&gt;, 127–35. PMLR.
&lt;/div&gt;
&lt;div id=&#34;ref-bertsimas2017personalized&#34; class=&#34;csl-entry&#34;&gt;
Bertsimas, Dimitris, Nathan Kallus, Alexander M Weinstein, and Ying Daisy Zhuo. 2017. &lt;span&gt;“Personalized Diabetes Management Using Electronic Medical Records.”&lt;/span&gt; &lt;em&gt;Diabetes Care&lt;/em&gt; 40 (2): 210–17.
&lt;/div&gt;
&lt;div id=&#34;ref-hadad2021confidence&#34; class=&#34;csl-entry&#34;&gt;
Hadad, Vitor, David A Hirshberg, Ruohan Zhan, Stefan Wager, and Susan Athey. 2021. &lt;span&gt;“Confidence Intervals for Policy Evaluation in Adaptive Experiments.”&lt;/span&gt; &lt;em&gt;Proceedings of the National Academy of Sciences&lt;/em&gt; 118 (15).
&lt;/div&gt;
&lt;div id=&#34;ref-hahn1998role&#34; class=&#34;csl-entry&#34;&gt;
Hahn, Jinyong. 1998. &lt;span&gt;“On the Role of the Propensity Score in Efficient Semiparametric Estimation of Average Treatment Effects.”&lt;/span&gt; &lt;em&gt;Econometrica&lt;/em&gt;, 315–31.
&lt;/div&gt;
&lt;div id=&#34;ref-howard2021uniform&#34; class=&#34;csl-entry&#34;&gt;
Howard, Steven R, Aaditya Ramdas, Jon McAuliffe, and Jasjeet Sekhon. 2021. &lt;span&gt;“Time-Uniform, Nonparametric, Non-Asymptotic Confidence Sequences.”&lt;/span&gt; &lt;em&gt;The Annals of Statistics&lt;/em&gt; Forthcoming.
&lt;/div&gt;
&lt;div id=&#34;ref-lai1985asymptotically&#34; class=&#34;csl-entry&#34;&gt;
Lai, Tze Leung, and Herbert Robbins. 1985. &lt;span&gt;“Asymptotically Efficient Adaptive Allocation Rules.”&lt;/span&gt; &lt;em&gt;Advances in Applied Mathematics&lt;/em&gt; 6 (1): 4–22.
&lt;/div&gt;
&lt;div id=&#34;ref-luedtke2016statistical&#34; class=&#34;csl-entry&#34;&gt;
Luedtke, Alexander R, and Mark J van der Laan. 2016. &lt;span&gt;“Statistical Inference for the Mean Outcome Under a Possibly Non-Unique Optimal Treatment Strategy.”&lt;/span&gt; &lt;em&gt;Annals of Statistics&lt;/em&gt; 44 (2): 713.
&lt;/div&gt;
&lt;div id=&#34;ref-murphy2005experimental&#34; class=&#34;csl-entry&#34;&gt;
Murphy, Susan A. 2005. &lt;span&gt;“An Experimental Design for the Development of Adaptive Treatment Strategies.”&lt;/span&gt; &lt;em&gt;Statistics in Medicine&lt;/em&gt; 24 (10): 1455–81.
&lt;/div&gt;
&lt;div id=&#34;ref-nie2018adaptively&#34; class=&#34;csl-entry&#34;&gt;
Nie, Xinkun, Xiaoying Tian, Jonathan Taylor, and James Zou. 2018. &lt;span&gt;“Why Adaptively Collected Data Have Negative Bias and How to Correct for It.”&lt;/span&gt; In &lt;em&gt;Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics&lt;/em&gt;, edited by Amos Storkey and Fernando Perez-Cruz, 84:1261–69. New York: PMLR. &lt;a href=&#34;http://proceedings.mlr.press/v84/nie18a.html&#34;&gt;http://proceedings.mlr.press/v84/nie18a.html&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-pena2008self&#34; class=&#34;csl-entry&#34;&gt;
Peña, Victor H de la, Tze Leung Lai, and Qi-Man Shao. 2008. &lt;em&gt;Self-Normalized Processes: Limit Theory and Statistical Applications&lt;/em&gt;. Berlin Heidelberg: Springer-Verlag.
&lt;/div&gt;
&lt;div id=&#34;ref-russo2020simple&#34; class=&#34;csl-entry&#34;&gt;
Russo, Daniel. 2020. &lt;span&gt;“Simple Bayesian Algorithms for Best-Arm Identification.”&lt;/span&gt; &lt;em&gt;Operations Research&lt;/em&gt; 68 (6): 1625–47. &lt;a href=&#34;https://doi.org/10.1287/opre.2019.1911&#34;&gt;https://doi.org/10.1287/opre.2019.1911&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-russo2018tutorial&#34; class=&#34;csl-entry&#34;&gt;
Russo, Daniel J., Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen. 2018. &lt;span&gt;“A Tutorial on Thompson Sampling.”&lt;/span&gt; &lt;em&gt;Found. Trends Mach. Learn.&lt;/em&gt; 11 (1): 1–96. &lt;a href=&#34;https://doi.org/10.1561/2200000070&#34;&gt;https://doi.org/10.1561/2200000070&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-schnabel2016recommendations&#34; class=&#34;csl-entry&#34;&gt;
Schnabel, Tobias, Adith Swaminathan, Ashudeep Singh, Navin Chandak, and Thorsten Joachims. 2016. &lt;span&gt;“Recommendations as Treatments: Debiasing Learning and Evaluation.”&lt;/span&gt; In &lt;em&gt;International Conference on Machine Learning&lt;/em&gt;, 1670–79. PMLR.
&lt;/div&gt;
&lt;div id=&#34;ref-zhan2021off&#34; class=&#34;csl-entry&#34;&gt;
Zhan, Ruohan, Vitor Hadad, David A Hirshberg, and Susan Athey. 2021. &lt;span&gt;“Off-Policy Evaluation via Adaptive Weighting with Data from Contextual Bandits.”&lt;/span&gt; In &lt;em&gt;Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp;amp; Data Mining&lt;/em&gt;, 2125–35.
&lt;/div&gt;
&lt;div id=&#34;ref-zhan2021policy&#34; class=&#34;csl-entry&#34;&gt;
Zhan, Ruohan, Zhimei Ren, Susan Athey, and Zhengyuan Zhou. 2021. &lt;span&gt;“Policy Learning with Adaptively Collected Data.”&lt;/span&gt; &lt;em&gt;arXiv Preprint arXiv:2105.02344&lt;/em&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Nonparametric inference based on statistical depth</title>
      <link>https://youngstats.github.io/post/2022/10/03/nonparametric-inference-based-on-statistical-depth/</link>
      <pubDate>Mon, 03 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/10/03/nonparametric-inference-based-on-statistical-depth/</guid>
      <description>


&lt;p&gt;&lt;strong&gt;Nonparametric inference based on statistical depth&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Monday, October 17th, 7:00 PT / 10:00 ET / 16:00 CET&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-10-03-nonparametric-inference-based-on-statistical-depth_files/cover2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The notion of center of an object, be it a set of observations, a physical object or a random variable, is difficult to define. This motivated the development of general ways to measure centrality via depth functions. Such mappings allow for comparing relative centrality of two locations and, consequently, providing a center-outward ordering. Many such mappings have been introduced in the literature in recent decades and this is subject to intense research in, among other, nonparametric statistics and functional data analysis.&lt;/p&gt;
&lt;p&gt;In the webinar, selected statisticians will present their recent works and elaborate on different aspects of this topic.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Monday, October 17th, 7:00 PT / 10:00 ET / 16:00 CET&lt;/li&gt;
&lt;li&gt;Online, via Zoom. The registration form is available &lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSdsX2FS-Dpmris0W1yfUbwYOUdXfA5qoY2T4ujP5XAuLGLFwA/viewform?vc=0&amp;amp;c=0&amp;amp;w=1&amp;amp;flr=0&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://dimitri.konen.web.ulb.be/&#34;&gt;Dimitri Konen&lt;/a&gt;, Université Libre de Bruxelles (ULB), Belgium, &lt;embed&gt; link to &lt;a href=&#34;https://drive.google.com/file/d/10D3bNPuAmzuUzJmaL1tE7788p_G_pJTu/view&#34;&gt;Presentation&lt;/a&gt;. &lt;/embed&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www2.karlin.mff.cuni.cz/~nagy/index.php&#34;&gt;Stanislav Nagy&lt;/a&gt;, Faculty of Mathematics and Physics, Charles University, Czech Republic, &lt;embed&gt; link to &lt;a href=&#34;https://drive.google.com/file/d/1yrouPnanWb29fQxj7-8yDrZpM37_WNZa/view&#34;&gt;Presentation&lt;/a&gt;. &lt;/embed&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://perso.telecom-paristech.fr/mozharovskyi/&#34;&gt;Pavlo Mozharovskyi&lt;/a&gt;, LTCI, Telecom Paris, Institut Polytechnique de Paris, France&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant: &lt;a href=&#34;https://sites.google.com/site/germainvanbever&#34;&gt;Germain Van Bever&lt;/a&gt;, Université de Namur, Belgium, &lt;embed&gt; link to &lt;a href=&#34;https://drive.google.com/file/d/1L-hbQ8Y0rh0TXeS9muNoiyJzhRL7oI3J/view&#34;&gt;Presentation&lt;/a&gt;. &lt;/embed&gt;&lt;/p&gt;
&lt;p&gt;The webinar is part of YoungStatS project of the Young Statisticians Europe initiative (FENStatS) supported by the Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics (IMS).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you missed this webinar, you can &lt;a href=&#34;https://www.youtube.com/watch?v=FxSWsoOi5jE&#34;&gt;watch the recording on our YouTube channel&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Recent challenges in model specification testing based on different data structures</title>
      <link>https://youngstats.github.io/post/2022/09/29/recent-challenges-in-model-specification-testing-based-on-different-data-structures/</link>
      <pubDate>Thu, 29 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/09/29/recent-challenges-in-model-specification-testing-based-on-different-data-structures/</guid>
      <description>


&lt;p&gt;&lt;strong&gt;Recent challenges in model specification testing based on different
data structures&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Wednesday, November 9th, 8:00 PT / 11:00 ET / 17:00 CET&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-09-29-recent-challenges-in-model-specification-testing-based-on-different-data-structures_files/Slika1-horz.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Model specification testing is one of the essential methodological tasks
in statistics. Recently, with the development of different data
structures, envisioning concepts from classical data setups to other
environments becomes very important.&lt;/p&gt;
&lt;p&gt;In the webinar, selected young statisticians will present their recent
works and elaborate on different aspects of this topic.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wednesday, November 9th, 8:00 PT / 11:00 ET / 17:00 CET&lt;/li&gt;
&lt;li&gt;Online, via Zoom. The registration form is available
&lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLScwfeWQzP-LQjweX0G9dg50G9BzTmmdVqqpmwlFkG94_3pgpQ/viewform?vc=0&amp;amp;c=0&amp;amp;w=1&amp;amp;flr=0&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://warwick.ac.uk/fac/sci/statistics/staff/academic-research/berrett/&#34;&gt;Thomas
Berrett&lt;/a&gt;,
University of Warwick, United Kingdom: &lt;em&gt;Hypothesis testing under
local differential privacy&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.math.uni-frankfurt.de/~gerstenb/&#34;&gt;Julian G.
Gerstenberg&lt;/a&gt;, Goethe
Universität Frankfurt, Institute for Mathematics, Germany: &lt;em&gt;On the
exchangeability theory and its applications in non-parametric
statistics&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Marija-Cuparic&#34;&gt;Marija
Cuparić&lt;/a&gt;,
University of Belgrade, Serbia: &lt;em&gt;Goodness-of-fit tests for randomly
censored data&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Charl-Pretorius&#34;&gt;Charl
Pretorius&lt;/a&gt;,
University of the Free State, Bloemfontein, South Africa:
&lt;em&gt;Goodness-of-fit tests for multivariate&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;&lt;em&gt;-stable
distributions with application to MGARCH models&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant: &lt;a href=&#34;http://poincare.matf.bg.ac.rs/~bojana/en/&#34;&gt;Bojana
Milošević&lt;/a&gt;, University of
Belgrade, Serbia&lt;/p&gt;
&lt;p&gt;The webinar is part of YoungStatS project of the Young Statisticians
Europe initiative (FENStatS) supported by the Bernoulli Society for
Mathematical Statistics and Probability and the Institute of
Mathematical Statistics (IMS).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you missed this webinar, you can &lt;a href=&#34;https://www.youtube.com/watch?v=eupF0Zs_5sk&#34;&gt;watch the recording on our YouTube channel&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Non-Homogeneous Poisson Process Intensity Modeling and Estimation using Measure Transport</title>
      <link>https://youngstats.github.io/post/2022/09/19/non-homogeneous-poisson-process-intensity-modeling-and-estimation-using-measure-transport/</link>
      <pubDate>Mon, 19 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/09/19/non-homogeneous-poisson-process-intensity-modeling-and-estimation-using-measure-transport/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;A NHPP defined on &lt;span class=&#34;math inline&#34;&gt;\({\cal S} \subset \mathbb{R}^{d}\)&lt;/span&gt; can be fully
characterized through its intensity function &lt;span class=&#34;math inline&#34;&gt;\(\lambda: {\cal S} \rightarrow [0, \infty)\)&lt;/span&gt;.
We present a general model for the intensity function of a non-homogeneous Poisson
process using measure transport. The model finds its roots in transportation of
probability measure &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-Marzouk2016&#34; role=&#34;doc-biblioref&#34;&gt;Marzouk et al. 2016&lt;/a&gt;)&lt;/span&gt;, an approach that has gained
popularity recently for its ability to model arbitrary probability
density functions. The basic idea of this approach is to construct a
“transport map” between the complex, unknown, intensity function of
interest, and a simpler, known, reference intensity function.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Measure Transport.&lt;/strong&gt; Consider two probability measures &lt;span class=&#34;math inline&#34;&gt;\(\mu_0(\cdot)\)&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(\mu_1(\cdot)\)&lt;/span&gt; defined on &lt;span class=&#34;math inline&#34;&gt;\({\cal X}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\({\cal Z}\)&lt;/span&gt;, respectively. A
transport map &lt;span class=&#34;math inline&#34;&gt;\(T: {\cal X} \rightarrow {\cal Z}\)&lt;/span&gt; is said to push forward
&lt;span class=&#34;math inline&#34;&gt;\(\mu_0(\cdot)\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\mu_1(\cdot)\)&lt;/span&gt; (written compactly as
&lt;span class=&#34;math inline&#34;&gt;\(T_{\#\mu_0}(\cdot) = \mu_1(\cdot)\)&lt;/span&gt;) if and only if &lt;span class=&#34;math display&#34;&gt;\[\begin{eqnarray}
\label{transport_map} \mu_1(B) = \mu_0(T^{-1}(B)), \quad \mbox{for any
Borel subset } B \subset {\cal Z}. \end{eqnarray}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\({\cal X}, {\cal Z} \subseteq \mathbb{R}^{d}\)&lt;/span&gt;, and that both
&lt;span class=&#34;math inline&#34;&gt;\(\mu_0(\cdot)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mu_1(\cdot)\)&lt;/span&gt; are absolutely continuous with respect
to the Lebesgue measure on &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^{d}\)&lt;/span&gt;, with densities
&lt;span class=&#34;math inline&#34;&gt;\(d\mu_0(x) = f_0(x) d x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(d\mu_1(z) = f_1(z) d z\)&lt;/span&gt;, respectively.
Furthermore, assume that the map &lt;span class=&#34;math inline&#34;&gt;\(T(\cdot)\)&lt;/span&gt; is bijective differentiable
with a differentiable inverse &lt;span class=&#34;math inline&#34;&gt;\(T^{-1}(\cdot)\)&lt;/span&gt; (i.e., assume that
&lt;span class=&#34;math inline&#34;&gt;\(T(\cdot)\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(C^{1}\)&lt;/span&gt; diffeomorphism), then we have &lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\label{eq:transport} f_0(x) = f_1(T(x)) \|\mbox{det}(\nabla T(x))\|,
\quad x \in {\cal X}. \end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Triangular Map.&lt;/strong&gt; One particular type of transport map is an
, that is, &lt;span class=&#34;math display&#34;&gt;\[\begin{eqnarray}
\label{triangular_map} T(x) = (T^{(1)}(x^{(1)}), T^{(2)}(x^{(1)},
x^{(2)}), \ldots, T^{(d)}(x^{(1)}, \ldots, x^{(d)}))&amp;#39;,\quad x \in {\cal
X}, \end{eqnarray}\]&lt;/span&gt; where, for &lt;span class=&#34;math inline&#34;&gt;\(k = 1,\dots, d,\)&lt;/span&gt; one has that
&lt;span class=&#34;math inline&#34;&gt;\(T^{(k)}(x^{(1)}, \ldots, x^{(k)})\)&lt;/span&gt; is monotonically increasing in
&lt;span class=&#34;math inline&#34;&gt;\(x^{(k)}\)&lt;/span&gt;. In particular, the Jacobian matrix of an increasing
triangular map, if it exists, is triangular with positive entries on its
diagonal.&lt;/p&gt;
&lt;p&gt;Various approaches to parameterize an increasing triangular map have been proposed (see, for example, &lt;span class=&#34;citation&#34;&gt;Germain et al. (&lt;a href=&#34;#ref-Germain2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;,
&lt;span class=&#34;citation&#34;&gt;Dinh, Krueger, and Bengio (&lt;a href=&#34;#ref-Dinh2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Dinh, Sohl-Dickstein, and Bengio (&lt;a href=&#34;#ref-Dinh2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;). One class of parameterizations is based on
the so-called “conditional networks” (&lt;span class=&#34;citation&#34;&gt;Papamakarios, Pavlakou, and Murray (&lt;a href=&#34;#ref-Papamakarios2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; and
&lt;span class=&#34;citation&#34;&gt;Huang et al. (&lt;a href=&#34;#ref-Huang2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;): &lt;span class=&#34;math display&#34;&gt;\[\begin{eqnarray} T_1^{(1)}(x^{(1)})
&amp;amp;=&amp;amp; S_1^{(1)}(x^{(1)}; \theta_{11}), \nonumber \\ T_1^{(k)}(x^{(1)},
\ldots, x^{(k)}) &amp;amp;=&amp;amp; S_1^{(k)}( x^{(k)}; \theta_{1k}(x^{(1)}, \ldots,
x^{(k-1)}; \vartheta_{1k}) ), \quad k = 2,\dots,d, \end{eqnarray}\]&lt;/span&gt; for
&lt;span class=&#34;math inline&#34;&gt;\(x \in {\cal X}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\theta_{1k}(x^{(1)}, \ldots, x^{(k-1)}; \vartheta_{1k}), k = 2,\dots,d\)&lt;/span&gt; is the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;th “conditional
network” that takes &lt;span class=&#34;math inline&#34;&gt;\(x^{(1)}, \ldots, x^{(k-1)}\)&lt;/span&gt; as inputs and is
parameterized by &lt;span class=&#34;math inline&#34;&gt;\(\vartheta_{1k}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(S_1^{(k)}(\cdot)\)&lt;/span&gt; is generally a
very simple univariate function of &lt;span class=&#34;math inline&#34;&gt;\(x^{(k)}\)&lt;/span&gt;, but with parameters that
depend in a relatively complex manner on &lt;span class=&#34;math inline&#34;&gt;\(x^{(1)}, \ldots, x^{(k-1)}\)&lt;/span&gt;
through the network. The class of neural autoregressive flows, proposed
by &lt;span class=&#34;citation&#34;&gt;Huang et al. (&lt;a href=&#34;#ref-Huang2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;, offers more flexibility where the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th component
of the map has the form &lt;span class=&#34;math display&#34;&gt;\[\begin{eqnarray} \label{uni_flow} S_1^{(k)}(
x^{(k)}; \theta_{1k} ) = \sigma^{-1} \Big( \sum_{i=1}^{M} w_{1ki}
\sigma( a_{1ki} x^{(k)} + b_{1ki}) \Big), \end{eqnarray}\]&lt;/span&gt; where
&lt;span class=&#34;math inline&#34;&gt;\(\sigma^{-1}(\cdot)\)&lt;/span&gt; is the logit function, and &lt;span class=&#34;math inline&#34;&gt;\(w_{1ki}\)&lt;/span&gt; is subject to
the constraint &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^{M} w_{1ki} = 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Composition of Increasing Triangular Maps.&lt;/strong&gt; Composition of Increasing
Triangular Mapsn does not break the required bijectivity of &lt;span class=&#34;math inline&#34;&gt;\(T(\cdot)\)&lt;/span&gt;,
since a bijective function of a bijective function is itself bijective.
Computations also remain tractable, since the determinant of the
gradient of the composition is simply the product of the determinants of
the individual gradients. Specifically, consider two increasing
triangular maps &lt;span class=&#34;math inline&#34;&gt;\(T_1(\cdot)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(T_2(\cdot)\)&lt;/span&gt;, each constructed using
the neural network approach described above. The composition
&lt;span class=&#34;math inline&#34;&gt;\(T_2 \circ T_1 (\cdot)\)&lt;/span&gt; is bijective, and its gradient at some
&lt;span class=&#34;math inline&#34;&gt;\(x \in \cal X\)&lt;/span&gt; has determinant,
&lt;span class=&#34;math display&#34;&gt;\[ \mbox{det}(\nabla T_2 \circ T_1(x)) = (\mbox{det} (\nabla T_1(x)))(\mbox{det} (\nabla T_2(T_1(x)))).\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;intensity-modeling-and-estimation-via-measure-transport&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Intensity Modeling and Estimation via Measure Transport&lt;/h2&gt;
&lt;p&gt; Consider a NHPP &lt;span class=&#34;math inline&#34;&gt;\({\cal P}\)&lt;/span&gt; defined on a bounded
domain &lt;span class=&#34;math inline&#34;&gt;\({\cal X} \subset \mathbb{R}^{d}\)&lt;/span&gt;. The density of a Poisson
process with respect to the density of the unit rate Poisson process is
given by &lt;span class=&#34;math display&#34;&gt;\[\begin{align} f_{\lambda}(X) &amp;amp;= \exp(\|{\cal X}\| -
\mu_{\lambda}({\cal X})) \prod_{x \in X} \lambda(x) \nonumber \\ &amp;amp;=
\exp \Big(\int_{{\cal X}} (\lambda(x) - 1)dx + \sum_{x \in X}
\log \lambda(x) \Big) \label{fdensity}. \end{align}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(|B|\)&lt;/span&gt; denote
the Lebesgue measure of a bounded set &lt;span class=&#34;math inline&#34;&gt;\(B \subset \mathbb{R}^{d}\)&lt;/span&gt;, and
let &lt;span class=&#34;math inline&#34;&gt;\(X \equiv \{x_1, \ldots, x_n\},\)&lt;/span&gt; where
&lt;span class=&#34;math inline&#34;&gt;\(x_i \in {\cal X}, i = 1,\ldots,n,\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n \ge 1,\)&lt;/span&gt; be a realization of
&lt;span class=&#34;math inline&#34;&gt;\({\cal P}\)&lt;/span&gt;. The unknown intensity function is estimated by a maximum
likelihood approach, which is equivalent to minimizing the
Kullback–Leibler (KL) divergence &lt;span class=&#34;math inline&#34;&gt;\(D_{KL}(p\|\|q) = \int p(x) \log (p(x)/ q(x))dx\)&lt;/span&gt; between the true density and the estimate:
&lt;span class=&#34;math display&#34;&gt;\[
\hat{\lambda}(\cdot) = \arg\min_{\rho(\cdot) \in {\cal A} }{{D_{KL}(f_{\lambda}\|f_{\rho})}}
\]&lt;/span&gt; where
&lt;span class=&#34;math inline&#34;&gt;\({\cal A}\)&lt;/span&gt; is some set of intensity functions, and &lt;span class=&#34;math inline&#34;&gt;\(f_\rho(\cdot)\)&lt;/span&gt; is
the density of a NHPP with intensity function &lt;span class=&#34;math inline&#34;&gt;\(\rho(\cdot)\)&lt;/span&gt; taken with
respect to the density of the unit rate Poisson process.&lt;/p&gt;
&lt;p&gt;In order to apply the measure transport approach to intensity function
estimation, we first define &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\rho}(\cdot) = \rho(\cdot) / \mu_{\rho}({\cal X})\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\lambda}(x) = \lambda(x) / \mu_{\lambda}({\cal X}),\)&lt;/span&gt; so that
&lt;span class=&#34;math inline&#34;&gt;\(\tilde{\rho}(\cdot)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\lambda}(\cdot)\)&lt;/span&gt; are valid density
functions with respect to Lebesgue measure. The KL divergence
&lt;span class=&#34;math inline&#34;&gt;\(D_{KL}(f_{\lambda}\|f_{\rho})\)&lt;/span&gt; can be written in terms of process
densities as follows,
&lt;span class=&#34;math display&#34;&gt;\[
D_{KL}(f_{\lambda}\|f_{\rho}) = \mu_{\rho}({\cal X)} -
\mu_{\lambda}({\cal X}) \int_{{\cal X}} \tilde{\lambda}(x) \log
\tilde{\rho}(x)dx - \mu_{\lambda}({\cal X}) \log
\mu_{\rho}({\cal X}) + \textrm{const.},
\]&lt;/span&gt; where “const.” captures other terms not dependent on
&lt;span class=&#34;math inline&#34;&gt;\(\mu_{\rho}({\cal X})\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\rho}(\cdot)\)&lt;/span&gt;. After further
approximations, we obtain the following optimization problem
&lt;span class=&#34;math display&#34;&gt;\[
\hat{\tilde{\lambda}}(\cdot) = \arg\min_{\tilde{\rho}(\cdot) \in {\cal \tilde{A}}} {-{\sum_{i=1}^{n} \log \tilde{\rho}(x_{i})}},
\]&lt;/span&gt; where
now &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\cal A}\)&lt;/span&gt; is some set of process densities.&lt;/p&gt;
&lt;p&gt;We model the process density &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\rho}(\cdot)\)&lt;/span&gt; using the transportation of
probability measure approach. Specifically, we seek a diffeomorphism
&lt;span class=&#34;math inline&#34;&gt;\(T: {\cal X} \rightarrow {\cal Z}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\({\cal Z}\)&lt;/span&gt; need not be the
same as &lt;span class=&#34;math inline&#34;&gt;\({\cal X}\)&lt;/span&gt;, such that
&lt;span class=&#34;math display&#34;&gt;\[ \tilde{\rho}(x) = \eta(T(x)) | \mbox{det}\nabla T(x)|, \quad x \in {\cal X},\]&lt;/span&gt;where
&lt;span class=&#34;math inline&#34;&gt;\(\eta(\cdot)\)&lt;/span&gt; is some simple reference density on &lt;span class=&#34;math inline&#34;&gt;\({\cal Z}\)&lt;/span&gt;, and
&lt;span class=&#34;math inline&#34;&gt;\(|\mbox{det}\nabla T(\cdot)| &amp;gt; 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We prove that the increasing triangular maps constructed using neural autoregressive flows satisfy a
universal property in the context of probability density approximation.&lt;/p&gt;
&lt;p&gt;Theorem 1. Let &lt;span class=&#34;math inline&#34;&gt;\(\cal P\)&lt;/span&gt; be a non-homogeneous Poisson process with positive continuous process density
&lt;span class=&#34;math inline&#34;&gt;\(\tilde{\lambda}(\cdot)\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\({\cal X} \subset \mathbb{R}^{d}\)&lt;/span&gt;. Suppose
further that the weak (Sobolev) partial derivatives of &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\lambda}\)&lt;/span&gt;
up to order &lt;span class=&#34;math inline&#34;&gt;\(d+1\)&lt;/span&gt; are integrable over &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^{d}\)&lt;/span&gt;. There exists a
sequence of triangular mappings &lt;span class=&#34;math inline&#34;&gt;\((T_i(\cdot))_{i}\)&lt;/span&gt; wherein the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;th
component of each map &lt;span class=&#34;math inline&#34;&gt;\(T^{(k)}_i(\cdot)\)&lt;/span&gt; has the form above, and wherein the corresponding conditional networks
are universal approximators (e.g., feedforward neural networks with
sigmoid activation functions), such that
&lt;span class=&#34;math display&#34;&gt;\[ \eta(T_i(\cdot)) \mbox{det} (\nabla T_i(\cdot)) \rightarrow \tilde{\lambda}(\cdot), \]&lt;/span&gt;
with respect to the sup norm on any compact subset of &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^{d}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;illustration&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Illustration&lt;/h2&gt;
&lt;p&gt;We apply our method for intensity function estimation to an earthquake
data set comprising 1000 seismic events of body-wave magnitude (MB) over
4.0. The data set is available from the
package. The events we analyze are those that occurred near Fiji from
1964 onwards. The left panel of Figure 1 shows a scatter plot of locations of the observed seismic events.&lt;/p&gt;
&lt;p&gt;We fitted our model using a composition of five triangular maps. The estimated
intensity surface and the standard error surface obtained using are
shown in the middle and right panels of Figure 1, respectively. The probability that the intensity function exceeds
various threshold can also be estimated using non-parametric bootstrap
resampling; some examples of these exceedance probability plots are
shown in Figure 2.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-09-19-non-homogeneous-poisson-process-intensity-modeling-and-estimation-using-measure-transport_files/quake_scatter-horz.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 1. Left panel: Scatter plot of earthquake events with
body-wave magnitude greater than 4.0 near Fiji since 1964. Middle
panel: Estimated intensity function obtained using measure transport.
Right panel: Estimated standard error of the intensity surface.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-09-19-non-homogeneous-poisson-process-intensity-modeling-and-estimation-using-measure-transport_files/quake_exceed_1-horz.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 2. Left panel: Estimated exceedance probability
&lt;span class=&#34;math inline&#34;&gt;\(P(\lambda(\cdot) &amp;gt; 1)\)&lt;/span&gt;. Middle panel: Estimated exceedance
probability &lt;span class=&#34;math inline&#34;&gt;\(P(\lambda(\cdot) &amp;gt; 5)\)&lt;/span&gt;. Right panel: Estimated exceedance
probability &lt;span class=&#34;math inline&#34;&gt;\(P(\lambda(\cdot) &amp;gt; 10)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-Dinh2015&#34; class=&#34;csl-entry&#34;&gt;
Dinh, Laurent, David Krueger, and Yoshua Bengio. 2015. &lt;span&gt;“&lt;span&gt;NICE:&lt;/span&gt; Non-Linear Independent Components Estimation.”&lt;/span&gt; In &lt;em&gt;Workshop Track Proceedings of the 3rd International Conference on Learning Representations&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-Dinh2017&#34; class=&#34;csl-entry&#34;&gt;
Dinh, Laurent, Jascha Sohl-Dickstein, and Samy Bengio. 2017. &lt;span&gt;“Density Estimation Using Real &lt;span&gt;NVP&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;Conference Track Proceedings of the 5th International Conference on Learning Representations&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-Germain2015&#34; class=&#34;csl-entry&#34;&gt;
Germain, Mathieu, Karol Gregor, Iain Murray, and Hugo Larochelle. 2015. &lt;span&gt;“&lt;span&gt;MADE&lt;/span&gt;: Masked Autoencoder for Distribution Estimation.”&lt;/span&gt; In &lt;em&gt;Proceedings of the 32nd International Conference on Machine Learning&lt;/em&gt;, 881–89.
&lt;/div&gt;
&lt;div id=&#34;ref-Huang2018&#34; class=&#34;csl-entry&#34;&gt;
Huang, Chin-Wei, David Krueger, Alexandre Lacoste, and Aaron Courville. 2018. &lt;span&gt;“Neural Autoregressive Flows.”&lt;/span&gt; In &lt;em&gt;Proceedings of the 35th International Conference on Machine Learning&lt;/em&gt;, 80:2078–87. Proceedings of Machine Learning Research.
&lt;/div&gt;
&lt;div id=&#34;ref-Marzouk2016&#34; class=&#34;csl-entry&#34;&gt;
Marzouk, Youssef, Tarek Moselhy, Matthew Parno, and Alessio Spantini. 2016. &lt;span&gt;“Sampling via Measure Transport: An Introduction.”&lt;/span&gt; In &lt;em&gt;Handbook of Uncertainty Quantification&lt;/em&gt;, 1–41.
&lt;/div&gt;
&lt;div id=&#34;ref-Papamakarios2017&#34; class=&#34;csl-entry&#34;&gt;
Papamakarios, George, Theo Pavlakou, and Iain Murray. 2017. &lt;span&gt;“Masked Autoregressive Flow for Density Estimation.”&lt;/span&gt; In &lt;em&gt;Advances in Neural Information Processing Systems 30&lt;/em&gt;, 2338–47. &lt;a href=&#34;http://papers.nips.cc/paper/6828-masked-autoregressive-flow-for-density-estimation.pdf&#34;&gt;http://papers.nips.cc/paper/6828-masked-autoregressive-flow-for-density-estimation.pdf&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Macroeconomic Determinants of Migration. A Comparative Analysis For Old vs New European Member States</title>
      <link>https://youngstats.github.io/post/2022/09/18/macroeconomic-determinants-of-migration-a-comparative-analysis-for-old-vs-new-european-member-states/</link>
      <pubDate>Sun, 18 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/09/18/macroeconomic-determinants-of-migration-a-comparative-analysis-for-old-vs-new-european-member-states/</guid>
      <description>


&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Migration is a multifaceted phenomenon including macro-, meso-, and
micro-triggers that, when combined, decide an individual’s final
decision to migrate. Apart from being an essential component of
population change, migration is also an important component of
population estimates and labor market force estimations. Jennissen
(2004) argues that the presence of migrant populations has a favorable
impact on natural population growth since migrants’ age characteristics
and fertility rates are often greater than those of the indigenous
population. Furthermore, the migration problem is far more significant
for population growth in European nations, which are generally seeing a
drop in natural population increase. In this environment, it’s critical
to identify and quantify the factors involved in the decision to
migrate. Income disparities or income inequality, economic development,
the tax system, the economic cycle, the availability of new job
opportunities, unemployment, and other variables have been highlighted
as factors affecting migration by several migration theories (Kumpikaite
and Zickute, 2012). People migrate for a variety of reasons, including
improved living conditions or an escape from adverse circumstances in
their native country. This is the foundation of Lee’s (1966) push and
pull hypothesis, which is one of the primary neo-classical migration
hypotheses. Individuals are influenced by supply-push forces to leave
their home country, while demand-pull variables draw migrants to the
destination country.&lt;/p&gt;
&lt;p&gt;Panel data analysis is the study of datasets in which entities are
observed through time and allows for the management of missing variables
without having to examine them. By examining changes in the dependent
variable across time, one may rule out the impact of neglected factors
that fluctuate between entities but remain constant over time. The basic
premise is that if unobserved factors (such as those peculiar to
nations) influence the dependent variable yet stay constant over time,
the changes in the dependent variable must come from other sources.&lt;/p&gt;
&lt;p&gt;The notation for panel data will be the following:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} (x_{1,it}, x_{2,it}, ..., x_{k,it}, y_{it}), i=1,2, ..., n; t=1,2, ..., T \end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;em&gt;i&lt;/em&gt; is the subscript for the entity being observed (in our case
study the country) and &lt;em&gt;t&lt;/em&gt; is the subscript for the date at which the
entity is observed (in our case study the year). Using these notations
we would have data for the variables
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} x_1, x_2, ..., x_k, y \end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The fixed effects approach&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The fixed effects model would be written as:&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} y_{it}=\beta_1 x_{1,it} + \beta_2 x_{2,it} + \dots + \beta_k x_{k,it} + \alpha_i + u_{it} \end{equation}\]&lt;/span&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_{j,it}\)&lt;/span&gt; represents the value of regressor &lt;em&gt;j&lt;/em&gt;, for entity &lt;em&gt;i&lt;/em&gt; and
time period &lt;em&gt;t&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_j\)&lt;/span&gt; represents the coefficients of the independent variables,
that do not vary across individuals.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha_i\)&lt;/span&gt; represents the entity specific intercepts.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;The random effects approach&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;the unobserved variable specific to the individual entity is encompassed
in the error term. The entities will have a common mean value for the
intercept (let’s denote this with &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;) and the specific differences
in the intercept values of each country would be reflected in an error
term (denoted with &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i\)&lt;/span&gt;).&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} y_{it}=\alpha + \sum_{j=1}^{k} \beta_j x_{j,it} + \epsilon_i + u_{it} \end{equation}\]&lt;/span&gt;
&lt;p&gt;We will obtain a composite error term, which is composed of
&lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i\)&lt;/span&gt;, the individual (cross section) specific error &lt;span class=&#34;math inline&#34;&gt;\(u_{it}\)&lt;/span&gt; and
a combined cross section and time series error:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} w_{it}=\epsilon_i + u_{it} \end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thus substituting both equations we obtain:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} y_{it}=\alpha + \sum_{j=1}^{k} \beta_j x_{j,it} + w_{it} \end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The main benefit of using panel data related techniques is that
individual heterogeneity of individual entities (countries) can be
explicitly taken into account in panel data estimation; additionally,
panel data offers “more informative data, more variability, less
collinearity, more degrees of freedom, and more efficiency” by combining
time series and cross-sectional observations (Baltagi, 2001).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The database involved&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Crude Rate of Net Migration plus adjustment is the dependent
variable in both estimated models. It is calculated as the annual ratio
of net migration to the average population. It is expressed in terms of
1000 people (of the average population). The difference between the
total number of immigrants and the total number of emigrants is referred
to as net migration; statistical adjustments refers to adjusting net
migration by taking the difference between total population change and
natural change; the indicator roughly covers the difference between
inward and outward migration. In the model, the variable is called “Net
Migration.”&lt;/p&gt;
&lt;p&gt;The independent variables in the models are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unemployment: The long-term unemployment rate is the percentage of
people who have been jobless for more than a year out of the total
number of people who are working.&lt;/li&gt;
&lt;li&gt;Earnings (adjusted): The adjusted gross disposable income of
households divided by the PPP (purchasing power parities) of the actual
individual consumption of households and by the entire resident
population (in purchasing power standard (PPS) per inhabitant) yields
gross disposable income of households per capita.&lt;/li&gt;
&lt;li&gt;Gini Coefficient (of equivalised disposable income): A relationship
between the cumulative shares of the population disposed based on
equivalised disposable income and the cumulative share of the
equivalised total disposable income received by the population is
defined as the Gini Coefficient (of equivalised disposable income).&lt;/li&gt;
&lt;li&gt;Poverty: The at-risk-of-poverty rate is the proportion of people whose
equivalised disposable income is less than the risk-of-poverty
threshold, which is set at 60% of the national median equivalised
disposable income.&lt;/li&gt;
&lt;li&gt;Economic Freedom: A 0–10 scale that assesses the degree of economic
liberty in five key areas: government size, legal system, sound money,
international trade freedom, and regulation.&lt;/li&gt;
&lt;li&gt;Hospital beds: the number of beds available in hospitals; the variable
is calculated per 100,000 people.&lt;/li&gt;
&lt;li&gt;Health spending: the overall amount spent on health as a proportion of
GDP ( percent ).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Eurostat is the data source for all variables except Economic Freedom.
The Fraser Institute is the source of data for the Economic Freedom
Index. Data was collected for 25 EU nations over a period of 18 years,
from 2000 to 2017.&lt;/p&gt;
&lt;p&gt;Figure 1 depicts the dispersion of average Net Migration rates for EU
nations for the period 2000–2017. Lithuania (-8.84), Latvia (-7.19),
and Romania (-7.19) are the nations with the lowest average Net
Migration between 2000 and 2017. (-5.48). Bulgaria, Estonia, Poland, and
Slovakia all had negative Net Migration averages over the time period
studied. This was to be expected, given that these economies in Central
and Eastern Europe are generally migration-sending countries (Jennissen,
2004). Luxembourg, Spain, and Sweden, on the other hand, have the
greatest average Net Migration rates, indicating that they are typically
migrant-receiving nations. The Net Migration Rate’s behavior proposes
that the group of nations be divided into two: typically sending and
traditionally receiving countries.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-09-18-macroeconomic-determinants-of-migration-a-comparative-analysis-for-old-vs-new-european-member-states_files/Figure%201.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 1 – Average Net Migration for the period 2000 – 2017 for the EU
countries; source: author’s processing in Tableau, based on Eurostat data.&lt;/p&gt;
&lt;p&gt;Figure 2 depicts the significant relationship between real GDP/capita,
chain linked volume (2010) in Euro/capita, and net migration rate
(averages from 2000 to 2017 were used). Nations with lower GDP per
capita also have negative or low Net Migration rates, indicating that
“poorer” countries are more likely to be the source of migrant flows
(sending countries). Nations with greater GDP/capita, on the other hand,
have higher migration rates, implying that they are primarily receiving
countries for migrants.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-09-18-macroeconomic-determinants-of-migration-a-comparative-analysis-for-old-vs-new-european-member-states_files/Figure%202.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 2 – Net Migration rate vs. GDP/capita – comparison between old
and new EU member states; source: author’s processing in Tableau, based
on Eurostat data.&lt;/p&gt;
&lt;p&gt;The tendency of Net Migration for old and new member states warrants the
dataset being separated into two categories. It is clear from this
research that the group of New Member States acts as Migrant Sending
Countries for the whole time under consideration, while the Old Member
States operate as Migrant Receiving Countries. As a result, the
empirical application will be split into two parts: one model for the
Old member nations and then another model (Table 1) for the New member
states (Table 2).&lt;/p&gt;
&lt;p&gt;Table 1. Results of estimation for fixed and random effects models for
Old Member States; source: author’s own results&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;23%&#34; /&gt;
&lt;col width=&#34;38%&#34; /&gt;
&lt;col width=&#34;38%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Fixed Effects&lt;/th&gt;
&lt;th&gt;Random Effects&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Unemployment&lt;/td&gt;
&lt;td&gt;-1.0468*** (0.0903)&lt;/td&gt;
&lt;td&gt;-1.0295*** (0.0881)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Ln_Income&lt;/td&gt;
&lt;td&gt;4.9042** (2.4037)&lt;/td&gt;
&lt;td&gt;8.1458*** (1.5631)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Gini&lt;/td&gt;
&lt;td&gt;-0.1777 (0.1691)&lt;/td&gt;
&lt;td&gt;-0.3282** (0.1391)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Poverty&lt;/td&gt;
&lt;td&gt;0.6876*** (0.1657)&lt;/td&gt;
&lt;td&gt;0.7331*** (0.1417)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Economic Freedom&lt;/td&gt;
&lt;td&gt;-4.5181*** (1.5230)&lt;/td&gt;
&lt;td&gt;-2.9796*** (1.0923)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Health Expenditure&lt;/td&gt;
&lt;td&gt;-1.4840*** (0.4175)&lt;/td&gt;
&lt;td&gt;-1.7844*** (0.2965)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Intercept&lt;/td&gt;
&lt;td&gt;-1.6247 (25.7912)&lt;/td&gt;
&lt;td&gt;-40.0461** (17.8813)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;R2&lt;/td&gt;
&lt;td&gt;Within 0.4184 Between 0.6589 Overall 0.5056&lt;/td&gt;
&lt;td&gt;Within 0.4109 Between 0.8405 Overall 0.5828&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;F&lt;/td&gt;
&lt;td&gt;29.85***&lt;/td&gt;
&lt;td&gt;229.47*** (Wald chi2)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Corr (u_i, xb)&lt;/td&gt;
&lt;td&gt;0.2190&lt;/td&gt;
&lt;td&gt;0 (assumed)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Sigma_u&lt;/td&gt;
&lt;td&gt;2.0795&lt;/td&gt;
&lt;td&gt;1.3274&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Sigma_e&lt;/td&gt;
&lt;td&gt;3.0081&lt;/td&gt;
&lt;td&gt;3.0081&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Rho&lt;/td&gt;
&lt;td&gt;0.3233&lt;/td&gt;
&lt;td&gt;0.1629&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;*** Significant at 0.01; ** Significant at 0.05&lt;/p&gt;
&lt;p&gt;(standard errors of the coefficients are reported in parenthesis)&lt;/p&gt;
&lt;p&gt;Table 2. Results of estimation for fixed and random effects models for
New Member States; source: author’s own results&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;23%&#34; /&gt;
&lt;col width=&#34;38%&#34; /&gt;
&lt;col width=&#34;38%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Fixed Effects&lt;/th&gt;
&lt;th&gt;Random Effects&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Unemployment&lt;/td&gt;
&lt;td&gt;-0.3900** (0.1547)&lt;/td&gt;
&lt;td&gt;-0.3893*** (0.1461)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Gini&lt;/td&gt;
&lt;td&gt;-0.3527** (0.1377)&lt;/td&gt;
&lt;td&gt;-0.5432*** (0.1226)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Economic Freedom&lt;/td&gt;
&lt;td&gt;4.3493*** (1.1280)&lt;/td&gt;
&lt;td&gt;4.3283*** (1.0755)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Health Expenditure&lt;/td&gt;
&lt;td&gt;-1.1240* (0.5841)&lt;/td&gt;
&lt;td&gt;-0.4513 (0.4765)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Intercept&lt;/td&gt;
&lt;td&gt;-15.6762 (8.0909)&lt;/td&gt;
&lt;td&gt;-13.3742** (7.5791)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;R2&lt;/td&gt;
&lt;td&gt;Within 0.1663 Between 0.0205 Overall 0.0855&lt;/td&gt;
&lt;td&gt;Within 0.1483 Between 0.5198 Overall 0.3147&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;F&lt;/td&gt;
&lt;td&gt;8.28***&lt;/td&gt;
&lt;td&gt;38.90*** (Wald chi2)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Corr (u_i, xb)&lt;/td&gt;
&lt;td&gt;-0.1069&lt;/td&gt;
&lt;td&gt;0 (assumed)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Sigma_u&lt;/td&gt;
&lt;td&gt;3.9181&lt;/td&gt;
&lt;td&gt;2.2254&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Sigma_e&lt;/td&gt;
&lt;td&gt;3.5999&lt;/td&gt;
&lt;td&gt;3.5999&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Rho&lt;/td&gt;
&lt;td&gt;0.5422&lt;/td&gt;
&lt;td&gt;0.2765&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;*** Significant at 0.01; ** Significant at 0.05; * Significant at
0.1&lt;/p&gt;
&lt;p&gt;(standard errors of the coefficients are reported in parenthesis)&lt;/p&gt;
&lt;p&gt;The influence of macroeconomic factors on the Crude Rate Net Migration
of European economies was quantified using panel data regression models.
A preliminary study of the dependent variable’s distribution reveals
that the net migration rate differs across Old Member States
(migrant-receiving nations) and New Member States (sending countries for
migrants). As a result, we opt to estimate two models for the two sets
of nations, following Mihi-Ramirez et al. (2017) method’s. Unemployment
rate, per capita income, Gini coefficient, poverty rate, Economic
Freedom Index, and two other characteristics relating to the health
system were included as independent variables (number of beds in
hospitals and health expenditure as percentage in GDP). The analysed
period was 2000 – 2017.&lt;/p&gt;
&lt;p&gt;The findings corroborated migratory economic theory. In terms of the
labor market, unemployment is a large and powerful supply push factor
for migration. Only for the Old Member states does income appear as a
key influence, validating the neo-classical economic theory of
migration, which claims that variations in earnings across nations are
one of the main factors driving labor movement (Massey et al, 1993).
Moving on to the social component, the Gini coefficient has been
established as a strong driving force behind migration in both Old and
New Member States. Poverty appears to be a factor with reduced
explanatory power, with a positive coefficient for the Old member states
and no significance for the New member states.&lt;/p&gt;
&lt;p&gt;In terms of Economic Freedom, the factor has a considerable beneficial
impact on net migration rates only in the New Member States.&lt;/p&gt;
&lt;p&gt;Furthermore, health-related macroeconomic factors were included in the
model, as well as the circular cumulative causation hypothesis, which
states that variations in standard of living across nations are the
primary cause of migration (Massey et al. 1993). The health system, on
the other hand, could not be proven to be a factor of migration.&lt;/p&gt;
&lt;p&gt;Because international migration has such a substantial impact on
European population dynamics, understanding and analyzing the factors
that influence international migration is critical. The findings of this
study might be utilized not just to produce migration forecasts, but
also to establish migration policies that would improve migrants’ labor
and social integration.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Smaranda Cimpoeru, PhD&lt;/p&gt;
&lt;p&gt;Department of Statistics and Econometrics, The Bucharest University of
Economic Studies&lt;/p&gt;
&lt;p&gt;E-mail:
&lt;a href=&#34;mailto:smaranda.cimpoeru@csie.ase.ro&#34; class=&#34;email&#34;&gt;smaranda.cimpoeru@csie.ase.ro&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
