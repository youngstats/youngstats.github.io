<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>YoungStatS</title>
    <link>https://youngstats.github.io/</link>
    <description>Recent content on YoungStatS</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 19 Apr 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://youngstats.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Theory and Methods for Inference in Multi-armed Bandit Problems</title>
      <link>https://youngstats.github.io/post/2022/04/19/theory-and-methods-for-inference-in-multi-armed-bandit-problems/</link>
      <pubDate>Tue, 19 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/04/19/theory-and-methods-for-inference-in-multi-armed-bandit-problems/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;Theory and Methods for Inference in Multi-armed Bandit Problems&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-04-19-theory-and-methods-for-inference-in-multi-armed-bandit-problems_files/Slika1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Multi-armed bandit (MAB) algorithms have been argued for decades as useful to conduct adaptively-randomized experiments. By skewing the allocation of the arms towards the more efficient or informative ones, they have the potential to enhance participants’ welfare, while resulting in a more flexible, efficient, and ethical alternative compared to traditional fixed studies. However, such allocation strategies complicate the problem of statistical inference. It is now recognized that traditional inference methods are typically not valid when used in MAB-collected data, leading to considerable biases in classical estimators and other relevant issues in hypothesis testing problems.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wednesday, May 11th, 7:00 PT / 10:00 EST / 16:00 CET.&lt;/li&gt;
&lt;li&gt;Online, via Zoom. The registration form is available &lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSexdudNXI46npEnpQZ0IwmcUNejJ8wSeZBjw1lNU4CftaFwUA/viewform?vc=0&amp;amp;c=0&amp;amp;w=1&amp;amp;flr=0&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.google.com/view/anandkalvit&#34;&gt;Anand Kalvit&lt;/a&gt;, Columbia University: “A Closer Look at the Worst-case Behavior of Multi-armed Bandit Algorithms”&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.stat.cmu.edu/~aramdas/&#34;&gt;Aaditya Ramdas&lt;/a&gt;, Carnegie Mellon University: “Safe, Anytime-Valid Inference in the face of 3 sources of bias in bandit data analysis”&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ruohanzhan.github.io/&#34;&gt;Ruohan Zhan&lt;/a&gt;, Stanford University: “Inference on Adaptively Collected Data”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prof. &lt;a href=&#34;https://www0.gsb.columbia.edu/faculty/azeevi/&#34;&gt;Assaf Zeevi&lt;/a&gt;, Columbia University&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The webinar is part of YoungStatS project of the Young Statisticians Europe initiative (FENStatS) supported by the Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics (IMS).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Selection of Priors in Bayesian Structural Equation Modeling</title>
      <link>https://youngstats.github.io/post/2022/02/14/selection-of-priors-in-bayesian-structural-equation-modeling/</link>
      <pubDate>Mon, 14 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/02/14/selection-of-priors-in-bayesian-structural-equation-modeling/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;Selection of Priors in Bayesian Structural Equation Modelling&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-02-14-selection-of-priors-in-bayesian-structural-equation-modeling_files/Cover2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Structural equation modeling (SEM) is an important framework within the social sciences that encompasses a wide variety of statistical models. Traditionally, estimation of SEMs has relied on maximum likelihood. Unfortunately, there also exist a variety of situations in which maximum likelihood performs subpar. This led researchers to turn to alternative estimation methods, in particular, Bayesian estimation of SEMs or BSEM. However, it is currently unclear how to specify the prior distribution in order to attain the advantages of Bayesian approaches.&lt;/p&gt;
&lt;p&gt;On the webinar, selected statisticians will present their recent works on the topic.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wednesday, April 20th, 7:00 PT / 10:00 ET / 16:00 Berlin/Amsterdam&lt;/li&gt;
&lt;li&gt;Online, via Zoom. The registration form is available
&lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSe-BAMdH2dhuON-kxKpVW3OgtnO2Qd6MWKlyUoaF_udbp5X2Q/viewform?vc=0&amp;amp;c=0&amp;amp;w=1&amp;amp;flr=0&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.mcgill.ca/psychology/milica-miocevic&#34;&gt;Milica Miočević&lt;/a&gt;, McGill University, Canada&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://winterstat.github.io/&#34;&gt;Sonja D. Winter&lt;/a&gt;, University of Missouri, USA&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://research.vu.nl/en/persons/mauricio-garnier-villarreal&#34;&gt;Mauricio Garnier-Villarreal&lt;/a&gt;, Vrije Universiteit Amsterdam, The Netherlands&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://saravanerp.com/&#34;&gt;Sara van Erp&lt;/a&gt;, Utrecht University, The Netherlands&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The webinar is part of YoungStatS project of the Young Statisticians Europe initiative (FENStatS) supported by the Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics (IMS).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Recent Advances in Approximate Bayesian Inference</title>
      <link>https://youngstats.github.io/post/2022/02/08/recent-advances-in-approximate-bayesian-inference/</link>
      <pubDate>Tue, 08 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/02/08/recent-advances-in-approximate-bayesian-inference/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;Recent Advances in Approximate Bayesian Inference&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-02-09-recent-advances-in-approximate-bayesian-inference_files/featured.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In approximate Bayesian computation, likelihood function is intractable and needs to be itself estimated using forward simulations of the statistical model (Beaumont et al., 2002; Marin et al., 2012; Sisson et al., 2019; Martin et al., 2020). Recent years have seen numerous advances in approximate inference methods, which have enabled Bayesian inference in increasingly challenging scenarios involving complex probabilistic models and large datasets.&lt;/p&gt;
&lt;p&gt;On the webinar, selected young statisticians will present their recent works on the topic.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wednesday, June 15th, 7:00 PT / 10:00 EST / 16:00 CET&lt;/li&gt;
&lt;li&gt;Online, via Zoom. The registration form is available &lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSdd1MztoBmjD8BXFzyURnSPoGolVnojTCoaZMgbQ4ooN5Fybw/viewform&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.lorenzopacchiardi.me/&#34;&gt;Lorenzo Pacchiardi&lt;/a&gt;, University of Oxford, United Kingdom&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.stats.ox.ac.uk/~pompe/&#34;&gt;Emilia Pompe&lt;/a&gt;, University of Oxford, United Kingdom&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.mimuw.edu.pl/~lr306321/&#34;&gt;Łukasz Rajkowski&lt;/a&gt;, University of Warsaw, Poland&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://theomoins.github.io/aboutme/&#34;&gt;Théo Moins&lt;/a&gt;, Inria Grenoble Rhône-Alpes, France&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.julyanarbel.com/&#34;&gt;Julyan Arbel&lt;/a&gt;, Inria Grenoble Rhône-Alpes, France&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The webinar is part of YoungStatS project of the Young Statisticians Europe initiative (FENStatS) supported by the Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics (IMS).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Recent Advancements in Applied Instrumental Variable Methods</title>
      <link>https://youngstats.github.io/post/2022/02/07/recent-advancements-in-applied-instrumental-variable-methods/</link>
      <pubDate>Mon, 07 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/02/07/recent-advancements-in-applied-instrumental-variable-methods/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;Recent Advancements in Applied Instrumental Variable Methods&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-02-09-recent-advancements-in-applied-instrumental-variable-methods_files/featured.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Instrumental variables (IV) is one of most important and widespread research designs in economics and statistics, as it can identify causal effects in the presence of unobserved confounding. Over the past 30 years the science of IV has advanced considerably, in part through the contributions of Nobel Laureates Joshua Angrist, Guido Imbens, and James Heckman. Recent years have brought significant advances in how IV is applied, in shift-share designs, with judge or examiner instruments, and in settings with rich or complex controls.&lt;/p&gt;
&lt;p&gt;In this webinar, selected econometricians will present their recent work on applied IV methods.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wednesday, June 29th, 8:00 PT / 11:00 EST / 17:00 CET&lt;/li&gt;
&lt;li&gt;Online, via Zoom. The registration form is available
&lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSe3w6IjY-oHDeRGT5HqE5_RgcZ0YGJNpTq5KDeyAdY77GqmKQ/viewform&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.google.com/view/borusyak/home&#34;&gt;Kirill Borusyak&lt;/a&gt;, University College London, United Kingdom: on shift-share IV and other composite instruments&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://som.yale.edu/faculty/paul-goldsmith-pinkham&#34;&gt;Paul Goldsmith-Pinkham&lt;/a&gt;, Yale University, USA: on judge/examiner instruments&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.publichealth.columbia.edu/people/our-faculty/sc4501&#34;&gt;Stephen C. Coussens&lt;/a&gt;, Columbia University, USA: on efficient estimation from compliance weighting&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://economics.uchicago.edu/directory/alexander-torgovitsky&#34;&gt;Alexander Torgovitsky&lt;/a&gt;, University of Chicago, USA: on when two-stage least squares estimates local average treatment effects&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://about.peterhull.net/&#34;&gt;Peter Hull&lt;/a&gt;, Brown University, USA&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The webinar is part of YoungStatS project of the Young Statisticians
Europe initiative (FENStatS) supported by the Bernoulli Society for
Mathematical Statistics and Probability and the Institute of
Mathematical Statistics (IMS).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Measuring dependence in the Wasserstein distance for Bayesian nonparametric models</title>
      <link>https://youngstats.github.io/post/2022/01/17/measuring-dependence-in-the-wasserstein-distance-for-bayesian-nonparametric-models/</link>
      <pubDate>Mon, 17 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/01/17/measuring-dependence-in-the-wasserstein-distance-for-bayesian-nonparametric-models/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;Bayesian nonparametric (BNP) models are a prominent tool for performing flexible inference with a natural quantification of uncertainty. Traditionallly, flexible inference within a &lt;strong&gt;homogeneous&lt;/strong&gt; sample is performed with &lt;strong&gt;exchangeable&lt;/strong&gt; models of the type &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots, X_n|\tilde \mu \sim T(\tilde \mu)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\tilde \mu\)&lt;/span&gt; is a random measure and &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; is a suitable transformation. Notable examples for &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; include normalization for random probabilities (Regazzini et al., 2003), kernel mixtures for densities (Lo, 1984) and for hazards (Dykstra and Laud, 1981; James, 2005), exponential transformations for survival functions (Doksum, 1974) and cumulative transformations for cumulative hazards (Hjort, 1990).&lt;/p&gt;
&lt;p&gt;Very often, though, the data presents some structural &lt;strong&gt;heterogeneity&lt;/strong&gt; one should carefully take into account, especially when analyzing data from different sources that are related in some way. For instance this happens in the study of clinical trials of a COVID-19 vaccine in different countries or when understanding the effects of a certain policy adopted by multiple regions. In these cases, besides modeling heterogeneity, one further aims at introducing some probabilistic mechanism that allows for &lt;strong&gt;borrowing information&lt;/strong&gt; across different studies. To achieve this goal, in the last 20 years a wealth of BNP models that rely on &lt;strong&gt;partial exchangeability&lt;/strong&gt; have been proposed (see Cifarelli &amp;amp; Regazzini (1978) for prioneering ideas, MacEachern (1999, 2000) for seminal results and Quintana et al. (2021) for a recent review). The general recipe consists in assigning a different random measure &lt;span class=&#34;math inline&#34;&gt;\(\tilde \mu_i\)&lt;/span&gt; to every data source or group of observations and then introduce flexible forms of dependence between the random measures. For simplicity we stick to the case of two groups, so that the partially exchangeable models are of the general form
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\tag{1}
X_1,\dots,X_{n_1}| (\tilde \mu_1, \tilde \mu_2) \stackrel{\text{iid}}{\sim} T(\tilde \mu_1); \qquad \qquad
Y_1, \dots, Y_{n_2} | (\tilde \mu_1, \tilde \mu_2) \stackrel{\text{iid}}{\sim} T(\tilde \mu_2);
\end{equation}\]&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\((\tilde \mu_1, \tilde \mu_2) \sim Q\)&lt;/span&gt; is a vector of dependent random measures. The borrowing of information across different groups is regulated by the amount of dependence between random measures. One can picture &lt;strong&gt;two extreme situations&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When the random measures are completely dependent, that is, &lt;span class=&#34;math inline&#34;&gt;\(\tilde \mu_1 = \tilde \mu_2\)&lt;/span&gt; almost surely, the observations are &lt;strong&gt;fully exchangeable&lt;/strong&gt; and there is no distinction between the different groups. In such case we denote the random measures as &lt;span class=&#34;math inline&#34;&gt;\((\tilde \mu_1^\text{ex}, \tilde \mu_2^\text{ex})\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;When the random measures are independent, the two groups of (exchangeable) observations are &lt;strong&gt;independent&lt;/strong&gt; and thus do not interact.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thus, it is crucial to translate one’s prior beliefs on the dependence structure into the specification of the model. Interestingly, though there have been many proposals on how to model dependence, few results on how to measure it are available. The current state-of-the-art is given by the pairwise linear correlation between &lt;span class=&#34;math inline&#34;&gt;\(T(\tilde \mu_1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(T(\tilde \mu_2)\)&lt;/span&gt;, which provides a useful proxy based on the first two moments of the random measures, but it does not take into account their whole infinite-dimensional structure. We thus look for a measure of dependence with the following properties:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;It is &lt;strong&gt;model non-specific&lt;/strong&gt; (i.e., it does not depend on the choice of transformation &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;);&lt;/li&gt;
&lt;li&gt;It is based on the &lt;strong&gt;whole infinite-dimensional distribution&lt;/strong&gt; of BNP priors;&lt;/li&gt;
&lt;li&gt;It can be naturally extended to &lt;strong&gt;more than 2 groups&lt;/strong&gt; of observations;&lt;/li&gt;
&lt;li&gt;It can be &lt;strong&gt;strictly bounded&lt;/strong&gt; in terms of the &lt;strong&gt;hyperparameters&lt;/strong&gt; of BNP models.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We point out that this last property is fundamental because it allows to fix the hyperparameters of BNP models so to achieve the desired level of dependence.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;proposal&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Proposal&lt;/h2&gt;
&lt;p&gt;Our proposal is based on two main ideas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In partially exchangeable models (1) all the dependence between different groups of observations is introduced at the level of the random measures. Thus, we can measure the dependence directly &lt;strong&gt;in terms of the underlying vector of random measures&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\((\tilde \mu_1, \tilde \mu_2)\)&lt;/span&gt;. This ensures Property 1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Complete dependence (&lt;span class=&#34;math inline&#34;&gt;\(\tilde \mu_1^{\text{ex}} = \mu_2^{\text{ex}}\)&lt;/span&gt; almost surely) coincides with the full exchangeability of the observations, which can thus be pictured as a degenerate distribution in the space of joint distributions &lt;span class=&#34;math inline&#34;&gt;\(\{ \mathcal{L}(\tilde \mu_1, \tilde \mu_2) | \mathcal{L}(\tilde \mu_1) = \mathcal{L}(\tilde \mu_2) \}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L}\)&lt;/span&gt; denotes the probability law of a random object (Figure 1). A natural way to measure dependence of any other joint distribution &lt;span class=&#34;math inline&#34;&gt;\((\tilde \mu_1, \tilde \mu_2)\)&lt;/span&gt; is then to measure the distance from the extreme case &lt;span class=&#34;math inline&#34;&gt;\((\tilde \mu_1^{\text{ex}}, \tilde \mu_2^{\text{ex}})\)&lt;/span&gt;. This ensures Property 2 and Property 3. Informally, we refer to the &lt;strong&gt;distance from exchangeability&lt;/strong&gt;, with the underlying idea that the observations in (1) are partially exchangeable and, under complete dependence of the random measures, one retrieves full exchangeability of the observations.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-01-17-measuring-dependence-in-the-wasserstein-distance-for-bayesian-nonparametric-models_files/1.png&#34; /&gt;&lt;/p&gt;
&lt;center&gt;
&lt;em&gt;Figure 1.&lt;/em&gt;
&lt;/center&gt;
&lt;p&gt;The choice of the distance on the laws of vectors of random measures is very delicate. On one hand we look for an intuitive notion of distance with a natural geometrical interpretation, on the other hand we also want to show Property 4, and these are clearly two competing aspects. We define
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\tag{2}
d_{\mathcal{W}}\bigg( \mathcal{L}\begin{pmatrix} \tilde \mu_1 \\ \tilde \mu_2 \end{pmatrix}, \mathcal{L} \begin{pmatrix} \tilde \mu_1^{\text{ex}} \\ \tilde \mu_2^{\text{ex}} \end{pmatrix} \bigg) = \sup_A W\bigg( \mathcal{L}\begin{pmatrix} \tilde \mu_1(A) \\ \tilde \mu_2(A) \end{pmatrix},  \mathcal{L} \begin{pmatrix} \tilde \mu_1^{\text{ex}}(A) \\ \tilde \mu_2^{\text{ex}} (A)\end{pmatrix} \bigg),
\end{equation}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; is the &lt;strong&gt;Wasserstein distance&lt;/strong&gt; (of order 2) between probability distributions in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^2\)&lt;/span&gt;. We briefly recall that if &lt;span class=&#34;math inline&#34;&gt;\(C(P,Q)\)&lt;/span&gt; = &lt;span class=&#34;math inline&#34;&gt;\(\{(X,Y): \mathcal{L}(X) = P; \mathcal{L}(Y) = Q\}\)&lt;/span&gt; denotes the set of &lt;em&gt;couplings&lt;/em&gt; between two probabilities &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;, then
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\tag{3}
W(P,Q)^2 = \inf_{(X,Y) \in C(P,Q)} \mathbb{E} \|X-Y\|^2.
\end{equation}\]&lt;/span&gt;
Because of its strong geometric properties, the Wasserstein distance is ideal to compare distributions with different support as in our case (see Figure 1). Indeed, other common distances or divergences would not provide an informative notion of discrepancy in this context (e.g. total variation distance, Hellinger distance, KL-divergence). As it is often the case, there is a trade-off between the naturalness of a distance and our ability to find closed forms expressions. In particular, we have to deal with the following challenges:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In general one can prove that the infimum in (3) is a minimum, that is, that there exists an &lt;em&gt;optimal coupling&lt;/em&gt;. Nonetheless, since we are dealing with probabilities in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^d\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(d&amp;gt;1\)&lt;/span&gt;, a general expression for the &lt;strong&gt;optimal coupling is not available&lt;/strong&gt; with very few exceptions (e.g., between multivariate normal distributions).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Even if one finds the optimal coupling, the expression of the Wasserstein distance amounts to evaluating a (difficult) &lt;strong&gt;multivariate integral&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Many noteworthy models in the BNP literature are based on dependent random measures with jointly independent vectors which, in analogy with the 1-dimensional case, we call &lt;em&gt;completely random vectors&lt;/em&gt;. The law of such vectors is specified in an indirect way through a &lt;strong&gt;multivariate Lévy measure&lt;/strong&gt;. Thus, we need to develop strict bounds in terms of the Lévy measures to find closed form expressions that depend on the hyperparameters of the models (Property 4).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;In our work (Catalano et al., 2021) we face the challenges above and obtain the following results:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;When dealing with completely random vectors, &lt;span class=&#34;math inline&#34;&gt;\((\tilde \mu_1(A), \tilde \mu_2(A))\)&lt;/span&gt; can always be approximated with compound Poisson (CP) distributions. We find upper bounds of the Wasserstein distance between completely random vectors in terms of the Wasserstein distance between the jumps of the corresponding CP approximations, whose law is expressed &lt;strong&gt;directly in terms of the Lévy measures&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We find a &lt;strong&gt;general expression for the optimal coupling&lt;/strong&gt; between the jumps of the CP approximation of &lt;span class=&#34;math inline&#34;&gt;\((\tilde \mu_1, \tilde \mu_2)\)&lt;/span&gt; and the ones of the CP approximation of &lt;span class=&#34;math inline&#34;&gt;\((\tilde \mu_1^\text{ex}, \tilde \mu_2^{\text{ex}})\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Once we have found the optimal coupling, the Wasserstein distance between the jumps of the CP approximations amounts to the evaluation of a multivariate integral. We develop &lt;strong&gt;case-by-case&lt;/strong&gt; analyses to evaluate these &lt;strong&gt;multivariate integrals&lt;/strong&gt; numerically.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Putting these three elements together, we are able to derive strict upper bounds of the distance in (2) for &lt;strong&gt;noteworthy models in the BNP literature&lt;/strong&gt;, which include compound random measures (Griffin &amp;amp; Leisen, 2017; Riva-Palacio &amp;amp; Leisen, 2019), Clayton Lévy copula (Tankov, 2003; Epifani &amp;amp; Lijoi, 2010) and additive models (Griffiths &amp;amp; Milne, 1978; Lijoi et al., 2014). These upper bounds are expressed in terms of the hyperparameters of the corresponding models (see Figure 2) and thus enable a principled prior specification of their dependence structure.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-01-17-measuring-dependence-in-the-wasserstein-distance-for-bayesian-nonparametric-models_files/2.png&#34; /&gt;&lt;/p&gt;
&lt;center&gt;
&lt;em&gt;Figure 2. On the left: distance from exchangeability for additive random measures. On the right: distance from exchangeability for compound random measures.&lt;/em&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;div id=&#34;discussion-and-further-work&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Discussion and further work&lt;/h2&gt;
&lt;p&gt;We highlight two take-home messages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We can use the Wasserstein distance to build a natural and tractable distance on a wide class of (vectors of) random measures. This opens the way to many possible uses of a distance between infinite dimensional random structures, going beyond the measurement of dependence.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A natural way to measure the dependence between infinite-dimensional random quantities is to evaluate the distance from an extreme situation, as the one of complete dependence.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In Catalano et al (2021) we have moved the first steps. Yet, there are a number of questions that still remain open. In particular,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We measure dependence in terms of distance from the extreme situation of complete dependence. How can we simultaneously quantify the &lt;strong&gt;discrepancy from&lt;/strong&gt; the other extreme, that is, &lt;strong&gt;independence&lt;/strong&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Using the distance from exchangeability to measure dependence is useful for relative comparisons between dependence structures (“&lt;span class=&#34;math inline&#34;&gt;\(\tilde \mu_1\)&lt;/span&gt; is more dependent than &lt;span class=&#34;math inline&#34;&gt;\(\tilde \mu_2\)&lt;/span&gt;”) but prevents absolute quantifications of dependence (“&lt;span class=&#34;math inline&#34;&gt;\(\tilde \mu\)&lt;/span&gt; has an intermediate dependence structure”). In order to extend the measure in this direction we need to find the &lt;strong&gt;maximum&lt;/strong&gt; possible value of the &lt;strong&gt;distance from exchangeability&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In Catalano et al (2022+) we deal with these questions. Their answer sheds light into the deep geometric properties of the Wasserstein distance, which are key to the definition of a &lt;strong&gt;Wasserstein Index of Dependence&lt;/strong&gt; in [0,1]: our goal since the beginning. This opens interesting research directions on how to use the index to assess heterogeneity in the data, going beyond the restrictive assumption of independence.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Catalano, M., Lavenant, H., Lijoi, A., Prünster, I. (2022+). A Wasserstein Index of Dependence for Random Measures. arXiv:2109.06646.&lt;/p&gt;
&lt;p&gt;Catalano, M., A. Lijoi, and I. Prünster (2021). Measuring dependence in the Wasserstein distance for Bayesian nonparametric models. Ann. Statist. 49(5), 2916–2947.&lt;/p&gt;
&lt;p&gt;Cifarelli, D.M. &amp;amp; Regazzini, E. (1978). Nonparametric statistical problems under partial exchangeability: The role of associative means. Technical Report.&lt;/p&gt;
&lt;p&gt;Doksum, K. (1974). Tailfree and neutral random probabilities and their posterior distributions. The Annals of Probability,
2(2):183–201.&lt;/p&gt;
&lt;p&gt;Dykstra, R. L. and Laud, P. (1981). A Bayesian nonparametric approach to reliability. The Annals of Statistics, 9(2):356– 367.&lt;/p&gt;
&lt;p&gt;Epifani, I. and Lijoi, A. (2010). Nonparametric priors for vectors of survival functions. Statist. Sinica 20 1455–1484.&lt;/p&gt;
&lt;p&gt;Hjort, N. L. (1990). Nonparametric Bayes estimators based on beta processes in models for life history data. The Annals of Statistics, 18(3):1259–1294.&lt;/p&gt;
&lt;p&gt;Griffin, J. E. &amp;amp; Leisen, F. (2017). Compound random measures and their use in bayesian non-parametrics. J. Royal Stat. Soc. Ser. B 79, 525–545.&lt;/p&gt;
&lt;p&gt;Griffiths, R. C. and Milne, R. K. (1978). A class of bivariate Poisson processes. J. Multivariate Anal. 8 380–395.&lt;/p&gt;
&lt;p&gt;James, L. F. (2005). Bayesian Poisson process partition calculus with an application to Bayesian Lévy moving averages. The Annals of Statistics, 33(4):1771–1799.&lt;/p&gt;
&lt;p&gt;Lijoi,A., Nipoti,B. and Prünster, I. (2014). Bayesian inference with dependent normalized completely random measures. Bernoulli 20 1260–1291.&lt;/p&gt;
&lt;p&gt;MacEachern, S. N. (1999). Dependent nonparametric processes. in ASA Proceedings of the Section on Bayesian Statistical Science, Alexandria, VA: American Statistical Association.&lt;/p&gt;
&lt;p&gt;MacEachern, S. N. (2000). Dependent Dirichlet processes. Tech. Report, Ohio State University.&lt;/p&gt;
&lt;p&gt;Riva Palacio, A. and Leisen, F. (2021). Compound vectors of subordinators and their associated positive Lévy copulas. J. Multivariate Anal. 183.&lt;/p&gt;
&lt;p&gt;Quintana, F. A., Müller, P., Jara, A., and MacEachern, S. N. (2021). The dependent Dirichlet process and related models. Statistical Science, to appear.&lt;/p&gt;
&lt;p&gt;Tankov, P. (2003). Dependence structure of spectrally positive multidimensional Lévy processes. Unpub- lished Manuscript.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;authors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Authors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://martacatalano.github.io/&#34;&gt;Marta Catalano&lt;/a&gt; is Assistant Professor in the Department of Statistics at the University of Warwick.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mypage.unibocconi.eu/antoniolijoi/&#34;&gt;Antonio Lijoi&lt;/a&gt; is Professor in the Department of Decision Sciences at Bocconi University.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mypage.unibocconi.eu/igorpruenster/&#34;&gt;Igor Prünster&lt;/a&gt; is Professor in the Department of Decision Sciences at Bocconi University.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Universal estimation with Maximum Mean Discrepancy (MMD)</title>
      <link>https://youngstats.github.io/post/2022/01/13/universal-estimation-with-maximum-mean-discrepancy-mmd/</link>
      <pubDate>Thu, 13 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/01/13/universal-estimation-with-maximum-mean-discrepancy-mmd/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;em&gt;This is an updated version of a blog post on RIKEN AIP Approximate Bayesian Inference team webpage:&lt;/em&gt; &lt;a href=&#34;https://team-approx-bayes.github.io/blog/mmd/&#34; class=&#34;uri&#34;&gt;https://team-approx-bayes.github.io/blog/mmd/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-01-13-universal-estimation-with-maximum-mean-discrepancy-mmd_files/cover2.png&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;INTRODUCTION&lt;/h2&gt;
&lt;p&gt;A very old and yet very exciting problem in statistics is the definition of a universal estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt;. An estimation procedure that would work all the time. Close your eyes, push the button, it works, for any model, in any context.&lt;/p&gt;
&lt;p&gt;Formally speaking, we want that for some metric &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; on probability distributions, for any statistical model &lt;span class=&#34;math inline&#34;&gt;\((P_\theta,\theta\in\Theta)\)&lt;/span&gt;, given &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt; drawn i.i.d from some &lt;span class=&#34;math inline&#34;&gt;\(P^0\)&lt;/span&gt; &lt;em&gt;not necessarily in the model&lt;/em&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d\left(P_{\hat{\theta}},P^0 \right) \leq \inf_{\theta\in\Theta} d\left(P_{\theta},P^0 \right) + r_n(\Theta),
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(r_n(\Theta) \rightarrow 0\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow \infty\)&lt;/span&gt; holds, either in expectation or with large probability.&lt;/p&gt;
&lt;p&gt;Why would this be nice? Well, first, if the model is well specified, that is, &lt;span class=&#34;math inline&#34;&gt;\(P^0 = P_{\theta^0}\)&lt;/span&gt; for some &lt;span class=&#34;math inline&#34;&gt;\(\theta^0\in\Theta\)&lt;/span&gt;, we would have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d\left(P_{\hat{\theta}},P^0 \right) \leq  r_n(\Theta) \rightarrow 0,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;so the estimator is consistent. But the ill-specified case is also relevant. Remember that “&lt;em&gt;all models are wrong, some models are useful&lt;/em&gt;”. A very interesting case is Huber’s contamination model: assume that the data is drawn from the model, but might be corrupted with small probability. That is, &lt;span class=&#34;math inline&#34;&gt;\(P^0 = (1-\varepsilon) P_{\theta^0} + \varepsilon Q\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\in[0,1]\)&lt;/span&gt; is small and &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;, the contamination distribution, can be absolutely whatever. Then we would have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d\left(P_{\hat{\theta}},P^0 \right) \leq d(P_{\theta^0},P^0) + r_n(\Theta) = d(P_{\theta^0},(1-\varepsilon) P_{\theta^0} + \varepsilon Q) + r_n(\Theta).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If the metric &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is such that &lt;span class=&#34;math inline&#34;&gt;\(d(P_{\theta^0},(1-\varepsilon) P_{\theta^0} + \varepsilon Q) \leq C \varepsilon\)&lt;/span&gt; for some constant &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;, we end up with&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d\left(P_{\hat{\theta}},P^0 \right) \leq C \varepsilon +  r_n(\Theta),
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which means that, as long as &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; remains quite small, the estimator is still not too bad. That is, the estimator is robust to contamination.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-mle-does-not-work&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;THE MLE DOES NOT WORK&lt;/h2&gt;
&lt;p&gt;In case you believe that popular estimators such as Maximum Likelihood Estimator (MLE) is universal, surprise: it’s not.&lt;/p&gt;
&lt;p&gt;First, if the model does not satisfy regularity assumptions, the MLE is not even defined. For example, consider a location model, that is, &lt;span class=&#34;math inline&#34;&gt;\(P_\theta\)&lt;/span&gt; has the density
&lt;span class=&#34;math display&#34;&gt;\[ p_\theta(x) = g(x-\theta) \]&lt;/span&gt;
and consider
&lt;span class=&#34;math display&#34;&gt;\[ g(x) = \frac{\exp(-|x|)}{2\sqrt{\pi|x|}} . \]&lt;/span&gt;
Obviously, the likelihood is infinite at each data point, that is, as soon as &lt;span class=&#34;math inline&#34;&gt;\(\theta\in\{X_1,\dots,X_n\}\)&lt;/span&gt;, we have
&lt;span class=&#34;math display&#34;&gt;\[ \prod_{i=1}^n p_\theta(X_i) = +\infty. \]&lt;/span&gt;
Even when the MLE is well defined, there are examples where it is known to be inconsistent [8]. It is also well known to be non robust to contamination in the data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-examples-of-universal-estimators&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;SOME EXAMPLES OF UNIVERSAL ESTIMATORS&lt;/h2&gt;
&lt;p&gt;The first example of universal estimator we are aware of: Yatracos’ estimator [7], with &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; being the total variation distance. It works with the rate &lt;span class=&#34;math inline&#34;&gt;\(r_n(\Theta) = [\mathrm{dim}(\Theta)/n]^{\frac{1}{2}}\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt; is in some finite dimensional space. And it doesn’t work for nonparametric estimation. Still, it’s nice, and the paper is beautiful (and short). Equally beautiful is the book by Devroye and Lugosi [9] which studies many estimations methods for the total variation distance, including variants of Yatracos’ estimator.&lt;/p&gt;
&lt;p&gt;Another example is Birgé, Barraud and Sart’s &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;-estimator [8], which satisfies a similar result for the Hellinger distance. If you want to read the paper, be aware: this is extremely difficult to prove! It is also very nice, because the Hellinger distance looks locally very similar to the KL in many models. In some sense, the &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;-estimator actually does what the MLE should do.&lt;/p&gt;
&lt;p&gt;By the way. We live in the big data era, high dimensional data, big networks, more layers, you know. So it must be said that Yatracos’ estimator, and the &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;-estimators, cannot be used in practice. They require an exhaustive search in a fine discretization of the parameter space, don’t expect to do that with a deep NN. Don’t expect to do it either for a very shallow NN, not even for a linear regression in dimension 50 (as discussed later, a variant of Yatracos’ estimator by Devroye and Lugosi might be feasible, though).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mmd-estimation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;MMD-ESTIMATION&lt;/h2&gt;
&lt;p&gt;Let us now describe yet another metric, and another estimator. This metric is based on kernels. So, let &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; be a kernel on the observations space, &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}\)&lt;/span&gt;. This means that there is an Hilbert space &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}\)&lt;/span&gt;, equipped with a norm &lt;span class=&#34;math inline&#34;&gt;\(\left\lVert\cdot\right\rVert _{\mathcal{H}}\)&lt;/span&gt;, and a continuous map &lt;span class=&#34;math inline&#34;&gt;\(\Phi:\mathcal{X}\rightarrow \mathcal{H}\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(K(x,x&amp;#39;) = \left&amp;lt;\Phi(x),\Phi(x&amp;#39;)\right&amp;gt;\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Given a probability distribution &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}\)&lt;/span&gt;, let us define the kernel mean embedding&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
 \mu(P) = \int \Phi(x) P(\mathrm{d} x) = \mathbb{E} _{X \sim P }[\Phi(X)].
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Wait. Of course, this is not always defined! It is, say, if &lt;span class=&#34;math inline&#34;&gt;\(\int \left\lVert\Phi(x)\right\rVert_{\mathcal{H}} P(\mathrm{d} x) &amp;lt;+\infty\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now, it appears that some kernels &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; are known such that:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-roman&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(k(x,x) = \left\| \Phi(x) \right\|_{\mathcal{H}}^2 \leq 1\)&lt;/span&gt; for any &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, which in turn ensures that &lt;span class=&#34;math inline&#34;&gt;\(\mu(P)\)&lt;/span&gt; is well defined for any &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P\mapsto \mu(P)\)&lt;/span&gt; is one to one.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For example, when &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}=\mathbb{R}^d\)&lt;/span&gt;, the Gaussian kernel&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
k(x,x&amp;#39;) = \exp\left(- \frac{\left\lVert x-x&amp;#39;\right\rVert^2}{\gamma^2} \right),
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;for some &lt;span class=&#34;math inline&#34;&gt;\(\gamma&amp;gt;0\)&lt;/span&gt;, satisfies (i) and (ii).&lt;/p&gt;
&lt;p&gt;For any such kernel,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d_{k}(P,Q)=\left\lVert \mu(P)-\mu(Q)\right\rVert_{\mathcal{H}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;is a distance between probability distributions. We can now define the MMD-estimator. Let &lt;span class=&#34;math inline&#34;&gt;\(\hat{P}_n\)&lt;/span&gt; denote the empirical probability distribution, that is,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{P} _n
= \frac{1}{n}\sum_{i=1}^{n} \delta_{X_i}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; is defined by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\theta} = \arg\min_{\theta\in\Theta}d_k(P_\theta,\hat{P}_n).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note: [12] below provide some conditions ensuring that the minimizer indeed exists. But actually, if if does not exist, just take any &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;-minimizer for &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; small enough, and all the good properties discussed below still hold.&lt;/p&gt;
&lt;p&gt;Note: if you believed the statement “&lt;em&gt;Close your eyes, push the button, it works&lt;/em&gt;” above, you will of course be disappointed. Life is not that simple. The choice of the kernel &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is far from easy, and is of course context dependent.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-shortest-consistency-proof-ever&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;THE SHORTEST CONSISTENCY PROOF EVER?&lt;/h2&gt;
&lt;p&gt;We now prove that, as long as the kernel satisfies (i) and (ii) above, for any statistical model &lt;span class=&#34;math inline&#34;&gt;\((P_\theta,\theta\in\Theta)\)&lt;/span&gt; (parametric, or not!), given &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt; drawn i.i.d from some &lt;span class=&#34;math inline&#34;&gt;\(P^0\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E} \left[ d_k\left(P_{\hat{\theta}},P^0 \right) \right] \leq \inf_{\theta\in\Theta} d_k\left(P_{\theta},P^0 \right) + \frac{2}{\sqrt{n}}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is taken from our paper [1]. (Note that the expectation is with respect to the sample: &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}=\hat{\theta}(X_1,\dots,X_n)\)&lt;/span&gt;, the dependence with respect to the sample is always dropped from the notation in statistics and in machine learning).&lt;/p&gt;
&lt;p&gt;First, for any &lt;span class=&#34;math inline&#34;&gt;\(\theta\in\Theta\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d_k\left(P_{\hat{\theta}_n},P^0 \right) \leq d_k\left(P_{\hat{\theta}_n},\hat{P}_n \right) +  d_k\left(\hat{P}_n,P^0 \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;by the triangle inequality. Using the defining property of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt;, that is, that it mimizes the first term in the right-hand side,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d_k\left(P_{\hat{\theta}_n},P^0 \right) \leq d_k\left(P_{\theta},\hat{P}_n \right) +  d_k\left(\hat{P}_n,P^0 \right),
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and using again the triangle inequality,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d_k\left(P_{\hat{\theta}_n},P^0 \right) \leq d_k \left(P_{\theta},P^0 \right) + 2 d_k\left(\hat{P}_n,P^0 \right).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Take the expectation of both sides, and keeping in mind that this holds for any &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, this gives:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E} \left[ d_k\left(P_{\hat{\theta}},P^0 \right) \right] \leq \inf_{\theta\in\Theta} d_k\left(P_{\theta},P^0 \right) + 2 \mathbb{E} \left[d_k\left(\hat{P}_n,P^0 \right) \right].
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So, it all boils down to a control of the expectation of &lt;span class=&#34;math inline&#34;&gt;\(d_k(\hat{P}_n,P^0 )\)&lt;/span&gt; in the right-hand side. Using Jensen’s inequality,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E} \left[ d_k\left(\hat{P}_n,P^0 \right)\right] \leq \sqrt{\mathbb{E} \left[d_k^2\left(\hat{P}_n,P^0 \right)\right]}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;so let us just focus on bounding the expected square distance. Using the definition of the MMD distance,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E} \left[d_k^2\left(\hat{P}_n,P^0 \right)\right] = \mathbb{E} \left[ \left\lVert \frac{1}{n} \sum_{i=1}^n \Phi(X_i)-\mathbb{E}_{X\sim P^0}[\Phi(X)]  \right\rVert_{\mathcal{H}}^2 \right] = \mathrm{Var}\left(\frac{1}{n} \sum_{i=1}^n \Phi(X_i) \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and so, as &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt; are i.i.d,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E} \left[d_k^2\left(\hat{P}_n,P^0 \right)\right] = \frac{1}{n} \mathrm{Var}[\Phi(X_1)] \leq \frac{1}{n} \mathbb{E}\left[ \left\lVert\Phi(X_1) \right\rVert_\mathcal{H}^2\right] \leq \frac{1}{n}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-compute-the-mmd-estimator&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;HOW TO COMPUTE THE MMD-ESTIMATOR?&lt;/h2&gt;
&lt;p&gt;Now, of course, one question remains: is it easier to compute &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; than Yatracos’ estimator?&lt;/p&gt;
&lt;p&gt;This question is discussed in depth in [1] and [12]. The main message is that the minimization of &lt;span class=&#34;math inline&#34;&gt;\(d_k^2(P_\theta,\hat{P}_{n})\)&lt;/span&gt; is usually a smooth, but non-convex problem (in [1] we exhibit one model for which the problem is convex, though). An unbiased estimate of the gradient of this quantity is easy to build. So, it is possible to use a stochastic gradient algorithm (SGA), but because of the non-convexity of the problem, it is not possible to show that this will lead to a global minimum. Still, in practice, the performances of the estimator obtained by using the SGA are excellent, see [1,12].&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;historical-references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;HISTORICAL REFERENCES&lt;/h2&gt;
&lt;p&gt;The idea to use an estimator of the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\theta} = \arg\min_{\theta\in\Theta} d(P_\theta,\hat{P}_n)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;goes back to the 50s, see [5], under the name “minimum distance estimation” (MDE). The paper [6] is followed by a discussion by Sture Holm who argues that this leads to robust estimators when the distance &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is bounded. The reader can try for example the Kolmogorov-Smirnov distance defined for &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}=\mathbb{R}\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d(P,Q) = \sup_{a\in\mathbb{R}} |P(X\leq a) - Q(X\leq a)|.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Another example is the total variation distance. Note that the initial procedure proposed by Yatracos [7] is &lt;em&gt;not&lt;/em&gt; the MDE with the TV distance (but it is an MDE with respect to another, model dependent semi-metric).&lt;/p&gt;
&lt;p&gt;Also, we mention that the procedure used by Barraud, Birgé and Sart in [8] &lt;em&gt;cannot&lt;/em&gt; be interpreted as minimum distance estimation.&lt;/p&gt;
&lt;p&gt;The MMD distance has been used in kernel methods for years, we refer the reader to the excellent tutorial [10]. However, up to your knowledge, the first time it was used as described in this blog post was in [11] where the authors used this technique to estimate &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; in a special type of model &lt;span class=&#34;math inline&#34;&gt;\((P_\theta,\theta\in\Theta)\)&lt;/span&gt; called Generative Adversarial Network (GAN, I guess you already heard about it). The first general study of MMD-estimation is [12], where the authors study the consistency and asymptotic normality of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; (among others!).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;advertisement-our-recent-works-on-mmd&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ADVERTISEMENT: OUR RECENT WORKS ON MMD&lt;/h2&gt;
&lt;p&gt;Our preprint on MMD-estimation and robustness study specifically the robustness properties of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt;. Namely, in Huber’s contamination model, we study in detail the dependence of &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{E}[d(P_{\hat{\theta}},P^0)]\)&lt;/span&gt; with respect to &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, the sample size, and &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;, the level of contamination. Moreover, in this paper, we also extend the consistency proof to the case where the observations are not independent.&lt;/p&gt;
&lt;p&gt;[1] &lt;a href=&#34;https://doi.org/10.3150/21-BEJ1338&#34;&gt;B.-E. Chérief-Abdellatif and P. Alquier (2019). Finite Sample Properties of Parametric MMD Estimation: Robustness to Misspecification and Dependence. Bernoulli, 2022, vol. 28(1), no. 1, pp. 181-213.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In the short paper on MMD-Bayes, we study a generalized Bayes procedure using the MMD distance:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\pi(\theta|X_1,\dots,X_n) \propto \exp\left(- \eta d_k^2(P_\theta,\hat{P}_n) \right) \pi(\theta)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; is some prior distribution and &lt;span class=&#34;math inline&#34;&gt;\(\eta&amp;gt;0\)&lt;/span&gt; some tuning parameter. This leads to robust Bayesian estimators.&lt;/p&gt;
&lt;p&gt;[2] &lt;a href=&#34;http://proceedings.mlr.press/v118/cherief-abdellatif20a.html&#34;&gt;B.-E. Chérief-Abdellatif and P. Alquier (2020). MMD-Bayes: Robust Bayesian Estimation via Maximum Mean Discrepancy. Proceedings of The 2nd Symposium on Advances in Approximate Bayesian Inference (AABI), Proceedings of Machine Learning Research, vol. 118, pp. 1-21.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We finally have two recent preprints on MMD-estimation in regression models and in copulas models, respectively. These models have in common that they are semi-parametric: we want to estimate a parameter that does not completely define the distribution of the data. For example, in the linear regression model &lt;span class=&#34;math inline&#34;&gt;\(Y_i = \left&amp;lt;\theta,X_i\right&amp;gt; + \varepsilon_i\)&lt;/span&gt;, we usually only specify the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;, say &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{N}(0,\sigma^2)\)&lt;/span&gt;. However, in order to use the MMD-estimator as defined above, one should also specify the distribution of the &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt;’s. In [4], we propose a trick to avoid that, but the analysis becomes immediately much more complex.&lt;/p&gt;
&lt;p&gt;[3] &lt;a href=&#34;https://doi.org/10.1080/01621459.2021.2024836&#34;&gt;P. Alquier, B.-E. Chérief-Abdellatif, A. Derumigny and J.-D. Fermanian. Estimation of Copulas via Maximum Mean Discrepancy, to appear in Journal of the American Statistical Association.&lt;/a&gt; The paper comes with the R package: &lt;a href=&#34;https://cran.r-project.org/web/packages/MMDCopula/&#34;&gt;MMDCopula&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;[4] &lt;a href=&#34;http://arxiv.org/abs/2006.00840&#34;&gt;P. Alquier and M. Gerber (2020). Universal Robust Regression via Maximum Mean Discrepancy. Preprint arxiv:2006.00840.&lt;/a&gt; We are currently working on the corresponding R package.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;REFERENCES&lt;/h2&gt;
&lt;p&gt;[5] &lt;a href=&#34;https://projecteuclid.org/download/pdf_1/euclid.aoms/1177707038&#34;&gt;J. Wolfowitz (1957). The minimum distance method. The Annals of Mathematical Statistics, 28(1), 75-88.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[6] P. J. Bickel (1976). Another Look at Robustness: A Review of Reviews and Some New Developments. Scandinavian Journal of Statistics 3, 145-168.&lt;/p&gt;
&lt;p&gt;[7] &lt;a href=&#34;https://projecteuclid.org/download/pdf1/euclid.aos/1176349553&#34;&gt;Y. G. Yatracos (1985). Rates of convergence of minimum distance estimators and Kolmogorov’s entropy. The Annals of Statistics, 13(2), 768-774.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[8] &lt;a href=&#34;https://link.springer.com/article/10.1007/s00222-016-0673-5&#34;&gt;Y. Baraud, L. Birgé and M. Sart (2017). A new method for estimation and model selection: &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;-estimation. Inventiones mathematicae, 207, 425-517.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[9] &lt;a href=&#34;https://www.springer.com/gp/book/9780387951171&#34;&gt;L. Devroye and G. Lugosi (2001). Combinatorial methods in density estimation. Springer.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[10] &lt;a href=&#34;https://www.nowpublishers.com/article/Details/MAL-060&#34;&gt;K. Muandet, K. Fukumizu, B. Sriperumbudur and B. Schölkopf, (2017). Kernel mean embedding of distributions: A review and beyond. Foundations and Trends in Machine Learning, 10(1-2), 1-141.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[11] &lt;a href=&#34;http://auai.org/uai2015/proceedings/papers/230.pdf&#34;&gt;G. K. Dziugaite, D. M. Roy and Z. Ghahramani (2015). Training generative neural networks via maximum mean discrepancy optimization. UAI’15: Proceedings of the Thirty-First Conference on Uncertainty in Artificial IntelligenceJuly 2015, 258-267.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[12] &lt;a href=&#34;http://arxiv.org/abs/1906.05944&#34;&gt;F.-X. Briol, A. Barp, A. B. Duncan and M. Girolami (2019). Statistical Inference for Generative Models via Maximum Mean Discrepancy. Preprint arXiv:1906.05944.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;about-the-author&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ABOUT THE AUTHOR&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://pierrealquier.github.io/&#34;&gt;Pierre Alquier&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Please visit the webpage of my co-authors on this topic:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://badreddinecheriefabdellatif.github.io/&#34;&gt;Badr-Eddine Chérief-Abdellatif&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://alexisderumigny.wordpress.com/&#34;&gt;Alexis Derumigny&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.crest.fr/pagesperso.php?user=2975&#34;&gt;Jean-David Fermanian&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://research-information.bris.ac.uk/en/persons/mathieu-gerber&#34;&gt;Mathieu Gerber&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Reconciling the Gaussian and Whittle Likelihood with an application to estimation in the frequency domain</title>
      <link>https://youngstats.github.io/post/2022/01/06/reconciling-the-gaussian-and-whittle-likelihood/</link>
      <pubDate>Thu, 06 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/01/06/reconciling-the-gaussian-and-whittle-likelihood/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(\{X_t: t\in \mathbb{Z}\}\)&lt;/span&gt; is a second order stationary time series where &lt;span class=&#34;math inline&#34;&gt;\(c(r) = \text{cov}(X_{t+r},X_t)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(f(\omega) = \sum_{r\in\mathbb{Z}}c(r)e^{ir\omega}\)&lt;/span&gt; are the corresponding autocovariance and spectral density function, respectively. For notational convenience, we assume the time series is centered, that is &lt;span class=&#34;math inline&#34;&gt;\(\textrm{E}(X_t)=0\)&lt;/span&gt;.
Our aim is to fit a parametric second-order stationary model (specified by &lt;span class=&#34;math inline&#34;&gt;\(\{c_{f_\theta}(r)\}\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(f_\theta(\omega)\)&lt;/span&gt;) to the observed time series &lt;span class=&#34;math inline&#34;&gt;\(\underline{X}_n = (X_1, ..., X_n)^\top\)&lt;/span&gt;.
There are two classical estimation methods based on the quasi-likelihood criteria. The first is a time-domain method which minimizes the (negative log) quasi Gaussian likelihood
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\mathcal{L}_n(\theta) = \frac{1}{n}\big(\underline{X}_n^{\top} \Gamma_n(f_\theta)\underline{X}_n + \log \det\Gamma_n(f_\theta) \big),
\end{equation}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\([\Gamma_n(f_\theta)]_{s,t} = c_{f_\theta}(s-t)\)&lt;/span&gt; is a covariance matrix of &lt;span class=&#34;math inline&#34;&gt;\(\underline{X}_n\)&lt;/span&gt;.
The second is a frequency-domain method which minimizes the Whittle likelihood
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
K_n(\theta) = \frac{1}{n}\sum_{k=1}^{n}\bigg(\frac{|J_n(\omega_k)|^2}{f_\theta(\omega_k)} + \log f_\theta(\omega_k)\bigg) \qquad \omega_k = \frac{2\pi k}{n},
\end{equation}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(J_n(\omega_k) =n^{-1/2}\sum_{t=1}^{n} X_t e^{it\omega_k}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(1 \leq k \leq n\)&lt;/span&gt;, is the discrete Fourier transform (DFT) of the (observed) time series.&lt;/p&gt;
&lt;p&gt;The Whittle likelihood is computationally a very attractive method. Thus, it has become a popular method for parameter estimation of both long and short memory stationary time series. Moreover, the Whittle likelihood has gained traction as a quasi-likelihood between the periodogram and conjectured spectral density.&lt;/p&gt;
&lt;p&gt;Despite its advantages, it is well-known that for small samples, the Whittle likelihood can give rise to estimators with a substantial bias. &lt;a href=&#34;https://doi.org/10.1214/aos/1176350838&#34;&gt;Dahlhaus (1988)&lt;/a&gt; showed that the finite sample bias in the periodogram impacts the performance of the
Whittle likelihood. To remedy this, he proposed the &lt;em&gt;tapered Whittle&lt;/em&gt; likelihood based on the tapered periodogram. He proved that the tapered periodogram led to a significant reduction in bias.&lt;/p&gt;
&lt;p&gt;However, as far as we are aware, there are no results that explain the “precise difference” between the Gaussian likelihood and the Whittle likelihood. The objective of our paper to bridge the gap between the time- and frequency-domain approaches by deriving an exact expression of the difference between the Gaussian and Whittle likelihood.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Contribution&lt;/strong&gt; of this paper is three-fold:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We obtain a linear transformation (which we named the &lt;em&gt;complete DFT&lt;/em&gt;) that is “biorthogonal” to the DFT of an observed time series.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We use the complete DFT to rewrite the Gaussian likelihood in the frequency domain.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Using an approximation for the difference, we propose a new frequency domain quasi-likelihood criteria — the &lt;em&gt;boundary corrected Whittle&lt;/em&gt; and the
&lt;em&gt;hybrid Whittle&lt;/em&gt; likelihood.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the following sections, we summarize each contribution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-complete-dft&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The complete DFT&lt;/h2&gt;
&lt;p&gt;We introduce our main theorem which obtains a transform that is biorthogonal to the DFT.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem 1 (The biorthogonal transform)&lt;/strong&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(\{X_t\}\)&lt;/span&gt; be a centered second order stationary time series with spectral density &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; which is bounded and strictly positive. For &lt;span class=&#34;math inline&#34;&gt;\(\tau \in \mathbb{Z}\)&lt;/span&gt;, let &lt;span class=&#34;math inline&#34;&gt;\(\hat{X}_{\tau,n}\)&lt;/span&gt; denote the best linear predictors of &lt;span class=&#34;math inline&#34;&gt;\(X_\tau\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(\underline{X}_n\)&lt;/span&gt;.
Let &lt;span class=&#34;math inline&#34;&gt;\(\widetilde{J_n}(\omega;f) = J_n(\omega) + \widehat{J_n}(\omega;f)\)&lt;/span&gt; be the &lt;em&gt;complete DFT&lt;/em&gt; where
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
\widehat{J_n}(\omega;f) = n^{-1/2} (\sum_{\tau \leq 0} \hat{X}_{\tau,n}e^{i\tau \omega} + 
\sum_{\tau &amp;gt; n} \hat{X}_{\tau,n}e^{i\tau \omega} ).
\end{equation*}\]&lt;/span&gt;
Then,
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
\text{cov}(\widetilde{J_n}(\omega_{k_1};f), J_n(\omega_{k_2}))
= f(\omega_{k_1}) \delta_{k_1 = k_2} \quad 1\leq k_1, k_2 \leq n,
\end{equation*}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\delta_{k_1 = k_2} = 1\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(k_1=k_2\)&lt;/span&gt; and zero otherwise.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;From the above theorem, the biorthogonal transform corresponding to the regular DFT—henceforth called the &lt;strong&gt;complete DFT&lt;/strong&gt;—contains the regular DFT &lt;em&gt;plus&lt;/em&gt; the Fourier transform of the best linear predictors outside the domain of observation. A visualization of the observations and the predictors that are involved in the construction of the complete DFT is given below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-01-06-reconciling-the-gaussian-and-whittle-likelihood_files/proj2.png&#34; style=&#34;width:70.0%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;contrasting-the-gaussian-and-whittle-likelihood&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Contrasting the Gaussian and Whittle likelihood&lt;/h2&gt;
&lt;p&gt;Using the complete DFT, we can represent the Gaussian likelihood in the frequency domain.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem 2 (A frequency domain representation of the Gaussian likelihood)&lt;/strong&gt; The quadratic term in the Gaussian likelihood can be rewritten as
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
\mathcal{L}_n(\theta) = \frac{1}{n}\underline{X}_n^{\top} \Gamma_n(f_\theta)\underline{X}_n 
= \frac{1}{n} \sum_{k=1}^{n} \frac{\widetilde{J_n}(\omega_{k};f_\theta) }{f_\theta(\omega_k)}.
\end{equation*}\]&lt;/span&gt; This yields the difference between the Gaussian and Whittle likelihood
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
\mathcal{L}_n(\theta) - K_n(\theta)
= \frac{1}{n} \sum_{k=1}^{n} \frac{\widehat{J_n}(\omega_{k};f_\theta) \overline{J_n(\omega_k)}}{f_\theta(\omega_k)}.
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;From the above theorem, we observe that the difference between the Gaussian and Whittle likelihood is due to the linear predictions outside the domain of observation. We interpret the Gaussian likelihood in terms of the information criterion. The Whittle likelihood estimator selects the spectral density &lt;span class=&#34;math inline&#34;&gt;\(f_\theta\)&lt;/span&gt; which best fits the periodogram. On the other hand, since the complete DFT is constructed based on &lt;span class=&#34;math inline&#34;&gt;\(f_\theta\)&lt;/span&gt;, the Gaussian likelihood estimator selects the spectral density which &lt;em&gt;simultaneously&lt;/em&gt; predicts and fits the time series.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;new-frequency-domain-quasi-likelihood-criteria&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;New frequency domain quasi-likelihood criteria&lt;/h2&gt;
&lt;p&gt;For both the specified and misspecified case, the Gaussian and Whittle likelihood estimate the &lt;em&gt;spectral divergence&lt;/em&gt; between the true spectral density &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; and the conjectured spectral density &lt;span class=&#34;math inline&#34;&gt;\(f_\theta\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
\mathbb{E}_f [\mathcal{L}_n(\theta)] 
~~\text{or} ~~ \mathbb{E}_f [K_n(\theta)]
= I(f;f_\theta) + O(n^{-1}),
\end{equation*}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(I(f;f_\theta) = n^{-1}\sum_{k=1}^{n}\big(\frac{f(\omega_k)}{f_\theta(\omega_k)} + \log f_\theta(\omega_k)\big)\)&lt;/span&gt;. Therefore, even for the misspecified case, asymptotically, both estimators have a meaningful interpretation. However, there is still a finite sample bias in both likelihoods which is of order &lt;span class=&#34;math inline&#34;&gt;\(O(n^{-1})\)&lt;/span&gt;. To remedy this, we introduce a new frequency domain quasi-likelihood criterion:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;(The boundary corrected Whittle likelihood)&lt;/strong&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
W_n(\theta;f) = \frac{1}{n}\sum_{k=1}^{n}\bigg(\frac{\widetilde{J_n}(\omega_{k};f)\overline{J_n(\omega_k)} }{f_\theta(\omega_k)}+
\log f_\theta(\omega_k)\bigg).
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Then, due to the Theorem 1, the (infeasible) boundary corrected Whittle likelihood is an &lt;em&gt;unbiased estimator&lt;/em&gt; of the spectral divergence.
To obtain the feasible boundary corrected Whittle criterion, we use the following approximations:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Estimate the true spectral density &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; by fitting AR&lt;span class=&#34;math inline&#34;&gt;\((p)\)&lt;/span&gt; model to the data (it is often called the &lt;em&gt;plug-in&lt;/em&gt; method).&lt;/li&gt;
&lt;li&gt;Calculate the complete DFT and the boundary corrected Whittle likelihood based on the best fitting AR&lt;span class=&#34;math inline&#34;&gt;\((p)\)&lt;/span&gt; spectral density,
&lt;span class=&#34;math inline&#34;&gt;\(\widehat{f}_p\)&lt;/span&gt;, described in 1.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The resulting feasible boundary corrected Whittle criterion, denotes &lt;span class=&#34;math inline&#34;&gt;\(W_{p,n}(\theta;\widehat{f_p})\)&lt;/span&gt;, gives a higher-order approximation of
the spectral divergence than the Gaussian and Whittle likelihoods. We use &lt;span class=&#34;math inline&#34;&gt;\(\widehat{\theta} = \arg\min_{\theta}W_{p,n}(\theta;\widehat{f}_p)\)&lt;/span&gt;
for a frequency-domain parameter estimation.&lt;/p&gt;
&lt;p&gt;We mention that by a similar argument to that given above, one can also
taper the regular DFT (but not the complete DFT) when defining the boundary corrected Whittle (the result we
called it the &lt;strong&gt;hybrid Whittle&lt;/strong&gt; likelihood). In the simulations, we have observed that this can often out perform the other likelihoods.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulation&lt;/h2&gt;
&lt;p&gt;To end this post, we present some estimation results under misspecification. For sample size &lt;span class=&#34;math inline&#34;&gt;\(n=20\)&lt;/span&gt;, the true data generating process is a Gaussian ARMA&lt;span class=&#34;math inline&#34;&gt;\((3,2)\)&lt;/span&gt; process, but we fit the following two models&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ARMA(1,1)&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(X_t = \phi X_{t-1} + \varepsilon_t + \psi \varepsilon_{t-1}\)&lt;/span&gt; and&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AR(2)&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(X_t = \phi_1 X_{t-1} + \phi_2 X_{t-2} + \varepsilon_t\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;to the time series.
Below shows (the logarithm of) the true spectral density and the best fitting ARMA&lt;span class=&#34;math inline&#34;&gt;\((1,1)\)&lt;/span&gt; and AR&lt;span class=&#34;math inline&#34;&gt;\((2)\)&lt;/span&gt; spectral densities.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-01-06-reconciling-the-gaussian-and-whittle-likelihood_files/armaspec.png&#34; style=&#34;width:70.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;All simulations are conducted over 1,000 replications and for each simulation, we calculate the parameter estimators of five quasi-likelihood criteria: (1) the Gaussian; (2) the Whittle; (3) the tapered Whittle; (4) the &lt;span style=&#34;color:blue&#34;&gt;boundary corrected Whittle&lt;/span&gt;; and (5) the &lt;span style=&#34;color:blue&#34;&gt;hybrid Whittle&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The table below shows the bias and the standard deviation (in the parentheses) of the estimated coefficients and spectral divergence. &lt;span style=&#34;color:red&#34;&gt;&lt;strong&gt;Red text&lt;/strong&gt;&lt;/span&gt; denotes the smallest root-mean-square error (RMSE) and &lt;span style=&#34;color:blue&#34;&gt;&lt;strong&gt;Blue text&lt;/strong&gt;&lt;/span&gt; denotes the second smallest RMSE.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2022-01-06-reconciling-the-gaussian-and-whittle-likelihood_files/result-table-no-caption.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Based on the above table, we conclude that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The new likelihoods tend to out perform the Whittle likelihood.&lt;/li&gt;
&lt;li&gt;The new likelihoods can significantly reduce the bias and have the smallest or second smallest RMSE.&lt;/li&gt;
&lt;li&gt;There is no clear winner, but the Gaussian, Tapered Whittle, and hybrid Whittle are close contenders.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;R. Dahlhaus. Small sample effects in time series analysis: a new asymptotic theory and a new estimate. &lt;em&gt;Ann. Statist.&lt;/em&gt;, 16(2):808-841, 1988.&lt;/p&gt;
&lt;p&gt;S. Subba Rao and J. Yang. Reconciling the Gaussian and Whittle Likelihood with an application to estimation in the frequency domain. &lt;em&gt;Ann. Statist.&lt;/em&gt;, 49(5):2774-2802, 2021.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;authors-biography&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Authors’ biography&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://sites.google.com/view/junhoyang&#34;&gt;Junho Yang&lt;/a&gt; is an Assistant Research Fellow in the Institute of Statistical Science at Academia Sinica, Taiwan. His research focuses on time series analysis, Toeplitz matrices, and spatial statistics and its applications.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.stat.tamu.edu/~suhasini/&#34;&gt;Suhasini Subba Rao&lt;/a&gt; is a Professor of Statistics at Texas A&amp;amp;M University, USA. Her research focuses on time series analysis, nonstationary processes, nonlinear processes, and spatio-temporal models.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Inclusion Process and Sticky Brownian Motions</title>
      <link>https://youngstats.github.io/post/2021/12/24/inclusion-process-and-sticky-brownian-motions/</link>
      <pubDate>Fri, 24 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/12/24/inclusion-process-and-sticky-brownian-motions/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;Inclusion Process and Sticky Brownian Motions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-12-24-inclusion-process-and-sticky-brownian-motions_files/Slika1w.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The ninth “One World webinar” organized by YoungStatS will take place on
February 9th, 2022. Inclusion process (IP) is a stochastic lattice gas
where particles perform random walks subjected to mutual attraction. For
the inclusion process in the condensation regime one can extract that
the scaling limit of two particles is a pair of sticky Brownian motions
which lead to interesting recent research. In a system of sticky
Brownian motions, particles behave as independent Brownian motions when
apart, but have a sticky interaction when they meet. Recently, exact
formulas for specific types of sticky interactions have been derived.&lt;/p&gt;
&lt;p&gt;Both the Inclusion process and the system of Sticky Brownian motions
satisfy a form of self-duality.&lt;/p&gt;
&lt;p&gt;Selected young researchers active in this area of probability and
stochastic processes will present their contributions on these topics.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wednesday, February 9th, 9:00 PT / 12:00 EST / 18:00 CET&lt;/li&gt;
&lt;li&gt;Online, via Zoom. The registration form is available
&lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSdPH7VOLfFE-WkOlR9elG9Njk-2hblnysrQe5jTgN3fSwefSg/viewform?vc=0&amp;amp;c=0&amp;amp;w=1&amp;amp;flr=0&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.almostsurelymario.com/&#34;&gt;Mario Ayala&lt;/a&gt;, Centre INRAE
PACA, Avignon, France. “Condensation of SIP particles and sticky Brownian motion”. &lt;embed&gt; Powerpoint &lt;a href=&#34;https://drive.google.com/file/d/1mHywzFD00kAW2ttagf4zNsEIILtUynWI/view&#34;&gt;presentation&lt;/a&gt;. &lt;/embed&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: &lt;em&gt;In this talk we will first introduce the symmetric inclusion process (SIP) in the condensation regime. We will describe how to obtain, with the help of self-duality, an explicit scaling for the variance of the density field in this regime, when initially started from a homogeneous product measure. This provides relevant new information on the coarsening dynamics of condensing interacting particle systems on the infinite lattice. We obtain our result by proving convergence to sticky Brownian motion for the difference of positions of two SIP particles in the sense of Mosco convergence of Dirichlet forms.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://warwick.ac.uk/fac/sci/masdoc/people/studentpages/students2017/brockington/&#34;&gt;Dominic Brockington&lt;/a&gt;,
University of Warwick, United Kingdom. “Fluctuations in the Howitt-Warren process”. &lt;embed&gt; Powerpoint &lt;a href=&#34;https://drive.google.com/file/d/19W1wT69fl1jS-o0CD93NXsETDrHiIrMU/view&#34;&gt;presentation&lt;/a&gt;. &lt;/embed&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: &lt;em&gt;Because of their consistency, sticky Brownian motions can be constructed as conditionally independent processes in a common random environment. The law of one such process, conditioned on the environment, is called the Howitt-Warren process. The Howitt-Warren process can also be viewed as the density of an infinite number of sticky Brownian motions. In the exactly solvable case, we study the fluctuations of this process in the large time limit.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.math.columbia.edu/~mrychnov/&#34;&gt;Mark Rychnovsky&lt;/a&gt;,
University of Southern California, USA. “Extremal particles for sticky Brownian motions.”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: &lt;em&gt;Sticky Brownian motions behave as independent Brownian motions when they are separated and interact when they touch, so that the coincidence time has positive Lebesgue measure with positive probability. For a specific sticky interaction we will show that as time and the number of particles are simultaneously sent to infinity, The fluctuations of the extremal particle are given by the GUE Tracy-Widom distribution. The proof involves realizing a specific sticky interaction as the limit of an exactly solvable statistical mechanics model, the Beta random walk in random environment, and using this to derive exact formulas. &lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.en.math.lmu.de/personen/mitarbeiter/swagner/index.html&#34;&gt;Stefan Wagner&lt;/a&gt;,
Ludwig-Maximilians University Munich, Germany. “The Inclusion Process on General Spaces: Reversible Measures, Consistency and Self-Duality”. &lt;embed&gt; Powerpoint &lt;a href=&#34;https://drive.google.com/file/d/1aHTNJcheA-x_1do7Q790wMZ61E9hR8W-/view&#34;&gt;presentation&lt;/a&gt;. &lt;/embed&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: &lt;em&gt;The symmetric inclusion process is a well-known process describing particles hopping on a discrete set. In this talk, we want to define an analogue dynamics on an arbitrary non-discrete space (e.g. &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^d\)&lt;/span&gt;). We consider whether properties like the existence of a reversible measure, consistency and self-duality can be transferred. Regarding the last point, we obtain a general machinery that can also be applied to other consistent particle systems (e.g. sticky Brownian motion) on general spaces.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Discussant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://research.tudelft.nl/en/persons/s-floreani&#34;&gt;Simone
Floreani&lt;/a&gt;, Delft
University of Technology, The Netherlands&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The webinar is part of YoungStatS project of the Young Statisticians
Europe initiative (FENStatS) supported by the Bernoulli Society for
Mathematical Statistics and Probability and the Institute of
Mathematical Statistics (IMS).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you missed this webinar, you can &lt;a href=&#34;https://www.youtube.com/watch?v=MU_saK2O9gI&#34;&gt;watch the recording on our YouTube channel&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Heterogeneous Treatment Effects with Instrumental Variables: A Causal Machine Learning Approach</title>
      <link>https://youngstats.github.io/post/2021/12/06/heterogeneous-treatment-effects-with-instrumental-variables-a-causal-machine-learning-approach/</link>
      <pubDate>Mon, 06 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/12/06/heterogeneous-treatment-effects-with-instrumental-variables-a-causal-machine-learning-approach/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;problem-setting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Problem Setting&lt;/h2&gt;
&lt;p&gt;In our forthcoming &lt;a href=&#34;https://arxiv.org/pdf/1905.12707.pdf&#34;&gt;paper&lt;/a&gt; on Annals of Applied Statistics, we propose a new method – which we call &lt;strong&gt;Bayesian Causal Forest with Instrumental Variable&lt;/strong&gt; (BCF-IV) – to interpretably discover the subgroups with the largest or smallest causal effects in an instrumental variable setting.&lt;/p&gt;
&lt;p&gt;These are many situations, ranging in complexity and importance, where one would like to estimate the causal effect of a defined intervention on a specific outcome. When the intervention is not randomized, researchers can recur to an instrumental variable (IV) to assess the causal effects. A valid instrument, &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;, is a variable that affects the receipt of the treatment, &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;, without directly affecting the outcome, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. Using an IV enables researchers to effectively control for potential confounding factors and estimate the local effect of the treatment on individuals who would take a treatment if assigned to it, and not take it if not assigned (the so-called &lt;em&gt;compliers&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;If the classical four IV assumptions (&lt;em&gt;monotonicity, exclusion restriction, unconfoundedness of the instrument, existence of compliers&lt;/em&gt;) hold, one can identify the causal effect of the treatment on the sub-population of compliers, the so-called Complier Average Causal Effect (CACE), that is:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} 
   \tau^{cace} = \frac{\mathbb{E}\left[Y_i\mid Z_i = 1\right]-\mathbb{E}\left[Y_i\mid Z_i = 0\right]}{\mathbb{E}\left[W_i\mid Z_i =
        1\right]-\mathbb{E}\left[W_i\mid Z_i = 0\right]}={ITT\over\pi_C},
    \end{equation}\]&lt;/span&gt;
where the numerator represents the average effect of the instrument, also referred to as &lt;em&gt;Intention-To-Treat&lt;/em&gt; (&lt;span class=&#34;math inline&#34;&gt;\(ITT\)&lt;/span&gt;) effect, and the denominator represents the overall proportion of units that comply with the treatment assignment, also referred to as &lt;em&gt;proportion of compliers&lt;/em&gt; (&lt;span class=&#34;math inline&#34;&gt;\(\pi_C\)&lt;/span&gt;). For example, researchers can make use of an IV – such as being eligible for additional school funding – to isolate the causal effects of the primary treatment – i.e., receiving the funding – on the outcome of interest – i.e., the performance of students.&lt;/p&gt;
&lt;p&gt;In IV settings, it may be of interest to disentangle the heterogeneity in the causal effects by estimating the causal effects within different subgroups. In our paper, we introduce and consider the following conditional version of CACE:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} 
   \tau^{cace}(x) = \frac{\mathbb{E}\left[Y_i\mid Z_i = 1, X_i=x\right]-\mathbb{E}\left[Y_i\mid Z_i = 0, X_i=x\right]}{\mathbb{E}\left[W_i\mid Z_i =
        1, X_i=x\right]-\mathbb{E}\left[W_i\mid Z_i = 0, X_i=x\right]}= {ITT_Y(x)\over\pi_C(x)}.
    \end{equation}\]&lt;/span&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\tau^{cace}(x)\)&lt;/span&gt; is critical as it enables researchers to investigate the heterogeneity in causal effects within different subgroups defined by partitions &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; of the features’ space.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;methodology&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Methodology&lt;/h2&gt;
&lt;p&gt;Various causal machine learning methods have been proposed to estimate conditional causal effects. However, few methods have been developed to discover and estimate heterogeneity in IVs scenarios. To account for this shortcoming, we propose the BCF-IV method. BCF-IV is a &lt;strong&gt;three steps algorithm&lt;/strong&gt; that can be used to interpretably discover the subgroups with the largest or smallest effects.&lt;/p&gt;
&lt;p&gt;In &lt;strong&gt;step one&lt;/strong&gt;, we divide the data into two subsamples: one to build the tree for the discovery of the heterogeneous effects (&lt;em&gt;discover subsample&lt;/em&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{I}^{dis}\)&lt;/span&gt;) and another for making inference (&lt;em&gt;inference subsample&lt;/em&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{I}^{inf}\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;In &lt;strong&gt;step two&lt;/strong&gt;, we discover the heterogeneity in the conditional CACE on &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{I}^{dis}\)&lt;/span&gt; by modeling separately the conditional ITT (&lt;span class=&#34;math inline&#34;&gt;\(ITT_Y(x)\)&lt;/span&gt;) and the conditional proportion of compliers (&lt;span class=&#34;math inline&#34;&gt;\(\pi_C(x)\)&lt;/span&gt;). To do so, we adapt the Bayesian Causal Forest (BCF) method – proposed by Hanh et al. (2020), and recently featured &lt;a href=&#34;https://youngstats.github.io/post/2021/01/26/machine-learning-for-causal-inference-that-works/&#34;&gt;on the YoungStats blog&lt;/a&gt; – for the estimation of &lt;span class=&#34;math inline&#34;&gt;\(ITT_Y(x)\)&lt;/span&gt;, by including the IV, &lt;span class=&#34;math inline&#34;&gt;\(Z_i\)&lt;/span&gt;, in functional form for the conditional expected value of the outcome:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} 
    \mathbb{E}[Y_i\mid Z_i=z, X_i=x] = \mu(\pi(x),x) + ITT_{Y}(x)  z
\end{equation}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\pi(x)\)&lt;/span&gt; is the propensity score for the IV:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
    \pi(x) =  \mathbb{E}[Z_i=1\mid X_i=x].
\end{equation}\]&lt;/span&gt;
Both functions &lt;span class=&#34;math inline&#34;&gt;\(\mu(\cdot)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(ITT_Y(\cdot)\)&lt;/span&gt; are Bayesian Additive Regression Trees (Chipman, 2010) and are given independent priors to model differently the contributions of the covariates and the treatment on &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. The conditional proportion of compliers can be expressed:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} 
    \mathbb{E}\left[W_i\mid Z_i = 1, X_i=x\right]-\mathbb{E}\left[W_i\mid Z_i = 0, X_i=x\right]=\delta(1,x)-\delta(0,x),
\end{equation}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\delta(z,x)\)&lt;/span&gt; can be estimated using the Bayesian machine learning methodology for causal effects estimation proposed by Hill (2011). The conditional CACE can be computed as the ratio between conditional ITT and conditional proportion of compliers:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
   \hat{\tau}^{cace}(x) =\frac{\mu(\hat{\pi}(x), x) + \hat{ITT}_{Y}(x)  z}{\hat{\delta}(1,x)-\hat{\delta}(0,x)}.
\end{equation}\]&lt;/span&gt;
One can then regress &lt;span class=&#34;math inline&#34;&gt;\(\hat{\tau}^{cace}(x)\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; via a binary decision tree to discover, in an interpretable manner, the drivers of the heterogeneity (see, e.g., Lee et al., 2020).&lt;/p&gt;
&lt;p&gt;In &lt;strong&gt;step three&lt;/strong&gt;, once the heterogeneous subgroups are learned, one can estimate the conditional CACE, &lt;span class=&#34;math inline&#34;&gt;\(\hat{\tau}^{cace}(x)\)&lt;/span&gt; on the inference subsample &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{I}^{inf}\)&lt;/span&gt;. To do so, one can use the method of moments IV estimator from Angrist et al. (1996) within all the different sub-populations that were detected in the previous step. Alternative estimation strategies, such as Two-Stages-Least-Squares, can be employed as well. Finally, multiple hypotheses tests adjustments are performed to control for familywise error rate or – less stringently – for the false discovery rate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;application&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Application&lt;/h2&gt;
&lt;p&gt;In our motivating application, implemented via the &lt;a href=&#34;https://github.com/fbargaglistoffi/BCF-IV&#34;&gt;BCF-IV package&lt;/a&gt;, we evaluate the effects of the Equal Educational Opportunity program, promoted in Flanders (Belgium) to provide additional funding for secondary schools with a significant share of disadvantaged students. We use the quasi-randomized assignment of the funding as an IV to assess the effect of additional financial resources on students’ performance in compliant schools. The Flemish Ministry of Education provided us with data on student level characteristics and school level characteristics for the universe of pupils in the first stage of education in the school year 2010/2011 (135,682 students).&lt;/p&gt;
&lt;p&gt;While the overall effects are negative but not significant (consistently with the findings of previous literature), there are significant differences among different sub-populations of students. Indeed, for students in schools with younger and less senior principals (i.e., principals younger than 55 years old and with less than 30 years of experience) the effects of the policy are larger (see Figure 1).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-12-06-heterogeneous-treatment-effects-with-instrumental-variables-a-causal-machine-learning-approach_files/tree.png&#34; style=&#34;width:70.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 1. Visualization of the heterogeneous Complier Average Causal Effects (CACE) of additional funding on student performance. The tree was discovered and estimated using the proposed BCF-IV model.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;By investigating the heterogeneity in the causal effects, BCF-IV expedites targeted policies. In fact, BCF-IV can shed light on the heterogeneity of causal effects in IVs scenarios and, in turn, provides a relevant knowledge for designing targeted interventions. Furthermore, in a Monte Carlo simulation study, we manifested that the BCF-IV technique outperforms other machine learning techniques tailored for causal inference in precisely estimating the causal effects and converges to an optimal large sample performance in identifying the subgroups with heterogeneous effects.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;essential-bibliography&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Essential bibliography&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;This article is based on:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Angrist, J. D., Imbens, G. W., &amp;amp; Rubin, D. B. (1996). &lt;em&gt;Identification of causal effects using instrumental variables.&lt;/em&gt; Journal of the American statistical Association, 91(434), 444-455.&lt;/p&gt;
&lt;p&gt;Bargagli-Stoffi, F. J., De-Witte, K. and Gnecco, G. (2021+) &lt;em&gt;Heterogeneous causal effects with imperfect compliance: a Bayesian machine learning approach.&lt;/em&gt; The Annals of Applied Statistics, forthcoming.&lt;/p&gt;
&lt;p&gt;Chipman, H. A., George, E. I., &amp;amp; McCulloch, R. E. (2010). &lt;em&gt;BART: Bayesian additive regression trees.&lt;/em&gt; The Annals of Applied Statistics, 4(1), 266-298.&lt;/p&gt;
&lt;p&gt;Lee, K., Bargagli-Stoffi, F. J., &amp;amp; Dominici, F. (2020). &lt;em&gt;Causal rule ensemble: Interpretable inference of heterogeneous treatment effects&lt;/em&gt;. arXiv preprint arXiv:2009.09036.&lt;/p&gt;
&lt;p&gt;Hahn, P. R., Murray, J. S., &amp;amp; Carvalho, C. M. (2020). &lt;em&gt;Bayesian regression tree models for causal inference: Regularization, confounding, and heterogeneous effects&lt;/em&gt;. Bayesian Analysis, 15(3), 965-1056.&lt;/p&gt;
&lt;p&gt;Hill, J. L. (2011). &lt;em&gt;Bayesian nonparametric modeling for causal inference.&lt;/em&gt; Journal of Computational and Graphical Statistics, 20(1), 217-240.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;authors-biography&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Authors’ biography&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-12-06-heterogeneous-treatment-effects-with-instrumental-variables-a-causal-machine-learning-approach_files/PhotoHandler%20(Custom).jpeg&#34; height=&#34;70&#34; /&gt;
&lt;a href=&#34;https://www.falcobargaglistoffi.com/&#34;&gt;&lt;strong&gt;Falco J. Bargagli Stoffi&lt;/strong&gt;&lt;/a&gt; is a Postdoctoral Research Fellow at the Harvard T.H. Chan School of Public Health.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Frozen percolation on the binary tree is nonendogenous</title>
      <link>https://youngstats.github.io/post/2021/11/25/frozen-percolation-on-the-binary-tree-is-nonendogenous/</link>
      <pubDate>Thu, 25 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/11/25/frozen-percolation-on-the-binary-tree-is-nonendogenous/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In frozen percolation on a graph, there is a barrier located on each
edge. Initially, the barriers are closed and they are assigned
i.i.d. uniformly distributed activation times. At its activation time,
a barrier opens, provided it is not frozen. At a fixed set &lt;span class=&#34;math inline&#34;&gt;\(\Xi\)&lt;/span&gt; of
freezing times, all barriers that percolate are frozen. In particular,
if &lt;span class=&#34;math inline&#34;&gt;\(\Xi\)&lt;/span&gt; is the whole unit interval, this means that clusters stop
growing as soon as they reach infinite size. Aldous (2000) showed that
such a process can be constructed on the infinite 3-regular tree. He
also proved uniqueness in law under natural conditions and asked
whether almost sure uniqueness holds. We answer this question
negatively. For general sets of freezing times, the answer turns out
to depend subtly on &lt;span class=&#34;math inline&#34;&gt;\(\Xi\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;definition-of-the-model&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;Definition of the model&lt;/h2&gt;
&lt;p&gt;To construct frozen percolation on a directed graph, we place a
&lt;em&gt;barrier&lt;/em&gt; on each directed edge. These barriers can be in three states:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-11-25-frozen-percolation-on-the-binary-tree-is-nonendogenous_files/screenshot-66.png&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We assign i.i.d. Unif&lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt; &lt;em&gt;activation times&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\((\tau_\mathbf{i})_{\mathbf{i}\in{\mathbb B}}\)&lt;/span&gt; to the barriers
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{i}\in{\mathbb B}\)&lt;/span&gt;, and we fix a closed set &lt;span class=&#34;math inline&#34;&gt;\(\Xi\subset[0,1]\)&lt;/span&gt; of
&lt;em&gt;freezing times&lt;/em&gt;. The states of barriers evolve according to the
following informal description:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Initially, all barriers are closed.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;At its activation time, a barrier opens, provided it is not frozen.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;At each freezing time &lt;span class=&#34;math inline&#34;&gt;\(t\in\Xi\)&lt;/span&gt;, all closed barriers that percolate
are frozen.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here, we say that a barrier percolates if there starts an infinite open
path just above it:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-11-25-frozen-percolation-on-the-binary-tree-is-nonendogenous_files/screenshot-71.png&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;More formally, we set &lt;span class=&#34;math display&#34;&gt;\[\begin{array}{r@{\,}c@{\,}l}
\displaystyle{\mathbb A}_t&amp;amp;:=&amp;amp;\displaystyle\big\{\mathbf{i}\in{\mathbb B}:\tau_\mathbf{i}\leq t\big\},\\[5pt]
\displaystyle{\mathbb F}&amp;amp;:=&amp;amp;\displaystyle\big\{\mathbf{i}\in{\mathbb B}:\mathbf{i}\mbox{ is frozen at the final time }1\big\}.
\end{array}\]&lt;/span&gt; Then &lt;span class=&#34;math inline&#34;&gt;\({\mathbb A}_t\backslash{\mathbb F}\)&lt;/span&gt; is the set of
barriers that are open at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. We write
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{i}\overset{{{\mathbb A}_t\backslash{\mathbb F}}}{\longrightarrow}\infty\)&lt;/span&gt;
if &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{i}\)&lt;/span&gt; percolates at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. Then the &lt;em&gt;Frozen Percolation
Equation&lt;/em&gt; reads:
&lt;span class=&#34;math display&#34;&gt;\[{\mathbb F}=\big\{\mathbf{i}\in{\mathbb B}:\mathbf{i}\overset{{{\mathbb A}_t\backslash{\mathbb F}}}{\longrightarrow}\infty
\mbox{ for some }t\in(0,\tau_\mathbf{i}]\cap\Xi\big\}\qquad({\rm FPE}).\]&lt;/span&gt;
It is clear from our informal description that if &lt;span class=&#34;math inline&#34;&gt;\(\Xi\)&lt;/span&gt; is finite, then
(FPE) has a solution &lt;span class=&#34;math inline&#34;&gt;\({\mathbb F}\)&lt;/span&gt;, which is a.s. unique given the
activation times &lt;span class=&#34;math inline&#34;&gt;\((\tau_\mathbf{i})_{\mathbf{i}\in{\mathbb B}}\)&lt;/span&gt;. For
general &lt;span class=&#34;math inline&#34;&gt;\(\Xi\)&lt;/span&gt; and general directed graphs, one can ask whether solutions
exist and whether they are unique in distribution or perhaps even almost
surely.&lt;/p&gt;
&lt;p&gt;Frozen percolation can also be defined on undirected graphs. Just
replace each undirected edge by two directed edges whose barriers are
activated at the same time. The case &lt;span class=&#34;math inline&#34;&gt;\(\Xi=[0,1]\)&lt;/span&gt; is of special interest,
since in this case clusters freeze as soon as they reach infinite size,
which leads to self-organised criticality. For this reason, in the year
2000 David Aldous introduced frozen percolation (Aldous, 2000). He proved that
the frozen percolation equation (FPE) has a solution on the undirected
3-regular tree, and that under natural assumptions this solution is
unique in law. On the other hand, Itai Benjamini and Oded Schramm
observed that (FPE) does not have a solution on the two-dimensional
integer lattice. A sketch of their proof can be found in Section 3 of
(van den Berg and Tóth, 2001).&lt;/p&gt;
&lt;p&gt;If from the directed binary tree &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;, we remove a finite subtree &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;
containing the root, then &lt;span class=&#34;math inline&#34;&gt;\(T\backslash U\)&lt;/span&gt; consists of finitely many
disjoint subtrees &lt;span class=&#34;math inline&#34;&gt;\(T_1,\ldots,T_n\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-11-25-frozen-percolation-on-the-binary-tree-is-nonendogenous_files/screenshot-70.png&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We say that a solution &lt;span class=&#34;math inline&#34;&gt;\({\mathbb F}\)&lt;/span&gt; satisfies the &lt;em&gt;natural conditions&lt;/em&gt;
if its restrictions to &lt;span class=&#34;math inline&#34;&gt;\(T_1,\ldots,T_n\)&lt;/span&gt; are i.i.d., equally distributed
with the process on the whole tree &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;, and independent of the
activation times in &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;. We label barriers in the obvious way and let
&lt;span class=&#34;math inline&#34;&gt;\([\mathbf{i}]\)&lt;/span&gt; denote the vertex just below &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{i}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-11-25-frozen-percolation-on-the-binary-tree-is-nonendogenous_files/screenshot-68.png&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is easy to see that the freezing time of the root
&lt;span class=&#34;math display&#34;&gt;\[Y_{[\varnothing]}:=\inf\big\{t\in\Xi:[\varnothing]\overset{{{\mathbb A}_t\backslash{\mathbb F}}}{\longrightarrow}\infty\big\}\]&lt;/span&gt;
solves the &lt;em&gt;Recursive Distributional Equation&lt;/em&gt;
&lt;span class=&#34;math display&#34;&gt;\[Y_{[\varnothing]}\stackrel{\scriptstyle{\rm d}}{=}\gamma(\tau_\varnothing,Y_{[1]},Y_{[2]}):=\left\{\begin{array}{ll}
Y_{[1]}\wedge Y_{[2]}\quad&amp;amp;\mbox{if }\tau_\varnothing&amp;lt;Y_{[1]}\wedge Y_{[2]},\\[5pt]
\infty\quad&amp;amp;\mbox{otherwise,}
\end{array}\right.\quad({\rm RDE}),\]&lt;/span&gt; where, because the solution to
(FPE) satisfies the natural conditions, &lt;span class=&#34;math inline&#34;&gt;\(Y_{[1]},Y_{[2]}\)&lt;/span&gt; are independent
of each other and equally distributed with &lt;span class=&#34;math inline&#34;&gt;\(Y_{[\varnothing]}\)&lt;/span&gt;, and
&lt;span class=&#34;math inline&#34;&gt;\(\tau_\varnothing\)&lt;/span&gt; is an independent Unif&lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt; distributed random
variable.&lt;/p&gt;
&lt;p&gt;The following theorem is a free formulation of of Theorem 1 and Lemma 5
from (Ráth, Swart and Szőke, 2021), which using Proposition 42 of (Ráth, Swart and Terpai, 2021) are translated
into the present setting.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; For each closed &lt;span class=&#34;math inline&#34;&gt;\(\Xi\subset[0,1]\)&lt;/span&gt;, there exists a unique
solution to (RDE) that yields a solution &lt;span class=&#34;math inline&#34;&gt;\({\mathbb F}\)&lt;/span&gt; to (FPE). In
particular, for each closed &lt;span class=&#34;math inline&#34;&gt;\(\Xi\subset[0,1]\)&lt;/span&gt;, (FPE) has a solution
that satisfies the natural conditions, and the joint law of
&lt;span class=&#34;math inline&#34;&gt;\(\big(\tau_\mathbf{i})_{\mathbf{i}\in{\mathbb B}},{\mathbb F}\big)\)&lt;/span&gt; is
uniquely determined.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For the case &lt;span class=&#34;math inline&#34;&gt;\(\Xi=[0,1]\)&lt;/span&gt;, this theorem was (basically) already proved in
(Aldous, 2000) by Aldous, who showed that in this case the freezing time of
the root has the law
&lt;span class=&#34;math display&#34;&gt;\[{\mathbb P}\big[Y_{[\varnothing]}&amp;gt;y\big]=1\wedge\frac{1}{2y}\qquad\big(y\in[0,1]\big),\]&lt;/span&gt;
with
&lt;span class=&#34;math inline&#34;&gt;\({\mathbb P}\big[Y_{[\varnothing]}=\infty\big]={\textstyle\frac{{1}}{{2}}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To study almost sure uniqueness in this and related problems, David
Aldous and Antar Bandyopadhyay (Aldous and Bandyopadhyay, 2005) developed a general theory of
&lt;em&gt;Recursive Tree Processes&lt;/em&gt;. In the general setting, almost sure
uniqueness is called &lt;em&gt;endogeny&lt;/em&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\({\mathbb F},{\mathbb F}&amp;#39;\)&lt;/span&gt; be
solutions to (FPE) that satisfy the natural conditions and are
conditionally independent given
&lt;span class=&#34;math inline&#34;&gt;\((\tau_\mathbf{i})_{\mathbf{i}\in{\mathbb B}}\)&lt;/span&gt;. Then
&lt;span class=&#34;math inline&#34;&gt;\((Y_{[\varnothing]},Y&amp;#39;_{[\varnothing]})\)&lt;/span&gt; solves the &lt;em&gt;bivariate RDE&lt;/em&gt;
&lt;span class=&#34;math display&#34;&gt;\[(Y_{[\varnothing]},Y&amp;#39;_{[\varnothing]})\stackrel{\scriptstyle{\rm d}}{=}
\big(\gamma(\tau_\varnothing,Y_{[1]},Y_{[2]}),\gamma(\tau_\varnothing,Y&amp;#39;_{[1]},Y&amp;#39;_{[2]})\big),\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; is the function from (RDE). In Theorem 11 of (Aldous and Bandyopadhyay, 2005), it
is proved that &lt;span class=&#34;math inline&#34;&gt;\({\mathbb F}={\mathbb F}&amp;#39;\)&lt;/span&gt; a.s. if and only if each
solution to the bivariate RDE with the right marginals is concentrated
on the diagonal. After a failed proof of endogeny that turned out to
contain an error (Bandyopadhyay, 2004), Antar Bandyopadhyay ran numerical
calculations that strongly suggested almost sure uniqueness does not
hold, but for 15 years nobody was able to prove this.&lt;/p&gt;
&lt;p&gt;We discovered the problem becomes easier if we place a geometrically
distributed number of barriers with mean one on each edge.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-11-25-frozen-percolation-on-the-binary-tree-is-nonendogenous_files/screenshot-67.png&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We call this the &lt;em&gt;Marked Binary Branching Tree&lt;/em&gt; (MBBT). The MBBT
naturally arises as the near-critical scaling limit of percolation on
regular trees. Since it is itself a scaling limit, the MBBT has a
natural scaling property. For fixed &lt;span class=&#34;math inline&#34;&gt;\(r\in[0,1]\)&lt;/span&gt;, the &lt;em&gt;pruned&lt;/em&gt; MBBT is
obtained by removing all parts of the tree that do not percolate at time
&lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; even in the absence of freezing. For the MBBT,
&lt;span class=&#34;math inline&#34;&gt;\({\mathbb P}\big[[\varnothing]\overset{{{\mathbb A}_r}}{\longrightarrow}\infty\big]=r\)&lt;/span&gt;
and conditional on this event, the pruned tree is equally distributed
with a scaled version of the original tree.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-11-25-frozen-percolation-on-the-binary-tree-is-nonendogenous_files/screenshot-69.png&#34; style=&#34;width:80.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The bivariate RDE gives an integral equation for
&lt;span class=&#34;math display&#34;&gt;\[F(s,t):={\mathbb P}\big[Y_{[\varnothing]}\leq s,\ Y&amp;#39;_{[\varnothing]}\leq t\big].\]&lt;/span&gt;
For the MBBT, scaling gives &lt;span class=&#34;math inline&#34;&gt;\(F(rs,rt)=rF(s,t)\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\((r,s,t\in[0,1])\)&lt;/span&gt;. The
bivariate RDE now reduces to an integral equation for a function of one
variable, which is easier to solve than the original bivariate RDE for
the binary tree. However, as demonstrated in Proposition 42 of (Ráth, Swart and Terpai, 2021),
frozen percolation on the binary tree and on the MBBT can be translated
into each other. The basic idea is that several barriers on one oriented
edge can be replaced by a single effective barrier, whose activation
time is the maximum of the activation times of the individual barriers.
As a result, we were able to obtain Theorem 3 of (Ráth, Swart and Terpai, 2021), which says:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; For frozen percolation on the binary tree with
&lt;span class=&#34;math inline&#34;&gt;\(\Xi=[0,1]\)&lt;/span&gt;, solutions to (FPE) are not almost surely unique.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In (Ráth, Swart and Szőke, 2021) we investigated almost sure uniqueness for frozen
percolation on the MBBT for sets of freezing times of the form
&lt;span class=&#34;math inline&#34;&gt;\(\Xi_\theta:=\{\theta^n:n\geq 0\}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(0&amp;lt;\theta&amp;lt;1\)&lt;/span&gt;. These sets behave
well under scaling, so we were able to use the same techniques as in
(Ráth, Swart and Terpai, 2021). To translate this back from the MBBT into the setting of the
binary tree, one has to replace &lt;span class=&#34;math inline&#34;&gt;\(\Xi_\theta\)&lt;/span&gt; by its image under the map
&lt;span class=&#34;math inline&#34;&gt;\(H(t):=1/(2-t)\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\((t\in[0,1])\)&lt;/span&gt;. Theorem 9 of (Ráth, Swart and Szőke, 2021) says:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; For frozen percolation on the MBBT and sets of freezing
times of the form &lt;span class=&#34;math inline&#34;&gt;\(\Xi_\theta\)&lt;/span&gt;, there exists a parameter
&lt;span class=&#34;math inline&#34;&gt;\(\theta^\ast=0.636\ldots\)&lt;/span&gt; such that, if &lt;span class=&#34;math inline&#34;&gt;\({\mathbb F},{\mathbb F}&amp;#39;\)&lt;/span&gt; are
solutions to (FPE) for &lt;span class=&#34;math inline&#34;&gt;\(\Xi_\theta\)&lt;/span&gt;, that satisfy the natural
conditions and are conditionally independent given
&lt;span class=&#34;math inline&#34;&gt;\((\tau_\mathbf{i})_{\mathbf{i}\in{\mathbb B}}\)&lt;/span&gt;, then
&lt;span class=&#34;math inline&#34;&gt;\({\mathbb F}={\mathbb F}&amp;#39;\)&lt;/span&gt; a.s. if and only if
&lt;span class=&#34;math inline&#34;&gt;\(\theta\leq\theta^\ast\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We have an explicit formula for the parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta^\ast\)&lt;/span&gt; (see Lemma 8
in (Ráth, Swart and Szőke, 2021)), which is rather complicated. The result shows that in
general, one cannot hope to settle the question of almost sure
uniqueness by “soft” arguments. Instead, any proof will always require
some hard calculations.&lt;/p&gt;
&lt;p&gt;A number of open problems remain, notably:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We don’t know if distributional or almost sure uniqueness still hold
if we drop the natural conditions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Almost sure (non-)uniqueness is open for &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-regular directed trees
with &lt;span class=&#34;math inline&#34;&gt;\(n\geq 3\)&lt;/span&gt; and for more general sets of freezing times &lt;span class=&#34;math inline&#34;&gt;\(\Xi\)&lt;/span&gt;.
The technical challenge is to replace perfect scale invariance in
the proofs by approximate scale invariance near the critical point.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Existence and/or uniqueness of solutions to (FPE) on more general
graphs, including &lt;span class=&#34;math inline&#34;&gt;\({\mathbb Z}^d\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(d\geq 3\)&lt;/span&gt;, is wide open.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 thebibliography&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;D.J. Aldous. The percolation process on a tree where infinite clusters
are frozen. &lt;em&gt;Math. Proc. Cambridge Philos. Soc.&lt;/em&gt; 128 (2000), 465–477.&lt;/p&gt;
&lt;p&gt;D.J. Aldous and A. Bandyopadhyay. A survey of max-type recursive
distributional equations. &lt;em&gt;Ann. Appl. Probab.&lt;/em&gt; 15(2) (2005), 1047–1110.&lt;/p&gt;
&lt;p&gt;A. Bandyopadhyay. Bivariate uniqueness and endogeny for recursive
distributional equations: two examples. Preprint (2004),
arXiv:math/0407175.&lt;/p&gt;
&lt;p&gt;J. van den Berg, B. Tóth. A signal-recovery system: asymptotic
properties, and construction of an infinite-volume process. &lt;em&gt;Stochastic
Process. Appl.&lt;/em&gt; 96(2) (2001), 177–190.&lt;/p&gt;
&lt;p&gt;B. Ráth, J.M. Swart, and M. Szőke. A phase transition between endogeny
and nonendogeny. Preprint (2021), arXiv:2103.14408.&lt;/p&gt;
&lt;p&gt;B. Ráth, J.M. Swart, and T. Terpai. Frozen percolation on the binary
tree is nonendogenous. &lt;em&gt;Ann. Probab.&lt;/em&gt; 49(5) (2021), 2272–2316.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
