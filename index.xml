<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>YoungStatS</title>
    <link>https://youngstats.github.io/</link>
    <description>Recent content on YoungStatS</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 06 Nov 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://youngstats.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Populations of Unlabeled Networks: Graph Space Geometry and Generalized Geodesic Principal Components</title>
      <link>https://youngstats.github.io/post/2023/11/06/populations-of-unlabeled-networks-graph-space-geometry-and-generalized-geodesic-principal-components/</link>
      <pubDate>Mon, 06 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2023/11/06/populations-of-unlabeled-networks-graph-space-geometry-and-generalized-geodesic-principal-components/</guid>
      <description>


&lt;div id=&#34;overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;Sets of graphs arise in many different applications, from medicine to
finance, from urban planning to social science. Analysing sets of graphs
is far from trivial as they are strongly non Euclidean data type. There
are two main sets of graphs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;labelled: where there is the same sets of nodes across each
observation;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- --&gt;
&lt;ul&gt;
&lt;li&gt;unlabelled: where there is no clear correspondence in the nodes
across networks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Unlabelled graphs poses high challenges from both the embedding
prospective (which geometrical embedding is suitable for such graphs)
and the statistical perspective (how can we extend basic tools to such
embedding).&lt;/p&gt;
&lt;p&gt;In the past years, scholars have been proposing different embedding
strategies for unlabelled graphs. Among existing models: &lt;span class=&#34;citation&#34;&gt;Ginestet et al. (&lt;a href=&#34;#ref-ginestet2017&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;
proposes a model where networks’ Laplacian matrices are smoothly
injected into a sub-manifold of a Euclidean space;
&lt;span class=&#34;citation&#34;&gt;Simpson et al. (&lt;a href=&#34;#ref-simpson2013permutation&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Severn, Dryden, and Preston (&lt;a href=&#34;#ref-severn2020non&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Durante, Dunson, and Vogelstein (&lt;a href=&#34;#ref-durante2017nonparametric&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;
face the problem of generating and performing tests on a population of
networks; &lt;span class=&#34;citation&#34;&gt;Lunagómez, Olhede, and Wolfe (&lt;a href=&#34;#ref-lunagomez2021modeling&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; provide Bayesian modelling for discrete
labeled graphs, and &lt;span class=&#34;citation&#34;&gt;Chowdhury and Mémoli (&lt;a href=&#34;#ref-chowdhury2019gromov&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; studies a metric space of
networks up to weak isomorphism, which allows the grouping of similar
nodes. In &lt;span class=&#34;citation&#34;&gt;Calissano, Feragen, and Vantini (&lt;a href=&#34;#ref-calissano2023populations&#34;&gt;2023&lt;/a&gt;)&lt;/span&gt;, we characterize geometrically the
Graph Space quotient space introduced by &lt;span class=&#34;citation&#34;&gt;Jain and Obermayer (&lt;a href=&#34;#ref-jain2009structure&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt; and we
extended principal component analysis to sets of unlabelled graphs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;graph-space&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Graph Space&lt;/h2&gt;
&lt;p&gt;Consider &lt;span class=&#34;math inline&#34;&gt;\(G_1,\dots, G_k\)&lt;/span&gt; where each &lt;span class=&#34;math inline&#34;&gt;\(G_i=(N_i,E_i,a_i)\)&lt;/span&gt; sets of nodes
&lt;span class=&#34;math inline&#34;&gt;\(N_i\)&lt;/span&gt;, edges &lt;span class=&#34;math inline&#34;&gt;\(E_i\)&lt;/span&gt;, and a real valued attribute function
&lt;span class=&#34;math inline&#34;&gt;\(a_i :E_i \rightarrow \mathbb{R}\)&lt;/span&gt;. Each graph can be described as a set
of adjacency matrix &lt;span class=&#34;math inline&#34;&gt;\(x \in X=\mathbb{R}^{n\times n}\)&lt;/span&gt;. We can embed
unlabelled graphs in a quotient space &lt;span class=&#34;math inline&#34;&gt;\(X / T\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; set of
permutation matrices. Each graph is now represented by its equivalence
class of permuted graphs: &lt;span class=&#34;math display&#34;&gt;\[[x]=\{t^T x t: t\in T\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-11-06-populations-of-unlabeled-networks-graph-space-geometry-and-generalized-geodesic-principal-components_files/exampleequivalenceclasses_graphspace.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 1: Conceptual visualization of Graph Space.&lt;/p&gt;
&lt;p&gt;By equipping the total space with a metric &lt;span class=&#34;math inline&#34;&gt;\((X,d_X)\)&lt;/span&gt; we can define a
quotient metric on &lt;span class=&#34;math inline&#34;&gt;\(X / T\)&lt;/span&gt; as:
&lt;span class=&#34;math display&#34;&gt;\[d_{X/T}([x_1],[x_2])=min_{t\in T}d_X (t^T x_1 t, x_2)\]&lt;/span&gt; Such metric
corresponds in finding the optimal candidate in the equivalence class
which minimize the distance in the total space. The minimization problem
is known as the Graph Matching problem, which is a broadly studied
problem in optimization (see &lt;span class=&#34;citation&#34;&gt;Conte et al. (&lt;a href=&#34;#ref-conte2004thirty&#34;&gt;2004&lt;/a&gt;)&lt;/span&gt; for a review). As we select
&lt;span class=&#34;math inline&#34;&gt;\(d_X\)&lt;/span&gt; to be the Frobenius norm, we use the FAQ Graph matching (&lt;span class=&#34;citation&#34;&gt;Vogelstein et al. (&lt;a href=&#34;#ref-vogelstein2015fast&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-11-06-populations-of-unlabeled-networks-graph-space-geometry-and-generalized-geodesic-principal-components_files/graphspace_totalspace_metric.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 2: Conceptual visualization of the distance in the Graph Space.&lt;/p&gt;
&lt;div id=&#34;short-geometrical-characterization-of-graph-space&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Short Geometrical Characterization of Graph Space&lt;/h3&gt;
&lt;p&gt;The graph space is a metric space &lt;span class=&#34;math inline&#34;&gt;\((X / T,d_{X/T})\)&lt;/span&gt;, but it is not a
manifold: The equivalence classes are often not of the same dimensions
(symmetries or blocks of zeros can cause the permutation to leave the
graph unchanged), thus the action is not free and the space is not a
quotient manifold (see &lt;span class=&#34;citation&#34;&gt;Lee (&lt;a href=&#34;#ref-lee2013smooth&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt; for details on quotient
manifolds). However, the total space &lt;span class=&#34;math inline&#34;&gt;\(X=\mathbb{R}^{n\times n}\)&lt;/span&gt; is
Euclidean. To overcome the complexity of the Graph Space and perform
statistics on unlabelled graphs, we define an algorithm relying on the
total space &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; for the computations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;align-all-and-compute&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Align All and Compute&lt;/h3&gt;
&lt;p&gt;The Align All and Compute Algorithm (AAC) allows to compute intrinsic
statistics on Graph Space by computing the estimators on the total
space. We illustrate it here for the estimation of the Fréchet Mean
(&lt;span class=&#34;citation&#34;&gt;Fréchet (&lt;a href=&#34;#ref-frechet1948elements&#34;&gt;1948&lt;/a&gt;)&lt;/span&gt;). Consider a set of &lt;span class=&#34;math inline&#34;&gt;\(\{[x_1],\dots,[x_k]\}\)&lt;/span&gt; graphs:
&lt;span class=&#34;math display&#34;&gt;\[[\bar{x}]=min_{[x]\in X / T }\sum_{i=1}^{n}d_{X /T}([\bar{x}],[x_i])\]&lt;/span&gt;
Notice that the Fréchet Mean results into the arithmetic mean in the
case of Euclidean data.&lt;/p&gt;
&lt;p&gt;The AAC operates as follows:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AAC algorithm for the the Fréchet Mean&lt;/strong&gt;&lt;br /&gt;
- Input: &lt;span class=&#34;math inline&#34;&gt;\(\{[x_1],\dots,[x_k]\} \subset X/T\)&lt;/span&gt;; a threshold
&lt;span class=&#34;math inline&#34;&gt;\(\varepsilon &amp;gt; 0\)&lt;/span&gt;&lt;br /&gt;
- Initialization: Randomly select
&lt;span class=&#34;math inline&#34;&gt;\(\tilde{x}=\tilde{x_i}\in [x_i] \in \{ [x_1],\dots,[x_k]\}\)&lt;/span&gt;&lt;br /&gt;
- While &lt;span class=&#34;math inline&#34;&gt;\(s&amp;gt;\varepsilon\)&lt;/span&gt;:&lt;br /&gt;
Obtain &lt;span class=&#34;math inline&#34;&gt;\(\tilde{x_i}\)&lt;/span&gt; optimally aligned wrt &lt;span class=&#34;math inline&#34;&gt;\(\tilde{x}\)&lt;/span&gt; for
&lt;span class=&#34;math inline&#34;&gt;\(i =\{ 1,\dots, k\}\)&lt;/span&gt;&lt;br /&gt;
Compute the Fréchet Mean &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; of
&lt;span class=&#34;math inline&#34;&gt;\(\{\tilde{x_1},\tilde{x_2},\dots,\tilde{x_k}\} \in X\)&lt;/span&gt;&lt;br /&gt;
Compute &lt;span class=&#34;math inline&#34;&gt;\(s=d(\tilde{x},\bar{x})\)&lt;/span&gt;&lt;br /&gt;
Set &lt;span class=&#34;math inline&#34;&gt;\(\tilde{x}=\bar{x}\)&lt;/span&gt;&lt;br /&gt;
- Output: &lt;span class=&#34;math inline&#34;&gt;\([\bar{x}]\)&lt;/span&gt;, an estimate of the Fréchet Mean of
&lt;span class=&#34;math inline&#34;&gt;\(\{[x_1], \ldots, [x_k]\} \in X/T\)&lt;/span&gt;.&lt;br /&gt;
&lt;/p&gt;
&lt;p&gt;The Figure 3 represents the algorithm graphically:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-11-06-populations-of-unlabeled-networks-graph-space-geometry-and-generalized-geodesic-principal-components_files/AAC_Scheme_correct_fm.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 3: Conceptual visualization of the AAC algorithm. The star
represents the current estimation of the Frechet Mean.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;generalized-geodesic-principal-components&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generalized Geodesic Principal Components&lt;/h2&gt;
&lt;p&gt;Let’s move now to more complex estimators. To extend the Principal
Component Analysis to Graph Space: (1) we firstly extend the concept of
geodesic to Graph Space; (2) we define a way to align an equivalent
class to a geodesic (i.e. optimal positioning).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 1 (Generalized Geodesics):&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;Denote by&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\Gamma(X)\)&lt;/span&gt; &lt;em&gt;the set of all straight lines (geodesics) in&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;&lt;em&gt;. Following &lt;span class=&#34;citation&#34;&gt;Huckemann, Hotz, and Munk (&lt;a href=&#34;#ref-huckemann&#34;&gt;2010&lt;/a&gt;)&lt;/span&gt;, a curve&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; &lt;em&gt;is a generalized geodesic
on the Graph Space&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(X/T\)&lt;/span&gt;&lt;em&gt;, if it is a projection of a straight line on&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;&lt;em&gt;:&lt;/em&gt; &lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
    \Gamma(X/T)=\{\delta=\pi \circ \gamma:\gamma\in\Gamma(X)\}.
\end{equation}\]&lt;/span&gt; &lt;em&gt;Where&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\pi: X\rightarrow X/T\)&lt;/span&gt; &lt;em&gt;is the canonical
quotient space projection.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Since Graph Space is not an inner product space, we define orthogonality
as:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 2:&lt;/strong&gt; &lt;em&gt;Two generalized geodesics&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\delta_1,\delta_2\in\Gamma(X/T)\)&lt;/span&gt; &lt;em&gt;are orthogonal if they have
representatives in&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\delta_1=\pi\circ\gamma_1,\delta_2=\pi\circ\gamma_2, \gamma_1,\gamma_2\in\Gamma(X)\)&lt;/span&gt;
&lt;em&gt;which are orthogonal&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(&amp;lt;\gamma_1,\gamma_2&amp;gt;_X=0\)&lt;/span&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In order to bridge computations in Graph Space &lt;span class=&#34;math inline&#34;&gt;\(X/T\)&lt;/span&gt; with computations
in the total space &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, we introduce a concept of alignment in &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 3 (Optimal position):&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;Given&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\tilde{x} \in X\)&lt;/span&gt; &lt;em&gt;and&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(t \in T\)&lt;/span&gt;&lt;em&gt;, the point&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(t^T \tilde{x} t\)&lt;/span&gt;
&lt;em&gt;is in&lt;/em&gt; &lt;em&gt;if&lt;/em&gt; &lt;span class=&#34;math display&#34;&gt;\[
d_X(t^T \tilde{x}t,x)= d_{X/T}([\tilde{x}],[x]).
\]&lt;/span&gt; &lt;em&gt;That is, the equivalence class&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\([\tilde{x}] \in X/T\)&lt;/span&gt; &lt;em&gt;contains (at
least) one point&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(t^T \tilde{x} t \in [\tilde{x}]\)&lt;/span&gt; &lt;em&gt;which has minimal
distance to&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;em&gt;, and this point is in optimal position to&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;em&gt;. Next,
consider&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\([x] \in X/T\)&lt;/span&gt;&lt;em&gt;,&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(t \in T\)&lt;/span&gt; &lt;em&gt;and&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; &lt;em&gt;a generalized
geodesic in&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(X/T\)&lt;/span&gt; &lt;em&gt;with representative&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\gamma\in \Gamma(X)\)&lt;/span&gt;&lt;em&gt;. The
graph representative&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(t^T x t\in X\)&lt;/span&gt; &lt;em&gt;is in optimal position to&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\gamma\in\Gamma(X)\)&lt;/span&gt; &lt;em&gt;if&lt;/em&gt; &lt;span class=&#34;math display&#34;&gt;\[d_X(t^T x t ,\gamma)=d_{X/T}([x],\delta).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Having concepts of generalized geodesic, optimal position and
orthogonality, we now define a set of geodesic principal components:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 4:&lt;/strong&gt; &lt;em&gt;Consider the canonical projection of the Graph Space&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\pi \colon X \rightarrow X/T\)&lt;/span&gt; &lt;em&gt;of&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; &lt;em&gt;and consider a set&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\{[x_1],\dots, [x_k]\} \subset X/T\)&lt;/span&gt; &lt;em&gt;of graphs,&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\([x]\in X/T\)&lt;/span&gt;&lt;em&gt;, and&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\delta \in \Gamma(X/T)\)&lt;/span&gt;&lt;em&gt;. The Generalized Geodesic Principal Components
(GGPCs) for the set&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\{[x_1],\dots, [x_k]\}\)&lt;/span&gt; &lt;em&gt;are defined as:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;The first generalized geodesic principal component&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\delta_1 \in \Gamma(X/T)\)&lt;/span&gt; &lt;em&gt;is the generalized geodesic minimizing
the sum of squared residuals:&lt;/em&gt; &lt;span class=&#34;math display&#34;&gt;\[\begin{equation}\label{eq:wrtdelta}
    \delta_1 = \underset{\delta \in \Gamma(X/T)}{\operatorname{argmin}} \sum_{i=1}^{k}{(d_{X/T}^2([x_i],\delta))}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;The second generalized geodesic principal component&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\delta_2 \in \Gamma(X/T)\)&lt;/span&gt; &lt;em&gt;minimizes (2) over all&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\delta \in \Gamma(X/T)\)&lt;/span&gt;&lt;em&gt;, having at least one point in common with&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\delta_1\)&lt;/span&gt; &lt;em&gt;and being orthogonal to&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\delta_1\)&lt;/span&gt; &lt;em&gt;at all points in
common with&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\delta_1\)&lt;/span&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;The point&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\mu\in X/T\)&lt;/span&gt; &lt;em&gt;is called Principal Component Mean if it
minimizes&lt;/em&gt; &lt;span class=&#34;math display&#34;&gt;\[\begin{equation}\label{eq:wrtpoint}
     \sum_{i=1}^{k}{(d_{X/T}^2([x_i],[\mu])^2)}
\end{equation}\]&lt;/span&gt; &lt;em&gt;where&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\([\mu]\)&lt;/span&gt; &lt;em&gt;only runs over points&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\tilde{x}\)&lt;/span&gt;
&lt;em&gt;in common with&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\delta_1\)&lt;/span&gt; &lt;em&gt;and&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\delta_2\)&lt;/span&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;The&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(j^{th}\)&lt;/span&gt; &lt;em&gt;generalized geodesic principal component is a&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\delta_j \in \Gamma(X/T)\)&lt;/span&gt; &lt;em&gt;if it minimizes (2) over all generalized
geodesics that meet orthogonally&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\delta_1,\dots, \delta_{j-1}\)&lt;/span&gt;
&lt;em&gt;and cross&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;&lt;em&gt;.&lt;/em&gt;&lt;br /&gt;
&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Conceptualized in Figure 4, the actual estimation of the GPCA in Graph
Space is done via AAC: (1) Randomly pick some candidates in the
equivalence classes; (2) estimate the standard PCA in &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;; (3) Select
new optimally aligned candidates wrt the current PCA estimation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-11-06-populations-of-unlabeled-networks-graph-space-geometry-and-generalized-geodesic-principal-components_files/AAC_Scheme_correct_pca.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 4: Conceptual visualization of the AAC for the estimation of the
first generalized geodesic principal component.&lt;/p&gt;
&lt;p&gt;The AAC converge in finite time and to a local minima as proven in
Theorem 2 and 3 (&lt;span class=&#34;citation&#34;&gt;Calissano, Feragen, and Vantini (&lt;a href=&#34;#ref-calissano2023populations&#34;&gt;2023&lt;/a&gt;)&lt;/span&gt;). All the algorithms and the
framework is implemeted in the geomstats python package.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;an-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An Example&lt;/h2&gt;
&lt;p&gt;As an intuitive visual example with real data and associated vectors
attributes, we subsample &lt;span class=&#34;math inline&#34;&gt;\(20\)&lt;/span&gt; cases of the letter A from the well known
hand written letters dataset (&lt;span class=&#34;citation&#34;&gt;Kersting et al. (&lt;a href=&#34;#ref-KKMMN2016&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Riesen and Bunke (&lt;a href=&#34;#ref-riesen2008iam&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt;). As shown in the
left panel of Figure 5, every network has node attributes consisting of
the node’s &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;- and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;-coordinates, and binary &lt;span class=&#34;math inline&#34;&gt;\((0/1\)&lt;/span&gt;) edge attributes
indicating whether nodes are connected by lines.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-11-06-populations-of-unlabeled-networks-graph-space-geometry-and-generalized-geodesic-principal-components_files/Letters_Plot.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 5: &lt;strong&gt;Left:&lt;/strong&gt; A datum extracted from the &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; dataset. Every
unlabelled node has a bi-dimensional real valued attribute, while every
edge has a &lt;span class=&#34;math inline&#34;&gt;\({0,1}\)&lt;/span&gt; attribute. The Fréchet mean. &lt;strong&gt;Right:&lt;/strong&gt; Visualization
of the GGPCs. &lt;span class=&#34;math inline&#34;&gt;\({0.1,0.25,0.5,0.75,0.9}\)&lt;/span&gt; quantile of the projected scores
are shown for the first three GGPCs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Graph Space is an intuitive embedding for sets of graphs with unlabelled
sets of nodes. Even if its geometry is far from trivial, we can easily
estimate intrinsic statistics by using the Align All and Compute
algorithm. In &lt;span class=&#34;citation&#34;&gt;Calissano, Feragen, and Vantini (&lt;a href=&#34;#ref-calissano2023populations&#34;&gt;2023&lt;/a&gt;)&lt;/span&gt; we detailed the geometry of
Graph Space, we define the AAC algorithm and the Generalized Geodesic
Principal Components for a set of graphs. Regression with unlabelled
network outputs is also available in &lt;span class=&#34;citation&#34;&gt;Calissano, Feragen, and Vantini (&lt;a href=&#34;#ref-calissano2022graph&#34;&gt;2022&lt;/a&gt;)&lt;/span&gt;. All the
framework is available as part of the geomstats python package
(&lt;span class=&#34;citation&#34;&gt;Miolane et al. (&lt;a href=&#34;#ref-miolane2020geomstats&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-calissano2022graph&#34; class=&#34;csl-entry&#34;&gt;
Calissano, Anna, Aasa Feragen, and Simone Vantini. 2022. &lt;span&gt;“Graph-Valued Regression: Prediction of Unlabelled Networks in a Non-Euclidean Graph Space.”&lt;/span&gt; &lt;em&gt;Journal of Multivariate Analysis&lt;/em&gt; 190: 104950.
&lt;/div&gt;
&lt;div id=&#34;ref-calissano2023populations&#34; class=&#34;csl-entry&#34;&gt;
———. 2023. &lt;span&gt;“Populations of Unlabelled Networks: Graph Space Geometry and Generalized Geodesic Principal Components.”&lt;/span&gt; &lt;em&gt;Biometrika&lt;/em&gt;, asad024.
&lt;/div&gt;
&lt;div id=&#34;ref-chowdhury2019gromov&#34; class=&#34;csl-entry&#34;&gt;
Chowdhury, Samir, and Facundo Mémoli. 2019. &lt;span&gt;“The Gromov–Wasserstein Distance Between Networks and Stable Network Invariants.”&lt;/span&gt; &lt;em&gt;Information and Inference: A Journal of the IMA&lt;/em&gt; 8 (4): 757–87.
&lt;/div&gt;
&lt;div id=&#34;ref-conte2004thirty&#34; class=&#34;csl-entry&#34;&gt;
Conte, Donatello, Pasquale Foggia, Carlo Sansone, and Mario Vento. 2004. &lt;span&gt;“Thirty Years of Graph Matching in Pattern Recognition.”&lt;/span&gt; &lt;em&gt;International Journal of Pattern Recognition and Artificial Intelligence&lt;/em&gt; 18 (03): 265–98.
&lt;/div&gt;
&lt;div id=&#34;ref-durante2017nonparametric&#34; class=&#34;csl-entry&#34;&gt;
Durante, Daniele, David B Dunson, and Joshua T Vogelstein. 2017. &lt;span&gt;“Nonparametric &lt;span&gt;B&lt;/span&gt;ayes Modeling of Populations of Networks.”&lt;/span&gt; &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 112 (520): 1516–30.
&lt;/div&gt;
&lt;div id=&#34;ref-frechet1948elements&#34; class=&#34;csl-entry&#34;&gt;
Fréchet, Maurice. 1948. &lt;span&gt;“Les &lt;span class=&#34;nocase&#34;&gt;é&lt;/span&gt;l&lt;span&gt;é&lt;/span&gt;ments Al&lt;span&gt;é&lt;/span&gt;atoires de Nature Quelconque Dans Un Espace Distanci&lt;span&gt;é&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;Annales de l’institut Henri Poincar&lt;span&gt;é&lt;/span&gt;&lt;/em&gt; 10 (4): 215–310.
&lt;/div&gt;
&lt;div id=&#34;ref-ginestet2017&#34; class=&#34;csl-entry&#34;&gt;
Ginestet, C. E., J. Li, P. Balachandran, S. Rosenberg, and E. D. Kolaczyk. 2017. &lt;span&gt;“Hypothesis Testing for Network Data in Functional Neuroimaging.”&lt;/span&gt; &lt;em&gt;The Annals of Applied Statistics&lt;/em&gt; 11 (2): 725–50. &lt;a href=&#34;https://doi.org/10.1214/16-AOAS1015&#34;&gt;https://doi.org/10.1214/16-AOAS1015&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-huckemann&#34; class=&#34;csl-entry&#34;&gt;
Huckemann, S., T. Hotz, and A. Munk. 2010. &lt;span&gt;“Intrinsic Shape Analysis: Geodesic &lt;span&gt;PCA&lt;/span&gt; for &lt;span&gt;R&lt;/span&gt;iemannian Manifolds Modulo Isometric &lt;span&gt;L&lt;/span&gt;ie Group Actions.”&lt;/span&gt; &lt;em&gt;Statist. Sinica&lt;/em&gt; 20 (1): 1–58.
&lt;/div&gt;
&lt;div id=&#34;ref-jain2009structure&#34; class=&#34;csl-entry&#34;&gt;
Jain, B. J., and K. Obermayer. 2009. &lt;span&gt;“Structure Spaces.”&lt;/span&gt; &lt;em&gt;Journal of Machine Learning Research&lt;/em&gt; 10 (Nov): 2667–2714.
&lt;/div&gt;
&lt;div id=&#34;ref-KKMMN2016&#34; class=&#34;csl-entry&#34;&gt;
Kersting, K., Kriege N. M., C. Morris, Mutzel. P., and M. Neumann. 2016. &lt;span&gt;“Benchmark Data Sets for Graph Kernels.”&lt;/span&gt; &lt;a href=&#34;http://graphkernels.cs.tu-dortmund.de&#34;&gt;http://graphkernels.cs.tu-dortmund.de&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-lee2013smooth&#34; class=&#34;csl-entry&#34;&gt;
Lee, John M. 2013. &lt;em&gt;Smooth Manifolds&lt;/em&gt;. Springer.
&lt;/div&gt;
&lt;div id=&#34;ref-lunagomez2021modeling&#34; class=&#34;csl-entry&#34;&gt;
Lunagómez, Simón, Sofia C Olhede, and Patrick J Wolfe. 2021. &lt;span&gt;“Modeling Network Populations via Graph Distances.”&lt;/span&gt; &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 116 (536): 2023–40.
&lt;/div&gt;
&lt;div id=&#34;ref-miolane2020geomstats&#34; class=&#34;csl-entry&#34;&gt;
Miolane, Nina, Nicolas Guigui, Alice Le Brigant, Johan Mathe, Benjamin Hou, Yann Thanwerdas, Stefan Heyder, et al. 2020. &lt;span&gt;“Geomstats: A Python Package for Riemannian Geometry in Machine Learning.”&lt;/span&gt; &lt;em&gt;Journal of Machine Learning Research&lt;/em&gt; 21 (223): 1–9.
&lt;/div&gt;
&lt;div id=&#34;ref-riesen2008iam&#34; class=&#34;csl-entry&#34;&gt;
Riesen, K., and H. Bunke. 2008. &lt;span&gt;“IAM Graph Database Repository for Graph Based Pattern Recognition and Machine Learning.”&lt;/span&gt; In &lt;em&gt;Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR)&lt;/em&gt;, 287–97. Springer.
&lt;/div&gt;
&lt;div id=&#34;ref-severn2020non&#34; class=&#34;csl-entry&#34;&gt;
Severn, Katie E, Ian L Dryden, and Simon P Preston. 2020. &lt;span&gt;“Non-Parametric Regression for Networks.”&lt;/span&gt; &lt;em&gt;arXiv Preprint arXiv:2010.00050&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-simpson2013permutation&#34; class=&#34;csl-entry&#34;&gt;
Simpson, Sean L, Robert G Lyday, Satoru Hayasaka, Anthony P Marsh, and Paul J Laurienti. 2013. &lt;span&gt;“A Permutation Testing Framework to Compare Groups of Brain Networks.”&lt;/span&gt; &lt;em&gt;Frontiers in Computational Neuroscience&lt;/em&gt; 7: 171.
&lt;/div&gt;
&lt;div id=&#34;ref-vogelstein2015fast&#34; class=&#34;csl-entry&#34;&gt;
Vogelstein, Joshua T, John M Conroy, Vince Lyzinski, Louis J Podrazik, Steven G Kratzer, Eric T Harley, Donniell E Fishkind, R Jacob Vogelstein, and Carey E Priebe. 2015. &lt;span&gt;“Fast Approximate Quadratic Programming for Graph Matching.”&lt;/span&gt; &lt;em&gt;PLOS One&lt;/em&gt; 10 (4): e0121002.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Stochastic Fluid Dynamics</title>
      <link>https://youngstats.github.io/post/2023/10/14/stochastic-fluid-dynamics/</link>
      <pubDate>Sat, 14 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2023/10/14/stochastic-fluid-dynamics/</guid>
      <description>


&lt;p&gt;&lt;strong&gt;Stochastic Fluid Dynamics&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Wednesday, November 15th, 6:00 PT / 9:00 ET / 15:00 CET&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-10-14-stochastic-fluid-dynamics_files/stochasticfluids_cover.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The study of fluid dynamics equations with white forcing is a classical topic in SPDEs and ergodic theory. Recently a new wave of interest, with a shift in focus towards transport noise, has risen, due to its connections with Stochastic Geometric Mechanics, Turbulence, Climate Modelling, Stochastic Parametrization and Uncertainty Quantification.&lt;/p&gt;
&lt;p&gt;In this webinar, selected young researchers will present their recent works on the topic.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wednesday, November 15th, 6:00 PT / 9:00 ET / 15:00 CET&lt;/li&gt;
&lt;li&gt;Online, via Zoom. The registration form is available &lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLScxPLuU4ItGqBbrcADlr-bub5xMs55KwaKrSLA2Fc3ehud5mA/viewform&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.google.com/view/dalonsoo&#34;&gt;Diego Alonso-Orán&lt;/a&gt;, Universidad de la Laguna, Tenerife, Spain&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Title: Regularity results for a stochastic transport equation with non-local velocity&lt;/p&gt;
&lt;p&gt;Abstract: In this talk, I will present different results regarding the well-posedness of a one-dimensional transport equation with non-local velocity under random perturbations. More precisely, we will discuss local and global existence of smooth solutions as well as the formation of finite time singularities depending on the nature of the noise. Time permitting, I will also pose some future research directions and open problems.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.imperial.ac.uk/people/daniel.goodair16&#34;&gt;Daniel Goodair&lt;/a&gt;, Imperial College London, UK&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Title: Pushing the Boundaries of Stochastic Fluid Dynamics&lt;/p&gt;
&lt;p&gt;Abstract: From the theoretical perspective, the recent introduction of transport noise into fluid dynamics models invites two interesting questions: do we retain the analytical properties of the deterministic equations for a wide class of such noise, and can we improve upon these properties for a specific choice of noise within the class. My talk will be focused on the former, with a brief eye to the latter. In particular, I will discuss the existence of solutions in the presence of a physical boundary. This will be guided by recent difficulties and successes surrounding the Navier-Stokes Equation with Stochastic Lie Transport, addressing the challenges arising in a bounded domain and what standard techniques can and cannot cope with. I will conclude with the mention of a conjecture as to how new developments in enhanced dissipation could provide a high probability stochastic answer to a famously open problem regarding the inviscid limit.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://people.utwente.nl/e.luesink&#34;&gt;Erwin Luesink&lt;/a&gt;, University of Twente, Netherlands&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Title: Stochastic fluid dynamics and its connections to geometry&lt;/p&gt;
&lt;p&gt;Abstract: Many equations in stochastic fluid dynamics can be derived through a geometric framework. This framework is known as stochastic geometric mechanics and guides also the numerical discretisation of models in stochastic fluid dynamics. In this talk I will discuss several versions of this framework and use them to derive stochastic models in fluid dynamics. For a special case, namely for stochastic fluids on the sphere, I show how the geometric framework helps to guide the numerical discretisation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://miloviviani.wordpress.com/&#34;&gt;Milo Viviani&lt;/a&gt;, Scuola Normale Superiore di Pisa, Italy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Title: Zero-Noise Selection for Point Vortex Dynamics after Collapse&lt;/p&gt;
&lt;p&gt;Abstract: In this talk, we discuss the continuation of point vortex dynamics after a vortex collapse by means of a regularization procedure consisting in introducing a small stochastic diffusive term, that corresponds to a vanishing viscosity. By a careful numerical investigation, we show that in contrast with deterministic regularization, in which a cutoff interaction selects in the limit a single trajectory of the system after collapse, the zero-noise method produces a probability distribution supported by trajectories satisfying relevant conservation laws of the point vortex system.&lt;/p&gt;
&lt;p&gt;Discussant: Umberto Pappalettera, University of Bielefeld, Germany&lt;/p&gt;
&lt;p&gt;The webinar is part of YoungStatS project of the Young Statisticians Europe initiative (FENStatS) supported by the Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics (IMS).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you missed this webinar, you can &lt;a href=&#34;https://www.youtube.com/watch?v=7Vl6aZVAiHg&#34;&gt;watch the recording on our YouTube channel&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Locally Sparse Functional Regression</title>
      <link>https://youngstats.github.io/post/2023/10/09/locally-sparse-functional-regression/</link>
      <pubDate>Mon, 09 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2023/10/09/locally-sparse-functional-regression/</guid>
      <description>


&lt;div id=&#34;overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this post we present a new estimation procedure for functional linear
regression useful when the regression surface – or curve – is supposed
to be exactly zero within specific regions of its domain. Our approach
involves regularization techniques, merging a B-spline representation of
the unknown coefficient function with a peculiar overlap group lasso
penalty. The methodology is illustrated on the well-known Swedish
mortality dataset and can be employed by &lt;span class=&#34;math inline&#34;&gt;\({\tt R}\)&lt;/span&gt;
users through the package &lt;span class=&#34;math inline&#34;&gt;\({\tt fdaSP}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;We consider the framework on which a functional response
&lt;span class=&#34;math inline&#34;&gt;\(y_i(s)\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(s \in \mathcal{S}\)&lt;/span&gt; is observed together with a functional covariate
&lt;span class=&#34;math inline&#34;&gt;\(x_i(t)\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(t \in \mathcal{T}\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(i=1, \dots, n\)&lt;/span&gt;. The simplest
modeling strategy for function-on-function regression is the &lt;em&gt;concurrent
model,&lt;/em&gt; which assumes that &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{T} = \mathcal{S}\)&lt;/span&gt; and the covariate &lt;span class=&#34;math inline&#34;&gt;\(x(\cdot)\)&lt;/span&gt; influences
&lt;span class=&#34;math inline&#34;&gt;\(y(s)\)&lt;/span&gt; only through its values &lt;span class=&#34;math inline&#34;&gt;\(x(s)\)&lt;/span&gt; at the domain point &lt;span class=&#34;math inline&#34;&gt;\(s \in \mathcal{S}\)&lt;/span&gt;.
The relation when both variables are centered is given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y_i(s) = x_i(s)\psi(s) + e_i(s)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\psi(s)\)&lt;/span&gt; is the regression function and
&lt;span class=&#34;math inline&#34;&gt;\(e_i(s)\)&lt;/span&gt; is a functional zero-mean random error.
The more general approach, named &lt;em&gt;nonconcurrent functional linear
model,&lt;/em&gt; allows &lt;span class=&#34;math inline&#34;&gt;\(y_i(s)\)&lt;/span&gt; to entirely depend on the
functional regressor in the following way&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y_i(s) = \int_{\mathcal{T}} x_i(t) \psi(t, s) dt + e_i(s) .
\]&lt;/span&gt; The model allows &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{T} \neq
\mathcal{S}\)&lt;/span&gt; and the bivariate function &lt;span class=&#34;math inline&#34;&gt;\(\psi(t,
s)\)&lt;/span&gt; represents the impact of &lt;span class=&#34;math inline&#34;&gt;\(x(\cdot)\)&lt;/span&gt; evaluated at &lt;span class=&#34;math inline&#34;&gt;\(t \in \mathcal{T}\)&lt;/span&gt; on
&lt;span class=&#34;math inline&#34;&gt;\(y_i(s)\)&lt;/span&gt; and usually is a “dense” function. Our
goal is to introduce a &lt;em&gt;nonconcurrent functional linear model&lt;/em&gt; that
allows for local sparsity patterns. Specifically, we want that
&lt;span class=&#34;math inline&#34;&gt;\(\psi(t, s) = 0\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\((t, s) \in
\mathcal{D}_0\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}_0\)&lt;/span&gt; being a suitable subset of the domain
&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\psi(\cdot)\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D} = \mathcal{S} \times
\mathcal{T}\)&lt;/span&gt;, thus, inducing locally sparse
Hilbert-Schmidt operators. Note that the set
&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}_0\)&lt;/span&gt; is unknown and need to be
estimated from data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;locally-sparse-functional-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Locally sparse functional model&lt;/h2&gt;
&lt;div id=&#34;b-splines-and-sparsity&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;B-splines and sparsity&lt;/h3&gt;
&lt;p&gt;In order to represent functional objects using basis expansion, we
select a basis &lt;span class=&#34;math inline&#34;&gt;\(\{\theta_l(s), l = 1, \dots , L\}\)&lt;/span&gt; of dimension &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; in the space of square
integrable functions on &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{S}\)&lt;/span&gt; and a basis
&lt;span class=&#34;math inline&#34;&gt;\(\{\varphi_m(t), m = 1, \dots ,M\}\)&lt;/span&gt; of
dimension &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; in the space of square integrable
functions on &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{T}\)&lt;/span&gt;. Exploiting a tensor
product expansion of these two, we represent the regression coefficient
&lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt; as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{eqnarray} \psi(t,s) &amp;amp; = \sum_{m=1}^M \sum_{l=1}^L
\psi_{ml} \varphi_m(t) \theta_l(s) = &amp;amp; (\varphi_1(t), \dots,
\varphi_M(t)) \left( \begin{matrix} \psi_{1,1} &amp;amp; \cdots &amp;amp;
\psi_{1,L} \\ \vdots &amp;amp; \ddots&amp;amp; \vdots \\ \psi_{M,1} &amp;amp; \cdots
&amp;amp; \psi_{M,L} \end{matrix}\right) \left( \begin{matrix}
\theta_{1}(s)\\ \vdots \\ \theta_{L}(s) \end{matrix}\right) =
\boldsymbol{\varphi}(t)^T \boldsymbol{\Psi}
\boldsymbol{\theta}(s), \label{eq:tensorproduct}
\end{eqnarray}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\psi_{ml} \in \mathbb{R}\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(l=1,
\dots, L\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(m=1, \dots, M\)&lt;/span&gt;.
A key point is to assume that elements in this representation are
B-splines (De Boor 1978) of order &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(L −
d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(M − d\)&lt;/span&gt; interior knots,
respectively. Suitable zero patterns in the B-spline basis coefficients
of &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Psi}\)&lt;/span&gt; induce sparsity in
&lt;span class=&#34;math inline&#34;&gt;\(\psi(t, s)\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\tau_1 &amp;lt; \dots &amp;lt; \tau_m
&amp;lt; \dots &amp;lt; \tau_{M−d+2}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_1 &amp;lt;
\dots &amp;lt; \sigma_l &amp;lt; \dots &amp;lt; \sigma_{L−d+2}\)&lt;/span&gt;
denote the knots defining the tensor product splines, with &lt;span class=&#34;math inline&#34;&gt;\(\tau_1,
\tau_{M−d+2}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_1,
\sigma_{L−d+2}\)&lt;/span&gt; being the boundaries of the two
domains, and let &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}_{ml} \in \mathcal{D}\)&lt;/span&gt; be the rectangular subset of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt; defined as &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}_{ml} = (\tau_m, \tau_{m+1})
\times (\sigma_l, \sigma_{l+1})\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(m = 1,
\dots ,M − d + 1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(l = 1, \dots , L − d +
1\)&lt;/span&gt;. To obtain &lt;span class=&#34;math inline&#34;&gt;\(\psi(t, s) = 0\)&lt;/span&gt;
for each &lt;span class=&#34;math inline&#34;&gt;\((t, s) \in \mathcal{D}_{ml}\)&lt;/span&gt;, it is
sufficient that all the coefficients &lt;span class=&#34;math inline&#34;&gt;\(\psi_{m&#39;l&#39;}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(m&#39; = m, \dots ,m + d − 1\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(l&#39; = l, \dots , l+d−1\)&lt;/span&gt; are jointly zero. The
following figure further clarifies this concept.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-10-09-locally-sparse-functional-regression_files/Image1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Bivariate example with tensor product cubic B-splines (d=4). The top
row shows different coefficient matrix patterns, while the bottom row
shows the corresponding spline, where dots represent knots and the set D
is highlighted in red. The first 2 columns show a coefficient matrix
with isolated zeros and a (d-1) x (d-1) block of zeros, respectively.
None of the two is able to produce a sparse function. In the last
column, conversely, an entire d x d block of coefficients is null and
the resulting function is indeed sparse.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The previous figure suggests that, in general, &lt;span class=&#34;math inline&#34;&gt;\(\psi(t, s)\)&lt;/span&gt; equals zero in the region identified by two pairs of
consecutive knots if the related &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; ×
&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; block of coefficients of
&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Psi}\)&lt;/span&gt; is entirely set to zero.
Therefore, &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Psi}\)&lt;/span&gt; should be suitably
partitioned in several blocks of dimensions &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; ×
&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; on which a joint sparsity penalty is induced.&lt;/p&gt;
&lt;p&gt;Note that in the simpler situation where a functional covariate
&lt;span class=&#34;math inline&#34;&gt;\(x_i(t)\)&lt;/span&gt; and a scalar response &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; are observed we have the following simplifications. The
functional linear model is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y_i = \int x_i(t) \psi(t) dt + e_i, \]&lt;/span&gt; the
regression function is expanded as &lt;span class=&#34;math inline&#34;&gt;\(\psi(t) = \sum_{m=1}^M
\psi_m \varphi_m(t)\)&lt;/span&gt; and the set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}_m
= (\tau_m, \tau_{m+1})\)&lt;/span&gt; is an interval of the real
line. To obtain &lt;span class=&#34;math inline&#34;&gt;\(\psi(t) = 0\)&lt;/span&gt; for each &lt;span class=&#34;math inline&#34;&gt;\(t \in
\mathcal{D}_{m}\)&lt;/span&gt;, it is sufficient that all the
coefficients &lt;span class=&#34;math inline&#34;&gt;\(\psi_{m&#39;}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(m&#39; = m,
\dots ,m + d − 1\)&lt;/span&gt; are jointly zero and
&lt;span class=&#34;math inline&#34;&gt;\(\psi(t)\)&lt;/span&gt; equals zero in the interval identified
by two pairs of consecutive knots if the related &lt;span class=&#34;math inline&#34;&gt;\(d-\)&lt;/span&gt;dimensional subvector of coefficients is entirely set to zero.
This behaviour is illustrated in the next figure.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-10-09-locally-sparse-functional-regression_files/Image2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Univariate cubic B-spline basis and resulting spline functions. Dashed
curves correspond to bases with a zero-valued coefficient. Only in the
case when d=4 consecutive coefficients are zero the resulting spline
function is null on a set of positive Lebesgue measure (in red).&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;overlap-group-lasso&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Overlap group Lasso&lt;/h3&gt;
&lt;p&gt;Having in mind the above mentioned B-splines sparsity properties, in
order to estimate a locally-sparse regression surface, we minimize the
following objective function&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{1}{2} \sum_{i=1}^n \int \bigg( y_i(s) - \int
x_i(t)\psi(t,s) dt \bigg)^2 ds + \lambda
\Omega(\boldsymbol{\Psi}), \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;but, instead of specifying the functional form for the penalty
&lt;span class=&#34;math inline&#34;&gt;\(\Omega(\cdot)\)&lt;/span&gt; as the widely-employed Lasso
(Tibshirani, 1996) or group-Lasso (Yuan and Lin, 2006) that would not
work properly (see paper for details), we propose to use something truly
tailored for the problem. First, instead of a disjoint partition, we
define an overlapping sequence of blocks of size &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; × &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. Specifically, we introduce the
block index &lt;span class=&#34;math inline&#34;&gt;\(b = 1, \dots , B\)&lt;/span&gt; with the total
number of blocks denoted by &lt;span class=&#34;math inline&#34;&gt;\(B = (M − d + 1) \times (L − d +
1)\)&lt;/span&gt;. Notably, there is a block for each set
&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}_{ml}\)&lt;/span&gt;. This overlapping group
structure allows &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}_0\)&lt;/span&gt; to be the union
of any set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}_{ml}\)&lt;/span&gt; by moving a block
of minimum size. An example of overlapping covering when
&lt;span class=&#34;math inline&#34;&gt;\(d=4\)&lt;/span&gt; (cubic splines) is shown in the following
figure.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-10-09-locally-sparse-functional-regression_files/Image3.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Overlapping covering of the coefficient matrix (L = M = 12) with the
first 4 blocks of size d × d, d = 4. Colors according to the balancing
vector.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This construction suggests specifying a penalty &lt;span class=&#34;math inline&#34;&gt;\(\Omega\)&lt;/span&gt; for overlapping groups of coefficients, which has attracted
significant interest in the last decade, see Jacob, Obozinski and Vert
(2009), Jenatton, Audibert and Bach (2011) and Lim and Hastie (2015).
Being interested in the sparsity structure of the matrix of coefficients
&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Psi}\)&lt;/span&gt; rather than its support we
particularize the previous problem as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{1}{2} \sum_{i=1}^n \int \left( y_i(s) - \int x_i(t)
\psi(t,s) dt \right)^2 ds + \lambda \sum_{b=1}^{B+1} || c_{b}
\odot \boldsymbol{\psi} ||_2, \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\lambda &amp;gt; 0\)&lt;/span&gt; is a fixed penalization term,
and &lt;span class=&#34;math inline&#34;&gt;\(\Omega\)&lt;/span&gt; specifies in the sum of &lt;span class=&#34;math inline&#34;&gt;\(B +
1\)&lt;/span&gt; Euclidean norms &lt;span class=&#34;math inline&#34;&gt;\(\lVert c_b \odot
\boldsymbol{\psi} \rVert_2\)&lt;/span&gt;, where
&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\psi} = \text{vec}(\boldsymbol{\Psi})\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\odot\)&lt;/span&gt; represents the Hadamard
product. The index &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; denotes the block of
coefficients in &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Psi}\)&lt;/span&gt;, with the
first &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; blocks being consistent with the
aforementioned construction and the last block containing all
coefficients in &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Psi}\)&lt;/span&gt;. Vectors of
size &lt;span class=&#34;math inline&#34;&gt;\(ML\)&lt;/span&gt;, denoted by &lt;span class=&#34;math inline&#34;&gt;\(c_b\)&lt;/span&gt;,
are needed to extract the correct subset of entries of
&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Psi}\)&lt;/span&gt; and contain also known
constants that equally balance the penalization of the coefficients.
This &lt;em&gt;balancing&lt;/em&gt; is needed because the parameters close to the
boundaries appear in fewer groups than the central ones. Note that this
penalty constitutes a special case of the norm defined by Jenatton,
Audibert, and Bach (2011).&lt;/p&gt;
&lt;p&gt;In the case of a scalar response, the objective function becomes&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{1}{2} \sum_{i=1}^n \left( y_i - \int x_i(t) \psi(t)
dt \right)^2 + \lambda \sum_{b=1}^{B+1} || c_{b} \odot
\boldsymbol{\psi} ||_2 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\psi}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c_b\)&lt;/span&gt; are vectors of dimension &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; and each of
the first &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; blocks contains &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; consecutive coefficients, as in the following example.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-10-09-locally-sparse-functional-regression_files/Image4.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Overlapping covering of the coefficient vector (M = 12) when the
response is scalar with the first 4 blocks of size d = 4. Colors
according to the balancing vector.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;computational-considerations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Computational Considerations&lt;/h2&gt;
&lt;p&gt;To develop an efficient computational strategy, we introduce the
empirical counterparts of the quantities described in the previous
section assuming to observe a sample of response curves
&lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(i=1, \dots,n\)&lt;/span&gt; on
a common grid of &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; points, i.e. &lt;span class=&#34;math inline&#34;&gt;\(y_i =
(y_i(s_1), \dots, y_i(s_G))^T\)&lt;/span&gt;. Let also
&lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; be the related functional covariate observed
on a possibly different but— common across &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;—grid of points, that for simplicity and without loss of
generality, we assume of length &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;. Let
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(n \times G\)&lt;/span&gt; matrix with &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; in the rows. Let
&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Phi}\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\Theta}\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(M \times
G\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(L \times G\)&lt;/span&gt; matrices
defined as &lt;span class=&#34;math display&#34;&gt;\[ \boldsymbol{\Phi} = \begin{pmatrix} \varphi_1(t_1)
&amp;amp; \cdots &amp;amp; \varphi_1(t_G) \\ \vdots&amp;amp;&amp;amp;\vdots \\ \varphi_m(t_1) &amp;amp;
\cdots &amp;amp; \varphi_m(t_G) \\ \vdots&amp;amp;&amp;amp;\vdots \\ \varphi_M(t_1) &amp;amp;
\cdots &amp;amp; \varphi_M(t_G) \\ \end{pmatrix}, \quad \quad
\boldsymbol{\Theta} = \begin{pmatrix} \theta_1(s_1) &amp;amp; \dots &amp;amp;
\theta_1(s_G) \\ \vdots&amp;amp;&amp;amp;\vdots \\ \theta_l(s_1) &amp;amp; \cdots &amp;amp;
\theta_l(s_G) \\ \vdots&amp;amp;&amp;amp;\vdots \\ \theta_L(s_1) &amp;amp; \cdots &amp;amp;
\theta_L(s_G) \\ \end{pmatrix}, \]&lt;/span&gt; and let
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\({\bf E}\)&lt;/span&gt;
be the &lt;span class=&#34;math inline&#34;&gt;\(n\times G\)&lt;/span&gt; matrices obtained as
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y} = (y_1, \dots, y_n)^T\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{E} = (e_1, \dots, e_n)^T\)&lt;/span&gt; , with
&lt;span class=&#34;math inline&#34;&gt;\(e_i = (e_i(s_1), \dots, e_i(s_G))^T\)&lt;/span&gt;. The
function-on-function linear regression model can be equivalently written
in matrix form as &lt;span class=&#34;math display&#34;&gt;\[ \mathbf{Y} = \mathbf{X} \boldsymbol{\Phi}^T
\boldsymbol{\Psi} \boldsymbol{\Theta} + {\bf E}. \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Applying the vectorization operator on each side of the equality above,
we obtain the new optimization problem&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{1}{2}\Vert \mathbf{y} - \mathbf{Z}
\boldsymbol{\psi}\Vert_2^2 + \lambda \sum_{b=1}^{B+1} \Vert
\mathbf{D}_{b}\boldsymbol{\psi} \Vert_2, \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{y}=\text{vec}(\mathbf{Y})\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\psi}=\text{vec}(\boldsymbol{\Psi})\)&lt;/span&gt; is the vector of coefficients of dimension &lt;span class=&#34;math inline&#34;&gt;\(LM\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Z} = \boldsymbol{\Theta}^T \otimes
\mathbf{X} \boldsymbol{\Phi}^T\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{D}_b=\mathrm{diag}(c_b)\)&lt;/span&gt; is a diagonal
matrix whose elements correspond to the elements of the vector
&lt;span class=&#34;math inline&#34;&gt;\(c_b\)&lt;/span&gt;. The minimization of the quantity above is
challenging because of the non-separability of the overlap group Lasso
penalty. We propose a Majorization-Minimization (MM) algorithm (Lange,
2016) to obtain the solution, although other choices are viable e.g.,
the Alternating Direction Method of Multipliers (ADMM), see Boyd et
al. (2011). The MM procedure works in two steps: in the first step a
majorizing function
&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{Q}(\boldsymbol{\psi}\vert\widehat{\boldsymbol{\psi}}^{k})\)&lt;/span&gt; based on the actual estimate is determined and in the second
step this function is minimized. Alternating between the two guarantees
the convergence to a solution of the original problem. The curious
reader can find the expression of
&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{Q}(\boldsymbol{\psi}\vert\widehat{\boldsymbol{\psi}}^{k})\)&lt;/span&gt; in the original paper, here we just point out that the function
is quadratic and admits an explicit solution in the form of generalized
ridge regression. A further speed-up, when the dimension of the problem
is high, is made exploiting the Sherman-Morrison-Woodbury matrix
identity, also known as the matrix inversion lemma.&lt;/p&gt;
&lt;p&gt;When the response is scalar, we have the following modifications. The
design matrix is &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Z} = \mathbf{X}
\boldsymbol{\Phi}^T\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{y}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\psi}\)&lt;/span&gt; are vectors of
dimension respectively &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; and the diagonal matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{D}_{b}\)&lt;/span&gt;
is of dimension &lt;span class=&#34;math inline&#34;&gt;\(M \times M\)&lt;/span&gt;. The optimization
problem is still valid and the same class of algorithms can be employed
to obtain the solution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;swedish-mortality-revisited&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Swedish mortality revisited&lt;/h2&gt;
&lt;p&gt;We apply the proposed locally sparse estimator to the Swedish mortality
dataset, where the aim is to predict the log-hazard function on a given
year from the same quantity on the previous year. We implement the model
with &lt;span class=&#34;math inline&#34;&gt;\(d = 4, M = L = 20\)&lt;/span&gt; basis functions on each
dimension and select the optimal value of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; by means of cross-validation. A perspective plot of the
estimated surface is depicted in the last figure of the post. The
estimate shows a marked positive diagonal confirming the positive
influence on the log-hazard rate at age &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; of the
previous year’s curve evaluated on a neighborhood of &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;. At the same time, the flat zero regions outside the diagonal
suggest that there is no influence of the curves evaluated at distant
ages. Our estimate is more regular than previous approaches and its
qualitative interpretation sharper and easier. Refer, for example, to
Figure 10.11 of Ramsay, Hooker, and Graves (2009).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-10-09-locally-sparse-functional-regression_files/Image5.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Estimated regression surface for the Swedish mortality dataset.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This result witnesses the practical relevance of adopting the proposed
approach. Indeed, the resulting estimate, while being reminiscent of a
concurrent model—inheriting its ease of interpretation—gives further
insights and improves the fit, representing the desired intermediate
solution between the concurrent and nonconcurrent models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tt-r-package&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;math inline&#34;&gt;\({\tt R}\)&lt;/span&gt; package&lt;/h2&gt;
&lt;p&gt;The method is implemented in &lt;span class=&#34;math inline&#34;&gt;\({\tt R}\)&lt;/span&gt; through the
package &lt;span class=&#34;math inline&#34;&gt;\({\tt fdaSP}\)&lt;/span&gt;, available on CRAN. The
function &lt;span class=&#34;math inline&#34;&gt;\({\tt f2fSP}\)&lt;/span&gt; can be used for a
functional response regression model while &lt;span class=&#34;math inline&#34;&gt;\({\tt f2sSP}\)&lt;/span&gt; for a scalar response one. The two counterparts
&lt;span class=&#34;math inline&#34;&gt;\(\texttt{f2fSP_cv}\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(\texttt{f2sSP_cv}\)&lt;/span&gt; are useful to select the
tuning parameter by means of cross-validation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;this-post-is-based-on&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;This post is based on&lt;/h2&gt;
&lt;p&gt;Bernardi, M., Canale, A. and Stefanucci, M. (2022). Locally Sparse
Function-on-Function Regression, Journal of Computational and Graphical
Statistics, 32:3, 985-999, DOI:
&lt;a href=&#34;https://doi.org/10.1080/10618600.2022.2130926&#34;&gt;10.1080/10618600.2022.2130926&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Tibshirani, R. (1996), “Regression Shrinkage and Selection via the
Lasso,” &lt;em&gt;Journal of the Royal Statistical Society,&lt;/em&gt; Series B, 58,
267–288.&lt;br /&gt;
Yuan, M., and Lin, Y. (2006), “Model Selection and Estimation in
Regression with Grouped Variables,” &lt;em&gt;Journal of the Royal Statistical
Society,&lt;/em&gt; Series B, 68, 49–67.&lt;br /&gt;
Jacob, L., Obozinski, G., and Vert, J.-P. (2009), “Group Lasso with
Overlap and Graph Lasso,” in &lt;em&gt;Proceedings of the 26th Annual
International Conference on Machine Learning,&lt;/em&gt; pp. 433–440.&lt;br /&gt;
Jenatton, R., Audibert, J.-Y., and Bach, F. (2011), “Structured Variable
Selection with Sparsity-Inducing Norms,” &lt;em&gt;Journal of Machine Learning
Research,&lt;/em&gt; 12, 2777–2824.&lt;br /&gt;
Lim, M., and Hastie, T. (2015), “Learning Interactions via Hierarchical
Group-Lasso Regularization,” &lt;em&gt;Journal of Computational and Graphical
Statistics,&lt;/em&gt; 24, 627–654.&lt;br /&gt;
Lange, K. (2016), &lt;em&gt;“MM optimization algorithms.”&lt;/em&gt; Society for Industrial
and Applied Mathematics, Philadelphia, PA.&lt;br /&gt;
Boyd, S., Parikh, N., Chu, E., Peleato, B. and Eckstein, J. (2011),
“Distributed optimization and statistical learning via the alternating
direction method of multipliers,” &lt;em&gt;Foundations and Trends® in Machine
Learning,&lt;/em&gt; 3, 1–122.&lt;br /&gt;
Ramsay, J. O., Hooker, G., and Graves, S. (2009), &lt;em&gt;Functional Data
Analysis with R and Matlab,&lt;/em&gt; NewYork: Springer.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;about-the-authors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;About the authors&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://scholar.google.it/citations?user=TejvXeQAAAAJ&amp;amp;hl=en&#34;&gt;Mauro Bernardi&lt;/a&gt;
is Associate Professor at University of Padua.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://tonycanale.github.io/&#34;&gt;Antonio Canale&lt;/a&gt; is Associate Professor
at University of Padua.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://marcostefanucci.github.io/&#34;&gt;Marco Stefanucci&lt;/a&gt; is Assistant
Professor at University of Rome Tor Vergata.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linear-cost unbiased estimator for large crossed random effect models via couplings</title>
      <link>https://youngstats.github.io/post/2023/09/27/linear-cost-unbiased-estimator-for-large-crossed-random-effect-models-via-couplings/</link>
      <pubDate>Wed, 27 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2023/09/27/linear-cost-unbiased-estimator-for-large-crossed-random-effect-models-via-couplings/</guid>
      <description>


&lt;style&gt;
body {
text-align: justify
fig.align = &#39;center&#34;}
&lt;/style&gt;
&lt;p&gt;In the following we show how it is possible to obtain &lt;strong&gt;parallelizable, unbiased and computationally cheap&lt;/strong&gt; estimates of Crossed random effects models with a &lt;strong&gt;linear cost&lt;/strong&gt; in the number of datapoints (and paramaters) exploiting &lt;strong&gt;couplings&lt;/strong&gt;.&lt;/p&gt;
&lt;div id=&#34;crossed-random-effects-models-crem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Crossed random effects models (CREM)&lt;/h2&gt;
&lt;p&gt;CREM model a continuous response variables &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; as depending on the sum of unknown effects of &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; categorical predictors. Think of the &lt;span class=&#34;math inline&#34;&gt;\(Y_n\)&lt;/span&gt; as the ratings given to university courses, along with some factors potentially impacting such a score, e.g. student identity, code of the course, department teaching it, professors ecc. Aim of the model is investigating the effect of each of those factors on the overall score. In their simplest version (i.e. linear, intercept-only case), the model takes the form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
	\label{eq:crem}
	\mathcal{L}(y_n) =  f \left(\mu +\sum_{k=1}^K a_{i_{k[n]}}^{(k)},\tau_0^{-1}\right) \text{ for } n=1,...,N,
\end{equation}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; indicates the density of some distribution whose mean is the sum of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, a global mean, and &lt;span class=&#34;math inline&#34;&gt;\(a^{(k)}_{i_{k[n]}}\)&lt;/span&gt;, i.e. the unknown effects of the student identity, the department teaching it ecc.&lt;/p&gt;
&lt;p&gt;We are interested in studying how the cost of estimating the unknown effects scales as the number of observations &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; and of parameters &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; grows to &lt;span class=&#34;math inline&#34;&gt;\(\infty\)&lt;/span&gt;. Our goal is an algorithm whose complexity scales linearly in &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, and we call such algorithms “scalable”. Both in the Frequentist and in the Bayesian literature, these models are difficult to estimate: works of &lt;span class=&#34;citation&#34;&gt;Gao and Owen (&lt;a href=&#34;#ref-gao_16&#34; role=&#34;doc-biblioref&#34;&gt;2016a&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Gao and Owen (&lt;a href=&#34;#ref-gao19&#34; role=&#34;doc-biblioref&#34;&gt;2016b&lt;/a&gt;)&lt;/span&gt; showed how the “vanilla” implementation of GLS and of Gibbs samplers have a computational cost that grows at best as &lt;span class=&#34;math inline&#34;&gt;\(O(N^\frac{3}{2})\)&lt;/span&gt;. Recent works by &lt;span class=&#34;citation&#34;&gt;Papaspiliopoulos, Roberts, and Zanella (&lt;a href=&#34;#ref-papaspiliopoulos2018scalable&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Ghosh, Hastie, and Owen (&lt;a href=&#34;#ref-gosh_back&#34; role=&#34;doc-biblioref&#34;&gt;2022&lt;/a&gt;)&lt;/span&gt; proposed, respectively, a collapsed Gibbs sampler and a “backfitting” iterative algorithm that exhibit computational costs linear in the number of observations; in particular the MCMC induced by the collapsed scheme is proved to have a mixing time that is &lt;span class=&#34;math inline&#34;&gt;\(O(1)\)&lt;/span&gt; under certain asymptotic regimes.&lt;/p&gt;
&lt;p&gt;It is possible to further improve the MCMC estimates exploiting couplings: as showed in &lt;span class=&#34;citation&#34;&gt;Jacob, O’Leary, and Atchadé (&lt;a href=&#34;#ref-jacob2019unbiased&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Glynn and Rhee (&lt;a href=&#34;#ref-glynn_rhee&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;, coupling two MCMC chains allows to derive unbiased estimators of posterior quantities, provided that the two coupled chains are exactly equal after a finite number of iterations. Furthermore, the same construction provides theoretical foundations for the early stopping of the chains (once met) and allows for the parallelization of independent experiments.&lt;/p&gt;
&lt;p&gt;The extra computational cost one has to pay is represented by the product between the cost of each iteration and the expected number of iterations needed for coalescence. As for the former, it is possible to devise many coupling algorithms for which the cost of each iteration is easily computable and linear in the number of observations. As for the expected meeting time, for chains arising from a Gibbs sampling scheme targeting Gaussian distributions, it is possible to show that is directly related to the mixing time of the single Markov chain, and indeed it differs from the latter only by a logarithmic factor up to some constants (see the Sections below for more details). Hence chains that mix fast also meet in a small number of iteration and therefore provide unbiased estimates with low computational cost.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;couplings-for-estimation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Couplings for estimation&lt;/h2&gt;
&lt;p&gt;Theoretically speaking, given &lt;span class=&#34;math inline&#34;&gt;\(X,Y\)&lt;/span&gt; random variables distributed according to &lt;span class=&#34;math inline&#34;&gt;\(P,Q\)&lt;/span&gt; respectively, a coupling of the two is random variables &lt;span class=&#34;math inline&#34;&gt;\((X, Y)\)&lt;/span&gt; on the joint space such that the marginal distribution of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; and the marginal distribution of Y is &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;.
Clearly, given two marginal distributions, there are infinitely many joint distributions with those as marginals. Below some of the possible couplings of a &lt;span class=&#34;math inline&#34;&gt;\(N(1,1)\)&lt;/span&gt; (on the x-axis) and &lt;span class=&#34;math inline&#34;&gt;\(N(0,1)\)&lt;/span&gt; on the y-axis. Starting clockwise from top left: maximal independent (i.e. a coupling maximizing the probability of equal draws), maximal reflection, independent (bivariate independent normal) and &lt;span class=&#34;math inline&#34;&gt;\(W2\)&lt;/span&gt;-optimal (maximally correlated draws).&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://youngstats.github.io/post/2023-09-27-linear-cost-unbiased-estimator-for-large-crossed-random-effect-models-via-couplings_files/gaussian_coupling.jpg&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Couplings of Gaussian distributions&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Couplings might be used for obtaining unbiased estimators in MCMC inference, as shown in &lt;span class=&#34;citation&#34;&gt;Jacob, O’Leary, and Atchadé (&lt;a href=&#34;#ref-jacob2019unbiased&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;.
Given a target probability distribution &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; and an integrable function &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt;, we are interested in estimating: &lt;span class=&#34;math display&#34;&gt;\[\mathbb{E}_{\pi}[h(\boldsymbol{\Theta})] = \int h(\boldsymbol{\theta}) \pi(d\boldsymbol{\theta}).\]&lt;/span&gt;
Usually, one would sample a chain according to some Markov kernel &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;, designed to leave the chain &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; invariant, i.e. a Gibbs kernel or random walk Metropolis &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-Metropolis&#34; role=&#34;doc-biblioref&#34;&gt;Hastings 1970&lt;/a&gt;)&lt;/span&gt;. Below the sample path of such a chain.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://youngstats.github.io/post/2023-09-27-linear-cost-unbiased-estimator-for-large-crossed-random-effect-models-via-couplings_files/Slide1.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Sample path of a single MC&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Instead of waiting until convergence, one could run two coupled Markov chains &lt;span class=&#34;math inline&#34;&gt;\((\boldsymbol{\Theta}^1_t)_{t\ge -1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\((\boldsymbol{\Theta}^2_t)_{t \ge0}\)&lt;/span&gt;, which marginally starts from some base distribution &lt;span class=&#34;math inline&#34;&gt;\(\pi_0\)&lt;/span&gt; and evolves according to the same kernel &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;, but some correlation is induced in order to let the chains meet after an almost surely finite number of iterations. Basically at iteration &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; instead of sampling from &lt;span class=&#34;math inline&#34;&gt;\(X_{t+1} \sim P(X_t, \cdot)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_{t+1} \sim P(Y_t, \cdot)\)&lt;/span&gt; independently, we sample from a coupling of the two distributions.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://youngstats.github.io/post/2023-09-27-linear-cost-unbiased-estimator-for-large-crossed-random-effect-models-via-couplings_files/Slide2.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Sample path of coupled MCs&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Then, for any fixed &lt;span class=&#34;math inline&#34;&gt;\(m \ge k\)&lt;/span&gt;, we can run coupled chains for &lt;span class=&#34;math inline&#34;&gt;\(\max(m, T)\)&lt;/span&gt; iterations and &lt;span class=&#34;math inline&#34;&gt;\(H_{k:m}\)&lt;/span&gt; is an unbiased estimator:
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
	H_{k:m} &amp;amp; = \frac{1}{m-k+1} \sum_{l=k}^m h(\boldsymbol{\Theta}^1_l) + \sum_{l=k+1}^{\tau} \min \left(1, \frac{l-k}{m-k+1}\right) \left(h(\boldsymbol{\Theta}^1_l)-h(\boldsymbol{\Theta}^2_{l})\right) \\ &amp;amp;= MCMC_{k:m} + BC_{k:m}.
\end{align*}\]&lt;/span&gt;
The form of the estimator includes two terms: the first term corresponds to a standard MCMC average with &lt;span class=&#34;math inline&#34;&gt;\(m-k+1\)&lt;/span&gt; total iterations and &lt;span class=&#34;math inline&#34;&gt;\(k-1\)&lt;/span&gt; burn-in steps, and the other term is a “bias correction”, the part that corrects the bias present in the MCMC average.&lt;/p&gt;
&lt;p&gt;For more details give a look up at &lt;a href=&#34;https://sites.google.com/site/pierrejacob/cmclectures&#34; class=&#34;uri&#34;&gt;https://sites.google.com/site/pierrejacob/cmclectures&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In order two yield small meeting times, we implement a “two-step” coupling strategy: whenever the chains are “far away” (in some notion that will be clarified later) use a coupling whose aim is to bring the realizations closer to each other; whenever “close enough”, choose a coupling maximizing the meeting probabilities. The heuristic for this construction is that whenever a maximal coupling fails, components are sampled far away in the space, thus reducing the coalescence probability for the next steps.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bound-on-coupling-time&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bound on coupling time&lt;/h2&gt;
&lt;p&gt;Consider &lt;span class=&#34;math inline&#34;&gt;\((\boldsymbol{\theta}_t)_{t\ge 1}=(\boldsymbol{\theta}^1_t, \boldsymbol{\theta}^2_t)_{t\ge 0}\)&lt;/span&gt;, two &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;-reversible Markov chains arising from a Gibbs sampler targeting &lt;span class=&#34;math inline&#34;&gt;\(\pi=N(\boldsymbol{\mu},\Sigma)\)&lt;/span&gt;. If a “two-step” strategy is implemented, with maximal reflection and &lt;span class=&#34;math inline&#34;&gt;\(W2\)&lt;/span&gt; optimal couplings, then for every &lt;span class=&#34;math inline&#34;&gt;\(\delta &amp;gt;0\)&lt;/span&gt;, the meeting time &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; is bounded by
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
		\mathbb{E}[T| \boldsymbol{\theta}_0] \le 5 + 3 \max \left(n^*_\delta, T_{rel} \left[\frac{\ln(T_{rel})}{2} + C_0 + C_\varepsilon \right] (1+\delta)  \right),
	\end{equation}\]&lt;/span&gt;on}
where &lt;span class=&#34;math inline&#34;&gt;\(C_0\)&lt;/span&gt; denotes a constant solely depending on &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\theta}^1_0, \boldsymbol{\theta}^2_0\)&lt;/span&gt; and the posterior variance &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(C_\varepsilon\)&lt;/span&gt; depends on the fixed parameters &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(n^*_\delta = inf_{n_0} \{ n_0 \ge 1: \forall n \ge n_0 \; 1-\| B^n \|^\frac{1}{n} \ge \frac{1-\rho(B)}{1+\delta} \}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Given the above, if one is able to design a single MCMC chain mixing in say &lt;span class=&#34;math inline&#34;&gt;\(O(1)\)&lt;/span&gt;, then the extra cost of an unbiased estimate is nothing more than a &lt;span class=&#34;math inline&#34;&gt;\(\ln\left( O(1) \right)\)&lt;/span&gt; plus a constant.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulations&lt;/h2&gt;
&lt;p&gt;We simulate data coming from the model for different asymptotic regimes and parameter specification. We study the behaviour of the meeting times as the dimensionality of the model increase. We implement the “two step” Algorithm, using maximal reflection and &lt;span class=&#34;math inline&#34;&gt;\(W2\)&lt;/span&gt;-optimal couplings.&lt;/p&gt;
&lt;p&gt;We consider two asymptotic regimes, called &lt;em&gt;outfill asymptotic&lt;/em&gt;, where both the number of parameters &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and the number of observation &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; increase, but with different speeds according to the chosen setting.
Consider first a model with &lt;span class=&#34;math inline&#34;&gt;\(K=2\)&lt;/span&gt; factors and &lt;span class=&#34;math inline&#34;&gt;\(I_1= I_2 = \{50, 100,250,500,750,1000 \}\)&lt;/span&gt; different possible levels. Suppose that the number of observations grows quadratically wrt the number of parameters, i.e. &lt;span class=&#34;math display&#34;&gt;\[ N= O(p^2)\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[ p \rightarrow +\infty.\]&lt;/span&gt;
Below we report the average meeting time for the sampler with fixed and free variances, alongside with the bound for the collapsed Gibbs sampler with fixed variances, for different values of &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://youngstats.github.io/post/2023-09-27-linear-cost-unbiased-estimator-for-large-crossed-random-effect-models-via-couplings_files/new_reg1_k2.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Average meeting times for Gaussian CREMS&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Results of the simulations suggest that the expected number of iterations converges to &lt;span class=&#34;math inline&#34;&gt;\(O(1)\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; increases, while diverges for the plain vanilla Gibbs sampler (not even plotted because of different scale). It is also clear that the bound closely resembles the estimated average meeting times.&lt;/p&gt;
&lt;p&gt;We consider now an even sparser asymptotic regime, where instead the number of observation grows at the same rate of the number of parameters.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{N}{p} \approx \alpha \]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[ p \rightarrow +\infty.\]&lt;/span&gt;
Estimates for the mean number of iterations for the fixed and free variance case are plotted below, alongside the corresponding theoretical bounds.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://youngstats.github.io/post/2023-09-27-linear-cost-unbiased-estimator-for-large-crossed-random-effect-models-via-couplings_files/new_reg2_k2.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Average meeting times for Gaussian CREMS&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Lastly, we want to highlight that while the bound presented in the Theorem above only applies for Gibbs samplers targeting Gaussian distributions, the methodology is still valid for general target distributions and provide small meeting times. We report below the average meeting times of chains targeting a CREM with Laplace response.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
	\mathcal{L}(y_n) = Laplace \left(\mu +\sum_{k=1}^K a_{i_{k[n]}}^{(k)} \right) \text{ for } n=1,...,N,
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://youngstats.github.io/post/2023-09-27-linear-cost-unbiased-estimator-for-large-crossed-random-effect-models-via-couplings_files/laplace_mh.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Average meeting times for Laplace CREMS&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;about-the-author&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;About the author&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://paoloceriani.github.io&#34;&gt;Paolo Ceriani&lt;/a&gt;, PhD student, Bocconi University, Italy.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.google.com/site/gzanellawebpage/home&#34;&gt;Giacomo Zanella&lt;/a&gt;, Assistant Professor, Bocconi University, Italy.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-gao_16&#34; class=&#34;csl-entry&#34;&gt;
Gao, Katelyn, and Art Owen. 2016a. &lt;span&gt;“Efficient Moment Calculations for Variance Components in Large Unbalanced Crossed Random Effects Models.”&lt;/span&gt; &lt;em&gt;Electronic Journal of Statistics&lt;/em&gt; 11 (January). &lt;a href=&#34;https://doi.org/10.1214/17-EJS1236&#34;&gt;https://doi.org/10.1214/17-EJS1236&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-gao19&#34; class=&#34;csl-entry&#34;&gt;
———. 2016b. &lt;span&gt;“Estimation and Inference for Very Large Linear Mixed Effects Models.”&lt;/span&gt; &lt;em&gt;Statistica Sinica&lt;/em&gt;, October. &lt;a href=&#34;https://doi.org/10.5705/ss.202018.0029&#34;&gt;https://doi.org/10.5705/ss.202018.0029&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-gosh_back&#34; class=&#34;csl-entry&#34;&gt;
Ghosh, Swarnadip, Trevor J. Hastie, and Art B. Owen. 2022. &lt;span&gt;“Backfitting for Large Scale Crossed Random Effects Regressions.”&lt;/span&gt; &lt;em&gt;The Annals of Statistics&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-glynn_rhee&#34; class=&#34;csl-entry&#34;&gt;
Glynn, Peter W., and Chang-han Rhee. 2014. &lt;span&gt;“Exact Estimation for Markov Chain Equilibrium Expectations.”&lt;/span&gt; &lt;em&gt;Journal of Applied Probability&lt;/em&gt; 51A: 377–89. &lt;a href=&#34;http://www.jstor.org/stable/43284129&#34;&gt;http://www.jstor.org/stable/43284129&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-Metropolis&#34; class=&#34;csl-entry&#34;&gt;
Hastings, W. K. 1970. &lt;span&gt;“Monte Carlo Sampling Methods Using Markov Chains and Their Applications.”&lt;/span&gt; &lt;em&gt;Biometrika&lt;/em&gt; 57 (1): 97–109. &lt;a href=&#34;http://www.jstor.org/stable/2334940&#34;&gt;http://www.jstor.org/stable/2334940&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-jacob2019unbiased&#34; class=&#34;csl-entry&#34;&gt;
Jacob, Pierre E., John O’Leary, and Yves F. Atchadé. 2020. &lt;span&gt;“Unbiased Markov Chain Monte Carlo Methods with Couplings.”&lt;/span&gt; &lt;em&gt;Journal of the Royal Statistical Society: Series B (Statistical Methodology)&lt;/em&gt; 82 (3): 543–600. https://doi.org/&lt;a href=&#34;https://doi.org/10.1111/rssb.12336&#34;&gt;https://doi.org/10.1111/rssb.12336&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-papaspiliopoulos2018scalable&#34; class=&#34;csl-entry&#34;&gt;
Papaspiliopoulos, O, G O Roberts, and G Zanella. 2019. &lt;span&gt;“&lt;span class=&#34;nocase&#34;&gt;Scalable inference for crossed random effects models&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;Biometrika&lt;/em&gt; 107 (1): 25–40. &lt;a href=&#34;https://doi.org/10.1093/biomet/asz058&#34;&gt;https://doi.org/10.1093/biomet/asz058&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Algorithmic Fairness</title>
      <link>https://youngstats.github.io/post/2023/09/19/algorithmic-fairness/</link>
      <pubDate>Tue, 19 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2023/09/19/algorithmic-fairness/</guid>
      <description>


&lt;p&gt;&lt;strong&gt;Algorithmic Fairness&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Tuesday, October 3rd, 2023, 7:30 PT / 10:30 ET / 16:30 CET&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-09-19-algorithmic-fairness_files/fairness_algo_img.jpeg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;2nd joint webinar of the &lt;a href=&#34;https://imstat.org/ims-groups/ims-new-researchers-group/&#34;&gt;IMS New Researchers Group&lt;/a&gt;, &lt;a href=&#34;https://math.ethz.ch/sfs/news-and-events/young-data-science.html&#34;&gt;Young Data Science Researcher Seminar Zürich&lt;/a&gt; and the YoungStatS Project.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tuesday, October 3rd, 2023, 7:30 PT / 10:30 ET / 16:30 CET&lt;/li&gt;
&lt;li&gt;Online, &lt;a href=&#34;https://washington.zoom.us/j/93421065642&#34;&gt;via Zoom&lt;/a&gt;. The registration form is available
&lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSdKUvtyyJZZb7dkF0RXnqak5OqBzvwCSUaoYxwvQKdAA3rQaw/viewform&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://linjunz.github.io/&#34;&gt;Linjun Zhang&lt;/a&gt;, Rutgers University: “&lt;em&gt;Fair conformal prediction&lt;/em&gt;”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Abstract: Multi-calibration is a powerful and evolving concept originating in the field of algorithmic fairness. For a predictor f that estimates the outcome y given covariates x, and for a function class C, multi-calibration requires that the predictor f(x) and outcome y are indistinguishable under the class of auditors in C. Fairness is captured by incorporating demographic subgroups into the class of functions C. Recent work has shown that, by enriching the class C to incorporate appropriate propensity reweighting functions, multi-calibration also yields target-independent learning, wherein a model trained on a source domain performs well on unseen, future target domains (approximately) captured by the re-weightings. The multi-calibration notion is extended, and the power of an enriched class of mappings is explored. HappyMap, a generalization of multi-calibration, is proposed, which yields a wide range of new applications, including a new fairness notion for uncertainty quantification (conformal prediction), a novel technique for conformal prediction under covariate shift, and a different approach to analyzing missing data, while also yielding a unified understanding of several existing seemingly disparate algorithmic fairness notions and target-independent learning approaches. A single HappyMap meta-algorithm is given that captures all these results, together with a sufficiency condition for its success.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://moonfolk.github.io/&#34;&gt;Mikhail Yurochkin&lt;/a&gt;, IBM Research and MIT-IBM Watson AI Lab: “&lt;em&gt;Operationalizing Individual Fairness&lt;/em&gt;”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Abstract: Societal applications of ML proved to be challenging due to algorithms replicating or even exacerbating biases in the training data. In response, there is a growing body of research on algorithmic fairness that attempts to address these issues, primarily via group definitions of fairness. In this talk, I will illustrate several shortcomings of group fairness and present an algorithmic fairness pipeline based on individual fairness (IF). IF is often recognized as the more intuitive notion of fairness: we want ML models to treat similar individuals similarly. Despite the benefits, challenges in formalizing the notion of similarity and enforcing equitable treatment prevented the adoption of IF. I will present our work addressing these barriers via algorithms for learning the similarity metric from data and methods for auditing and training fair models utilizing the intriguing connection between individual fairness and adversarial robustness. Finally, I will demonstrate applications of IF with Large Language Models.&lt;/p&gt;
&lt;p&gt;Discussant: &lt;a href=&#34;https://raziehnabi.com/&#34;&gt;Razieh Nabi&lt;/a&gt;, Emory University&lt;/p&gt;
&lt;p&gt;YoungStatS project of the Young Statisticians Europe initiative (FENStatS) is supported by the Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics (IMS).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you missed this webinar, you can &lt;a href=&#34;https://www.youtube.com/watch?v=faPooozTKGk&#34;&gt;watch the recording on our YouTube channel&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Illustration of Graphical Gaussian Process models to analyze highly multivariate spatial data</title>
      <link>https://youngstats.github.io/post/2023/07/07/illustration-of-graphical-gaussian-process-models-to-analyze-highly-multivariate-spatial-data/</link>
      <pubDate>Fri, 07 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2023/07/07/illustration-of-graphical-gaussian-process-models-to-analyze-highly-multivariate-spatial-data/</guid>
      <description>



&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Abundant multivariate spatial data from the natural and environmental sciences demands research on the joint distribution of multiple spatially dependent variables (&lt;span class=&#34;citation&#34;&gt;Wackernagel (&lt;a href=&#34;#ref-wackernagel2013multivariate&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Cressie and Wikle (&lt;a href=&#34;#ref-creswikle11&#34;&gt;2011&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Banerjee and Gelfand (&lt;a href=&#34;#ref-ban14&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;). Here, our goal is to estimate associations over spatial locations for each variable and those among the variables.&lt;/p&gt;
&lt;p&gt;In this document, we will present an example of a simulated multivariate spatial data and how we can use Graphical Gaussian processes (GGP) (&lt;span class=&#34;citation&#34;&gt;Dey and Banerjee (&lt;a href=&#34;#ref-dey2021ggp&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt;) to model the data. First, we’ll introduce the general multivariate spatial model, and then we will introduce a variable graph and how to simulate a Graphical Gaussian process (Matérn) for that variable graph. Next, we will lay out the estimation steps of GGP parameters and how the estimated parameters compare against the truth.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model&lt;/h1&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(y(s)\)&lt;/span&gt; denote a &lt;span class=&#34;math inline&#34;&gt;\(q\times 1\)&lt;/span&gt; vector of spatially-indexed dependent outcomes for any location &lt;span class=&#34;math inline&#34;&gt;\(s \in \mathcal D \subset \mathbb{R}^d\)&lt;/span&gt; with typically &lt;span class=&#34;math inline&#34;&gt;\(d=2\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(3\)&lt;/span&gt;. On our spatial domain &lt;span class=&#34;math inline&#34;&gt;\(\mathcal D\)&lt;/span&gt;, a multivariate spatial regression model provides a marginal spatial regression model for each outcome as follows:
&lt;span class=&#34;math display&#34; id=&#34;eq:mgp&#34;&gt;\[\begin{equation}
y_i(s) = x_i(s)^{T}\beta_i + w_i(s) + \epsilon_i(s)\;,\quad i=1,2,\ldots,q,\; s \in \mathcal D
\tag{1}
\end{equation}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(y_i(s)\)&lt;/span&gt; is the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th element of &lt;span class=&#34;math inline&#34;&gt;\(y(s)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(x_i(s)\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(p_i\times 1\)&lt;/span&gt; vector of predictors, &lt;span class=&#34;math inline&#34;&gt;\(\beta_i\)&lt;/span&gt; is the &lt;span class=&#34;math inline&#34;&gt;\(p_i\times 1\)&lt;/span&gt; vector of slopes, &lt;span class=&#34;math inline&#34;&gt;\(w_i(s)\)&lt;/span&gt; is a spatially correlated process and &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i(s) \stackrel{ind}{\sim} N(0,\tau^2_i)\)&lt;/span&gt; is the random error in outcome &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. We usually assume that &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i(s)\)&lt;/span&gt; are independent across &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, whereas &lt;span class=&#34;math inline&#34;&gt;\(w(s)=(w_1(s), w_2(s),\ldots, w_q(s))^T\)&lt;/span&gt; is a zero-centred multivariate Gaussian process (GP) with a zero mean and a cross-covariance function that introduces dependency across space and among the &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; variables. Let &lt;span class=&#34;math inline&#34;&gt;\(\mathcal S_i\)&lt;/span&gt; represent the collection of places where the &lt;span class=&#34;math inline&#34;&gt;\(i-\)&lt;/span&gt;th variable was observed. The reference set of locations for our approach is &lt;span class=&#34;math inline&#34;&gt;\(\mathcal L = \cup_i\mathcal S_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The definition of &lt;span class=&#34;math inline&#34;&gt;\(w(s)\)&lt;/span&gt; as the &lt;span class=&#34;math inline&#34;&gt;\(q \times 1\)&lt;/span&gt; multivariate graphical Matérn GP (&lt;span class=&#34;citation&#34;&gt;Dey and Banerjee (&lt;a href=&#34;#ref-dey2021ggp&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt;) with respect to a decomposable variable graph &lt;span class=&#34;math inline&#34;&gt;\({\mathcal G}_{\mathcal V}\)&lt;/span&gt; yields the distribution of each &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt;. The marginal parameters for each component Matérn process &lt;span class=&#34;math inline&#34;&gt;\(w_i(\cdot)\)&lt;/span&gt; are denoted by &lt;span class=&#34;math inline&#34;&gt;\(\{\phi_{ii}, \sigma_{ii} | i = 1.\ldots,q\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;It is sufficient to limit the intra-site covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma=(\sigma_{ij})\)&lt;/span&gt; to be of the following form to assure a valid multivariate Matérn cross-covariance function (Theorem 1 of &lt;span class=&#34;citation&#34;&gt;Apanasovich and Sun (&lt;a href=&#34;#ref-apanasovich2012valid&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt;):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:constraints&#34;&gt;\[\begin{equation}
\begin{array}{cc}
   \nu_{ij} =&amp;amp; \frac 12 (\nu_{ii} + \nu_{jj}) + \Delta_A (1 - A_{ij}) \mbox{ where } \Delta_A \geq 0, A=(A_{ij}) \mbox{ for all } i \geq 0, A_{ii}=1 \\
    \sum_{i,j} c_ic_j\phi_{ij} \leq 0  &amp;amp;\mbox{ is a conditionally non-negative definite matrix } \\
    \sigma_{ij} =&amp;amp; b_{ij} \frac{\Gamma(\frac 12 (\nu_{ii}+\nu_{jj} + d))\Gamma(\nu_{ij})}{\phi_{ij}^{2\Delta_A+\nu_{ii}+\nu_{jj}}\Gamma(\nu_{ij} + \frac d2)} \mbox{ where } \Delta_A \geq 0, \mbox{ and } B=(b_{ij}) &amp;gt; 0, \mbox{ i.e., is p.d.}
\end{array}
\tag{2}
\end{equation}\]&lt;/span&gt;
This is equal to &lt;span class=&#34;math inline&#34;&gt;\(\Sigma = (B \odot (\gamma_{ij}))\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{ij}\)&lt;/span&gt;’s are constants collecting the components in &lt;a href=&#34;#eq:constraints&#34;&gt;(2)&lt;/a&gt; involving just &lt;span class=&#34;math inline&#34;&gt;\(\nu_{ij}\)&lt;/span&gt;’s and &lt;span class=&#34;math inline&#34;&gt;\(\phi_{ij}\)&lt;/span&gt;’s, and &lt;span class=&#34;math inline&#34;&gt;\(\odot\)&lt;/span&gt; indicates the Hadamard (element-wise) product.&lt;/p&gt;
&lt;p&gt;In the following example, we’ll assume &lt;span class=&#34;math inline&#34;&gt;\(\Delta_A=0, \nu_{ij} = \frac{\nu_{ii}+\nu_{jj}}{2}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\phi_{ij}^2 = \frac{\phi_{ii}^2 + \phi_{jj}^2}{2}\)&lt;/span&gt;. The constraints in &lt;a href=&#34;#eq:constraints&#34;&gt;(2)&lt;/a&gt; simplifies to -&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:constraints-simp&#34;&gt;\[\begin{equation}
\begin{array}{cc}
    \sigma_{ij} =&amp;amp; (\sigma_{ii} \sigma_{jj})^{\frac{1}{2}} * \frac{\phi_{ii}^{\nu_{ii}}\phi_{jj}^{\nu_{jj}}}{\phi_{ij}^{\nu_{ij}}}  \frac{\Gamma(\nu_{ij})}{\Gamma(\nu_{ii})^{\frac 12} \Gamma(\nu_{ij})^{\frac 12}} r_{ij} \mbox{ where }, \mbox{ and } R=(r_{ij}) &amp;gt; 0, \mbox{ i.e., is p.d.}
\end{array}
\tag{3}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We also take &lt;span class=&#34;math inline&#34;&gt;\(\nu_{ii} = \nu_{jj} = 0.5\)&lt;/span&gt; in our case and hence, we only need to estimate the marginal scale (&lt;span class=&#34;math inline&#34;&gt;\(\phi_{ii}\)&lt;/span&gt;) and variance parameters (&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{ii}\)&lt;/span&gt;) and cross-correlation parameters &lt;span class=&#34;math inline&#34;&gt;\(r_{ij}\)&lt;/span&gt;. We also assume variable graph (&lt;span class=&#34;math inline&#34;&gt;\({\mathcal G}_{\mathcal V}\)&lt;/span&gt;) to be decomposable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simulation&lt;/h1&gt;
&lt;p&gt;As a first step of simulation, we need to fix a decomposable variable graph (&lt;span class=&#34;math inline&#34;&gt;\({\mathcal G}_{\mathcal V}\)&lt;/span&gt;). Depending on the perfect ordering of cliques in the graph, the density function of the full process can be factorized. Using this property, we can calculate the variance-covarinace matrix of the GGP and use it to simulate observation from a multivariate GGP.&lt;/p&gt;
&lt;div id=&#34;decomposable-variable-graph&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Decomposable variable graph&lt;/h2&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\mathcal G_\mathcal V\)&lt;/span&gt; is decomposable, it has a perfect clique sequence &lt;span class=&#34;math inline&#34;&gt;\(\{K_1,K_2,\cdots,K_p\}\)&lt;/span&gt; with separators &lt;span class=&#34;math inline&#34;&gt;\(\{S_2,\ldots,S_m\}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(w(\mathcal L)\)&lt;/span&gt;’s joint density may be factorized as follows.
&lt;span class=&#34;math display&#34; id=&#34;eq:ggp-fact-2&#34;&gt;\[\begin{equation}
    f_M(w(\mathcal L)) = \frac{\Pi_{m=1}^{p} f_C(w_{K_m}( \mathcal L))}{\Pi_{m=2}^{p} f_C(w_{S_m}( \mathcal L))}\;,
\tag{4}
\end{equation}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(S_m=F_{m-1} \cap K_m\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(F_{m-1}= K_1 \cup \cdots \cup K_{m-1}\)&lt;/span&gt;, and
&lt;span class=&#34;math inline&#34;&gt;\(f_A\)&lt;/span&gt; denotes the the density of a GP over &lt;span class=&#34;math inline&#34;&gt;\(\mathcal L\)&lt;/span&gt; with covariance function &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(A \in \{M,C\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Here, we assume we have &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt; variables with each being observed at &lt;span class=&#34;math inline&#34;&gt;\(250\)&lt;/span&gt; locations uniformly chosen from the &lt;span class=&#34;math inline&#34;&gt;\((0,1) \times (0,1)\)&lt;/span&gt; square. We assume the variable graph to be as follows.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-07-07-illustration-of-graphical-gaussian-process-models-to-analyze-highly-multivariate-spatial-data_files/unnamed-chunk-3-1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We calculate the perfect ordering of the cliques of the graph above and list the cliques and separators below. To visualize the decomposition, we also plot the junction tree (definition below) of the graph -&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;junction graph&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; of a decomposable graph &lt;span class=&#34;math inline&#34;&gt;\(\mathcal G_\mathcal V\)&lt;/span&gt; is a complete graph with the cliques of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal G_\mathcal V\)&lt;/span&gt; as its nodes. Every edge in the junction graph is denoted as a link, which is also the intersection of the two cliques, and can be empty. A &lt;strong&gt;spanning tree&lt;/strong&gt; of a graph is defined as a subgraph comprising all the vertices of the original graph and is a tree (acyclic graph). If a spanning tree &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; of the junction graph of &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; satisfies the following property: for any two cliques &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; of the graph, every node in the unique path between &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; in the tree contains &lt;span class=&#34;math inline&#34;&gt;\(C \cap D\)&lt;/span&gt;. Then &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; is called the &lt;strong&gt;junction tree&lt;/strong&gt; for the graph &lt;span class=&#34;math inline&#34;&gt;\(\mathcal G_\mathcal V\)&lt;/span&gt;. Here, the junction tree of the graph has the perfectly ordered cliques as its nodes and the separators denoted as edges.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-07-07-illustration-of-graphical-gaussian-process-models-to-analyze-highly-multivariate-spatial-data_files/unnamed-chunk-4-1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-07-07-illustration-of-graphical-gaussian-process-models-to-analyze-highly-multivariate-spatial-data_files/unnamed-chunk-5-1.png&#34; /&gt;
Hence, in this case, the likelihood of the decomposable GGP would be as follows -
&lt;span class=&#34;math display&#34; id=&#34;eq:ggp-fact-ex&#34;&gt;\[\begin{equation}
    f_M(w(\mathcal L)) = \frac{f_C(w_{(1,2,3)}(\mathcal L)) f_C(w_{(2,3,4)}(\mathcal L)) f_C(w_{(4,5,6)}(\mathcal L)) f_C(w_{(6,7,8)}(\mathcal L)) f_C(w_{(8,9,10)}(\mathcal L))}{f_C(w_{(2,3)}(\mathcal L)) f_C(w_{(4)}(\mathcal L)) f_C(w_{(6)}(\mathcal L)) f_C(w_{(8)}(\mathcal L))}\;,
\tag{5}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulating-latent-ggp&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulating latent GGP&lt;/h2&gt;
&lt;p&gt;First we fix the marginal and cross-covariance parameters of the process.&lt;/p&gt;
&lt;p&gt;Now, the precision matrix of the GGP &lt;span class=&#34;math inline&#34;&gt;\(w(\mathcal L)\)&lt;/span&gt; satisfies (Lemma 5.5 of &lt;span class=&#34;citation&#34;&gt;Lauritzen (&lt;a href=&#34;#ref-lauritzen1996graphical&#34;&gt;1996&lt;/a&gt;)&lt;/span&gt;)
&lt;span class=&#34;math display&#34; id=&#34;eq:m-decomp&#34;&gt;\[\begin{equation}
M(\mathcal L,\mathcal L)^{-1} = \sum_{m=1}^{p} [{C}_{[K_m \boxtimes \mathcal{G_L}]}^{-1}]^{\mathcal V \times \mathcal L} - \sum_{m=2}^{p} [{C}_{[S_m \boxtimes \mathcal{G_L}]}^{-1}] ^{\mathcal V \times \mathcal L}\;, %= \sum_{m=1}^{p} [{M}_{[K_m \boxtimes \mathcal{G_L}]}^{-1}] ^\mathcal V - \sum_{m=2}^{p} [{M}_{[S_m \boxtimes \mathcal{G_L}]}^{-1}] ^\mathcal V\;
\tag{6}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;#eq:m-decomp&#34;&gt;(6)&lt;/a&gt; and &lt;a href=&#34;#eq:ggp-fact-2&#34;&gt;(4)&lt;/a&gt; shows that inverting the full cross-covariance matrix only requires inverting the clique and separator specific covariance matrices. Hence, the computational complexity for calculating likelihood of a multivariate GGP boils down to &lt;span class=&#34;math inline&#34;&gt;\(O(n^3 c^3)\)&lt;/span&gt; (here, &lt;span class=&#34;math inline&#34;&gt;\(O(250^3 * 27)\)&lt;/span&gt;) where &lt;span class=&#34;math inline&#34;&gt;\(c= 3\)&lt;/span&gt; is the maximum size of a clique in the perfect clique ordering of the graph. On the contrary, the likelihood of a full GGP would need &lt;span class=&#34;math inline&#34;&gt;\(O(n^3 q^3)\)&lt;/span&gt; complexity, i.e. &lt;span class=&#34;math inline&#34;&gt;\(O(250^3 * 1000)\)&lt;/span&gt; in our case.&lt;/p&gt;
&lt;p&gt;We use the &lt;a href=&#34;#eq:m-decomp&#34;&gt;(6)&lt;/a&gt; to calculate the covariance of the latent GGP and we use it to simulate the latent process.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulating-multivariate-spatial-outcome&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulating multivariate spatial outcome&lt;/h2&gt;
&lt;p&gt;Now we use &lt;a href=&#34;#eq:mgp&#34;&gt;(1)&lt;/a&gt; to simulate our multivariate outcome &lt;span class=&#34;math inline&#34;&gt;\(y_i(.), i= 1, \cdots, 10\)&lt;/span&gt;. In order to do that, we fix some covariates &lt;span class=&#34;math inline&#34;&gt;\(x_i(\cdot)\)&lt;/span&gt; and simulate error process &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{i}(\cdot)\)&lt;/span&gt; independently from &lt;span class=&#34;math inline&#34;&gt;\(N(0,\tau^2_{ii})\)&lt;/span&gt;. We take &lt;span class=&#34;math inline&#34;&gt;\(\tau_{ii}^2=\tau^2=0.25\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For analyzing the prediction accuracy of our method, we randomly pick &lt;span class=&#34;math inline&#34;&gt;\(20\%\)&lt;/span&gt; of the locations for each outcome variable and consider them missing. We will only be working with the training set to fit the model and judge our prediction accuracy on the test set.&lt;/p&gt;
&lt;p&gt;Now we visualize the surface of the training data by variables below.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data analysis&lt;/h1&gt;
&lt;p&gt;The analysis of our simulated data can be broken down in the following steps.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Marginal parameter estimation&lt;/strong&gt;: We estimate the marginal scale (&lt;span class=&#34;math inline&#34;&gt;\(\phi_{ii}\)&lt;/span&gt;), variance (&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{ii}\)&lt;/span&gt;) and smoothness parameters (&lt;span class=&#34;math inline&#34;&gt;\(\nu_{ii}\)&lt;/span&gt;) from the component Gaussian processes. We also estimate the error variance (&lt;span class=&#34;math inline&#34;&gt;\(\tau^2_{ii}\)&lt;/span&gt;) for each marginal processes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Initialize Gibbs sampling&lt;/strong&gt;: For this step, we need the following components:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Processing the variable graph&lt;/strong&gt;: We fix our variable graph for the estimation process. First we calculate cliques, separators of the graph. Then we color the nodes and edges of the graph for parallel computation purposes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Starting cross-correlation&lt;/strong&gt;: We use the estimated marginal parameters and an initial correlation parameter to start off.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Gibbs sampling&lt;/strong&gt;: We run our Gibbs sampler in two steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Sampling latent spatial processes&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sampling latent correlations&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;marginal-parameter-estimation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Marginal parameter estimation&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We first estimate the marginal process parameters using &lt;strong&gt;BRISC&lt;/strong&gt; package in R. We estimate scale (&lt;span class=&#34;math inline&#34;&gt;\(\phi_{ii}\)&lt;/span&gt;) and variance (&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{ii}\)&lt;/span&gt;) parameters. We fix the marginal smoothness (&lt;span class=&#34;math inline&#34;&gt;\(\nu_ii\)&lt;/span&gt;) and cross-smoothness (&lt;span class=&#34;math inline&#34;&gt;\(\nu_{ij}\)&lt;/span&gt;) parameters at &lt;span class=&#34;math inline&#34;&gt;\(0.5\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Same as the true cross-covariance, we calculate the cross-scale parameters as: &lt;span class=&#34;math inline&#34;&gt;\(\phi_{ij}= \sqrt{\frac{\phi_{ii}^2 + \phi_{jj}^2}{2}}\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;We estimate the marginal regression coefficients (&lt;span class=&#34;math inline&#34;&gt;\(\beta_i\)&lt;/span&gt;).&lt;/li&gt;
&lt;li&gt;We estimate the marginal error variance (&lt;span class=&#34;math inline&#34;&gt;\(\tau_{ii}^2\)&lt;/span&gt;).&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;initialize-gibbs-sampling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Initialize Gibbs sampling&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Process the variable graph&lt;/strong&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;First, we color the nodes of the variable graph. This will allow us to simulate the latent processes belonging to the same color in parallel.&lt;/p&gt;
&lt;p&gt;We also construct a new graph &lt;span class=&#34;math inline&#34;&gt;\(\mathcal G_E(\mathcal G_V)=(E_\mathcal V,E^*)\)&lt;/span&gt; which denotes this graph on the set of edges &lt;span class=&#34;math inline&#34;&gt;\(E_\mathcal V\)&lt;/span&gt;, i.e., there is an edge &lt;span class=&#34;math inline&#34;&gt;\(((i,j),(i&amp;#39;,j&amp;#39;))\)&lt;/span&gt; in this new graph &lt;span class=&#34;math inline&#34;&gt;\(\mathcal G_E(\mathcal G_V)\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\{i,i&amp;#39;,j,j&amp;#39;\}\)&lt;/span&gt; are in some clique &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal G_V\)&lt;/span&gt;. This allows us to facilitate parallel update of cross-correlation parameters corresponding to edges in the same color.&lt;/p&gt;
&lt;p&gt;These two procedures are examples of chromatic Gibbs sampling.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-07-07-illustration-of-graphical-gaussian-process-models-to-analyze-highly-multivariate-spatial-data_files/unnamed-chunk-13-1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;2.&lt;strong&gt;Calculate initial cross-covariance matrix&lt;/strong&gt;: Start with an initial cross-correlation matrix. We Take a convex combination: &lt;span class=&#34;math inline&#34;&gt;\(0.5*diag(q) + 0.5*cor(Y.train)\)&lt;/span&gt;. Then we use this initial cross-correlation parameters and estimated marginal parameters to store the cross covariance matrices for cliques and separators only.&lt;/p&gt;
&lt;p&gt;The largest matrix we need to store is of size &lt;span class=&#34;math inline&#34;&gt;\(nq_c \times nq_c\)&lt;/span&gt; matrix where &lt;span class=&#34;math inline&#34;&gt;\(q_c\)&lt;/span&gt; is the size of maximum clique or separator).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gibbs-sampling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gibbs sampling&lt;/h2&gt;
&lt;p&gt;We iteratively perform the following steps until we reach a desired number of samples (&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;).&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Sampling latent processes&lt;/strong&gt; : Using random draws from multivariate normal distribution&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sampling correlations&lt;/strong&gt;: Metropolis-hastings&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Jumping between graphs&lt;/strong&gt;: Reversible jump MCMC.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;sampling-latent-processes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sampling latent processes&lt;/h3&gt;
&lt;p&gt;To update the latent random effects &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;, let &lt;span class=&#34;math inline&#34;&gt;\(\mathcal L=\{s_1,\ldots,s_n\}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(o_i=\mbox{diag}(I(s_1 \in \mathcal S_i), \ldots, I(s_n \in \mathcal S_i))\)&lt;/span&gt; denote the vector of missing observations for the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th outcome. With &lt;span class=&#34;math inline&#34;&gt;\(X_{i}(\mathcal L) = (x_i(s_1),\ldots,x_i(s_n))^T\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(y_i(\mathcal L)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(w_i(\mathcal L)\)&lt;/span&gt; defined similarly, we obtain
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
\begin{split}
p(w_i(\mathcal L) | \cdot) &amp;amp; \sim N\left(\mathcal M_i^{-1}\mu_i, \mathcal M_i^{-1}\right)\;, \mbox{ where } \\
\mathcal M_i &amp;amp; =\frac{1}{\tau_i^2}\mbox{diag}(o_i) + \sum_{K \ni i}M_{\{i\} \times \mathcal L|(K\setminus \{i\}) \times \mathcal L}^{-1} - \sum_{S \ni i}M^{-1}_{\{i\} \times \mathcal L|(S\setminus \{i\}) \times \mathcal L}\;, \\
\mu_i &amp;amp;= \frac{(y_i(\mathcal L) - x_i(\mathcal L)^{T}\beta_i)\odot o_i}{\tau_i^2} + \\
&amp;amp; \qquad \sum_{K \ni i}  T_{i}(K) w({(K\setminus \{i\}) \times \mathcal L}) - \sum_{S \ni i} T_i(S) w({(S\setminus \{i\}) \times \mathcal L})\;, \\
T_{i}(A) &amp;amp;= M_{\{i\} \times \mathcal L|(A\setminus \{i\}) \times \mathcal L}^{-1}  M_{\{i\} \times \mathcal L,(A\setminus \{i\}) \times \mathcal L}M_{(A\setminus \{i\}) \times \mathcal L}^{-1}, \mbox{ for } A \in \{K,S\}.\\
% U_i(S) &amp;amp;= M_{\{i\} \times \mathcal L|(S\setminus \{i\}) \times \mathcal L}^{-1}  M_{(S\setminus \{i\}) \times \mathcal L,\{i\} \times \mathcal L}M_{(S\setminus \{i\}) \times \mathcal L}^{-1}.
\end{split}\label{eqn: gibbs-sam-latent}
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;%We denote &lt;span class=&#34;math inline&#34;&gt;\(\mathscr T_i= \mathcal L \setminus \mathscr S_i\)&lt;/span&gt;.
% for a clique &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; in variable graph &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{G_V}\)&lt;/span&gt;, the set - &lt;span class=&#34;math inline&#34;&gt;\(K \times \mathcal L = K \times \mathscr S\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(S \times \mathcal L= S \times \mathscr S\)&lt;/span&gt;,
%&lt;span class=&#34;math inline&#34;&gt;\(i_{K}=\{K \setminus i\} \times \mathscr S\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(i \times \mathscr S_i = i_{\mathscr S}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(i \times \mathscr T_i= i_{\mathscr T}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(i_{K} \cup i_{\mathscr T}= i_{K\mathscr T}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(i_{K} \cup i_{\mathscr S}= i_{K \times \mathcal L}\)&lt;/span&gt; . Also, for a clique &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; in containing a variable &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, let’s denote &lt;span class=&#34;math inline&#34;&gt;\(M_{i_{{\mathscr S}.K}} = M_{i_{\mathscr S}} - M_{i_{\mathscr S}, i_{K\mathscr T}}M_{i_{K\mathscr T}}^{-1}M_{i_{K\mathscr T},i_{\mathscr S}}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(M_{i_{{\mathscr T}.K}} = M_{i_{\mathscr T}} - M_{i_{\mathscr T}, i_{K \times \mathcal L}}M_{i_{K \times \mathcal L}}^{-1}M_{i_{K \times \mathcal L},i_{\mathscr T}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sampling-cross-correlation-parameters&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sampling cross-correlation parameters&lt;/h3&gt;
&lt;p&gt;Requires only checking positive-definiteness of the clique-specific cross-covariance matrix and inverting it. The largest matrix inversion across all these updates is of the order &lt;span class=&#34;math inline&#34;&gt;\(nc \times nc\)&lt;/span&gt;, corresponding to the largest clique. The largest matrix that needs storing is also of dimension &lt;span class=&#34;math inline&#34;&gt;\(nc \times nc\)&lt;/span&gt;. These result in appreciable reduction of computations from any multivariate Matérn model that involves &lt;span class=&#34;math inline&#34;&gt;\(nq \times nq\)&lt;/span&gt; matrices and positive-definiteness checks for &lt;span class=&#34;math inline&#34;&gt;\(q \times q\)&lt;/span&gt; matrices at every iteration.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;For every correlation parameter corresponding to edges in the current graph, we draw a new correlation value from the proposal distribution.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We accept or reject the proposal based on the Metropolis-Hastings acceptance probability.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Results&lt;/h1&gt;
&lt;p&gt;We create three plots below among true and estimated parameters for - (1) product of marginal scale and variance parameter (&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{ii}\)&lt;/span&gt;), (2) cross-correlation parameter (&lt;span class=&#34;math inline&#34;&gt;\(r_{ij}\)&lt;/span&gt;) and (3) product of cross-covariance and cross-scale parameter (&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{ij}*\phi_{ij}\)&lt;/span&gt;). The points on all the plots align on the &lt;span class=&#34;math inline&#34;&gt;\(y=x\)&lt;/span&gt; line thus showing the accuracy of our estimation. We also create a plot across &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt; variables showing true test data values and predicted values from our model. The points align on &lt;span class=&#34;math inline&#34;&gt;\(y=x\)&lt;/span&gt; line and mean-square error varied from &lt;span class=&#34;math inline&#34;&gt;\(0.403\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(1.064\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-07-07-illustration-of-graphical-gaussian-process-models-to-analyze-highly-multivariate-spatial-data_files/unnamed-chunk-15-1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-07-07-illustration-of-graphical-gaussian-process-models-to-analyze-highly-multivariate-spatial-data_files/unnamed-chunk-15-2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-07-07-illustration-of-graphical-gaussian-process-models-to-analyze-highly-multivariate-spatial-data_files/unnamed-chunk-15-3.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-07-07-illustration-of-graphical-gaussian-process-models-to-analyze-highly-multivariate-spatial-data_files/unnamed-chunk-15-4.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;about-the-authors&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;About the authors&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://debangandey.rbind.io&#34;&gt;Debangan Dey&lt;/a&gt;, National Institute of Mental Health.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://abhidatta.com/&#34;&gt;Abhirup Datta&lt;/a&gt;, Dept. of Biostatistics, Johns Hopkins Bloomberg School of Public Health.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sudipto.bol.ucla.edu/&#34;&gt;Sudipto Banerjee&lt;/a&gt;, Dept. of Biostatistics, UCLA Fielding School of Public Health.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;bibliography&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;Bibliography&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-apanasovich2012valid&#34; class=&#34;csl-entry&#34;&gt;
Apanasovich, Marc G Genton, Tatiyana V, and Ying Sun. 2012. &lt;span&gt;“A Valid Matern Class of Cross-Covariance Functions for Multivariate Random Fields with Any Number of Components.”&lt;/span&gt; &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 107 (497): 180–93.
&lt;/div&gt;
&lt;div id=&#34;ref-ban14&#34; class=&#34;csl-entry&#34;&gt;
Banerjee, B. P. Carlin, S., and A. E. Gelfand. 2014. &lt;em&gt;Hierarchical Modeling and Analysis for Spatial Data&lt;/em&gt;. Chapman &amp;amp; Hall/CRC, Boca Raton, FL.
&lt;/div&gt;
&lt;div id=&#34;ref-creswikle11&#34; class=&#34;csl-entry&#34;&gt;
Cressie, Noel A. C., and Christopher K. Wikle. 2011. &lt;em&gt;Statistics for Spatio-Temporal Data&lt;/em&gt;. Wiley Series in Probability and Statistics. Wiley, Hoboken, NJ.
&lt;/div&gt;
&lt;div id=&#34;ref-dey2021ggp&#34; class=&#34;csl-entry&#34;&gt;
Dey, Abhirup Datta, Debangan, and Sudipto Banerjee. 2021. &lt;span&gt;“Graphical Gaussian Process Models for Highly Multivariate Spatial Data.”&lt;/span&gt; &lt;em&gt;Biometrika&lt;/em&gt; December.
&lt;/div&gt;
&lt;div id=&#34;ref-lauritzen1996graphical&#34; class=&#34;csl-entry&#34;&gt;
Lauritzen, Steffen L. 1996. &lt;em&gt;Graphical Models&lt;/em&gt;. Vol. 17. Clarendon Press.
&lt;/div&gt;
&lt;div id=&#34;ref-wackernagel2013multivariate&#34; class=&#34;csl-entry&#34;&gt;
Wackernagel, Hans. 2013. &lt;em&gt;Multivariate Geostatistics: An Introduction with Applications&lt;/em&gt;. Springer Science &amp;amp; Business Media.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The scaling limit of Baxter permutations</title>
      <link>https://youngstats.github.io/post/2023/07/02/the-scaling-limit-of-baxter-permutations/</link>
      <pubDate>Sun, 02 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2023/07/02/the-scaling-limit-of-baxter-permutations/</guid>
      <description>


&lt;div id=&#34;meanders-monotone-meanders-and-baxter-permutations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Meanders, monotone meanders, and Baxter permutations&lt;/h2&gt;
&lt;p&gt;Versions of the following question can be traced back at least to the work of Henri Poincaré (&lt;span class=&#34;citation&#34;&gt;Poincaré (&lt;a href=&#34;#ref-poincare-meander&#34;&gt;1912&lt;/a&gt;)&lt;/span&gt;):&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In how many ways a simple loop in the plane can cross a line a specified number of times?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Despite many efforts, this question remains open after more than a century.
Let us start by mathematically formalizing the latter question.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 1.1.&lt;/strong&gt; A &lt;em&gt;meander&lt;/em&gt; of size &lt;span class=&#34;math inline&#34;&gt;\(n\in\mathbb{N}\)&lt;/span&gt; is a simple loop &lt;span class=&#34;math inline&#34;&gt;\(\ell\)&lt;/span&gt; (i.e., a homeomorphic image of the circle) in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R^2}\)&lt;/span&gt; which crosses the real line &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}\)&lt;/span&gt; exactly &lt;span class=&#34;math inline&#34;&gt;\(2n\)&lt;/span&gt; times and does not touch the line without crossing it, viewed modulo orientation-preserving homeomorphisms from &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R^2}\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R^2}\)&lt;/span&gt; which take &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-07-02-the-scaling-limit-of-baxter-permutations_files/Figure%201.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 1: &lt;strong&gt;Left:&lt;/strong&gt; A meander of size &lt;span class=&#34;math inline&#34;&gt;\(6\)&lt;/span&gt;, with the intersection points of the loop and the line labeled by the order in which they are hit by the loop (in black) and the line (in red) starting at the blue vertex.
The associated meandric permutation is &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\ell}=1,4,3,2,5,12,7,8,9,10,11,6\)&lt;/span&gt; (in one-line notation); see Definition 1.1.
&lt;strong&gt;Right:&lt;/strong&gt; The same meander of size &lt;span class=&#34;math inline&#34;&gt;\(6\)&lt;/span&gt; seen as a pair of Jordan curves in the sphere.&lt;/p&gt;
&lt;p&gt;See the left-hand sides of Figures 1 and 2 for some illustrations of meanders.
A meander of size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; can equivalently be thought of as a pair of Jordan curves in the sphere which intersect exactly &lt;span class=&#34;math inline&#34;&gt;\(2n\)&lt;/span&gt; times and which do not intersect without crossing, viewed modulo orientation-preserving homeomorphisms from the sphere to itself.
The correspondence with Definition 1.1 is obtained by viewing &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}\)&lt;/span&gt; as a curve in the Riemann sphere.
See the right-hand side of Figure 1.&lt;/p&gt;
&lt;p&gt;Each meander can be bijectively encoded with the corresponding &lt;em&gt;meandric permutation&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 1.2.&lt;/strong&gt; For a meander &lt;span class=&#34;math inline&#34;&gt;\(\ell\)&lt;/span&gt; of size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, the corresponding &lt;em&gt;meandric permutation&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\ell}\)&lt;/span&gt; is the permutation of &lt;span class=&#34;math inline&#34;&gt;\([1,2n]\cap\mathbb{Z}\)&lt;/span&gt; obtained as follows.
Order the &lt;span class=&#34;math inline&#34;&gt;\(2n\)&lt;/span&gt; intersection points of &lt;span class=&#34;math inline&#34;&gt;\(\ell\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}\)&lt;/span&gt; from left to right.
Then, consider the second order in which these intersection points are hit by &lt;span class=&#34;math inline&#34;&gt;\(\ell\)&lt;/span&gt; when we start &lt;span class=&#34;math inline&#34;&gt;\(\ell\)&lt;/span&gt; from the leftmost intersection point and traverse it counterclockwise.
Then &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\ell (i)=j\)&lt;/span&gt; if some intersection point is the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th visited point by the first order and the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th visited point by the second one.&lt;/p&gt;
&lt;p&gt;See Figure 1 for an example and the right-hand side of Figure 2 for the plots of two large uniform meandric permutations.
Note that any meandric permutation fixes 1.
It is easily verified that the mapping &lt;span class=&#34;math inline&#34;&gt;\(\ell\mapsto\sigma_{\ell}\)&lt;/span&gt; is a bijection from the set of meanders of size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; to the set of meandric permutations of size &lt;span class=&#34;math inline&#34;&gt;\(2n\)&lt;/span&gt;.
The study of meandric permutations dates back to at least the paper &lt;span class=&#34;citation&#34;&gt;Rosenstiehl (&lt;a href=&#34;#ref-rosentiehl-meander&#34;&gt;1984&lt;/a&gt;)&lt;/span&gt;, where they are called “planar permutations”.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-07-02-the-scaling-limit-of-baxter-permutations_files/Meander256-side.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 2: &lt;strong&gt;Left:&lt;/strong&gt; Two large uniform meanders &lt;span class=&#34;math inline&#34;&gt;\(\ell\)&lt;/span&gt; of size 256 and 2048.
&lt;strong&gt;Right:&lt;/strong&gt; The plots of the two corresponding meandric permutations &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\ell}\)&lt;/span&gt;; the blue dots correspond to the values &lt;span class=&#34;math inline&#34;&gt;\((i,\sigma_\ell(i))_{i = 1,\dots,|\sigma_\ell|}\)&lt;/span&gt;.
These simulations are obtained using the Markov chain Monte Carlo algorithm from &lt;span class=&#34;citation&#34;&gt;Heitsch and Tetali (&lt;a href=&#34;#ref-heitsch2011meander&#34;&gt;2011&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The term “meander” was first coined by &lt;span class=&#34;citation&#34;&gt;Arnold (&lt;a href=&#34;#ref-arnold-meander&#34;&gt;1988&lt;/a&gt;)&lt;/span&gt;.
Meanders are of interest in physics and computational biology as models of polymer folding &lt;span class=&#34;citation&#34;&gt;Di Francesco, Golinelli, and Guitter (&lt;a href=&#34;#ref-dgg-meander-folding&#34;&gt;1997&lt;/a&gt;)&lt;/span&gt;.
There is a vast mathematical literature devoted to the enumeration of various types of meanders, which has connections to many different subjects, from combinatorics to theoretical physics and more recently to the geometry of moduli spaces (&lt;span class=&#34;citation&#34;&gt;Delecroix et al. (&lt;a href=&#34;#ref-Delecroix-enum-meand&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;).
See &lt;span class=&#34;citation&#34;&gt;Zvonkin (&lt;a href=&#34;#ref-zvonkin-meander-survey&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; for a brief recent survey of this literature and &lt;span class=&#34;citation&#34;&gt;La Croix (&lt;a href=&#34;#ref-lacroix-meander-survey&#34;&gt;2003&lt;/a&gt;)&lt;/span&gt; for a more detailed account.
Additionally, meanders are the connected components of &lt;em&gt;meandric systems&lt;/em&gt;, recently studied in &lt;span class=&#34;citation&#34;&gt;Curien et al. (&lt;a href=&#34;#ref-ckst-noodle&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Kargin (&lt;a href=&#34;#ref-Kargin-cycles-meander&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Goulden, Nica, and Puder (&lt;a href=&#34;#ref-Goulden-Asymptotics-meandric&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Féray and Thévenin (&lt;a href=&#34;#ref-Feray-components-mendric&#34;&gt;2022&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Fukuda and Nechita (&lt;a href=&#34;#ref-fn-meander-system&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Borga, Gwynne, and Park (&lt;a href=&#34;#ref-borga2022geometry&#34;&gt;2022&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The problem of enumerating &lt;em&gt;all&lt;/em&gt; meanders of size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; – this is the Question (1.1) – is open and seemingly quite difficult. But, it was conjectured by Di Francesco, Golinelli, and Guitter (&lt;span class=&#34;citation&#34;&gt;Di Francesco, Golinelli, and Guitter (&lt;a href=&#34;#ref-dgg-meander-asymptotics&#34;&gt;2000&lt;/a&gt;)&lt;/span&gt;) that the number of meanders of size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; behaves asymptotically like &lt;span class=&#34;math inline&#34;&gt;\(C \cdot A^n n^{-\alpha}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\alpha = (29 + \sqrt{145})/12\)&lt;/span&gt;. This conjecture was later numerically tested by Jensen and Gutmann (&lt;span class=&#34;citation&#34;&gt;Jensen and Guttmann (&lt;a href=&#34;#ref-Jensen-num-meanders&#34;&gt;2000&lt;/a&gt;)&lt;/span&gt;), where they also proposed the numerical estimate of &lt;span class=&#34;math inline&#34;&gt;\(A \approx 12.26\)&lt;/span&gt;. The best known rigorous bounds for the constant &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; are &lt;span class=&#34;math inline&#34;&gt;\(11.380\leq A \leq 12.901\)&lt;/span&gt;, as proved in &lt;span class=&#34;citation&#34;&gt;Albert and Paterson (&lt;a href=&#34;#ref-Albert-bounds-meanders&#34;&gt;2005&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A candidate scaling limit for meanders and meandric permutations was recently proposed and studied in &lt;span class=&#34;citation&#34;&gt;Borga, Gwynne, and Sun (&lt;a href=&#34;#ref-borga2022permutons&#34;&gt;2022&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In this paper, we will address a simpler but very related model, i.e.&lt;em&gt;monotone meanders&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 1.3.&lt;/strong&gt; A &lt;strong&gt;monotone meander&lt;/strong&gt; of size &lt;span class=&#34;math inline&#34;&gt;\(n\in\mathbb{N}\)&lt;/span&gt; is a pair of simple curves &lt;span class=&#34;math inline&#34;&gt;\(\ell_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\ell_2\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\([0,1]^2\)&lt;/span&gt; which cross each other exactly &lt;span class=&#34;math inline&#34;&gt;\(2n-1\)&lt;/span&gt; times and such that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\ell_1\)&lt;/span&gt; starts on the left-hand side of &lt;span class=&#34;math inline&#34;&gt;\([0,1]^2\)&lt;/span&gt;, ends on the right-hand side of &lt;span class=&#34;math inline&#34;&gt;\([0,1]^2\)&lt;/span&gt; and never moves in the left direction.
(Equivalently, it is the graph of a continuous function from &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt;.)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\ell_2\)&lt;/span&gt; starts on the bottom side of &lt;span class=&#34;math inline&#34;&gt;\([0,1]^2\)&lt;/span&gt;, ends on the top side of &lt;span class=&#34;math inline&#34;&gt;\([0,1]^2\)&lt;/span&gt; and never moves in the bottom direction.
(Equivalently, it is the graph of a continuous function from &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt; rotated by 90 degrees.)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As in the case of meanders, to each monotone meander we can associate the corresponding &lt;strong&gt;monotone meandric permutation&lt;/strong&gt;. Two monotone meanders are equivalent if they induce the same monotone meandric permutation.&lt;/p&gt;
&lt;p&gt;See Figure 3 for an example. Note that a monotone meandric permutation maps even numbers to even numbers, and odd numbers to odd numbers. The permutation mapping odd numbers is called a &lt;strong&gt;Baxter permutation&lt;/strong&gt;, while the permutation mapping even numbers is called a &lt;strong&gt;reduced Baxter permutation&lt;/strong&gt;. It can be shown &lt;span class=&#34;citation&#34;&gt;Baxter (&lt;a href=&#34;#ref-baxter-permutation&#34;&gt;1964&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;William M. Boyce (&lt;a href=&#34;#ref-MR0250516&#34;&gt;1967&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;William Martin Boyce (&lt;a href=&#34;#ref-boyce1981&#34;&gt;1981&lt;/a&gt;)&lt;/span&gt; that the Baxter permutation corresponding to a monotone meandric permutation uniquely determine the monotone meandric permutation itself.
Hence there is a natural bijection between monotone meandric permutations and Baxter permutations.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-07-02-the-scaling-limit-of-baxter-permutations_files/Figure%203.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 3: &lt;strong&gt;Left:&lt;/strong&gt; A monotone meander of size &lt;span class=&#34;math inline&#34;&gt;\(6\)&lt;/span&gt;, with the intersection points labeled by the order in which they are hit by the curve &lt;span class=&#34;math inline&#34;&gt;\(\ell_1\)&lt;/span&gt; (in red) and the curve &lt;span class=&#34;math inline&#34;&gt;\(\ell_2\)&lt;/span&gt; (in blue).
&lt;strong&gt;Right:&lt;/strong&gt; The associated monotone meandric permutation: the Baxter permutation corresponds to the black dots, while the reduced Baxter permutation corresponds to the white dots.&lt;/p&gt;
&lt;p&gt;Baxter permutations were introduced by Glen Baxter in 1964 &lt;span class=&#34;citation&#34;&gt;Baxter (&lt;a href=&#34;#ref-baxter-permutation&#34;&gt;1964&lt;/a&gt;)&lt;/span&gt; while studying fixed points of commuting functions. They are classical examples of pattern-avoiding permutations, which have been intensively studied both in the probabilistic and combinatorial literature (see e.g. &lt;span class=&#34;citation&#34;&gt;William M. Boyce (&lt;a href=&#34;#ref-MR0250516&#34;&gt;1967&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Chung et al. (&lt;a href=&#34;#ref-MR491652&#34;&gt;1978&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Mallows (&lt;a href=&#34;#ref-MR555815&#34;&gt;1979&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Bousquet-Mélou (&lt;a href=&#34;#ref-MR2028288&#34;&gt;2002/03&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Canary (&lt;a href=&#34;#ref-MR2679559&#34;&gt;2010&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Felsner et al. (&lt;a href=&#34;#ref-MR2763051&#34;&gt;2011&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Bouvel et al. (&lt;a href=&#34;#ref-MR3882946&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;). They are known to be connected with various other interesting combinatorial structures, such as bipolar orientations &lt;span class=&#34;citation&#34;&gt;Bonichon, Bousquet-Mélou, and Fusy (&lt;a href=&#34;#ref-BBMF11&#34;&gt;2010&lt;/a&gt;)&lt;/span&gt;, walks in cones &lt;span class=&#34;citation&#34;&gt;Kenyon et al. (&lt;a href=&#34;#ref-KMSW19&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;, certain pairs of binary trees and a family of triples of non-intersecting lattice paths &lt;span class=&#34;citation&#34;&gt;Felsner et al. (&lt;a href=&#34;#ref-MR2763051&#34;&gt;2011&lt;/a&gt;)&lt;/span&gt;, and domino tilings of Aztec diamonds &lt;span class=&#34;citation&#34;&gt;Canary (&lt;a href=&#34;#ref-MR2679559&#34;&gt;2010&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The diagrams of two large uniform Baxter permutations is plotted in Figure 4.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-07-02-the-scaling-limit-of-baxter-permutations_files/Figure4.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 4: The diagrams of two uniform Baxter permutations of size 3253 and 4520.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;main-results-the-permuton-limit-of-baxter-permutations-and-its-intensity-measure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Main results: the permuton limit of Baxter permutations and its intensity measure&lt;/h2&gt;
&lt;p&gt;In recent years there has been an increasing interest in studying limits of random non-uniform permutations. One approach is to look at the convergence of relevant statistics, such as the number of cycles, the number of inversions, or the length of the longest increasing subsequence. For a brief overview of this approach see e.g. &lt;span class=&#34;citation&#34;&gt;Borga (&lt;a href=&#34;#ref-borga2021random&#34;&gt;2021a&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The more recent approach is to directly determine the scaling limits of permutation diagrams. Here given a permutation &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; of size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, its &lt;em&gt;diagram&lt;/em&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; table with &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; points at position &lt;span class=&#34;math inline&#34;&gt;\((i,\sigma(i))\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(i\in[n]:=\{1,2,\dots,n\}\)&lt;/span&gt;. (See the right-hand side of Figure 3 for an example.) Their scaling limits are called &lt;em&gt;permutons&lt;/em&gt;. See e.g. &lt;span class=&#34;citation&#34;&gt;Borga (&lt;a href=&#34;#ref-borga2021random&#34;&gt;2021a&lt;/a&gt;)&lt;/span&gt; for an overview of this approach.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 2.1.&lt;/strong&gt; A Borel probability measure &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; on the unit square &lt;span class=&#34;math inline&#34;&gt;\([0,1]^2\)&lt;/span&gt; is a &lt;strong&gt;permuton&lt;/strong&gt; if both of its marginals are uniform, i.e., &lt;span class=&#34;math inline&#34;&gt;\(\mu([a,b]\times[0,1]) = \mu([0,1]\times[a,b])=b-a\)&lt;/span&gt; for any &lt;span class=&#34;math inline&#34;&gt;\(0\le a&amp;lt;b\le 1\)&lt;/span&gt;.
A permutation &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; can be viewed as a permuton &lt;span class=&#34;math inline&#34;&gt;\(\mu_{\sigma}\)&lt;/span&gt; by uniformly distributing mass to the squares &lt;span class=&#34;math inline&#34;&gt;\(\{[\frac{i-1}{n}, \frac{i}{n}]\times [\frac{\sigma(i)-1}{n}, \frac{\sigma(i)}{n}]: i \in [n]\}.\)&lt;/span&gt; More precisely, &lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
        \mu_\sigma(A)
        =
        n
        \sum_{i=1}^n
        Leb
        \big([(i-1)/n, i/n]
        \times
        [(\sigma(i)-1)/n,\sigma(i)/n]
        \cap
        A\big),
    \end{equation*}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is a Borel measurable set of &lt;span class=&#34;math inline&#34;&gt;\([0,1]^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For a &lt;em&gt;deterministic&lt;/em&gt; sequence of permutations &lt;span class=&#34;math inline&#34;&gt;\(\sigma_n\)&lt;/span&gt;, we say that &lt;span class=&#34;math inline&#34;&gt;\(\sigma_n\)&lt;/span&gt; &lt;em&gt;converge in the permuton sense&lt;/em&gt; to a limiting permuton &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, if the permutons &lt;span class=&#34;math inline&#34;&gt;\(\mu_{\sigma_n}\)&lt;/span&gt; induced by &lt;span class=&#34;math inline&#34;&gt;\(\sigma_n\)&lt;/span&gt; converge weakly to &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;.
The set of permutons equipped with the topology of weak convergence of measures can be viewed as a compact metric space.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;Dokos and Pak (&lt;a href=&#34;#ref-MR3238333&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt; studied the expected limiting permuton of the so-called &lt;em&gt;doubly alternating Baxter permutations&lt;/em&gt;.
The authors raised the question of proving the existence of the &lt;em&gt;Baxter permuton&lt;/em&gt; as the scaling limit of uniform Baxter permutations, and determine its expected density.
The existence of the Baxter permuton was established in &lt;span class=&#34;citation&#34;&gt;Borga and Maazoun (&lt;a href=&#34;#ref-BM20&#34;&gt;2022a&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 2.2&lt;/strong&gt; (&lt;span class=&#34;citation&#34;&gt;Borga and Maazoun (&lt;a href=&#34;#ref-bm-baxter-permutation&#34;&gt;2022b&lt;/a&gt;)&lt;/span&gt;, Theorem 1.9) Let &lt;span class=&#34;math inline&#34;&gt;\(\sigma_n\)&lt;/span&gt; be a uniform Baxter permutation of size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;.
The following convergence w.r.t. the permuton topology holds: &lt;span class=&#34;math inline&#34;&gt;\(\mu_{\sigma_n}\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\xrightarrow{d}\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\mu_B\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\mu_B\)&lt;/span&gt; is a random permuton called the &lt;em&gt;Baxter permuton&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remark 2.3.&lt;/strong&gt; In &lt;span class=&#34;citation&#34;&gt;Borga (&lt;a href=&#34;#ref-borga2021skewperm&#34;&gt;2021b&lt;/a&gt;)&lt;/span&gt;, a two-parameter family of permutons called the &lt;em&gt;skew Brownian permuton&lt;/em&gt; was introduced.
This family includes the Baxter permuton and a well-studied one-parameter family of permutons, called the &lt;em&gt;biased Brownian separable permuton&lt;/em&gt; (&lt;span class=&#34;citation&#34;&gt;Bassino et al. (&lt;a href=&#34;#ref-bassino2018separable&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Bassino et al. (&lt;a href=&#34;#ref-bassino2017universal&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;), as special cases.&lt;/p&gt;
&lt;p&gt;The Baxter permuton &lt;span class=&#34;math inline&#34;&gt;\(\mu_B\)&lt;/span&gt; is a &lt;em&gt;random&lt;/em&gt; probability measure on the unit square (with uniform marginals); recall Figure 4.
The next result is an explicit expression of its intensity measure, defined by &lt;span class=&#34;math inline&#34;&gt;\(\mathbb E [\mu_{B}](\cdot):=\mathbb E [\mu_{B}(\cdot)]\)&lt;/span&gt;, which answers &lt;span class=&#34;citation&#34;&gt;Dokos and Pak (&lt;a href=&#34;#ref-MR3238333&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 2.4.&lt;/strong&gt; (&lt;span class=&#34;citation&#34;&gt;Borga et al. (&lt;a href=&#34;#ref-bhsy-baxter-permuton&#34;&gt;2023&lt;/a&gt;)&lt;/span&gt;, Theorem 1.3) Consider the Baxter permuton &lt;span class=&#34;math inline&#34;&gt;\(\mu_B\)&lt;/span&gt;.
Define the function &lt;span class=&#34;math display&#34;&gt;\[\begin{equation}\label{eqn-rho}
        \rho(t, x, r):= \frac{1}{t^2} \left(\left(\frac{3rx}{2t}-1\right)e^{-\frac{r^2+x^2-rx}{2t}}+e^{-\frac{(x+r)^2}{2t}}\right).
    \end{equation}\]&lt;/span&gt; Then the intensity measure &lt;span class=&#34;math inline&#34;&gt;\(\mathbb E [\mu_B]\)&lt;/span&gt; is absolutely continuous with respect to the Lebesgue measure on &lt;span class=&#34;math inline&#34;&gt;\([0,1]^2\)&lt;/span&gt;.
{Moreover}, it has the following density function &lt;span class=&#34;math display&#34;&gt;\[\begin{equation}\label{eqn-baxter-density}
        p_B(x, y) = c\int_{\max\{0, x+y-1\}}^{\min\{x, y\}}\int_{\mathbb R_{+}^4}\rho(y-z, \ell_1, \ell_2)\rho(z, \ell_2, \ell_3)\rho(x-z, \ell_3, \ell_4)\rho(1+z-x-y, \ell_4, \ell_1)\,\ d\ell_1 d\ell_2 d\ell_3 d\ell_4\, dz,
    \end{equation}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is a normalizing constant.&lt;/p&gt;
&lt;p&gt;We remark that, further computation of the integral (2.2) is tricky, as it involves integrating a four-dimensional Gaussian in the first quadrant.
We also recall that the intensity measure of other universal random limiting permutons has been investigated in the literature.
For instance, the intensity measure of the &lt;em&gt;biased Brownian separable permuton&lt;/em&gt;, was determined by Maazoun in &lt;span class=&#34;citation&#34;&gt;Maazoun (&lt;a href=&#34;#ref-MR4079636&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;.
We recall that the biased Brownian separable permuton &lt;span class=&#34;math inline&#34;&gt;\(\mu^q_S\)&lt;/span&gt;, defined for all &lt;span class=&#34;math inline&#34;&gt;\(q\in[0,1]\)&lt;/span&gt;, is a one-parameter universal family of limiting permutons arising form pattern-avoiding permutations. In &lt;span class=&#34;citation&#34;&gt;Maazoun (&lt;a href=&#34;#ref-MR4079636&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;, it was proved that for all &lt;span class=&#34;math inline&#34;&gt;\(q\in(0,1)\)&lt;/span&gt;, the intensity measure &lt;span class=&#34;math inline&#34;&gt;\(\mathbb E[\mu^q_S]\)&lt;/span&gt; of the biased Brownian separable permuton is absolutely continuous with respect to the Lebesgue measure on &lt;span class=&#34;math inline&#34;&gt;\([0,1]^2\)&lt;/span&gt;.
Furthermore, &lt;span class=&#34;math inline&#34;&gt;\(\mathbb E[\mu^q_S]\)&lt;/span&gt; has the following density function&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation*}
    p^q_S(x, y) =\int_{\max\{0,x+y -1\}}^{\min\{x,y\}} \frac{3q^2(1-q)^2 \,da }
    {2\pi(a(x-a)(1-x-y+a)(y-a))^{3/2}{\left(\frac{q^2}{a}+\frac{(1-q)^2}{(x-a)}+\frac{q^2}{(1-x-y+a)}+\frac{(1-q)^2}{(y-a)}\right)^{5/2}}}.
\end{equation*}\]&lt;/span&gt; The proof of &lt;span class=&#34;citation&#34;&gt;Maazoun (&lt;a href=&#34;#ref-MR4079636&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; relies on an explicit construction of the biased Brownian separable permuton &lt;span class=&#34;math inline&#34;&gt;\(\mu^q_S\)&lt;/span&gt; from a one-dimensional Brownian excursion decorated with i.i.d. plus and minus signs.
To the best of our knowledge this proof cannot be easily extended to the Baxter permuton case.
Our proof of Theorem 2.4 uses the combination of two main achievements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A connection (&lt;span class=&#34;citation&#34;&gt;Borga (&lt;a href=&#34;#ref-borga2021skewperm&#34;&gt;2021b&lt;/a&gt;)&lt;/span&gt;) between the Baxter permuton (and more generally the skew Brownian permuton of Remark 2.3) and some well-known families of random curves and random surfaces: the &lt;strong&gt;Schramm-Loewner evolution (SLE)&lt;/strong&gt; curves and the &lt;strong&gt;Liouville quantum gravity (LQG)&lt;/strong&gt; spheres.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Some integrability results (&lt;span class=&#34;citation&#34;&gt;Ang, Holden, and Sun (&lt;a href=&#34;#ref-AHS20&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;) for SLE curves and LQG spheres which go under the name of &lt;strong&gt;conformal welding of quantum surfaces&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Explaining the results above would require a long digression; hence we just conclude this article with some plots of &lt;span class=&#34;math inline&#34;&gt;\(p_B(x, y)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p^q_S(x, y)\)&lt;/span&gt; using numerical approximations of the integrals.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-07-02-the-scaling-limit-of-baxter-permutations_files/Figure5.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 5: &lt;strong&gt;From left to right:&lt;/strong&gt; The diagrams of the densities &lt;span class=&#34;math inline&#34;&gt;\(p_S^{0.1}(x,y)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(p_S^{0.4}(x,y)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(p_S^{0.5}(x,y)\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(p_B(x,y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-07-02-the-scaling-limit-of-baxter-permutations_files/Figure6.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 6: Some sections of the densities &lt;span class=&#34;math inline&#34;&gt;\(p_S^{0.5}(x,y)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p_B(x,y)\)&lt;/span&gt;.
&lt;strong&gt;From left to right:&lt;/strong&gt; In red (resp. in blue) we plot the diagrams of &lt;span class=&#34;math inline&#34;&gt;\(p_S^{0.5}(x,x)\)&lt;/span&gt; (resp. &lt;span class=&#34;math inline&#34;&gt;\(p_B(x,x)\)&lt;/span&gt;), &lt;span class=&#34;math inline&#34;&gt;\(p_S^{0.5}(x,1/2)\)&lt;/span&gt; (resp. &lt;span class=&#34;math inline&#34;&gt;\(p_B(x,1/2)\)&lt;/span&gt;), and &lt;span class=&#34;math inline&#34;&gt;\(p_S^{0.5}(x,1/4)\)&lt;/span&gt; (resp. &lt;span class=&#34;math inline&#34;&gt;\(p_B(x,1/4)\)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bibliography&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;Bibliography&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-Albert-bounds-meanders&#34; class=&#34;csl-entry&#34;&gt;
Albert, M. H., and M. S. Paterson. 2005. &lt;span&gt;“Bounds for the Growth Rate of Meander Numbers.”&lt;/span&gt; &lt;em&gt;J. Combin. Theory Ser. A&lt;/em&gt; 112 (2): 250–62. &lt;a href=&#34;https://doi.org/10.1016/j.jcta.2005.02.006&#34;&gt;https://doi.org/10.1016/j.jcta.2005.02.006&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-AHS20&#34; class=&#34;csl-entry&#34;&gt;
Ang, Morris, Nina Holden, and Xin Sun. 2020. &lt;span&gt;“Conformal Welding of Quantum Disks.”&lt;/span&gt; &lt;em&gt;arXiv Preprint:2009.08389&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-arnold-meander&#34; class=&#34;csl-entry&#34;&gt;
Arnold, V. I. 1988. &lt;span&gt;“The Branched Covering &lt;span&gt;&lt;span class=&#34;math inline&#34;&gt;\({\bf C}{\rm P}^2\to S^4\)&lt;/span&gt;&lt;/span&gt;, Hyperbolicity and Projective Topology.”&lt;/span&gt; &lt;em&gt;Sibirsk. Mat. Zh.&lt;/em&gt; 29 (5): 36–47, 237. &lt;a href=&#34;https://doi.org/10.1007/BF00970265&#34;&gt;https://doi.org/10.1007/BF00970265&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-bassino2017universal&#34; class=&#34;csl-entry&#34;&gt;
Bassino, Frédérique, Mathilde Bouvel, Valentin Féray, Lucas Gerin, Mickaël Maazoun, and Adeline Pierrot. 2020. &lt;span&gt;“Universal Limits of Substitution-Closed Permutation Classes.”&lt;/span&gt; &lt;em&gt;J. Eur. Math. Soc. (JEMS)&lt;/em&gt; 22 (11): 3565–3639. &lt;a href=&#34;https://doi.org/10.4171/jems/993&#34;&gt;https://doi.org/10.4171/jems/993&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-bassino2018separable&#34; class=&#34;csl-entry&#34;&gt;
Bassino, Frédérique, Mathilde Bouvel, Valentin Féray, Lucas Gerin, and Adeline Pierrot. 2018. &lt;span&gt;“The &lt;span&gt;B&lt;/span&gt;rownian Limit of Separable Permutations.”&lt;/span&gt; &lt;em&gt;Ann. Probab.&lt;/em&gt; 46 (4): 2134–89. &lt;a href=&#34;https://doi.org/10.1214/17-AOP1223&#34;&gt;https://doi.org/10.1214/17-AOP1223&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-baxter-permutation&#34; class=&#34;csl-entry&#34;&gt;
Baxter, Glen. 1964. &lt;span&gt;“On Fixed Points of the Composite of Commuting Functions.”&lt;/span&gt; &lt;em&gt;Proc. Amer. Math. Soc.&lt;/em&gt; 15: 851–55.
&lt;/div&gt;
&lt;div id=&#34;ref-BBMF11&#34; class=&#34;csl-entry&#34;&gt;
Bonichon, Nicolas, Mireille Bousquet-Mélou, and Éric Fusy. 2010. &lt;span&gt;“Baxter Permutations and Plane Bipolar Orientations.”&lt;/span&gt; &lt;em&gt;S&lt;span&gt;é&lt;/span&gt;minaire Lotharingien de Combinatoire&lt;/em&gt; 61: B61Ah.
&lt;/div&gt;
&lt;div id=&#34;ref-borga2021random&#34; class=&#34;csl-entry&#34;&gt;
Borga, Jacopo. 2021a. &lt;span&gt;“Random Permutations – a Geometric Point of View.”&lt;/span&gt; &lt;em&gt;arXiv Preprint:2107.09699 (Ph.D. Thesis)&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-borga2021skewperm&#34; class=&#34;csl-entry&#34;&gt;
———. 2021b. &lt;span&gt;“The Skew &lt;span&gt;B&lt;/span&gt;rownian Permuton: A New Universality Class for Random Constrained Permutations.”&lt;/span&gt; &lt;em&gt;Proceedings of the London Mathematical Society (to Appear)&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-borga2022geometry&#34; class=&#34;csl-entry&#34;&gt;
Borga, Jacopo, Ewain Gwynne, and Minjae Park. 2022. &lt;span&gt;“On the Geometry of Uniform Meandric Systems.”&lt;/span&gt; &lt;em&gt;arXiv Preprint:2212.00534&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-borga2022permutons&#34; class=&#34;csl-entry&#34;&gt;
Borga, Jacopo, Ewain Gwynne, and Xin Sun. 2022. &lt;span&gt;“Permutons, Meanders, and &lt;span&gt;SLE&lt;/span&gt;-Decorated &lt;span&gt;L&lt;/span&gt;iouville Quantum Gravity.”&lt;/span&gt; &lt;em&gt;arXiv Preprint:2207.02319&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-bhsy-baxter-permuton&#34; class=&#34;csl-entry&#34;&gt;
Borga, Jacopo, Nina Holden, Xin Sun, and Pu Yu. 2023. &lt;span&gt;“Baxter Permuton and &lt;span&gt;L&lt;/span&gt;iouville Quantum Gravity.”&lt;/span&gt; &lt;em&gt;Probability Theory and Related Fields&lt;/em&gt;, 1–49.
&lt;/div&gt;
&lt;div id=&#34;ref-bm-baxter-permutation&#34; class=&#34;csl-entry&#34;&gt;
Borga, Jacopo, and Mickaël Maazoun. 2022b. &lt;span&gt;“Scaling and Local Limits of &lt;span&gt;B&lt;/span&gt;axter Permutations and Bipolar Orientations Through Coalescent-Walk Processes.”&lt;/span&gt; &lt;em&gt;Ann. Probab.&lt;/em&gt; 50 (4): 1359–1417. &lt;a href=&#34;https://doi.org/10.1214/21-aop1559&#34;&gt;https://doi.org/10.1214/21-aop1559&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-BM20&#34; class=&#34;csl-entry&#34;&gt;
———. 2022a. &lt;span&gt;“Scaling and Local Limits of &lt;span&gt;B&lt;/span&gt;axter Permutations and Bipolar Orientations Through Coalescent-Walk Processes.”&lt;/span&gt; &lt;em&gt;Ann. Probab.&lt;/em&gt; 50 (4): 1359–1417. &lt;a href=&#34;https://doi.org/10.1214/21-aop1559&#34;&gt;https://doi.org/10.1214/21-aop1559&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-MR2028288&#34; class=&#34;csl-entry&#34;&gt;
Bousquet-Mélou, M. 2002/03. &lt;span&gt;“Four Classes of Pattern-Avoiding Permutations Under One Roof: Generating Trees with Two Labels.”&lt;/span&gt; &lt;em&gt;Electron. J. Combin.&lt;/em&gt; 9 (2): Research paper 19, 31. &lt;a href=&#34;http://www.combinatorics.org/Volume_9/Abstracts/v9i2r19.html&#34;&gt;http://www.combinatorics.org/Volume_9/Abstracts/v9i2r19.html&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-MR3882946&#34; class=&#34;csl-entry&#34;&gt;
Bouvel, Mathilde, Veronica Guerrini, Andrew Rechnitzer, and Simone Rinaldi. 2018. &lt;span&gt;“Semi-&lt;span&gt;B&lt;/span&gt;axter and Strong-&lt;span&gt;B&lt;/span&gt;axter: Two Relatives of the &lt;span&gt;B&lt;/span&gt;axter Sequence.”&lt;/span&gt; &lt;em&gt;SIAM J. Discrete Math.&lt;/em&gt; 32 (4): 2795–2819. &lt;a href=&#34;https://doi.org/10.1137/17M1126734&#34;&gt;https://doi.org/10.1137/17M1126734&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-MR0250516&#34; class=&#34;csl-entry&#34;&gt;
Boyce, William M. 1967. &lt;span&gt;“Generation of a Class of Permutations Associated with Commuting Functions.”&lt;/span&gt; &lt;em&gt;Math. Algorithms 2 (1967), 19–26; Addendum, Ibid.&lt;/em&gt; 3: 25–26.
&lt;/div&gt;
&lt;div id=&#34;ref-boyce1981&#34; class=&#34;csl-entry&#34;&gt;
Boyce, William Martin. 1981. &lt;span&gt;“Baxter Permutations and Functional Composition.”&lt;/span&gt; &lt;em&gt;Houston Journal Of Mathematics&lt;/em&gt; 7 (2).
&lt;/div&gt;
&lt;div id=&#34;ref-MR2679559&#34; class=&#34;csl-entry&#34;&gt;
Canary, Hal. 2010. &lt;span&gt;“Aztec Diamonds and &lt;span&gt;B&lt;/span&gt;axter Permutations.”&lt;/span&gt; &lt;em&gt;Electron. J. Combin.&lt;/em&gt; 17 (1): Research Paper 105, 12. &lt;a href=&#34;http://www.combinatorics.org/Volume_17/Abstracts/v17i1r105.html&#34;&gt;http://www.combinatorics.org/Volume_17/Abstracts/v17i1r105.html&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-MR491652&#34; class=&#34;csl-entry&#34;&gt;
Chung, Fan-Rong K., Ronald L. Graham, Verner Emil Hoggatt Jr., and Mark Kleiman. 1978. &lt;span&gt;“The Number of &lt;span&gt;B&lt;/span&gt;axter Permutations.”&lt;/span&gt; &lt;em&gt;J. Combin. Theory Ser. A&lt;/em&gt; 24 (3): 382–94. &lt;a href=&#34;https://doi.org/10.1016/0097-3165(78)90068-7&#34;&gt;https://doi.org/10.1016/0097-3165(78)90068-7&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-ckst-noodle&#34; class=&#34;csl-entry&#34;&gt;
Curien, Nicolas, Gady Kozma, Vladas Sidoravicius, and Laurent Tournier. 2019. &lt;span&gt;“Uniqueness of the Infinite Noodle.”&lt;/span&gt; &lt;em&gt;Ann. Inst. Henri Poincaré D&lt;/em&gt; 6 (2): 221–38. &lt;a href=&#34;https://doi.org/10.4171/AIHPD/70&#34;&gt;https://doi.org/10.4171/AIHPD/70&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-Delecroix-enum-meand&#34; class=&#34;csl-entry&#34;&gt;
Delecroix, Vincent, Élise Goujard, Peter Zograf, and Anton Zorich. 2020. &lt;span&gt;“Enumeration of Meanders and &lt;span&gt;M&lt;/span&gt;asur-&lt;span&gt;V&lt;/span&gt;eech Volumes.”&lt;/span&gt; &lt;em&gt;Forum Math. Pi&lt;/em&gt; 8: e4, 80. &lt;a href=&#34;https://doi.org/10.1017/fmp.2020.2&#34;&gt;https://doi.org/10.1017/fmp.2020.2&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-dgg-meander-folding&#34; class=&#34;csl-entry&#34;&gt;
Di Francesco, P., O. Golinelli, and E. Guitter. 1997. &lt;span&gt;“Meander, Folding, and Arch Statistics.”&lt;/span&gt; &lt;em&gt;Math. Comput. Modelling&lt;/em&gt; 26 (8-10): 97–147. &lt;a href=&#34;https://doi.org/10.1016/S0895-7177(97)00202-1&#34;&gt;https://doi.org/10.1016/S0895-7177(97)00202-1&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-dgg-meander-asymptotics&#34; class=&#34;csl-entry&#34;&gt;
———. 2000. &lt;span&gt;“Meanders: Exact Asymptotics.”&lt;/span&gt; &lt;em&gt;Nuclear Phys. B&lt;/em&gt; 570 (3): 699–712. &lt;a href=&#34;https://doi.org/10.1016/S0550-3213(99)00753-1&#34;&gt;https://doi.org/10.1016/S0550-3213(99)00753-1&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-MR3238333&#34; class=&#34;csl-entry&#34;&gt;
Dokos, Theodore, and Igor Pak. 2014. &lt;span&gt;“The Expected Shape of Random Doubly Alternating &lt;span&gt;B&lt;/span&gt;axter Permutations.”&lt;/span&gt; &lt;em&gt;Online J. Anal. Comb.&lt;/em&gt; 9: 12.
&lt;/div&gt;
&lt;div id=&#34;ref-MR2763051&#34; class=&#34;csl-entry&#34;&gt;
Felsner, Stefan, Éric Fusy, Marc Noy, and David Orden. 2011. &lt;span&gt;“Bijections for &lt;span&gt;B&lt;/span&gt;axter Families and Related Objects.”&lt;/span&gt; &lt;em&gt;J. Combin. Theory Ser. A&lt;/em&gt; 118 (3): 993–1020. &lt;a href=&#34;https://doi.org/10.1016/j.jcta.2010.03.017&#34;&gt;https://doi.org/10.1016/j.jcta.2010.03.017&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-Feray-components-mendric&#34; class=&#34;csl-entry&#34;&gt;
Féray, Valentin, and Paul Thévenin. 2022. &lt;span&gt;“&lt;span class=&#34;nocase&#34;&gt;Components in meandric systems and the infinite noodle&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;ArXiv e-Prints&lt;/em&gt;, January. &lt;a href=&#34;https://arxiv.org/abs/\arxiv{2201.11572}&#34;&gt;https://arxiv.org/abs/\arxiv{2201.11572}&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-fn-meander-system&#34; class=&#34;csl-entry&#34;&gt;
Fukuda, Motohisa, and Ion Nechita. 2019. &lt;span&gt;“Enumerating Meandric Systems with Large Number of Loops.”&lt;/span&gt; &lt;em&gt;Ann. Inst. Henri Poincaré D&lt;/em&gt; 6 (4): 607–40. &lt;a href=&#34;https://doi.org/10.4171/AIHPD/80&#34;&gt;https://doi.org/10.4171/AIHPD/80&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-Goulden-Asymptotics-meandric&#34; class=&#34;csl-entry&#34;&gt;
Goulden, I. P., Alexandru Nica, and Doron Puder. 2020. &lt;span&gt;“Asymptotics for a Class of Meandric Systems, via the &lt;span&gt;H&lt;/span&gt;asse Diagram of &lt;span&gt;&lt;span class=&#34;math inline&#34;&gt;\({\rm NC}(n)\)&lt;/span&gt;&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;Int. Math. Res. Not. IMRN&lt;/em&gt;, no. 4: 983–1034. &lt;a href=&#34;https://doi.org/10.1093/imrn/rny044&#34;&gt;https://doi.org/10.1093/imrn/rny044&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-heitsch2011meander&#34; class=&#34;csl-entry&#34;&gt;
Heitsch, Christine E, and Prasad Tetali. 2011. &lt;span&gt;“Meander Graphs.”&lt;/span&gt; In &lt;em&gt;Discrete Mathematics and Theoretical Computer Science&lt;/em&gt;, 469–80. Discrete Mathematics; Theoretical Computer Science.
&lt;/div&gt;
&lt;div id=&#34;ref-Jensen-num-meanders&#34; class=&#34;csl-entry&#34;&gt;
Jensen, Iwan, and Anthony J. Guttmann. 2000. &lt;span&gt;“Critical Exponents of Plane Meanders.”&lt;/span&gt; &lt;em&gt;J. Phys. A&lt;/em&gt; 33 (21): L187–92. &lt;a href=&#34;https://doi.org/10.1088/0305-4470/33/21/101&#34;&gt;https://doi.org/10.1088/0305-4470/33/21/101&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-Kargin-cycles-meander&#34; class=&#34;csl-entry&#34;&gt;
Kargin, Vladislav. 2020. &lt;span&gt;“Cycles in Random Meander Systems.”&lt;/span&gt; &lt;em&gt;J. Stat. Phys.&lt;/em&gt; 181 (6): 2322–45. &lt;a href=&#34;https://doi.org/10.1007/s10955-020-02665-2&#34;&gt;https://doi.org/10.1007/s10955-020-02665-2&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-KMSW19&#34; class=&#34;csl-entry&#34;&gt;
Kenyon, Richard, Jason Miller, Scott Sheffield, and David B Wilson. 2019. &lt;span&gt;“Bipolar Orientations on Planar Maps and &lt;span&gt;SLE&lt;/span&gt;&lt;span class=&#34;math inline&#34;&gt;\(_{12}\)&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;The Annals of Probability&lt;/em&gt; 47 (3): 1240–69.
&lt;/div&gt;
&lt;div id=&#34;ref-lacroix-meander-survey&#34; class=&#34;csl-entry&#34;&gt;
La Croix, Michael. 2003. &lt;span&gt;“&lt;span&gt;A&lt;/span&gt;pproaches to the Enumerative Theory of Meanders.”&lt;/span&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-MR4079636&#34; class=&#34;csl-entry&#34;&gt;
Maazoun, Mickaël. 2020. &lt;span&gt;“On the &lt;span&gt;B&lt;/span&gt;rownian Separable Permuton.”&lt;/span&gt; &lt;em&gt;Combin. Probab. Comput.&lt;/em&gt; 29 (2): 241–66. &lt;a href=&#34;https://doi.org/10.1017/s0963548319000300&#34;&gt;https://doi.org/10.1017/s0963548319000300&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-MR555815&#34; class=&#34;csl-entry&#34;&gt;
Mallows, Colin L. 1979. &lt;span&gt;“Baxter Permutations Rise Again.”&lt;/span&gt; &lt;em&gt;J. Combin. Theory Ser. A&lt;/em&gt; 27 (3): 394–96.
&lt;/div&gt;
&lt;div id=&#34;ref-poincare-meander&#34; class=&#34;csl-entry&#34;&gt;
Poincaré, Henri. 1912. &lt;span&gt;“&lt;span&gt;S&lt;/span&gt;ur Un t&lt;span&gt;é&lt;/span&gt;or&lt;span&gt;è&lt;/span&gt;me de g&lt;span&gt;é&lt;/span&gt;om&lt;span&gt;é&lt;/span&gt;trie.”&lt;/span&gt; &lt;em&gt;Rend. Del Circ. Mat. Palermo&lt;/em&gt; 33: 375–407.
&lt;/div&gt;
&lt;div id=&#34;ref-rosentiehl-meander&#34; class=&#34;csl-entry&#34;&gt;
Rosenstiehl, P. 1984. &lt;span&gt;“Planar Permutations Defined by Two Intersecting &lt;span&gt;J&lt;/span&gt;ordan Curves.”&lt;/span&gt; In &lt;em&gt;Graph Theory and Combinatorics (&lt;span&gt;C&lt;/span&gt;ambridge, 1983)&lt;/em&gt;, 259–71. Academic Press, London.
&lt;/div&gt;
&lt;div id=&#34;ref-zvonkin-meander-survey&#34; class=&#34;csl-entry&#34;&gt;
Zvonkin, Alexander. 2021. &lt;span&gt;“&lt;span&gt;M&lt;/span&gt;eanders: A &lt;span&gt;P&lt;/span&gt;ersonal &lt;span&gt;P&lt;/span&gt;erspective.”&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian nonparametric modeling of conditional multidimensional dependence structures</title>
      <link>https://youngstats.github.io/post/2023/05/19/bayesian-nonparametric-modeling-of-conditional-multidimensional-dependence-structures/</link>
      <pubDate>Fri, 19 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2023/05/19/bayesian-nonparametric-modeling-of-conditional-multidimensional-dependence-structures/</guid>
      <description>


&lt;script type=&#34;text/x-mathjax-config&#34;&gt;
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: &#34;AMS&#34; } }
});
&lt;/script&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;In many real data applications we are often required to model jointly &lt;span class=&#34;math inline&#34;&gt;\(d\geq 3\)&lt;/span&gt; continuous random variables, denoted as &lt;span class=&#34;math inline&#34;&gt;\(Y_1,\dots,Y_d\)&lt;/span&gt; . The multivariate distribution, which allows us to describe the joint behaviour of those variables, can be denoted as &lt;span class=&#34;math inline&#34;&gt;\(F(Y_1,\dots,Y_d)=P(Y_1\le y_1,\dots,Y_d,\le y_d)\)&lt;/span&gt; . However, complex relations between data, particularly asymmetric and tail dependent associations, are often difficult to be modelled. The copula approach allows us to express the multivariate distribution of a set of variables by separating the marginals from the dependence structure. Furthermore, the idea of modelling the effect of covariates on the dependence structure described by copulas has recently attracted increasing attention.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;proposal&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Proposal&lt;/h2&gt;
&lt;p&gt;In &lt;span class=&#34;citation&#34;&gt;Barone and Dalla Valle (&lt;a href=&#34;#ref-barone2023bayesian&#34; role=&#34;doc-biblioref&#34;&gt;2023&lt;/a&gt;)&lt;/span&gt; we provide a flexible Bayesian mixture model that returns easy-to-interpret results, estimating the effects of covariates on high-dimensional dependence structures and showing good performances in both clustering with unknown number of components and density estimation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dirichlet-process-mixture-of-conditional-vines&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dirichlet process mixture of conditional vines&lt;/h2&gt;
&lt;p&gt;Let us consider &lt;span class=&#34;math inline&#34;&gt;\(Y_1, \ldots, Y_d\)&lt;/span&gt; , which are continuous random variables of interest and let &lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}=(X_1, \ldots, X_p)\)&lt;/span&gt; be a vector of covariates that may affect the dependence between &lt;span class=&#34;math inline&#34;&gt;\(Y_1, \ldots, Y_d\)&lt;/span&gt;. Then, the conditional joint distribution function of &lt;span class=&#34;math inline&#34;&gt;\((Y_1, \ldots, Y_d)\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}= \textbf{x}\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
F_x (y_1, \ldots, y_d) = P(Y_1 \leq y_1, \ldots, Y_d \leq y_d | \textbf{X}= \textbf{x}),
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;under the assumption that such conditional distribution exists (see &lt;span class=&#34;citation&#34;&gt;Gijbels et al. (&lt;a href=&#34;#ref-gijbels2012multivariate&#34; role=&#34;doc-biblioref&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Abegaz, Gijbels, and Veraverbeke (&lt;a href=&#34;#ref-abegaz2012semiparametric&#34; role=&#34;doc-biblioref&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Acar, Craiu, and Yao (&lt;a href=&#34;#ref-acar2011dependence&#34; role=&#34;doc-biblioref&#34;&gt;2011&lt;/a&gt;)&lt;/span&gt; ). We denote the conditional marginals of &lt;span class=&#34;math inline&#34;&gt;\(F_x\)&lt;/span&gt; as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray*}F_{1,x}(y_1) &amp;amp; = &amp;amp; P(Y_1 \leq y_1 | \textbf{X}= \textbf{x}), \\&amp;amp; &amp;amp; \ldots \\F_{d,x}(y_d) &amp;amp; = &amp;amp; P(Y_d \leq y_d | \textbf{X}= \textbf{x}). \end{eqnarray*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If the marginals are continuous, then Sklar’s theorem &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-sklar1959fonctions&#34; role=&#34;doc-biblioref&#34;&gt;Sklar 1959&lt;/a&gt;)&lt;/span&gt; allows us to write&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
C_x (u_1, \ldots, u_d) = F_x \left( F_{1,x}^{-1} (u_1), \ldots, F_{d,x}^{-1} (u_d) \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(F_{j,x}^{-1} (u_j) = \inf \left\{ y_j : F_{j,x} \geq u_j \right\}\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(j = 1,\ldots, d\)&lt;/span&gt; , are the conditional quantile functions and &lt;span class=&#34;math inline&#34;&gt;\(u_j = F_{j,x}(y_j)\)&lt;/span&gt;. The conditional copula &lt;span class=&#34;math inline&#34;&gt;\(C_x\)&lt;/span&gt; fully describes the conditional dependence structure of &lt;span class=&#34;math inline&#34;&gt;\((Y_1, \ldots, Y_d)\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}= \textbf{x}\)&lt;/span&gt;. Therefore, the conditional joint distribution function can be written as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
F_x (Y_1, \ldots, Y_d) = C_x \left( F_{1,x}(y_1), \ldots, F_{d,x} (y_d) \right).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let us denote the copula density corresponding to the distribution &lt;span class=&#34;math inline&#34;&gt;\(C_x \left( F_{1x}(y_1), \ldots, F_{dx} (y_d) \right)\)&lt;/span&gt; as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
c_x \left(u_1, \ldots, u_d \right) = c_{\boldsymbol{\theta}} (u_1, \ldots, u_d | \textbf{x}) = c_{\boldsymbol{\theta}(\textbf{x})} (u_1, \ldots, u_d),
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\theta}\)&lt;/span&gt; is the parameter vector of the &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;-variate copula density. We assume that the function &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\theta}(\textbf{x})\)&lt;/span&gt; depends on a vector of parameters &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\beta}\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\label{condcop}
c_{\boldsymbol{\theta}(\textbf{x})} (u_1, \ldots, u_d) = c_{\boldsymbol{\theta}(\textbf{x}| \boldsymbol{\beta})} (u_1, \ldots, u_d) = c_{1:d} (u_1, \ldots, u_d \,  |  \, \boldsymbol{\theta}(\textbf{x}| \boldsymbol{\beta})).
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The \eqref{condcop} can be written in terms of vines &lt;span class=&#34;citation&#34;&gt;Czado (&lt;a href=&#34;#ref-czado2019analyzing&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;, where each pair-copula depends on the vector of covariates &lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}\)&lt;/span&gt;.&lt;/p&gt;
&lt;center&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://youngstats.github.io/post/2023-05-19-bayesian-nonparametric-modeling-of-conditional-multidimensional-dependence-structures_files/vine.png&#34; width=&#34;350&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Trivariate vine representation.&lt;/p&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;p&gt;The vine representation can be generalized to special vine distribution classes, the most popular of which are D-vines (see &lt;span class=&#34;citation&#34;&gt;Bedford and Cooke (&lt;a href=&#34;#ref-bedford2001probability&#34; role=&#34;doc-biblioref&#34;&gt;2001&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Aas et al. (&lt;a href=&#34;#ref-aas2009pair&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Czado (&lt;a href=&#34;#ref-czado2019analyzing&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;).The conditional D-vine decomposition takes the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{multline*}c_{1:d} (u_1,\ldots, u_d \,| \,\boldsymbol{\theta}(\textbf{x}| \boldsymbol{\beta})) = \\ \prod_{\ell=1}^{d-1}\prod_{k=1}^{d-\ell}c_{k,\ell+k; k+1, \ldots, k+\ell-1}\left\{F_{k | k+1, \ldots, k+\ell-1,x} (y_k | y_{k+1, \ldots, k+\ell-1}),  \right. \\ \left. F_{\ell+k | k+1, \ldots, k+\ell-1,x} (y_{\ell+k}| y_{k+1, \ldots, k+\ell-1}) \,  |  \,  \boldsymbol{\theta}_{k,\ell+k; k+1, \ldots, k+\ell-1}(\textbf{x}| \boldsymbol{\beta})\right\}.\end{multline*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In &lt;span class=&#34;citation&#34;&gt;Barone and Dalla Valle (&lt;a href=&#34;#ref-barone2023bayesian&#34; role=&#34;doc-biblioref&#34;&gt;2023&lt;/a&gt;)&lt;/span&gt;, we model multivariate dependence structures specified as the product of &lt;span class=&#34;math inline&#34;&gt;\(\nu=d(d-1)/2\)&lt;/span&gt; pair copulas, indexed by the &lt;span class=&#34;math inline&#34;&gt;\(\nu\times(q+1)\)&lt;/span&gt;-dimensional vector of parameters &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\beta}\)&lt;/span&gt;. The covariates &lt;span class=&#34;math inline&#34;&gt;\(f_{h}(x_h)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(h=1,\dots,p\)&lt;/span&gt;, are independent random variables with parameters &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\phi}=\left (\phi_{1},\dots,\phi_p \right)\)&lt;/span&gt;. Note that &lt;span class=&#34;math inline&#34;&gt;\(q\ge p\)&lt;/span&gt; and its value depends on the chosen link function; for example if the link is linear &lt;span class=&#34;math inline&#34;&gt;\(q=p\)&lt;/span&gt;. Let the vector of parameters &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\xi}=\left (\boldsymbol{\beta},\boldsymbol{\phi} \right)\)&lt;/span&gt; be defined on the parameter space &lt;span class=&#34;math inline&#34;&gt;\(\Xi\)&lt;/span&gt;. We rewrite the density &lt;span class=&#34;math inline&#34;&gt;\(\textbf{f}_{G}(\textbf{x}) \cdot c_G(\cdot, \ldots, \cdot | \textbf{x})\)&lt;/span&gt; as an infinite mixture of conditional vine copulas with kernel &lt;span class=&#34;math inline&#34;&gt;\(\textbf{f}_{\boldsymbol{\xi}}(\textbf{x})\cdot c_{\boldsymbol{\xi}}(\cdot, \ldots, \cdot | \textbf{x})\)&lt;/span&gt; with respect to the mixing measure &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;, that is&lt;span class=&#34;math display&#34;&gt;\[
 \textbf{f}_{G} (\textbf{x}) c_{G} (u_1, \ldots, u_d  | \textbf{x}) = 
\int  \textbf{f}_{\boldsymbol{\phi_j}}(\textbf{x})  c_{1:d}(u_1, \ldots, u_d \, | \, \boldsymbol{\theta}(\textbf{x} | \boldsymbol{\beta}_j))dG(\boldsymbol{\xi}).
\]&lt;/span&gt;With a Dirichlet Process (DP) prior on &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;, we get a Dirichlet Process Mixture (DPM) of conditional vine copulas, which may be alternatively represented as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\textbf{f}_{\boldsymbol{\phi}} (\textbf{x}) c_{ \boldsymbol{\theta}(\textbf{x} | \boldsymbol{\beta}) } (u_1, \ldots, u_d \, | \, \textbf{x}) = \sum_{j=1}^{\infty} \omega_j \,\textbf{f}_{\boldsymbol{\phi_j}}(\textbf{x})  c_{1:d}(u_1, \ldots, u_d \, | \, \boldsymbol{\theta}(\textbf{x} | \boldsymbol{\beta}_j)),
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the weights &lt;span class=&#34;math inline&#34;&gt;\(\omega_j\)&lt;/span&gt; sum to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;. The posterior distribution &lt;span class=&#34;math inline&#34;&gt;\(\Pi(G| \textbf{Y},\textbf{X})\)&lt;/span&gt; is a mixture of DP models, mixing with respect to the latent variables &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\xi}_i\)&lt;/span&gt; specific to each observation &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\((F_1(y_{1i}),\dots,F_d(y_{di}))\)&lt;/span&gt; &lt;/p&gt;
&lt;p&gt;for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,N\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
G| \textbf{Y},\textbf{X} \sim\int DP\left (MG_0+\sum_{i=1}^N\delta_{\boldsymbol{\phi}_i\boldsymbol{\beta}_i } \right )d\Pi(\boldsymbol{\phi},\boldsymbol{\beta}|\textbf{y},\textbf{x}),
\]&lt;/span&gt;where &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; is the concentration parameter, &lt;span class=&#34;math inline&#34;&gt;\(G_0\)&lt;/span&gt; is the centring measure and &lt;span class=&#34;math inline&#34;&gt;\(\delta_t\)&lt;/span&gt; denotes the Dirac measure at &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Posterior inference is performed via MCMC sampling by using a Pólya-urn scheme for integrating out of the model the random distribution function from the Dirichlet process &lt;span class=&#34;citation&#34;&gt;MacEachern and Müller (&lt;a href=&#34;#ref-maceachern1998estimating&#34; role=&#34;doc-biblioref&#34;&gt;1998&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;financial-development-and-natural-disasters-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Financial development and natural disasters data&lt;/h2&gt;
&lt;p&gt;We present an application to a heterogeneous dataset to study the impacts of worldwide natural disasters on international financial development. We define a &lt;span class=&#34;math inline&#34;&gt;\(4\)&lt;/span&gt;-dimensional vine copula with marginals denoting the FD index in &lt;span class=&#34;math inline&#34;&gt;\(4\)&lt;/span&gt; consecutive years and consider the occurrence of a natural disaster as a binary covariate taking value &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; if the total damage is over 100 million dollars and &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; otherwise. The pair copulas parameters are associated to the covariates through a link function &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt;, such that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\rho (\textbf{x} | \boldsymbol{\beta}) = g^{-1} ( \eta(\textbf{x} | \boldsymbol{\beta}) )
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(g^{-1}\)&lt;/span&gt; is the Fisher’s transform and &lt;span class=&#34;math inline&#34;&gt;\(\eta(\cdot)\)&lt;/span&gt; is the calibration function &lt;span class=&#34;math inline&#34;&gt;\(\eta=\beta_0 +\beta_1 X\)&lt;/span&gt;. We set &lt;span class=&#34;math inline&#34;&gt;\(M=1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(G_0\)&lt;/span&gt; as a flat multivariate Gaussian distribution centred on a vector of zeros.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://youngstats.github.io/post/2023-05-19-bayesian-nonparametric-modeling-of-conditional-multidimensional-dependence-structures_files/FDdata.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;The top left panel shows the barplot of the number of observations allocated to the two estimated mixture components. The top right panel compares the posterior densities of the calibration function parameter &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1_{12}}\)&lt;/span&gt; (which is related to the first time interval (12)) for the first (solid line) and the second (dashed line) mixture components. The left and right bottom panels show, for the first and second mixture components, the boxplots of the calibration function parameters &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt; for the first, second and third time intervals (12; 23; 34).&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The two estimated mixture components present substantial differences in terms of how they are impacted by natural disasters. For the first cluster (&lt;span class=&#34;math inline&#34;&gt;\(\psi=1\)&lt;/span&gt;) the model estimates a general negative effect which tends to remain constant until the fourth year;  instead, for the second cluster (&lt;span class=&#34;math inline&#34;&gt;\(\psi=2\)&lt;/span&gt;) the model estimates a positive effect of the natural disaster on the time dependence between yearly FD indexes. &lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;take-home-message&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Take home message&lt;/h2&gt;
&lt;p&gt;In &lt;span class=&#34;citation&#34;&gt;Barone and Dalla Valle (&lt;a href=&#34;#ref-barone2023bayesian&#34; role=&#34;doc-biblioref&#34;&gt;2023&lt;/a&gt;)&lt;/span&gt; we present an innovative methodology that allows for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;flexible modeling of high-dimensional dependency structures, also considering the impact of one or more covariates and accounting for individual as well as temporal heterogeneity in a natural way;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;clustering without assuming the number of components a priori and density estimation;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;easy interpretation of the results.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;about-the-authors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;About the authors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://economia.uniroma2.it/faculty/675/barone-rosario&#34;&gt;Rosario Barone&lt;/a&gt;, University of Rome Tor Vergata.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.plymouth.ac.uk/staff/luciana-dalla-valle&#34;&gt;Luciana Dalla Valle&lt;/a&gt;, University of Plymouth.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-aas2009pair&#34; class=&#34;csl-entry&#34;&gt;
Aas, Kjersti, Claudia Czado, Arnoldo Frigessi, and Henrik Bakken. 2009. &lt;span&gt;“Pair-Copula Constructions of Multiple Dependence.”&lt;/span&gt; &lt;em&gt;Insurance: Mathematics and Economics&lt;/em&gt; 44 (2): 182–98.
&lt;/div&gt;
&lt;div id=&#34;ref-abegaz2012semiparametric&#34; class=&#34;csl-entry&#34;&gt;
Abegaz, Fentaw, Irène Gijbels, and Noël Veraverbeke. 2012. &lt;span&gt;“Semiparametric Estimation of Conditional Copulas.”&lt;/span&gt; &lt;em&gt;Journal of Multivariate Analysis&lt;/em&gt; 110: 43–73.
&lt;/div&gt;
&lt;div id=&#34;ref-acar2011dependence&#34; class=&#34;csl-entry&#34;&gt;
Acar, Elif F, Radu V Craiu, and Fang Yao. 2011. &lt;span&gt;“Dependence Calibration in Conditional Copulas: A Nonparametric Approach.”&lt;/span&gt; &lt;em&gt;Biometrics&lt;/em&gt; 67 (2): 445–53.
&lt;/div&gt;
&lt;div id=&#34;ref-barone2023bayesian&#34; class=&#34;csl-entry&#34;&gt;
Barone, Rosario, and Luciana Dalla Valle. 2023. &lt;span&gt;“Bayesian Nonparametric Modeling of Conditional Multidimensional Dependence Structures.”&lt;/span&gt; &lt;em&gt;Journal of Computational and Graphical Statistics&lt;/em&gt;, 1–10.
&lt;/div&gt;
&lt;div id=&#34;ref-bedford2001probability&#34; class=&#34;csl-entry&#34;&gt;
Bedford, Tim, and Roger M Cooke. 2001. &lt;span&gt;“Probability Density Decomposition for Conditionally Dependent Random Variables Modeled by Vines.”&lt;/span&gt; &lt;em&gt;Annals of Mathematics and Artificial Intelligence&lt;/em&gt; 32 (1-4): 245–68.
&lt;/div&gt;
&lt;div id=&#34;ref-czado2019analyzing&#34; class=&#34;csl-entry&#34;&gt;
Czado, Claudia. 2019. &lt;em&gt;Analyzing Dependent Data with Vine Copulas&lt;/em&gt;. &lt;em&gt;Lecture Notes in Statistics, Springer&lt;/em&gt;. Springer.
&lt;/div&gt;
&lt;div id=&#34;ref-gijbels2012multivariate&#34; class=&#34;csl-entry&#34;&gt;
Gijbels, Irene, Marek Omelka, Noël Veraverbeke, et al. 2012. &lt;span&gt;“Multivariate and Functional Covariates and Conditional Copulas.”&lt;/span&gt; &lt;em&gt;Electronic Journal of Statistics&lt;/em&gt; 6: 1273–1306.
&lt;/div&gt;
&lt;div id=&#34;ref-maceachern1998estimating&#34; class=&#34;csl-entry&#34;&gt;
MacEachern, Steven N, and Peter Müller. 1998. &lt;span&gt;“Estimating Mixture of Dirichlet Process Models.”&lt;/span&gt; &lt;em&gt;Journal of Computational and Graphical Statistics&lt;/em&gt; 7 (2): 223–38.
&lt;/div&gt;
&lt;div id=&#34;ref-sklar1959fonctions&#34; class=&#34;csl-entry&#34;&gt;
Sklar, A. 1959. &lt;span&gt;“Fonctions d&lt;span&gt;é&lt;/span&gt; Repartition &lt;span class=&#34;nocase&#34;&gt;à&lt;/span&gt; n Dimension Et Leurs Marges.”&lt;/span&gt; &lt;em&gt;Universit&lt;span&gt;é&lt;/span&gt; Paris&lt;/em&gt; 8 (3.2): 1–3.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Chaotic mixing and the statistical properties of scalar turbulence</title>
      <link>https://youngstats.github.io/post/2023/04/10/chaotic-mixing-and-the-statistical-properties-of-scalar-turbulence/</link>
      <pubDate>Mon, 10 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2023/04/10/chaotic-mixing-and-the-statistical-properties-of-scalar-turbulence/</guid>
      <description>


&lt;p&gt;Passive scalar turbulence is the study of how a scalar quantity, such as
temperature or salinity, is transported by an incompressible fluid. This
process is modeled by the &lt;em&gt;advection diffusion equation&lt;/em&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\partial_tg_t + u_t\cdot\nabla g_t - \kappa \Delta g_t = s_t,\label{eqAD}\tag{AD}
\end{equation}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(g_t\)&lt;/span&gt; is the scalar quantity, &lt;span class=&#34;math inline&#34;&gt;\(u_t\)&lt;/span&gt; is an
incompressible velocity field, &lt;span class=&#34;math inline&#34;&gt;\(\kappa&amp;gt;0\)&lt;/span&gt; is the diffusivity parameter
and &lt;span class=&#34;math inline&#34;&gt;\(s_t\)&lt;/span&gt; is a replenishing source. As &lt;span class=&#34;math inline&#34;&gt;\(g_t\)&lt;/span&gt; evolves, it often settles
into a statistical steady state and complex self-similar structures
arise due to repeated stretching and folding by the velocity field.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/A7T4U4sYDAs&#34; frameborder=&#34;0&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;Figure 1: A numerical simulation of scalar turbulence on &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{T}^2\)&lt;/span&gt;
advected by the stochastic Navier-Stokes equations (all rights to the video content belong to Sam Punshon-Smith).&lt;/p&gt;
&lt;p&gt;In his 1959 work (&lt;span class=&#34;citation&#34;&gt;Batchelor (&lt;a href=&#34;#ref-Batchelor59&#34;&gt;1959&lt;/a&gt;)&lt;/span&gt;) Batchelor made a significant step toward
understanding these structures. He predicted that, on average, the &lt;span class=&#34;math inline&#34;&gt;\(L^2\)&lt;/span&gt;
power spectral density of &lt;span class=&#34;math inline&#34;&gt;\(g_t\)&lt;/span&gt; displays a &lt;span class=&#34;math inline&#34;&gt;\(|k|^{-1}\)&lt;/span&gt; power law
(Batchelor’s law) for the &lt;span class=&#34;math inline&#34;&gt;\(L^2\)&lt;/span&gt; power spectral density of &lt;span class=&#34;math inline&#34;&gt;\(g_t\)&lt;/span&gt; along
frequencies &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; in the so-called &lt;em&gt;viscous convective range&lt;/em&gt;, i.e.,
length-scales sufficiently small such that the fluid motion is
viscosity-dominated but large enough so as not to be dissipated by
molecular diffusion. This law has since been verified in physical,
numerical, and experimental settings (e.g. &lt;span class=&#34;citation&#34;&gt;Grant et al. (&lt;a href=&#34;#ref-GrantEtAl68&#34;&gt;1968&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Antonia and Orlandi (&lt;a href=&#34;#ref-AO03&#34;&gt;2003&lt;/a&gt;)&lt;/span&gt;,
&lt;span class=&#34;citation&#34;&gt;Gibson and Schwarz (&lt;a href=&#34;#ref-GibsonSchwarz63&#34;&gt;1963&lt;/a&gt;)&lt;/span&gt;) and is frequently used by scientists to predict the
distribution of pollutants and biological matter in the ocean and
atmosphere. Despite this success, Batchelor’s law has evaded rigorous
mathematical proof.&lt;/p&gt;
&lt;p&gt;The purpose of this post is to report progress with Jacob Bedrossian and
Alex Blumenthal on the development of rigorous mathematical tools for
studying Batchelor’s law when &lt;span class=&#34;math inline&#34;&gt;\(u_t\)&lt;/span&gt; evolves according to a randomly
forced fluid model. The primary example is the &lt;em&gt;incompressible
stochastic Navier-Stokes equations&lt;/em&gt; on &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{T}^2\)&lt;/span&gt;,&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\partial_t u_t + u_t\cdot\nabla u_t + \nabla p_t - \nu \Delta u_t = \xi_t, \\
\mathbf{div} u_t=0,\label{eqSNS}\tag{SNS}
\end{equation}\]&lt;/span&gt;
&lt;p&gt;though other well-posed models (not restricted two dimensions) can be
considered. Here the stochastic forcing &lt;span class=&#34;math inline&#34;&gt;\(\xi_t\)&lt;/span&gt; is assumed to be a
non-degenerate, white-in-time, spatially Sobolev regular Gaussian
forcing. The viscosity parameter &lt;span class=&#34;math inline&#34;&gt;\(\nu &amp;gt; 0\)&lt;/span&gt; can be considered the inverse
Reynolds number.&lt;/p&gt;
&lt;p&gt;For this model and a host of other fluid models, in &lt;span class=&#34;citation&#34;&gt;Bedrossian, Blumenthal, and Punshon-Smith (&lt;a href=&#34;#ref-BBPSbatch19&#34;&gt;2019a&lt;/a&gt;)&lt;/span&gt; we
prove a version Batchelor’s prediction on the cumulative power spectrum,
when the viscosity parameter &lt;span class=&#34;math inline&#34;&gt;\(\nu&amp;gt;0\)&lt;/span&gt; is fixed.&lt;/p&gt;
&lt;p&gt;Theorem 1 &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-BBPSbatch19&#34;&gt;Bedrossian, Blumenthal, and Punshon-Smith 2019a&lt;/a&gt;)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\Pi_{\leq N}\)&lt;/span&gt; denote the projection onto Fourier modes with
&lt;span class=&#34;math inline&#34;&gt;\(|k|\leq N\)&lt;/span&gt;. Let the source &lt;span class=&#34;math inline&#34;&gt;\(s_t\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{eqAD}\)&lt;/span&gt; be a white-in-time
Gaussian process and &lt;span class=&#34;math inline&#34;&gt;\(u_t\)&lt;/span&gt; be given by &lt;span class=&#34;math inline&#34;&gt;\(\eqref{eqSNS}\)&lt;/span&gt; as described above.
Then there exists a unique stationary probability measure &lt;span class=&#34;math inline&#34;&gt;\(\mu^\kappa\)&lt;/span&gt;
for the Markov process &lt;span class=&#34;math inline&#34;&gt;\((u_t,g_t)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\kappa\)&lt;/span&gt;-independent constants
&lt;span class=&#34;math inline&#34;&gt;\(C \geq 1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\ell_0\leq 1\)&lt;/span&gt; such that&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\frac{1}{C_0}\log N \leq \mathbb{E}_{\mu^\kappa} \|\Pi_{\leq N}g\|^{2}_{L^2}  \leq C_0\log N \quad \text{for}\quad \ell_{0}^{-1} \leq |k|\leq \kappa^{-1/2}.\label{eqBL}\tag{BL}
\end{equation}\]&lt;/span&gt;
&lt;div id=&#34;uniform-in-kappa-exponential-mixing-and-batchelors-law&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Uniform-in-&lt;span class=&#34;math inline&#34;&gt;\(\kappa\)&lt;/span&gt; exponential mixing and Batchelor’s law&lt;/h2&gt;
&lt;p&gt;The key ingredient in obtaining Batchelor’s law is the mixing properties
of the velocity field &lt;span class=&#34;math inline&#34;&gt;\(u_t\)&lt;/span&gt;, and a quantitative understanding of how
that mixing interacts with the diffusion. In the absence of a scalar
source (&lt;span class=&#34;math inline&#34;&gt;\(s_t =0\)&lt;/span&gt;) and molecular diffusivity (&lt;span class=&#34;math inline&#34;&gt;\(\kappa = 0\)&lt;/span&gt;), the velocity
field &lt;span class=&#34;math inline&#34;&gt;\(u_t\)&lt;/span&gt; filaments &lt;span class=&#34;math inline&#34;&gt;\(g_t\)&lt;/span&gt; and forms small scales as it homogenizes, a
process known as &lt;em&gt;mixing&lt;/em&gt; (see Figure 2).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-03-28-chaotic-mixing-and-the-statistical-properties-of-scalar-turbulence_files/Figure%202.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 2: Mixing of a circular blob, showing filamentation and formation
of small scales.&lt;/p&gt;
&lt;p&gt;Mixing of the scalar &lt;span class=&#34;math inline&#34;&gt;\(g_t\)&lt;/span&gt; (assuming it is mean zero) can be quantified
using a negative Sobolev norm. Commonly chosen is the &lt;span class=&#34;math inline&#34;&gt;\(H^{-1}\)&lt;/span&gt; norm
&lt;span class=&#34;math inline&#34;&gt;\(\|g_t\|_{H^{-1}} := \|(-\Delta)^{-1/2}g_t\|_{L^2}\)&lt;/span&gt;, which essentially
measures the average filamentation width, though there are many other
expedient choices &lt;span class=&#34;citation&#34;&gt;Thiffeault (&lt;a href=&#34;#ref-Thiffeault2012-xs&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In &lt;span class=&#34;citation&#34;&gt;Bedrossian, Blumenthal, and Punshon-Smith (&lt;a href=&#34;#ref-BBPS19II&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; we show that solutions to &lt;span class=&#34;math inline&#34;&gt;\(\eqref{eqSNS}\)&lt;/span&gt; cause the advection
diffusion equation (without source but with diffusion) to mix
exponentially fast with a rate that is &lt;em&gt;uniform in the diffusivity&lt;/em&gt;
parameter &lt;span class=&#34;math inline&#34;&gt;\(\kappa\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Theorem 2 &lt;span class=&#34;citation&#34;&gt;(Uniform-in-diffusivity mixing, &lt;a href=&#34;#ref-BBPS19II&#34;&gt;Bedrossian, Blumenthal, and Punshon-Smith 2021&lt;/a&gt;)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Let&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(u_t\)&lt;/span&gt; solve &lt;span class=&#34;math inline&#34;&gt;\(\eqref{eqSNS}\)&lt;/span&gt; with non-degenerate noise. There exists
a &lt;strong&gt;deterministic&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\gamma &amp;gt; 0\)&lt;/span&gt;, independent of &lt;span class=&#34;math inline&#34;&gt;\(\kappa\)&lt;/span&gt;, such that for
all initial &lt;span class=&#34;math inline&#34;&gt;\(u_0\)&lt;/span&gt;, and all &lt;span class=&#34;math inline&#34;&gt;\(\kappa \in [0,1]\)&lt;/span&gt; there is a random constant
&lt;span class=&#34;math inline&#34;&gt;\(D_\kappa = D_\kappa(u_0,\omega)\)&lt;/span&gt; so that for all zero-mean
&lt;span class=&#34;math inline&#34;&gt;\(g_0 \in H^1\)&lt;/span&gt; and all &lt;span class=&#34;math inline&#34;&gt;\(t&amp;gt;0\)&lt;/span&gt; the following holds almost surely
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\|g_t\|_{H^{-1}} \leq D_\kappa e^{-\gamma t}\|g_0\|_{H^1}.\label{eqEM}\tag{EM}
\end{equation}\]&lt;/span&gt; &lt;em&gt;The random constant&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(D_\kappa\)&lt;/span&gt;, has finite second
moment &lt;strong&gt;uniformly bounded in&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\kappa\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Theorem 2 can be seen as a direct consequence of Theorem 1 and follows
from a fairly straight forward argument using the mild form of
&lt;span class=&#34;math inline&#34;&gt;\(\eqref{eqAD}\)&lt;/span&gt; and estimates on the stochastic convolution. This argument
is carried out in &lt;span class=&#34;citation&#34;&gt;Bedrossian, Blumenthal, and Punshon-Smith (&lt;a href=&#34;#ref-BBPSbatch19&#34;&gt;2019a&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lagrangian-chaos&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Lagrangian chaos&lt;/h2&gt;
&lt;p&gt;It has long been understood in the physics community (e.g. &lt;span class=&#34;citation&#34;&gt;Bohr et al. (&lt;a href=&#34;#ref-BMOV05&#34;&gt;2005&lt;/a&gt;)&lt;/span&gt;,
&lt;span class=&#34;citation&#34;&gt;Antonsen Jr and Ott (&lt;a href=&#34;#ref-AntonsenOtt1991&#34;&gt;1991&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Ott (&lt;a href=&#34;#ref-ott-1999&#34;&gt;1999&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Shraiman and Siggia (&lt;a href=&#34;#ref-ShraimanSiggia00&#34;&gt;2000&lt;/a&gt;)&lt;/span&gt;) that the predominant
mechanism for mixing in spatially regular fluids is the chaotic motion
of the particle trajectories &lt;span class=&#34;math inline&#34;&gt;\(x_t = \phi^t(x)\)&lt;/span&gt; of the &lt;strong&gt;Lagrangian flow
map&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\phi^t : \mathbb{T}^d \to \mathbb{T}^d\)&lt;/span&gt; associated to the
velocity field &lt;span class=&#34;math inline&#34;&gt;\(u_t\)&lt;/span&gt;, defined by &lt;span class=&#34;math display&#34;&gt;\[
  \frac{d}{dt} \phi^t(x) = u_t(\phi^t(x)), \quad \phi^0(x) = x\in \mathbb{T}d.
\]&lt;/span&gt; Here we characterize chaos through having a &lt;em&gt;positive Lyapunov
exponent&lt;/em&gt; &lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
0&amp;lt; \lambda_1 := \lim_{t\to \infty} \frac{1}{t}\log|D_x\phi^t| \, .\label{eqPE}\tag{PE}
\end{equation}\]&lt;/span&gt; This property is typically referred to as &lt;strong&gt;Lagrangian
chaos&lt;/strong&gt; in the fluid mechanics literature.&lt;/p&gt;
&lt;p&gt;In the deterministic setting, proving positivity of Lyapunov exponents
as in &lt;span class=&#34;math inline&#34;&gt;\(\eqref{eqPE}\)&lt;/span&gt; is currently hopelessly out of reach due to the
possible formation of coherent structures and lack of ergodicity.
However, starting with the seminal work of Furstenberg
(&lt;span class=&#34;citation&#34;&gt;Furstenberg (&lt;a href=&#34;#ref-furstenberg1963noncommuting&#34;&gt;1963&lt;/a&gt;)&lt;/span&gt;), significant success has been achieved in
proving existence and positivity of Lyapunov exponents in the context of
&lt;em&gt;random dynamical systems&lt;/em&gt; (&lt;span class=&#34;citation&#34;&gt;Arnold (&lt;a href=&#34;#ref-arnold2013random&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Kifer (&lt;a href=&#34;#ref-kifer2012ergodic&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt;,
&lt;span class=&#34;citation&#34;&gt;P. H. Baxendale (&lt;a href=&#34;#ref-baxendale1989lyapunov&#34;&gt;1989&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Ledrappier and Young (&lt;a href=&#34;#ref-ledrappier1985metric&#34;&gt;1985&lt;/a&gt;)&lt;/span&gt;). Ideas in this vein are
what enabled us to prove the following Lagrangian chaos result, the
first step in the proof of Theorem 3.&lt;/p&gt;
&lt;p&gt;Theorem 3 &lt;span class=&#34;citation&#34;&gt;(Lagrangian chaos, &lt;a href=&#34;#ref-BBPS18&#34;&gt;Bedrossian, Blumenthal, and Punshon-Smith 2018&lt;/a&gt;)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(u_t\)&lt;/span&gt; solve &lt;span class=&#34;math inline&#34;&gt;\(\eqref{eqSNS}\)&lt;/span&gt; with non-degenerate noise as above, then
there exists a &lt;strong&gt;deterministic&lt;/strong&gt; constant &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1 &amp;gt; 0\)&lt;/span&gt; (independent
of &lt;span class=&#34;math inline&#34;&gt;\(u_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt;) for which &lt;span class=&#34;math inline&#34;&gt;\(\eqref{eqPE}\)&lt;/span&gt; holds almost surely.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;decay-of-correlations-and-mixing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Decay of correlations and mixing&lt;/h2&gt;
&lt;p&gt;Let us now address how &lt;span class=&#34;math inline&#34;&gt;\(\eqref{eqEM}\)&lt;/span&gt; is obtained in the case
&lt;span class=&#34;math inline&#34;&gt;\(\kappa = 0\)&lt;/span&gt;. In this case, the solution &lt;span class=&#34;math inline&#34;&gt;\(g_t\)&lt;/span&gt; is given by
&lt;span class=&#34;math inline&#34;&gt;\(g_t = g_0 \circ (\phi^t)^{-1}\)&lt;/span&gt;. In view of this, &lt;span class=&#34;math inline&#34;&gt;\(\eqref{eqPE}\)&lt;/span&gt; suggests
that &lt;span class=&#34;math inline&#34;&gt;\(g_t\)&lt;/span&gt; is `stretched out’ considerably as &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; increases, leading to
a rapid generation of high frequencies as oppositely signed values of
the concentration profile &lt;span class=&#34;math inline&#34;&gt;\(g_t\)&lt;/span&gt; ``pile up’’ against each other almost
everywhere in the domain. Indeed, this local-to-global mechanism is
widely used in dynamics. It is known as &lt;em&gt;decay of correlations&lt;/em&gt; and
takes the form &lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\left|\int (f\circ \phi^t)\, g dx \right| \leq D_\kappa e^{-\gamma t}\|f\|_{H^1}\|g\|_{H^1}\label{eq1}\tag{1}
\end{equation}\]&lt;/span&gt; for each mean zero &lt;span class=&#34;math inline&#34;&gt;\(f,g\in H^1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(t&amp;gt;0\)&lt;/span&gt;. This is
equivalent to exponential mixing &lt;span class=&#34;math inline&#34;&gt;\(\eqref{eqEM}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Despite this simple picture, passing from &lt;span class=&#34;math inline&#34;&gt;\(\eqref{eqPE}\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\eqref{eq1}\)&lt;/span&gt;
requires serious work. When random driving is present, the &lt;strong&gt;two-point
process&lt;/strong&gt; is a powerful tool for proving exponential correlation decay
(&lt;span class=&#34;citation&#34;&gt;P. Baxendale and Stroock (&lt;a href=&#34;#ref-baxendale1988large&#34;&gt;1988&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Dolgopyat et al. (&lt;a href=&#34;#ref-dolgopyat2004sample&#34;&gt;2004&lt;/a&gt;)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;In our context, this is the Markov process that simultaneously tracks
two particles subjected to the same velocity field
&lt;span class=&#34;math inline&#34;&gt;\((u_t,\phi^t(x),\phi^t(y))\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(x \neq y\)&lt;/span&gt;. Correlation decay for
generic noise realizations is connected with the rate at which the
probabilistic law of &lt;span class=&#34;math inline&#34;&gt;\((u_t, \phi^t(x), \phi^t(y))\)&lt;/span&gt; relaxes to its
equilibrium statistics (known as &lt;em&gt;geometric ergodicity&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;The proof of Theorem 2 with &lt;span class=&#34;math inline&#34;&gt;\(\kappa = 0\)&lt;/span&gt; utilizes this connection using
tools from the theory of Markov chains, particularly the Harris theorem
&lt;span class=&#34;citation&#34;&gt;Meyn and Tweedie (&lt;a href=&#34;#ref-meyn2012markov&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt;. The main difficulty to overcome here is the degeneracy
in the &lt;span class=&#34;math inline&#34;&gt;\((u_t, \phi^t(x), \phi^t(y))\)&lt;/span&gt; process near the diagonal
&lt;span class=&#34;math inline&#34;&gt;\(\{x=y\}\)&lt;/span&gt;. One needs to show that any time two particles are close, they
separate again exponentially fast. This effectively amounts to a large
deviation estimate on the convergence of finite-time Lyapunov exponents
to the asymptotic Lyapunov exponent deduced in Theorem 3, and is carried
out in &lt;span class=&#34;citation&#34;&gt;Bedrossian, Blumenthal, and Punshon-Smith (&lt;a href=&#34;#ref-BBPS19I&#34;&gt;2019b&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;It remains to incorporate molecular diffusion (&lt;span class=&#34;math inline&#34;&gt;\(\kappa &amp;gt; 0\)&lt;/span&gt;) into this
scheme. This comes down again to the two-point process, now with
Lagrangian flow &lt;span class=&#34;math inline&#34;&gt;\(\phi^t_\kappa\)&lt;/span&gt; augmented by an additional white noise
term with variance &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\kappa}\)&lt;/span&gt; to account for molecular diffusivity.
The primary step is to show that one can pass to the singular limit
&lt;span class=&#34;math inline&#34;&gt;\(\kappa \to 0\)&lt;/span&gt; in the dominant eigenvalue, eigenfunction pair for the
Perron-Frobenius operator corresponding to
&lt;span class=&#34;math inline&#34;&gt;\((u_t, \phi^t_\kappa(x), \phi^t_\kappa(y))\)&lt;/span&gt;; this is carried out in
&lt;span class=&#34;citation&#34;&gt;Bedrossian, Blumenthal, and Punshon-Smith (&lt;a href=&#34;#ref-BBPS19II&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bibliography&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;Bibliography&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-AO03&#34; class=&#34;csl-entry&#34;&gt;
Antonia, RA, and P Orlandi. 2003. &lt;span&gt;“Effect of Schmidt Number on Small-Scale Passive Scalar Turbulence.”&lt;/span&gt; &lt;em&gt;Appl. Mech. Rev.&lt;/em&gt; 56 (6): 615–32.
&lt;/div&gt;
&lt;div id=&#34;ref-AntonsenOtt1991&#34; class=&#34;csl-entry&#34;&gt;
Antonsen Jr, Thomas M, and Edward Ott. 1991. &lt;span&gt;“Multifractal Power Spectra of Passive Scalars Convected by Chaotic Fluid Flows.”&lt;/span&gt; &lt;em&gt;Physical Review A&lt;/em&gt; 44 (2): 851.
&lt;/div&gt;
&lt;div id=&#34;ref-arnold2013random&#34; class=&#34;csl-entry&#34;&gt;
Arnold, Ludwig. 2013. &lt;em&gt;Random Dynamical Systems&lt;/em&gt;. Springer Science &amp;amp; Business Media.
&lt;/div&gt;
&lt;div id=&#34;ref-Batchelor59&#34; class=&#34;csl-entry&#34;&gt;
Batchelor, George K. 1959. &lt;span&gt;“Small-Scale Variation of Convected Quantities Like Temperature in Turbulent Fluid Part 1. General Discussion and the Case of Small Conductivity.”&lt;/span&gt; &lt;em&gt;Journal of Fluid Mechanics&lt;/em&gt; 5 (1): 113–33.
&lt;/div&gt;
&lt;div id=&#34;ref-baxendale1989lyapunov&#34; class=&#34;csl-entry&#34;&gt;
Baxendale, Peter H. 1989. &lt;span&gt;“Lyapunov Exponents and Relative Entropy for a Stochastic Flow of Diffeomorphisms.”&lt;/span&gt; &lt;em&gt;Probability Theory and Related Fields&lt;/em&gt; 81 (4): 521–54.
&lt;/div&gt;
&lt;div id=&#34;ref-baxendale1988large&#34; class=&#34;csl-entry&#34;&gt;
Baxendale, PH, and DW Stroock. 1988. &lt;span&gt;“Large Deviations and Stochastic Flows of Diffeomorphisms.”&lt;/span&gt; &lt;em&gt;Probability Theory and Related Fields&lt;/em&gt; 80 (2): 169–215.
&lt;/div&gt;
&lt;div id=&#34;ref-BBPSbatch19&#34; class=&#34;csl-entry&#34;&gt;
Bedrossian, Jacob, Alex Blumenthal, and Sam Punshon-Smith. 2019a. &lt;span&gt;“The &lt;span&gt;Batchelor&lt;/span&gt; Spectrum of Passive Scalar Turbulence in Stochastic Fluid Mechanics.”&lt;/span&gt; &lt;em&gt;Comm. Pure Appl. Math.&lt;/em&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-BBPS18&#34; class=&#34;csl-entry&#34;&gt;
Bedrossian, Jacob, Alex Blumenthal, and Samuel Punshon-Smith. 2018. &lt;span&gt;“Lagrangian Chaos and Scalar Advection in Stochastic Fluid Mechanics.”&lt;/span&gt; &lt;em&gt;J. Euro. Math. Soc.&lt;/em&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-BBPS19I&#34; class=&#34;csl-entry&#34;&gt;
———. 2019b. &lt;span&gt;“Almost-Sure Exponential Mixing of Passive Scalars by the Stochastic &lt;span&gt;N&lt;/span&gt;avier-&lt;span&gt;S&lt;/span&gt;tokes Equations.”&lt;/span&gt; &lt;em&gt;Ann. Of Prob.&lt;/em&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-BBPS19II&#34; class=&#34;csl-entry&#34;&gt;
———. 2021. &lt;span&gt;“Almost-Sure Enhanced Dissipation and Uniform-in-Diffusivity Exponential Mixing for Advection-Diffusion by Stochastic &lt;span&gt;N&lt;/span&gt;avier-&lt;span&gt;S&lt;/span&gt;tokes.”&lt;/span&gt; &lt;em&gt;Prob. Theory Rel. Fields&lt;/em&gt; 179 (3): 777–834.
&lt;/div&gt;
&lt;div id=&#34;ref-BMOV05&#34; class=&#34;csl-entry&#34;&gt;
Bohr, Tomas, Mogens H Jensen, Giovanni Paladin, and Angelo Vulpiani. 2005. &lt;em&gt;Dynamical Systems Approach to Turbulence&lt;/em&gt;. Cambridge University Press.
&lt;/div&gt;
&lt;div id=&#34;ref-dolgopyat2004sample&#34; class=&#34;csl-entry&#34;&gt;
Dolgopyat, Dmitry, Vadim Kaloshin, Leonid Koralov, et al. 2004. &lt;span&gt;“Sample Path Properties of the Stochastic Flows.”&lt;/span&gt; &lt;em&gt;The Annals of Probability&lt;/em&gt; 32 (1A): 1–27.
&lt;/div&gt;
&lt;div id=&#34;ref-furstenberg1963noncommuting&#34; class=&#34;csl-entry&#34;&gt;
Furstenberg, Harry. 1963. &lt;span&gt;“Noncommuting Random Products.”&lt;/span&gt; &lt;em&gt;Transactions of the American Mathematical Society&lt;/em&gt; 108 (3): 377–428.
&lt;/div&gt;
&lt;div id=&#34;ref-GibsonSchwarz63&#34; class=&#34;csl-entry&#34;&gt;
Gibson, CH, and WH Schwarz. 1963. &lt;span&gt;“The Universal Equilibrium Spectra of Turbulent Velocity and Scalar Fields.”&lt;/span&gt; &lt;em&gt;Journal of Fluid Mechanics&lt;/em&gt; 16 (3): 365–84.
&lt;/div&gt;
&lt;div id=&#34;ref-GrantEtAl68&#34; class=&#34;csl-entry&#34;&gt;
Grant, HL, BA Hughes, WM Vogel, and A Moilliet. 1968. &lt;span&gt;“The Spectrum of Temperature Fluctuations in Turbulent Flow.”&lt;/span&gt; &lt;em&gt;Journal of Fluid Mechanics&lt;/em&gt; 34 (3): 423–42.
&lt;/div&gt;
&lt;div id=&#34;ref-kifer2012ergodic&#34; class=&#34;csl-entry&#34;&gt;
Kifer, Yuri. 2012. &lt;em&gt;Ergodic Theory of Random Transformations&lt;/em&gt;. Vol. 10. Springer Science &amp;amp; Business Media.
&lt;/div&gt;
&lt;div id=&#34;ref-ledrappier1985metric&#34; class=&#34;csl-entry&#34;&gt;
Ledrappier, François, and L-S Young. 1985. &lt;span&gt;“The Metric Entropy of Diffeomorphisms: Part i: Characterization of Measures Satisfying Pesin’s Entropy Formula.”&lt;/span&gt; &lt;em&gt;Annals of Mathematics&lt;/em&gt;, 509–39.
&lt;/div&gt;
&lt;div id=&#34;ref-meyn2012markov&#34; class=&#34;csl-entry&#34;&gt;
Meyn, Sean P, and Richard L Tweedie. 2012. &lt;em&gt;Markov Chains and Stochastic Stability&lt;/em&gt;. Springer Science &amp;amp; Business Media.
&lt;/div&gt;
&lt;div id=&#34;ref-ott-1999&#34; class=&#34;csl-entry&#34;&gt;
Ott, E. 1999. &lt;span&gt;“The Role of Lagrangian Chaos in the Creation of Multifractal Measures.”&lt;/span&gt; In &lt;em&gt;Fundamental Problematic Issues in Turbulence&lt;/em&gt;, edited by Albert Gyr, Wolfgang Kinzelbach, and Arkady Tsinober, 381–403. Basel: Birkh&lt;span&gt;ä&lt;/span&gt;user Basel.
&lt;/div&gt;
&lt;div id=&#34;ref-ShraimanSiggia00&#34; class=&#34;csl-entry&#34;&gt;
Shraiman, Boris I, and Eric D Siggia. 2000. &lt;span&gt;“Scalar Turbulence.”&lt;/span&gt; &lt;em&gt;Nature&lt;/em&gt; 405 (6787): 639.
&lt;/div&gt;
&lt;div id=&#34;ref-Thiffeault2012-xs&#34; class=&#34;csl-entry&#34;&gt;
Thiffeault, Jean-Luc. 2012. &lt;span&gt;“Using Multiscale Norms to Quantify Mixing and Transport.”&lt;/span&gt; &lt;em&gt;Nonlinearity&lt;/em&gt; 25 (2): R1–44.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Recent advances in extreme value theory</title>
      <link>https://youngstats.github.io/post/2023/04/04/recent-advances-in-extreme-value-theory/</link>
      <pubDate>Tue, 04 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2023/04/04/recent-advances-in-extreme-value-theory/</guid>
      <description>


&lt;p&gt;&lt;strong&gt;Recent advances in extreme value theory&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Thursday, April 20th, 6:00 PT / 9:00 ET / 15:00 CET&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2023-04-04-recent-advances-in-extreme-value-theory_files/cover.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Extreme value theory is concerned with the accurate statistical assessment of the risk of rare events. Such extreme events have small occurrence probabilities but often entail large economic and ecological costs in addition to their potential for severe impacts on human health. There is active research to model and infer complex extremal dependence structures in multivariate data, with connections to high-dimensional statistics, machine learning and other fields. Applications include the analysis of financial crises, prediction of flood risk and the modeling of record-shattering climate extremes.&lt;/p&gt;
&lt;p&gt;In the webinar, selected statisticians will present their recent works and elaborate on different aspects of this topic.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Thursday, April 20th, 6:00 PT / 9:00 ET / 15:00 CET&lt;/li&gt;
&lt;li&gt;Online, via Zoom. The registration form is available &lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSdeixPUhPzfQfGMGC8YcSNiK9kfgGpNujyj_l6p5z7TwUE5sQ/viewform?vc=0&amp;amp;c=0&amp;amp;w=1&amp;amp;flr=0&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docenti.unicatt.it/ppd2/it/docenti/85092/stefano-rizzelli&#34;&gt;Stefano Rizzelli&lt;/a&gt;, Department of Statistical Sciences, Catholic University of Milan&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Title: Mathematical foundations of empirical Bayesian analysis of maxima&lt;/p&gt;
&lt;p&gt;Abstract: Predicting future observations is the central goal of several statistical applications concerning extreme value data. Under mild assumptions, extreme-value theory justifies modelling linearly normalized sample maxima by max-stable distributions, which provide asymptotic approximations to the actual data generating mechanism. The Bayesian paradigm offers a direct approach to forecasting and uncertainty quantification. Various proposals for Bayesian inferential procedures have been formulated in recent years, though they typically disregard the model convergence bias inherent in the use of max-stable distributions, incorporating no information on norming sequences in prior specifications for scale and location parameters. We propose an empirical Bayes approach which suitably addresses this point via data-dependent priors. We illustrate the resulting asymptotic posterior concentration properties and pinpoint their implications for estimation and prediction of future observations. This talk is based on a joint work with Simone A. Padoan.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.math.ku.dk/english/about/news/new-names/nicola-gnecco-postdoc/&#34;&gt;Nicola Gnecco&lt;/a&gt;, Department of Mathematical Sciences, University of Copenhagen&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Title: Extremal Random Forests&lt;/p&gt;
&lt;p&gt;Abstract: Classical methods for quantile regression fail in cases where the quantile of interest is extreme and only few or no training data points exceed it. Asymptotic results from extreme value theory can be used to extrapolate beyond the range of the data, and several approaches exist that use linear regression, kernel methods or generalized additive models. Most of these methods break down if the predictor space has more than a few dimensions or if the regression function of extreme quantiles is complex. We propose a method for extreme quantile regression that combines the flexibility of random forests with the theory of extrapolation. Our extremal random forest (ERF) estimates the parameters of a generalized Pareto distribution, conditional on the predictor vector, by maximizing a local likelihood with weights extracted from a quantile random forest. Under certain assumptions, we show consistency of the estimated parameters. Furthermore, we penalize the shape parameter in this likelihood to regularize its variability in the predictor space. Simulation studies show that our ERF outperforms both classical quantile regression methods and existing regression approaches from extreme value theory. We apply our methodology to extreme quantile prediction for U.S. wage data.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.imsv.unibe.ch/about_us/staff/dr_koh_jonathan_boon_han/index_eng.html&#34;&gt;Jonathan Koh&lt;/a&gt;, Oeschger Centre for Climate Change Research, University of Bern&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Title: Predicting risks of temperature extremes using large-scale circulation patterns with r–Pareto processes&lt;/p&gt;
&lt;p&gt;Abstract: Many severe weather patterns in the mid-latitudes have been found to be connected to a particular atmospheric pattern known as blocking. This pattern obstructs the prevailing westerly large-scale atmospheric flow, changing flow anomalies in the vicinity of the blocking system to sustain weather conditions in the immediate region of its occurrence. Blockings’ presence and characteristics are thus important for the development of temperature extremes, which are rarely isolated in space, so one must not just account for their occurrence probabilities and intensities but also their spatial dependencies when assessing their associated risk. Here we propose a methodology that does so by combining tools from the spatial extremes and machine learning literature, to incorporate 500hPa geopotential (Z500) anomalies over the North Atlantic and European region as covariates to predict surface temperature extremes. This involves fitting Generalized r-Pareto processes with appropriate risk functionals to high-impact positive and negative temperature anomaly events across central Europe from 1979–2020, using loss functions motivated by extreme-value theory in a boosting algorithm. We check by simulation that the model parameters are identifiable and can be estimated adequately. We find which circulation patterns in the Euro-Atlantic sector are most important in determining the characteristics of these extremes, and show how they affect it.&lt;/p&gt;
&lt;p&gt;Discussant: &lt;a href=&#34;https://www.unige.ch/gsem/en/research/faculty/all/sebastian-engelke/&#34;&gt;Sebastian Engelke&lt;/a&gt;, Research Center for Statistics, University of Geneva&lt;/p&gt;
&lt;p&gt;The webinar is part of YoungStatS project of the Young Statisticians Europe initiative (FENStatS) supported by the Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics (IMS).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you missed this webinar, you can &lt;a href=&#34;https://www.youtube.com/watch?v=Mpzqz4y1oYY&#34;&gt;watch the recording on our YouTube channel&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
