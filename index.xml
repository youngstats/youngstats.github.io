<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>YoungStatS</title>
    <link>https://youngstats.github.io/</link>
    <description>Recent content on YoungStatS</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 30 Sep 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://youngstats.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Advancements in Symbolic Data Analysis</title>
      <link>https://youngstats.github.io/post/2021/09/30/advancements-in-symbolic-data-analysis/</link>
      <pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/09/30/advancements-in-symbolic-data-analysis/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;Advancements in Symbolic Data Analysis&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-09-30-advancements-in-symbolic-data-analysis_files/cover-sda.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The sixth “One World webinar” organized by YoungStatS will take place on
October 27th, 2021. With the development of digital systems, very large
datasets have become routine. However, standard statistical approaches
do not have the power or flexibility to analyse these efficiently, and
extract the required knowledge. Symbolic Data Analysis provides a
framework allowing for the representation of data with intrinsic
variability, where the observed “values” are not just single real values
or categories, but finite sets, intervals or distributions over a given
domain. Methods for the (multivariate) analysis of such symbolic data
have been developed, following different approaches, and using distinct
criteria, which allow taking data variability into account.&lt;/p&gt;
&lt;p&gt;Selected young researchers active in the area will present their recent
contributions on this developing topic.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;p&gt;Wednesday, October 27th, 16:30 CEST&lt;/p&gt;
&lt;p&gt;Online, via Zoom. The registration form is available
&lt;a href=&#34;https://docs.google.com/forms/d/1WfJ6NG4hWq3-6C0q6gs9axaDR8THX9uP1UPS-5iB-dU/edit&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.researchgate.net/scientific-contributions/Yuying-Sun-2145350022&#34;&gt;Yuying
Sun&lt;/a&gt;,
Chinese Academy of Sciences, Beijing, China&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.borisberanger.com/&#34;&gt;Boris Beranger&lt;/a&gt;, University of New
South Wales, Australia&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=--uDCpYAAAAJ&amp;amp;hl=en&#34;&gt;Bruno
Pimentel&lt;/a&gt;,
Universidade Federal de Pernambuco, Brazil&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.estg.ipvc.pt/~sdias/&#34;&gt;Sónia Dias&lt;/a&gt;, Polytechnic Institute
of Viana do Castelo, Portugal&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prof. Paula Brito, University of Porto, Portugal&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The webinar is part of YoungStatS project of the Young Statisticians
Europe initiative (FENStatS) supported by the Bernoulli Society for
Mathematical Statistics and Probability and the Institute of
Mathematical Statistics (IMS).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Advances in Difference-in-Differences in Econometrics</title>
      <link>https://youngstats.github.io/post/2021/09/30/advances-in-difference-in-differences-in-econometrics/</link>
      <pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/09/30/advances-in-difference-in-differences-in-econometrics/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;Advances in Difference-in-Differences in Econometrics&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-09-30-advances-in-difference-in-differences-in-econometrics_files/cover_did_roth_image.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The eighth “One World webinar” organized by YoungStatS will take place
on December 13th, 2021. The difference-in-differences design is a
quasi-experimental identification strategy for estimating causal effects
which has become the single most popular research design in the
quantitative social sciences, and as such, it merits careful study by
researchers everywhere. It is also a flourishing field of present
research in econometrics. Selected younger researchers active in the
area will present their recent contributions on this topic.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;p&gt;Wednesday, December 15th, 9:00 PT / 12:00 EST / 18:00 CEST&lt;/p&gt;
&lt;p&gt;Online, via Zoom. The registration form is available
&lt;a href=&#34;https://docs.google.com/forms/d/1cdQT9YwjzIEjuNo_OMVjnPgKpO_6M2U6tBq9E471Df0/edit&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.google.com/site/clementdechaisemartin/&#34;&gt;Clément de
Chaisemartin&lt;/a&gt;,
Sciences Po, Paris, France&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jonathandroth.github.io/&#34;&gt;Jonathan Roth&lt;/a&gt;, Brown University,
USA&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://bcallaway11.github.io/&#34;&gt;Brantly Callaway&lt;/a&gt;, University of
Georgia, USA&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lihualei71.github.io/&#34;&gt;Lihua Lei&lt;/a&gt;, Stanford University, USA&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prof. Pedro H.C. Sant&#39;Anna, Microsoft and Vanderbilt University,
USA&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The webinar is part of YoungStatS project of the Young Statisticians
Europe initiative (FENStatS) supported by the Bernoulli Society for
Mathematical Statistics and Probability and the Institute of
Mathematical Statistics (IMS).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Optimal disclosure risk assessment</title>
      <link>https://youngstats.github.io/post/2021/09/30/optimal-disclosure-risk-assessment/</link>
      <pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/09/30/optimal-disclosure-risk-assessment/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;disclosure-risk-for-microdata&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Disclosure risk for microdata&lt;/h1&gt;
&lt;p&gt;Protection against disclosure is a legal and ethical obligation for
agencies releasing microdata files for public use. Consider a microdata
sample &lt;span class=&#34;math inline&#34;&gt;\({X}_n=(X_{1},\ldots,X_{n})\)&lt;/span&gt; of size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; from a finite
population of size &lt;span class=&#34;math inline&#34;&gt;\(\bar{n}=n+\lambda n\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(\lambda&amp;gt;0\)&lt;/span&gt;, such that
each sample record &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; contains two disjoint types of information:
identifying categorical information and sensitive information.
Identifying information consists of a set of categorical variables which
might be matchable to known units of the population. A risk of
disclosure results from the possibility that an intruder might succeed
in identifying a microdata unit through such a matching, and hence be
able to disclose sensitive information on this unit. To quantify the
risk of disclosure, sample records &lt;span class=&#34;math inline&#34;&gt;\({X}_n\)&lt;/span&gt; are typically
cross-classified according to identifying variables. That is, &lt;span class=&#34;math inline&#34;&gt;\({X}_n\)&lt;/span&gt;
is partitioned in &lt;span class=&#34;math inline&#34;&gt;\(K_{n}\leq n\)&lt;/span&gt; cells, with &lt;span class=&#34;math inline&#34;&gt;\(Y_{j,n}\)&lt;/span&gt; being the number
of &lt;span class=&#34;math inline&#34;&gt;\(X_{i}\)&lt;/span&gt;’s belonging to cell &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(j=1,\ldots, K_{n}\)&lt;/span&gt;, such that
&lt;span class=&#34;math inline&#34;&gt;\(\sum_{1\leq j\leq K_{n}}Y_{j,n}=n\)&lt;/span&gt;; we refer to the number of
occurrences &lt;span class=&#34;math inline&#34;&gt;\(Y_{j,n}\)&lt;/span&gt; as the sample frequency of cell &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. We also
indicate by &lt;span class=&#34;math inline&#34;&gt;\(Y_{j,\bar{n}}\)&lt;/span&gt; the same quantities referring to the entire
population of size &lt;span class=&#34;math inline&#34;&gt;\(\bar{n}\)&lt;/span&gt;. Then, a risk of disclosure arises from
cells in which both sample frequencies and population frequencies are
small. Of special interest are cells with frequency &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; (singletons or
uniques) since, assuming no errors in the matching process or data
sources, for these cells the match is guaranteed to be correct. This has
motivated inferences on measures of disclosure risk that are suitable
functionals of the number of uniques, the most common being the number
&lt;span class=&#34;math inline&#34;&gt;\(\tau_{1}\)&lt;/span&gt; of sample uniques which are also population uniques, namely
the following functional:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\tau_{1}=\sum_{j\geq 1}\mathbf{1}_{\{Y_{j,n}=1\}}\mathbf{1}_{\{Y_{j,\bar{n}}=1\}},\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{1}\)&lt;/span&gt; denotes the indicator function.&lt;/p&gt;
&lt;p&gt;We first introduce a class of nonparametric estimators of &lt;span class=&#34;math inline&#34;&gt;\(\tau_{1}\)&lt;/span&gt;, we
then show that they provably estimate &lt;span class=&#34;math inline&#34;&gt;\(\tau_{1}\)&lt;/span&gt; all of the way up to
the sampling fraction &lt;span class=&#34;math inline&#34;&gt;\((\lambda+1)^{-1}\propto (\log n)^{-1}\)&lt;/span&gt;, with
vanishing normalized mean squared error (NMSE) for large sample size
&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. More importantly we prove that
&lt;span class=&#34;math inline&#34;&gt;\((\lambda+1)^{-1}\propto (\log n)^{-1}\)&lt;/span&gt; is the smallest possible
sampling fraction for consistently estimating &lt;span class=&#34;math inline&#34;&gt;\(\tau_{1}\)&lt;/span&gt;, thus the
estimators’ NMSE is near optimal. Our paper also provides a rigorous
answer to an open question raised by Skinner and Elliot (2002) about the feasibility of nonparametric estimation of &lt;span class=&#34;math inline&#34;&gt;\(\tau_{1}\)&lt;/span&gt; and for a sampling fraction
&lt;span class=&#34;math inline&#34;&gt;\((\lambda+1)^{-1}&amp;lt;1/2\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;nonparametric-estimation-of-tau_1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Nonparametric estimation of &lt;span class=&#34;math inline&#34;&gt;\(\tau_{1}\)&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;A nonparametric estimator for &lt;span class=&#34;math inline&#34;&gt;\(\tau_1\)&lt;/span&gt; may be simply deduced by
comparing expectations. Indeed, under a suitable Poisson abundance model
for the cells’ proportions, it easy to see that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\label{eq:comparingE}
\mathbb{E} [\tau_1] =  \sum_{i \geq 0}(-1)^i \lambda^i (i+1) \mathbb{E} [Z_{i+1, n}],\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Z_{i,n}\)&lt;/span&gt; denotes the number of cells with frequency &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; out of
the sample &lt;span class=&#34;math inline&#34;&gt;\({X}_n\)&lt;/span&gt;. Thus, according to identity , we can define the
following estimator of &lt;span class=&#34;math inline&#34;&gt;\(\tau_1\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\label{eq:estimator&amp;lt;1}
\hat{\tau}_{1}=\sum_{i \geq 0} (-1)^{i} (i+1)\lambda^{i}Z_{i+1}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;By construction, the estimator is unbiased and it admits a natural
interpretation as a nonparametric empirical Bayes estimator in the sense
of Robbins(1956). The use of estimator is legitimated under the assumption
&lt;span class=&#34;math inline&#34;&gt;\(\lambda&amp;lt;1\)&lt;/span&gt;. For &lt;span class=&#34;math inline&#34;&gt;\(\lambda \geq 1\)&lt;/span&gt; it becomes useless, because of its
high variance due to the exponential growth of the coefficients
&lt;span class=&#34;math inline&#34;&gt;\(\lambda^i\)&lt;/span&gt;. Unfortunately, the assumption &lt;span class=&#34;math inline&#34;&gt;\(\lambda &amp;lt; 1\)&lt;/span&gt; is
unrealistic in the context of disclosure risk assessment, where the size
&lt;span class=&#34;math inline&#34;&gt;\(\lambda n\)&lt;/span&gt; of the unobserved population is typically much bigger than
the size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; of the observed sample. Thus the estimator requires an
adjustment via suitable smoothing techniques, along similar lines as
Orlitsky et al. (2016) in the context of the nonparametric estimation of the number of unseen species. We propose a smoothed version of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\tau}_{1}\)&lt;/span&gt; by
truncating the series at an independent random location &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;, and then
averaging over the distribution of &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;, i.e.,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\label{eq:estimator&amp;gt;1}
\hat{\tau}_{1}^L&amp;amp;= {\mathbb{E}}_L \left[ \sum_{i =1}^L (-1)^{i} (i+1) \lambda^i Z_{i+1,n}\right],\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; is supposed to be a Poisson or a Binomial random variable, but
other choices are possible.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;main-results&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Main results&lt;/h1&gt;
&lt;p&gt;We have evaluated the performance of the estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat{\tau}_{1}^L\)&lt;/span&gt; in
terms of the normalized mean squared error (NMSE). The NMSE is the mean
squared error (MSE) of the estimator normalized by the maximum value of
&lt;span class=&#34;math inline&#34;&gt;\(\tau_{1}\)&lt;/span&gt; (which is exactly &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;). Thus, the performance of an estimator
is evaluated in terms of the rate of convergence to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; of the NMSE as
&lt;span class=&#34;math inline&#34;&gt;\(n \to +\infty\)&lt;/span&gt;. See also Orlitsky et al. (2016) for a definition of NMSE. In our
paper we have proved that &lt;span class=&#34;math inline&#34;&gt;\(\hat{\tau}_{1}^L\)&lt;/span&gt; provably estimate
&lt;span class=&#34;math inline&#34;&gt;\(\tau_{1}\)&lt;/span&gt; all of the way up to the sampling fraction
&lt;span class=&#34;math inline&#34;&gt;\((\lambda+1)^{-1}\propto (\log n)^{-1}\)&lt;/span&gt; of the population, with
vanishing normalized mean-square error (NMSE) as &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; becomes large.
Then, by relying on recent techniques developed in Wu and Yang (2019) in
the context of nonparametric estimation of the support size of discrete
distributions, we are also able to provide us with a lower bound for the
NMSE of any estimator of the disclosure risk &lt;span class=&#34;math inline&#34;&gt;\(\tau_1\)&lt;/span&gt;. The lower bound
we find has an important implication: without imposing any parametric
assumption on the model, one can estimate &lt;span class=&#34;math inline&#34;&gt;\(\tau_1\)&lt;/span&gt; with vanishing NMSE
all the way up to &lt;span class=&#34;math inline&#34;&gt;\(\lambda \propto \log n\)&lt;/span&gt;. It is then impossible to
determine an estimator having provable guarantees, in terms of vanishing
NMSE, when &lt;span class=&#34;math inline&#34;&gt;\(\lambda = \lambda (n)\)&lt;/span&gt; goes to &lt;span class=&#34;math inline&#34;&gt;\(+\infty\)&lt;/span&gt; much faster than
&lt;span class=&#34;math inline&#34;&gt;\(\log (n)\)&lt;/span&gt;, as a function of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. Moreover it follows that the ``limit
of predictability&#34; of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\tau}_{1}^L\)&lt;/span&gt; in is near optimal, because it
matches (asymptotically) with its maximum possible value
&lt;span class=&#34;math inline&#34;&gt;\(\lambda \propto \log (n)\)&lt;/span&gt;, under suitable choices of the smoothing
distribution &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-09-30-optimal-disclosure-risk-assessment_files/unif.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 1: The normalized mean squared error as a function of the sampling
fraction &lt;span class=&#34;math inline&#34;&gt;\((1+\lambda)^{-1}\)&lt;/span&gt; when the cell’s probabilities are uniform
distributed. Each curve corresponds to a different estimator of
&lt;span class=&#34;math inline&#34;&gt;\(\tau_1\)&lt;/span&gt;: i) the nonparametric estimator with Binomial smoothing
&lt;span class=&#34;math inline&#34;&gt;\(\hat{\tau}_1^{L_b}\)&lt;/span&gt;; ii) the nonparametric estimator with Poisson
smoothing &lt;span class=&#34;math inline&#34;&gt;\(\hat{\tau}_1^{L_p}\)&lt;/span&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The performance of our nonparametric approach is shown in Figure 1.&lt;/p&gt;
&lt;p&gt;In order to do that, we generated a collection of synthetic
tables with &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; cells, where &lt;span class=&#34;math inline&#34;&gt;\(C=3 \cdot 10^6\)&lt;/span&gt;. The population size is
fixed to &lt;span class=&#34;math inline&#34;&gt;\(\bar{n}=10^{6}\)&lt;/span&gt;, and we evaluated the NMSE for different
values of the sample size &lt;span class=&#34;math inline&#34;&gt;\(n=\bar{n}(\lambda+1)^{-1}\)&lt;/span&gt;. The underlying
true cells’ probabilities are generated according to a uniform
distribution over the total number of cells. The figure shows how the
NMSE varies as a function of the sampling fraction &lt;span class=&#34;math inline&#34;&gt;\((1+\lambda)^{-1}\)&lt;/span&gt;,
for the estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat{\tau}_1^L\)&lt;/span&gt;, under a Poisson and a Binomial
smoothing. All the estimates are averaged over 100 iterations. The
sampling fractions considered in our simulation study are above the
limiting threshold &lt;span class=&#34;math inline&#34;&gt;\((\log n)^{-1}\)&lt;/span&gt; and the better performance seems to
be achieved under the Binomial smoothing.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;authors&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Authors&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-09-30-optimal-disclosure-risk-assessment_files/federico.jpg&#34; height=&#34;100&#34; /&gt;
&lt;strong&gt;Federico Camerlenghi&lt;/strong&gt; is an Assistant Professor of Statistics at the
University of Milano-Bicocca (Italy).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-09-30-optimal-disclosure-risk-assessment_files/favaro.jpg&#34; height=&#34;100&#34; /&gt;
&lt;strong&gt;Stefano Favaro&lt;/strong&gt; is a Full Professor of Statistics at the University
of Torino (Italy), and he is also a Carlo Alberto Chair at Collegio
Carlo Alberto (Torino, Italy).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-09-30-optimal-disclosure-risk-assessment_files/zacharie.png&#34; height=&#34;100&#34; /&gt;
&lt;strong&gt;Zacharie Naulet&lt;/strong&gt; is a Maître de Conférence at the Department of
Mathematics of Université Paris-Sud (France).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-09-30-optimal-disclosure-risk-assessment_files/panero.jpeg&#34; height=&#34;100&#34; /&gt;
&lt;strong&gt;Francesca Panero&lt;/strong&gt; is finishing her Ph.D. in Statistics at the
University of Oxford (UK).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p&gt;Good, I.J. and Toulmin, G.H. (1956). The number of new species, and the
increase in population coverage, when a sample is increased.
&lt;em&gt;Biometrika&lt;/em&gt; &lt;strong&gt;43&lt;/strong&gt;, 45–63.&lt;/p&gt;
&lt;p&gt;Orlitsky, A., Suresh, A.T. and Wu, Y. (2017). Optimal prediction of the
number of unseen species. &lt;em&gt;Proc. Natl. Acad. Sci. USA&lt;/em&gt; &lt;strong&gt;113&lt;/strong&gt;,
13283–13288.&lt;/p&gt;
&lt;p&gt;Robbins, H. (1956). An empirical Bayes approach to statistics. &lt;em&gt;Proc.
3rd Berkeley Symp.&lt;/em&gt;,&lt;strong&gt;1&lt;/strong&gt;, 157–163.&lt;/p&gt;
&lt;p&gt;Skinner, C.J. and Elliot, M.J. (2002). A measure of disclosure risk for
microdata. &lt;em&gt;J. Roy. Statist. Soc. B&lt;/em&gt; &lt;strong&gt;64&lt;/strong&gt;, 855–867.&lt;/p&gt;
&lt;p&gt;Skinner, C., Marsh, C., Openshaw, S. and Wymer, C. (1994). Disclosure
control for census microdata. &lt;em&gt;J. Off. Stat.&lt;/em&gt; &lt;strong&gt;10&lt;/strong&gt;, 31–51.&lt;/p&gt;
&lt;p&gt;Skinner, and Shlomo, N. (2008). Assessing identification risk in survey
microdata using log-linear models. &lt;em&gt;J. Amer. Statist. Assoc.&lt;/em&gt; &lt;strong&gt;103&lt;/strong&gt;,
989–1001.&lt;/p&gt;
&lt;p&gt;Wu, Y. and Yang, P. (2019). Chebyshev polynomials, moment matching, and
optimal estimation of the unseen. &lt;em&gt;Ann. Statist.&lt;/em&gt;, &lt;strong&gt;47&lt;/strong&gt;, 857–883.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Depth Quantile Functions</title>
      <link>https://youngstats.github.io/post/2021/07/01/depth-quantile-functions/</link>
      <pubDate>Thu, 01 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/07/01/depth-quantile-functions/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-07-01-depth-quantile-functions_files/f1.png&#34; /&gt;
&lt;em&gt;Figure 1: Depth quantile functions for the wine data (d=13), class 2 vs class 3. Blue curves correspond to between class comparisons, red/pink correspond to within class comparisons.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A common technique in modern statistics is the so-called kernel trick, where data is mapped into a (usually) infinite-dimensional feature space, where various statistical tasks can be carried out. Relatedly, we introduce the &lt;strong&gt;depth quantile function&lt;/strong&gt; (DQF), &lt;span class=&#34;math inline&#34;&gt;\(q_{ij}(\alpha)\)&lt;/span&gt; which similarly maps observations into an infinite dimensional space (the double index will become clear below), though in this case, these new representations of the data are functions of a one-dimensional variable &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; which allows plotting. By construction, described below, these functions encode geometric information about the underlying data set. Consequently, we obtain a tool that permits an interpretable visualization of point cloud geometry regardless of dimension of the data. Additionally, tools from functional data analysis can now be used to solve problems (classification, anomaly detection, etc).&lt;/p&gt;
&lt;p&gt;The primary tool used is that of Tukey’s half space depth (HSD), which provides a higher dimensional analog to the order statistics as a measure of centrality of an observation in a data set (where, for instance, the median is the most central or “deepest” point). In fact, the one dimensional version of HSD (&lt;span class=&#34;math inline&#34;&gt;\(D(x) = \min\{F_n(x), 1-F_n(x)\}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(F_n\)&lt;/span&gt; the EDF) is all we need, as we consider projections of our data onto lines before computing centrality, see below.&lt;/p&gt;
&lt;p&gt;It is known that for HSD in any dimension, the level sets &lt;span class=&#34;math inline&#34;&gt;\(\{x:D(x)\geq\lambda\}\)&lt;/span&gt; are necessarily convex, thus not conforming to the shape of the underlying density. Additionally, in high dimensions, it’s likely that most points live near the boundary of the point cloud (the convex hull), i.e. we expect almost all points to be “non-central”. To get around this second problem, we instead consider, for every pair of points (&lt;span class=&#34;math inline&#34;&gt;\(x_i, x_j\)&lt;/span&gt;), the midpoint &lt;span class=&#34;math inline&#34;&gt;\(m_{ij} = \frac{x_i + x_j}{2}\)&lt;/span&gt; as the base point in the construction of our feature functions &lt;span class=&#34;math inline&#34;&gt;\(q_{ij}(\alpha)\)&lt;/span&gt;. Thus, we construct a &lt;em&gt;matrix&lt;/em&gt; of feature functions, with each observation corresponding to &lt;span class=&#34;math inline&#34;&gt;\(n-1\)&lt;/span&gt; feature functions, one for every other observation in the data set (though current work uses only the average of these functions or appropriate subsets of them). The choice of the midpoint as base point is motivated as follows. Consider a 2-class modal classification problem, where each class is represented by a component of a mixture of two normal distributions for which the corresponding cluster centers (the means) are sufficiently separated. When considering a pair of observations from different classes, their midpoint is likely to live in a region between the two point clouds with few nearby observations, in other words, a low density region with a high measure of centrality. The opposite can be expected for within class comparisons, i.e. two observations from the same class.&lt;/p&gt;
&lt;p&gt;To addresses the convexity issue alluded to above, we use “local” versions of the HSD. This is done by taking random subsets of the data space containing &lt;span class=&#34;math inline&#34;&gt;\(m_{ij}\)&lt;/span&gt; and computing the HSD for this point after projection of the subset onto the line &lt;span class=&#34;math inline&#34;&gt;\(\ell_{ij}\)&lt;/span&gt; that is determined by the two points &lt;span class=&#34;math inline&#34;&gt;\(x_i,x_j\)&lt;/span&gt;. Specifically, the subsets are given by the data residing in randomly selected spherical cones of a fixed angle (which is a tuning parameter) with axis of symmetry &lt;span class=&#34;math inline&#34;&gt;\(\ell_{ij}\)&lt;/span&gt; (see figure 2.) We define a distribution of cone tips along &lt;span class=&#34;math inline&#34;&gt;\(\ell_{ij}\)&lt;/span&gt;, which induces a distribution of “local” depths (HSD), and define the DQF as the corresponding quantile function of this distribution. Using directions determined by pairs of points (i.e. the lines &lt;span class=&#34;math inline&#34;&gt;\(\ell_{ij}\)&lt;/span&gt;) addresses a challenge with high dimensional data: which direction should one look to capture interesting feature of the data? It also results in this method being automatically adaptive to sparseness (data living in a lower dimensional subspace of our data space).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-07-01-depth-quantile-functions_files/f2.png&#34; /&gt;
&lt;em&gt;Figure 2: A local depth for midpoint in red. Depth value will be 2 for this cone tip.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As a result of this construction, we end up with a multi-scale method, a function defined on [0,1], that is non-degenerate at both boundaries (in contrast to most multi-scale methods). One can show that the derivative of &lt;span class=&#34;math inline&#34;&gt;\(q_{ij}(\alpha)\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(\alpha\to 0\)&lt;/span&gt; yields information about the density at &lt;span class=&#34;math inline&#34;&gt;\(m_{ij}\)&lt;/span&gt;, while &lt;span class=&#34;math inline&#34;&gt;\(q_{ij}(1)\)&lt;/span&gt; is related to its centrality in the entire point cloud. The manner in which &lt;span class=&#34;math inline&#34;&gt;\(q_{ij}\)&lt;/span&gt; grows with increasing &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, while less interpretable, yields valuable information about the observations it corresponds to.&lt;/p&gt;
&lt;p&gt;As an example of how this information might be used, we again consider the 2-class classification problem. Each observation is described by two functions, the average function &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{|C_1|}\sum_{j\in C_1}q_{ij}(\alpha)\)&lt;/span&gt; for comparisons with class 1 (&lt;span class=&#34;math inline&#34;&gt;\(C_1\)&lt;/span&gt;), and similarly the average function for comparisons with class 2. In line with the heuristic laid out above, it can be seen in figure 1 that for an observation from class 1, comparisons with class two tends to yield functions that have low density (so are slow to grow for small quantile levels) and large value for &lt;span class=&#34;math inline&#34;&gt;\(\alpha=1\)&lt;/span&gt; corresponding to high centrality. In-class comparisons have the opposite properties. A simple heuristic for solving this classification problem might be to use the first few loadings from an fPCA and your favorite classifier for Euclidean data. Results on several data sets using an untuned SVM were competitive with existing methods with extensive tuning.&lt;/p&gt;
&lt;p&gt;Finally, the construction only depends on the data via inner products, meaning that DQFs can be constructing on any data type for which a kernel is defined, for instance persistence diagrams in topological data analysis, allowing for a visualization of non-Euclidean data in addition to high (including infinite) dimensional Euclidean data.&lt;/p&gt;
&lt;p&gt;Reference: Chandler, G. and Polonik, W. “Multiscale geometric feature extraction for high-dimensional and non-Euclidean data with applications.” Ann. Statist. 49 (2) 988 - 1010, April 2021. (&lt;a href=&#34;https://arxiv.org/abs/1811.10178&#34; class=&#34;uri&#34;&gt;https://arxiv.org/abs/1811.10178&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Concentration Inequalities in Machine Learning</title>
      <link>https://youngstats.github.io/post/2021/06/30/concentration-inequalities-in-machine-learning/</link>
      <pubDate>Wed, 30 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/06/30/concentration-inequalities-in-machine-learning/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;The fifth “One World webinar” organized by YoungStatS will take place on September 15th, 2021. Selected young European researchers active in the areas of probability and machine learning will present their recent contributions. The webinar is joint cooperation between the Young Researchers Committee of the Bernoulli Society and the YoungStatS project.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wednesday, September 15th, 17:00 CEST&lt;/li&gt;
&lt;li&gt;Online, via Zoom. The registration form is available &lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSe3S-SK35qjIG2bNUs1lBNQsq1uXWBYkO3VMqpvxywM9LdXpQ/viewform?vc=0&amp;amp;c=0&amp;amp;w=1&amp;amp;flr=0&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://amarchina.perso.math.cnrs.fr/index_en.html&#34;&gt;Antoine Marchina&lt;/a&gt; (Université de Paris): »Concentration inequalities for suprema of unbounded empirical processes« (Based on the &lt;a href=&#34;https://hal.archives-ouvertes.fr/hal-01545101/&#34;&gt;preprint&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.google.com/view/geoffreychinot&#34;&gt;Geoffrey Chinot&lt;/a&gt; (ETH Zurich): »Adaboost and robust one-bit compressed sensing« (based on the &lt;a href=&#34;https://arxiv.org/pdf/2105.02083.pdf&#34;&gt;preprint&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prof. Gábor Lugosi, ICREA Research Professor at Pompeu Fabra University and Barcelona GSE Research Professor, Blackwell Lecture speaker at the Bernoulli-IMS 2021 World Congress in Probability and Statistics&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The webinar is part of YoungStatS project of the Young Statisticians Europe initiative (FENStatS) supported by the Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics (IMS).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-06-30-concentration-inequality-in-machine-learning_files/webinar1.jpg&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Optional stopping with Bayes factors: possibilities and limitations</title>
      <link>https://youngstats.github.io/post/2021/06/10/optional-stopping-with-bayes-factors-possibilities-and-limitations/</link>
      <pubDate>Thu, 10 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/06/10/optional-stopping-with-bayes-factors-possibilities-and-limitations/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In recent years, a surprising number of scientific results have failed
to hold up to continued scrutiny. Part of this ‘replicability crisis’
may be caused by practices that ignore the assumptions of traditional
(frequentist) statistical methods &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-john-2012-measur-preval&#34; role=&#34;doc-biblioref&#34;&gt;John, Loewenstein, and Prelec 2012&lt;/a&gt;)&lt;/span&gt;. One of
these assumptions is that the experimental protocol should be completely
determined upfront. In practice, researchers often adjust the protocol
due to unforeseen circumstances or collect data until a point has been
proven. This practice, which is referred to as &lt;em&gt;optional stopping&lt;/em&gt;, can
cause true hypotheses to be wrongly rejected much more often than these
statistical methods promise.&lt;br /&gt;
Bayes factor hypothesis testing has long been advocated as an
alternative to traditional testing that can resolve several of its
problems; in particular, it was claimed early on that Bayesian methods
continue to be valid under optional stopping
&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-lindley-1957-statis-parad&#34; role=&#34;doc-biblioref&#34;&gt;Lindley 1957&lt;/a&gt;; &lt;a href=&#34;#ref-RaiffaS61&#34; role=&#34;doc-biblioref&#34;&gt;Raiffa and Schlaifer 1961&lt;/a&gt;; &lt;a href=&#34;#ref-edwards-1963-bayes-statis&#34; role=&#34;doc-biblioref&#34;&gt;Edwards, Lindman, and Savage 1963&lt;/a&gt;)&lt;/span&gt;. In
light of the replicability crisis, such claims have received much
renewed interest
&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-wagenmakers-2007-pract-solut&#34; role=&#34;doc-biblioref&#34;&gt;Wagenmakers 2007&lt;/a&gt;; &lt;a href=&#34;#ref-rouder-2014-option&#34; role=&#34;doc-biblioref&#34;&gt;Jeffrey N. Rouder 2014&lt;/a&gt;; &lt;a href=&#34;#ref-schonbrodt-2017-sequen-hypot&#34; role=&#34;doc-biblioref&#34;&gt;Schönbrodt et al. 2017&lt;/a&gt;; &lt;a href=&#34;#ref-yu-2013-when-decis&#34; role=&#34;doc-biblioref&#34;&gt;Yu et al. 2014&lt;/a&gt;; &lt;a href=&#34;#ref-sanborn-2013-frequen-implic&#34; role=&#34;doc-biblioref&#34;&gt;Sanborn and Hills 2014&lt;/a&gt;)&lt;/span&gt;.
But what do they mean mathematically? It turns out that different
authors mean quite different things by ‘Bayesian methods handle optional
stopping’; moreover, such claims are often shown to hold only in an
informal sense, or in restricted contexts. In the paper
&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hendriksen2020optional&#34; role=&#34;doc-biblioref&#34;&gt;Hendriksen, Heide, and Grünwald 2020&lt;/a&gt;)&lt;/span&gt; we give a systematic overview and
formalization of such claims, and explain their relevance for practice:
can we effectively rely on Bayes factor testing to do a good job under
optional stopping or not? As we shall see, the answer is subtle.
Secondly, we extend the reach of such claims to more general settings,
for which they have never been formally verified and for which
verification is not always trivial. In the paper &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-heide2020optional&#34; role=&#34;doc-biblioref&#34;&gt; Heide and Grünwald 2020&lt;/a&gt;)&lt;/span&gt;,
we explain claims about optional stopping for an audience of
methodologists and applied statisticians with the help of computer
simulations.&lt;br /&gt;
&lt;/p&gt;
&lt;div id=&#34;bayesian-inference&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;Bayesian inference&lt;/h1&gt;
&lt;p&gt;Bayesianism is about a certain interpretation of the concept
probability: as &lt;em&gt;degrees of belief&lt;/em&gt;. A Bayesian first expresses this
belief as a probability function. We call this the prior distribution,
and we denote it by &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{P}(\theta)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is the
parameter (or several parameters) of the model. After the specification
of the prior, we obtain the data &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; and the likelihood
&lt;span class=&#34;math inline&#34;&gt;\(\mathbb{P}(D | \theta)\)&lt;/span&gt;. Now we can compute the &lt;em&gt;posterior
distribution&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{P}(\theta | D)\)&lt;/span&gt; with the help of &lt;em&gt;Bayes’
theorem&lt;/em&gt;: &lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\mathbb{P}(\theta | D) = \frac{\mathbb{P}(D | \theta) \mathbb{P}(\theta)}{\mathbb{P}(D)}.\end{aligned}\]&lt;/span&gt;
Suppose we want to test a null hypothesis &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_0\)&lt;/span&gt; against an
alternative hypothesis &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_1\)&lt;/span&gt;. We can do this in a Bayesian way
with &lt;em&gt;Bayes factors&lt;/em&gt;: we start with the &lt;em&gt;prior odds&lt;/em&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\mathbb{P}(\mathcal{H}_1) / \mathbb{P}(\mathcal{H}_0)\)&lt;/span&gt;, our belief
before seeing the data. Often we believe that both hypotheses are
equally probable, then our prior odds are 1-to-1. Next we gather our
data &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;, and update our odds with the new knowledge, using Bayes’
theorem: &lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\text{posterior odds}(\mathcal{H}_1 \text{ vs. } \mathcal{H}_0) = \frac{\mathbb{P}(\mathcal{H}_1 | D)}{\mathbb{P}(\mathcal{H}_0 | D)} = \frac{\mathbb{P}(\mathcal{H}_1)}{\mathbb{P}(\mathcal{H}_0)} \frac{\mathbb{P}(D | \mathcal{H}_1)}{\mathbb{P}(D | \mathcal{H}_0)}.\end{aligned}\]&lt;/span&gt;
The posterior odds is our updated belief about which hypothesis is more
likely.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;three-notions-of-optional-stopping&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;Three notions of optional stopping&lt;/h1&gt;
&lt;p&gt;Validity under optional stopping is a desirable property of hypothesis
testing: we gather some data, look at the results, and decide whether we
stop of gather some additional data. Informally, we call ‘peeking at the
results to decide whether to collect more data’ &lt;em&gt;optional stopping&lt;/em&gt;, but
if we want to make more precise what it means if we say that a test can
handle optional stopping, it turns out that different approaches
(frequentist, subjective Bayesian and objective Bayesian) lead to
different interpretations and definitions. It tuns out that we can
discern three main mathematical concepts of handling optional stopping,
which we identify and formally define in the paper
&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hendriksen2020optional&#34; role=&#34;doc-biblioref&#34;&gt;Hendriksen, Heide, and Grünwald 2020&lt;/a&gt;)&lt;/span&gt;.&lt;br /&gt;
The first concept we call &lt;em&gt;subjective Bayesian optional stopping&lt;/em&gt; or
&lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;-independence. If one considers a purely subjective Bayesian
setting, appropriate if one truly believes one’s prior, then Bayesian
updating from prior to posterior is not affected by the employed
stopping rule: one ends up with the same posterior if one had decided
the sample size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; in advance, or if it had been determined, for
example, because one was satisfied with the result at this &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. In this
sense a subjective Bayesian procedure does not depend on the stopping
rule.&lt;br /&gt;
The second sense of optional stopping we call &lt;em&gt;calibration&lt;/em&gt;. As
&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-rouder-2014-option&#34; role=&#34;doc-biblioref&#34;&gt;Jeffrey N. Rouder 2014&lt;/a&gt;)&lt;/span&gt; writes: ‘If a replicate experiment yielded a
posterior odds of 3.5-to-1 in favor of the null, then we expect that the
null was 3.5 times as probable as the alternative to have produced the
data.’ In more mathematical language, this can be expressed as
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\text{post-odds} (\mathcal{H}_1 \text{ vs. } \mathcal{H}_0 | ``\text{post-odds} (\mathcal{H}_1 \text{ vs. } \mathcal{H}_0 | D ) = a&amp;quot;) = a.\end{aligned}\]&lt;/span&gt;
We say this equation expresses &lt;em&gt;calibration of the posterior odds&lt;/em&gt;. It
turns out that this calibration fails to hold if one does not adhere to
a purely subjective Bayesian view, in particular, it does not hold for
the &lt;em&gt;default&lt;/em&gt; priors the Bayesian psychology community is advocating
&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-wagenmakers-2007-pract-solut&#34; role=&#34;doc-biblioref&#34;&gt;Wagenmakers 2007&lt;/a&gt;; &lt;a href=&#34;#ref-rouder-2012-default-bayes&#34; role=&#34;doc-biblioref&#34;&gt;J. N. Rouder et al. 2012&lt;/a&gt;)&lt;/span&gt;. To get a
first idea of one of the issues: default priors sometimes depend on the
data. Then it is unclear what &lt;em&gt;optional stopping&lt;/em&gt; really means, because
if, using prior &lt;span class=&#34;math inline&#34;&gt;\(P_1(\theta)\)&lt;/span&gt; based on a sample of size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, one had
stopped at sample size &lt;span class=&#34;math inline&#34;&gt;\(n&amp;#39;&amp;lt; n\)&lt;/span&gt;, one should have really used prior
&lt;span class=&#34;math inline&#34;&gt;\(P&amp;#39;_1(\theta)\)&lt;/span&gt; based on sample of size &lt;span class=&#34;math inline&#34;&gt;\(n&amp;#39;\)&lt;/span&gt;...but then one would have
stopped at yet another sample size &lt;span class=&#34;math inline&#34;&gt;\(n&amp;#39;&amp;#39;\)&lt;/span&gt;, and so on. See our paper
&lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-heide2020optional&#34; role=&#34;doc-biblioref&#34;&gt; Heide and Grünwald 2020&lt;/a&gt;)&lt;/span&gt; for an extensive discussion and many examples.&lt;br /&gt;
&lt;/p&gt;
&lt;p&gt;The third sense is a frequentist interpretation of handling optional
stopping, which is about controlling the Type I error of an experiment.
A Type I error occurs when we reject the null hypothesis when it is
true, also called &lt;em&gt;false positive&lt;/em&gt;. The frequentist interpretation of
handling optional stopping is that the Type I error guarantee holds if
we do not determine the sampling plan — and thus the stopping rule —
in advance, but we may stop when we see significant results. In the case
&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_0\)&lt;/span&gt; is &lt;em&gt;simple&lt;/em&gt; (containing just one hypothesis), there is a
well-known intriguing connection between Bayes factors and Type I error
probabilities: if we reject &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_0\)&lt;/span&gt; iff the posterior odds in
favor of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_0\)&lt;/span&gt; are smaller than some fixed level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;,
then we are guaranteed a Type I error of at most &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. And
interestingly, this holds not just for fixed sample sizes but even under
optional stopping. However, for &lt;em&gt;composite&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_0\)&lt;/span&gt; this does
not continue to hold. Except for the special case where &lt;em&gt;all&lt;/em&gt; free
parameters in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_0\)&lt;/span&gt; are nuisance parameters observing a group
structure and equipped with the corresponding right-Haar prior, and are
shared with &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_1\)&lt;/span&gt;, as we prove in &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hendriksen2020optional&#34; role=&#34;doc-biblioref&#34;&gt;Hendriksen, Heide, and Grünwald 2020&lt;/a&gt;)&lt;/span&gt;.
But for general priors and composite &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_0\)&lt;/span&gt;, this is typically
not the case.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-06-09-optional-stopping-with-bayes-factors-possibilities-and-limitations_files/screenshot.138.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 1: Posterior odds in an experiment of testing whether the mean of a normal distribution is 0
(&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_0\)&lt;/span&gt;), versus non-zero (&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_1\)&lt;/span&gt;), from 20; 000 replicate experiments. (a) The empirical sampling distribution
of the posterior odds as a histogram under &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_0\)&lt;/span&gt; (blue) and &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_1\)&lt;/span&gt; (pink). (b) Calibration plot: the observed
posterior odds as a function of the nominal posterior odds.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;One can give three distinct mathematical meanings to the notion of
&lt;em&gt;optional stopping&lt;/em&gt;. Whether or not we can say that ‘the Bayes factor
method can handle optional stopping’ in practice is a subtle matter,
depending on the specifics of the given situation: what models are used,
what priors, and what is the goal of the analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;authors&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Authors&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-06-09-optional-stopping-with-bayes-factors-possibilities-and-limitations_files/Allard.jpg&#34; height=&#34;70&#34; /&gt;
Allard Hendriksen, CWI Amsterdam&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-06-09-optional-stopping-with-bayes-factors-possibilities-and-limitations_files/Rianne.jpg&#34; height=&#34;70&#34; /&gt;
dr. Rianne de Heide, Leiden University &amp;amp; CWI Amsterdam&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-06-09-optional-stopping-with-bayes-factors-possibilities-and-limitations_files/Peter.jpg&#34; height=&#34;70&#34; /&gt;
prof. Peter Grünwald, Leiden University &amp;amp; CWI Amsterdam&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-edwards-1963-bayes-statis&#34; class=&#34;csl-entry&#34;&gt;
Edwards, Ward, Harold Lindman, and Leonard J. Savage. 1963. &lt;span&gt;“Bayesian Statistical Inference for Psychological Research.”&lt;/span&gt; &lt;em&gt;Psychological Review&lt;/em&gt; 70 (3): 193–242. &lt;a href=&#34;https://doi.org/10.1037/h0044139&#34;&gt;https://doi.org/10.1037/h0044139&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-heide2020optional&#34; class=&#34;csl-entry&#34;&gt;
 Heide, Rianne, and Peter D. Grünwald. 2020. &lt;span&gt;“Why Optional Stopping Can Be a Problem for Bayesians.”&lt;/span&gt; &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, Advance Publication&lt;/em&gt;, 1–18.
&lt;/div&gt;
&lt;div id=&#34;ref-hendriksen2020optional&#34; class=&#34;csl-entry&#34;&gt;
Hendriksen, Allard, Rianne Heide, and Peter Grünwald. 2020. &lt;span&gt;“Optional Stopping with Bayes Factors: A Categorization and Extension of Folklore Results, with an Application to Invariant Situations.”&lt;/span&gt; &lt;em&gt;Bayesian Analysis, Advance Publication&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-john-2012-measur-preval&#34; class=&#34;csl-entry&#34;&gt;
John, Leslie K, George Loewenstein, and Drazen Prelec. 2012. &lt;span&gt;“Measuring the Prevalence of Questionable Research Practices with Incentives for Truth Telling.”&lt;/span&gt; &lt;em&gt;Psychological Science&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-lindley-1957-statis-parad&#34; class=&#34;csl-entry&#34;&gt;
Lindley, D. V. 1957. &lt;span&gt;“A Statistical Paradox.”&lt;/span&gt; &lt;em&gt;Biometrika&lt;/em&gt; 44 (1/2): 187–92. &lt;a href=&#34;https://doi.org/10.2307/2333251&#34;&gt;https://doi.org/10.2307/2333251&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-RaiffaS61&#34; class=&#34;csl-entry&#34;&gt;
Raiffa, H., and R. Schlaifer. 1961. &lt;em&gt;Applied Statistical Decision Theory&lt;/em&gt;. Cambridge, MA: Harvard University Press.
&lt;/div&gt;
&lt;div id=&#34;ref-rouder-2012-default-bayes&#34; class=&#34;csl-entry&#34;&gt;
Rouder, J N, R D Morey, P L Speckman, and J M Province. 2012. &lt;span&gt;“Default &lt;span&gt;B&lt;/span&gt;ayes Factors for &lt;span&gt;ANOVA&lt;/span&gt; Designs.”&lt;/span&gt; &lt;em&gt;Journal of Mathematical Psychology&lt;/em&gt; 56 (5): 356–74.
&lt;/div&gt;
&lt;div id=&#34;ref-rouder-2014-option&#34; class=&#34;csl-entry&#34;&gt;
Rouder, Jeffrey N. 2014. &lt;span&gt;“Optional Stopping: No Problem for &lt;span&gt;B&lt;/span&gt;ayesians.”&lt;/span&gt; &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt; 21 (2): 301–8. &lt;a href=&#34;https://doi.org/10.3758/s13423-014-0595-4&#34;&gt;https://doi.org/10.3758/s13423-014-0595-4&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-sanborn-2013-frequen-implic&#34; class=&#34;csl-entry&#34;&gt;
Sanborn, Adam N., and Thomas T. Hills. 2014. &lt;span&gt;“The Frequentist Implications of Optional Stopping on &lt;span&gt;B&lt;/span&gt;ayesian Hypothesis Tests.”&lt;/span&gt; &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt; 21 (2): 283–300. &lt;a href=&#34;https://doi.org/10.3758/s13423-013-0518-9&#34;&gt;https://doi.org/10.3758/s13423-013-0518-9&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-schonbrodt-2017-sequen-hypot&#34; class=&#34;csl-entry&#34;&gt;
Schönbrodt, Felix D., Eric-Jan Wagenmakers, Michael Zehetleitner, and Marco Perugini. 2017. &lt;span&gt;“Sequential Hypothesis Testing with &lt;span&gt;B&lt;/span&gt;ayes Factors: Efficiently Testing Mean Differences.”&lt;/span&gt; &lt;em&gt;Psychological Methods&lt;/em&gt; 22 (2): 322–39. &lt;a href=&#34;https://doi.org/10.1037/met0000061&#34;&gt;https://doi.org/10.1037/met0000061&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-wagenmakers-2007-pract-solut&#34; class=&#34;csl-entry&#34;&gt;
Wagenmakers, Eric-Jan. 2007. &lt;span&gt;“A Practical Solution to the Pervasive Problems of p Values.”&lt;/span&gt; &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt; 14 (5): 779–804. &lt;a href=&#34;https://doi.org/10.3758/bf03194105&#34;&gt;https://doi.org/10.3758/bf03194105&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-yu-2013-when-decis&#34; class=&#34;csl-entry&#34;&gt;
Yu, Erica C., Amber M. Sprenger, Rick P. Thomas, and Michael R. Dougherty. 2014. &lt;span&gt;“When Decision Heuristics and Science Collide.”&lt;/span&gt; &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt; 21 (2): 268–82. &lt;a href=&#34;https://doi.org/10.3758/s13423-013-0495-z&#34;&gt;https://doi.org/10.3758/s13423-013-0495-z&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Spatiotemporal modeling and real-time prediction of origin-destination traffic demand</title>
      <link>https://youngstats.github.io/post/2021/06/09/spatiotemporal-modeling-and-real-time-prediction-of-origin/</link>
      <pubDate>Wed, 09 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/06/09/spatiotemporal-modeling-and-real-time-prediction-of-origin/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the past decades, intelligent transportation system (ITS) has brought advanced technology that enables a data-rich environment and unprecedented opportunities for traffic prediction, which is considered as one of the most prevalent issues facing ITS (Li et al., 2015). We discuss the online prediction of the origin-destination (OD) demand count in traffic networks, which represents the number of trips between certain combinations of an origin and a destination. The study of OD demand prediction based on count data has a growing impact on many traffic control and management policies (Ashok, 1996, Ashok and Ben-Akiva, 2002, Li, 2005, Hazelton, 2008, Shao et al., 2014). For example, dynamic OD demand prediction is critical in planning for the charging services of the electrical vehicles (EV; Zhang et al., 2017). A well-designed charging facility network is necessary to extend the vehicle range and popularize the use of EVs. In particular, the dynamic demand between nodes of the traffic network plays a key role in determining the availability of the charging facilities, planning the multi-period charging schedules, and meeting the customer needs at the maximum extent (Zhang et al., 2017, Brandstatter et al., 2017). The objective of this study is to appropriately model the stochastic OD traffic demand counts considering the spatiotemporal correlations between different routes and epochs, while incorporating physical knowledge of the traffic network in the estimation. The estimation results are expected to enhance the prediction accuracy and robustness of the online traffic demand prediction for future epochs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Model and method&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We investigate a multivariate Poisson log-normal model with a
block-diagonal covariance matrix and incorporate domain knowledge of the
traffic network features to account for spatial correlations. Let
&lt;span class=&#34;math inline&#34;&gt;\(N_{\text{ijt}}\)&lt;/span&gt; denote the observed traffic demand (i.e., the count of
vehicles) for route &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; on day &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, at epoch &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. Based on the natural
characteristics of the demand counts, it is reasonable to model each
observation &lt;span class=&#34;math inline&#34;&gt;\(N_{\text{ijt}}\)&lt;/span&gt; with a Poisson log-linear model (Perrakis
et al., 2014, Xian et al. 2018) such that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[N_{\text{ijt}}\sim\text{Poisson}\left( \lambda_{\text{ijt}} \right),u_{\text{ijt}} = \log\lambda_{\text{ijt}}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{\text{ijt}}\)&lt;/span&gt; is the intensity of the Poisson process, and
&lt;span class=&#34;math inline&#34;&gt;\(u_{\text{ijt}}\)&lt;/span&gt; is the log transformation of the intensity. To
characterize the spatiotemporal correlations across different routes and
time points, we model &lt;span class=&#34;math inline&#34;&gt;\(u_{\text{ijt}}\)&lt;/span&gt; as a mixed-effect Gaussian
process based on &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; basis functions &lt;span class=&#34;math inline&#34;&gt;\(B_{k}(t)\)&lt;/span&gt; that&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math display&#34;&gt;\[u_{\text{ijt}} = \mu_{\text{jt}} + \sum_{k = 1}^{K}{\gamma_{\text{jk}}B_{k}(t)} + Z_{\text{ijt}}.\]&lt;/span&gt; (1)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\mu =}\left\lbrack \mu_{11},\mu_{12},\ \cdots,\ \mu_{\text{JT}} \right\rbrack\mathbf{&amp;#39;}\)&lt;/span&gt;
is the fixed effect coefficient that models the common characteristics
of the whole traffic network, and
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\gamma}_{k}\mathbf{=}\left\lbrack \gamma_{1k},\ \gamma_{2k},\ \cdots,\ \gamma_{\text{Jk}} \right\rbrack\)&lt;/span&gt;
is the random effect coefficient with prior distribution
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\gamma}_{k}\sim N(0,\ \mathbf{R}_{\theta_{y}})\)&lt;/span&gt; that
characterizes the uniqueness of different routes. Here
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{R}_{\theta_{y}}\)&lt;/span&gt; is the correlation matrix which takes into
consideration of the traffic network information, where
&lt;span class=&#34;math inline&#34;&gt;\(\left\lbrack \mathbf{R}_{\theta_{y}} \right\rbrack_{j_{1},j_{2}} = \sigma_{j_{1},\ j_{2}}\exp\left\{ - \theta_{y}\left| \mathbf{y}_{j_{1}} - \mathbf{y}_{j_{2}} \right|^{2} \right\}\)&lt;/span&gt;.
In this expression, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{y}_{j}\)&lt;/span&gt; denotes the unique features of
route &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, such as information about the origin and destination, the
maximum speed limit on a route, and the travel distance. The term
&lt;span class=&#34;math inline&#34;&gt;\(Z_{\text{ijt}}\)&lt;/span&gt; in model (1) is the random error that follows a
Gaussian distribution which has the covariance structure&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{cov}\left( Z_{ij_{1}t_{1}},Z_{ij_{2}t_{2}} \right) = \sigma_{j_{1},\ j_{2}}\exp\left\{ - \theta_{y}\left| \mathbf{y}_{j_{1}} - \mathbf{y}_{j_{2}} \right|^{2} \right\} \cdot \tau^{2}\exp\left\{ - \theta_{t}\left| t_{1} - t_{2} \right| \right\}.\]&lt;/span&gt; (2)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Which depends on both the features of routes &lt;span class=&#34;math inline&#34;&gt;\(j_{1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j_{2}\)&lt;/span&gt;, and
the time points &lt;span class=&#34;math inline&#34;&gt;\(t_{1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(t_{2}\)&lt;/span&gt;, which we refer to as the spatial
and temporal covariance structures, respectively.&lt;/p&gt;
&lt;p&gt;Denote the log-transformed intensity of the OD traffic demand on day &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;
as
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{u}_{i} = \left( u_{i11},u_{i12},\ldots,u_{\text{iJT}} \right)^{&amp;#39;}\)&lt;/span&gt;.
We can further derive that conditioning on parameters
&lt;span class=&#34;math inline&#34;&gt;\(\left( \mathbf{\mu},\ \theta_{y},\theta_{t},\mathbf{\sigma,\ }\tau^{2} \right)\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{u}_{i}\)&lt;/span&gt; follows normal distribution
&lt;span class=&#34;math inline&#34;&gt;\(N\left( \mathbf{\mu},\ \mathbf{\Sigma} \right)\)&lt;/span&gt;, where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathbf{\mu} = \left( \mu_{11},\mu_{12},\ \cdots,\ \mu_{\text{JT}} \right)^{&amp;#39;},\]&lt;/span&gt;&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathbf{\Sigma} = \mathbf{R}_{\theta_{y}}\bigotimes\left\lbrack \mathbf{R}_{B} + \tau^{2}\mathbf{R}_{\theta_{t}} \right\rbrack.\]&lt;/span&gt; (3)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here, the symbol &lt;span class=&#34;math inline&#34;&gt;\(\bigotimes\)&lt;/span&gt; denotes the Kronecker product,
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{R}_{B}\)&lt;/span&gt; is a fixed &lt;span class=&#34;math inline&#34;&gt;\(T \times T\)&lt;/span&gt; matrix with the &lt;span class=&#34;math inline&#34;&gt;\((t_{1},t_{2})\)&lt;/span&gt;
element equal to &lt;span class=&#34;math inline&#34;&gt;\(\sum_{k = 1}^{K}{B_{k}(t_{1})B_{k}(t_{2})}\)&lt;/span&gt;, and
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{R}_{\theta_{t}}\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(T \times T\)&lt;/span&gt; matrix with the
&lt;span class=&#34;math inline&#34;&gt;\((t_{1},t_{2})\)&lt;/span&gt; element equal to
&lt;span class=&#34;math inline&#34;&gt;\(\exp\left\{ - \theta_{t}|t_{1} - t_{2}| \right\}\)&lt;/span&gt;. Therefore, the large
covariance matrix is parametrized based on only the parameters
&lt;span class=&#34;math inline&#34;&gt;\(\theta_{y},\theta_{t},\mathbf{\sigma}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\tau^{2}\)&lt;/span&gt;. This
parsimonious model has several advantages, such as high interpretability
tailored to the traffic demand count data, increased stability of the
estimation results, and reduced computational burden for parameter
estimation. We treat &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{u}\)&lt;/span&gt; as a latent variable and further
employ the EM algorithm to obtain the maximum likelihood estimation
(MLE) for the parameters.&lt;/p&gt;
&lt;p&gt;In this way, we can fully explore the complicated spatiotemporal
correlation structure of the traffic network demand and automatically
cluster the routes with high correlations, without introducing a large
number of parameters that impact the estimation accuracy. Besides
transportation systems, the proposed method can be easily extended to
other network applications with count data through few modifications,
such as communication systems, supply chain management, smart grid, or
even three-dimensional networks (Wang et al., 2018).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case study&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We apply the proposed method to a real New York yellow taxi dataset
which is collected from June 1&lt;sup&gt;st&lt;/sup&gt; to July 31&lt;sup&gt;st&lt;/sup&gt; in 2017 (NYC taxi,
2017). The dataset records all yellow taxi trips during the
aforementioned time period including the pick-up and drop-off dates and
times, pick-up and drop-off locations, trip distances, and payment
information about the trips. We focus on the trips between the four
busiest zones in Manhattan and investigate the structure of the travel
demand counts on these zones as OD pairs. The details of the four taxi
zones are shown in Figure 1.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-06-10-spatiotemporal-modeling-and-real-time-prediction-of-origin_files/f1.jpg&#34; /&gt;&lt;/p&gt;
&lt;table style=&#34;width:97%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;14%&#34; /&gt;
&lt;col width=&#34;24%&#34; /&gt;
&lt;col width=&#34;58%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Index&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Borough&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Zones&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Manhattan&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Lincoln Square East&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Manhattan&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Times Square/Theatre District&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Manhattan&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Upper East Side North&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Manhattan&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Upper East Side South&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;Figure 1. Illustration of the taxi zones in the case study&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Figure 2 further shows the specific taxi demand prediction results of
two routes &lt;span class=&#34;math inline&#34;&gt;\((4,\ 3)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\((4,\ 4)\)&lt;/span&gt; for four test days. The solid black
line in this figure represents the true dynamic traffic demand counts,
where it can be observed that the true taxi demand indeed exhibits high
spatial and temporal variation and strong correlations for observations
between the routes and across different epochs. The solid red line in is
the predicted demand using the proposed method, and the dashed error
bars show the 90% confidence interval of the prediction based on the
variance derivation in equation (7), which significantly outperforms the
existing method shown in black dotted line.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-06-10-spatiotemporal-modeling-and-real-time-prediction-of-origin_files/f2.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 2. Taxi demand prediction results for routes &lt;span class=&#34;math inline&#34;&gt;\((4,\ 3)\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\((4,\ 4)\)&lt;/span&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;This post is based on&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Xian, X., Ye, H., Wang, X., and Liu, K. (2021). Spatiotemporal modeling and real-time prediction of origin-destination traffic demand. &lt;em&gt;Technometrics&lt;/em&gt;, 63(1), 77-89.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Li, L., Su, X., Zhang, Y., Lin, Y., and Li, Z. (2015). Trend modeling for traffic time series analysis: An integrated study. &lt;em&gt;IEEE Transactions on Intelligent Transportation Systems&lt;/em&gt;, 16(6), 3430-3439.&lt;/li&gt;
&lt;li&gt;Ashok, K. (1996). Estimation and prediction of time-dependent origin-destination flows (Doctoral dissertation, Massachusetts Institute of Technology).&lt;/li&gt;
&lt;li&gt;Ashok, K., and Ben-Akiva, M. E. (2002). Estimation and prediction of time-dependent origin-destination flows with a stochastic mapping to path flows and link flows. &lt;em&gt;Transportation Science&lt;/em&gt;, 36(2), 184-198.&lt;/li&gt;
&lt;li&gt;Brandstatter, G., Kahr, M., and Leitner, M. (2017). Determining optimal locations for charging stations of electric car-sharing systems under stochastic demand. &lt;em&gt;Transportation Research Part B: Methodological&lt;/em&gt;, 104, 17-35.&lt;/li&gt;
&lt;li&gt;Li, B. (2005). Bayesian inference for origin-destination matrices of transport networks using the EM algorithm. &lt;em&gt;Technometrics&lt;/em&gt;, 47(4), 399-408.&lt;/li&gt;
&lt;li&gt;Hazelton, M. L. (2008). Statistical inference for time varying origin–destination matrices. &lt;em&gt;Transportation Research Part B: Methodological&lt;/em&gt;, 42(6), 542-552.
NYC taxi, (2017). Retrieved from &lt;a href=&#34;http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml&#34; class=&#34;uri&#34;&gt;http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Perrakis, K., Karlis, D., Cools, M., Janssens, D., Vanhoof, K., and Wets, G. (2012). A Bayesian approach for modeling origin-destination matrices. &lt;em&gt;Transportation Research Part A: Policy and Practice&lt;/em&gt;, 46(1), 200-212.&lt;/li&gt;
&lt;li&gt;Shao, H., Lam, W. H., Sumalee, A., Chen, A., and Hazelton, M. L. (2014). Estimation of mean and covariance of peak hour origin-destination demands from day-to-day traffic counts. &lt;em&gt;Transportation Research Part B: Methodological&lt;/em&gt;, 68, 52-75.&lt;/li&gt;
&lt;li&gt;Wang, D., Liu, K., and Zhang, X. (2018), “Modeling of a three-dimensional dynamic thermal field under grid-based sensor networks in grain storage”, in press, &lt;em&gt;IISE Transactions&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Zhang, A., Kang, J. E., and Kwon, C. (2017). Incorporating demand dynamics in multi-period capacitated fast-charging location planning for electric vehicles. &lt;em&gt;Transportation Research Part B: Methodological&lt;/em&gt;, 103, 5-29.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;About the authors&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-06-10-spatiotemporal-modeling-and-real-time-prediction-of-origin_files/xiaochen.jpg&#34; height=&#34;75&#34; /&gt;
Dr. Xiaochen Xian is an assistant professor from the Department of Industrial and Systems Engineering at the University of Florida.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-06-10-spatiotemporal-modeling-and-real-time-prediction-of-origin_files/ye.jpg&#34; height=&#34;75&#34; /&gt;
Honghan Ye is a Ph.D. candidate from the Department of Industrial and Systems Engineering at UW-Madison.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-06-10-spatiotemporal-modeling-and-real-time-prediction-of-origin_files/wang.jpg&#34; height=&#34;75&#34; /&gt;
Dr. Xin Wang is an assistant professor from the Department of Industrial and Systems Engineering at UW-Madison.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-06-10-spatiotemporal-modeling-and-real-time-prediction-of-origin_files/liu.png&#34; height=&#34;75&#34; /&gt;
Dr. Kaibo Liu is an associate professor from the Department of Industrial and Systems Engineering at UW-Madison.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Recent Advances in Functional Data Analysis</title>
      <link>https://youngstats.github.io/post/2021/04/29/fda-webinar/</link>
      <pubDate>Thu, 29 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/04/29/fda-webinar/</guid>
      <description>&lt;p&gt;The fourth &amp;ldquo;&lt;em&gt;One World webinar&lt;/em&gt;&amp;rdquo; organized by YoungStatS will take place
on June 30th, 2021. The topic of this webinar is on Functional Data
Analysis. Selected young European researchers active in this area of
research will present their contributions on spherical functional
autoregressions, additive models, and clustering methods for functional
data, with the focus on both theoretical developments and applications.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Wednesday, June 30th, 16:30 CEST&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Online&lt;/em&gt;, via Zoom. The registration form is available &lt;a href=&#34;https://docs.google.com/forms/d/1efH13r__c5v-GHC5MsxzTlS0Csps_f4YHr622717STY/viewform?edit_requested=true&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://people.epfl.ch/alessia.caponera/?lang=en&#34;&gt;Alessia Caponera&lt;/a&gt; (École polytechnique fédérale de Lausanne, Switzerland): &amp;ldquo;Asymptotics for spherical functional autoregressions&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.wiwi.hu-berlin.de/en/Professorships/vwl/statistik/team/volkmale&#34;&gt;Alexander Volkmann&lt;/a&gt; (Humboldt-Universität zu Berlin, Germany): &amp;ldquo;Multivariate Functional Additive Mixed Models&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.dii.unina.it/page.php?tabella=livello4&amp;amp;id_livello=272&amp;amp;flag=persona&amp;amp;livello1=1&amp;amp;livello2=53&amp;amp;livello3=25&amp;amp;lang=en&#34;&gt;Fabio Centofanti&lt;/a&gt; (University of Naples Federico II, Italy): &amp;ldquo;Sparse and Smooth Functional Data Clustering&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.math-evry.cnrs.fr/members/Jpark/welcome&#34;&gt;Juhyun Park&lt;/a&gt; (École nationale supérieure d&#39;informatique pour
l&#39;industrie et l&#39;entreprise, France)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;The webinar is part of YoungStatS project of the Young Statisticians Europe initiative (FENStatS) supported by the Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics (IMS).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For more information, please visit our
&lt;a href=&#34;https://youngstats.github.io/post/2020/09/10/webinars/&#34;&gt;website&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Composite-Based Structural Equation Modeling: Developments and Perspectives</title>
      <link>https://youngstats.github.io/post/2021/04/28/csem-webinar/</link>
      <pubDate>Wed, 28 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/04/28/csem-webinar/</guid>
      <description>&lt;p&gt;The third &amp;ldquo;&lt;em&gt;One World webinar&lt;/em&gt;&amp;rdquo; organized by YoungStatS will take place
on May 19th, 2021. The focus of this webinar will be on composite-based
structural equation modeling, particularly on partial least squares path
modeling (Wold, 1982; Lohmöller, 1989) and approaches to assess
composite models. The webinar will present some of the most interesting
and recent theoretical developments and applications from younger
scholars active in this area of research.&lt;/p&gt;
&lt;p&gt;When &amp;amp; Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Wednesday, May 19th, 16:00 CEST&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Online&lt;/em&gt;, via Zoom. The registration form is available &lt;a href=&#34;https://docs.google.com/forms/d/19ozVQ71vZJi0_g_GYUC29w_xHnD9ub5HNC9sWbYhhvc/edit&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://econ.au.dk/research/researchcentres/creates/people/research-fellows/benjamin-d-liengaard/&#34;&gt;Benjamin Liengaard&lt;/a&gt; (Aarhus University, Denmark): &amp;lsquo;Measurement Invariance Testing with Latent Variable Scores using Partial Least Squares Path Modeling&amp;rsquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://nicholasdanks.com/&#34;&gt;Nicholas Danks&lt;/a&gt; (Trinity College Dublin, Ireland): &amp;lsquo;The Role of Prediction in Composite Modeling&amp;rsquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.florianschuberth.com/&#34;&gt;Florian Schuberth&lt;/a&gt; (University of Twente, The Netherlands): &amp;lsquo;Confirmatory Composite Analysis&amp;rsquo;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Discussant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.henseler.com/&#34;&gt;Jörg Henseler&lt;/a&gt; (University of Twente, The Netherlands)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;The webinar is part of YoungStatS project of the Young Statisticians Europe initiative (FENStatS) supported by the Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics (IMS).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For more information, please visit our
&lt;a href=&#34;https://youngstats.github.io/post/2020/09/10/webinars/&#34;&gt;website&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A small step to understand Generative Adversarial Networks</title>
      <link>https://youngstats.github.io/post/2021/04/27/a-small-step-to-understand-gda/</link>
      <pubDate>Tue, 27 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/04/27/a-small-step-to-understand-gda/</guid>
      <description>
&lt;script src=&#34;https://youngstats.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In the last decade, there have been spectacular advances on the practical side of machine learning.
One of the most impressive may be the success of Generative Adversarial Networks (GANs) for image generation &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-GoPoMiXuWaOzCoBe14&#34; role=&#34;doc-biblioref&#34;&gt;Goodfellow et al. 2014&lt;/a&gt;)&lt;/span&gt;.
State of the art models are capable of producing &lt;a href=&#34;https://www.youtube.com/watch?v=XOxxPcy5Gr4&#34;&gt;portraits of fake persons&lt;/a&gt; that look perfectly authentic to you and me (see e.g. &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-SaGoZaChRaCg16&#34; role=&#34;doc-biblioref&#34;&gt;Salimans et al. 2016&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-Karras2018&#34; role=&#34;doc-biblioref&#34;&gt;Karras et al. 2018&lt;/a&gt;)&lt;/span&gt;).
Other domains such as inpainting, text to image and speech are also concerned by outstanding results (see &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-Go16&#34; role=&#34;doc-biblioref&#34;&gt;Goodfellow 2016&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-JaLiBo20&#34; role=&#34;doc-biblioref&#34;&gt;Jabbar, Li, and Bourahla 2020&lt;/a&gt;)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Since their introduction by &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-GoPoMiXuWaOzCoBe14&#34; role=&#34;doc-biblioref&#34;&gt;Goodfellow et al. 2014&lt;/a&gt;)&lt;/span&gt;, GANs have unleashed passions in the community of machine learning, leading to a large volume of variants and possible applications, often referred to as &lt;a href=&#34;https://github.com/hindupuravinash/the-gan-zoo&#34;&gt;the GAN Zoo&lt;/a&gt;.
However, despite increasingly spectacular applications, little was known few years ago about the statistical properties of GANs.&lt;/p&gt;
&lt;p&gt;This post sketches the paper entitled ``Some Theoretical Properties of GANs’’ (G. Biau, B. Cadre, M. Sangnier and U. Tanielian, The Annals of Statistics, 2020),
which aims at building a statistical analysis of GANs in order to better understand their mathematical mechanism.
In particular, it proves a non-asymptotic bound on the excess of Jensen-Shannon error and the asymptotic normality of the parametric estimator.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathematical-framework&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mathematical framework&lt;/h2&gt;
&lt;div id=&#34;overview-of-the-method&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Overview of the method&lt;/h3&gt;
&lt;p&gt;The objective of GANs is to randomly generate artificial contents similar to some data.
Put another way, they are aimed at sampling according to an unknown distribution &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt;, based solely on i.i.d. observations &lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt; drawn according to &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt;.
Obviously, a naive approach would be to:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Estimate the distribution &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt; by some &lt;span class=&#34;math inline&#34;&gt;\(\hat P\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Sample according to &lt;span class=&#34;math inline&#34;&gt;\(\hat P\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;However, both tasks are difficult in themselves.
In particular, density estimation is made arduous by the complexity and high dimensionality of the data involved in the domain, relegating both standard parametric and nonparametric approaches unworkable.
Thus, GANs offer a completely different way to achieve our goal, often compared to the struggle between a police team, trying to distinguish true banknotes (the observed data &lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt;) from false ones (the generated data), and a counterfeiters team (the generator), slaving to produce banknotes as credible as possible and to mislead the police.&lt;/p&gt;
&lt;p&gt;To be a bit more specific, there are two brilliant ideas at the core of GANs:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Sample data in a very straightforward manner thanks to the transform method:
let &lt;span class=&#34;math inline&#34;&gt;\(\mathscr G=\left \{G_{\theta}: \mathbb R^\ell \to \mathbb R^d, \theta \in \Theta \right\}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\ell\)&lt;/span&gt; is the dimension of what is called the latent space and &lt;span class=&#34;math inline&#34;&gt;\(\Theta \subset \mathbb R^p\)&lt;/span&gt;, be a class of measurable functions, called generators (in practice &lt;span class=&#34;math inline&#34;&gt;\(\mathscr G\)&lt;/span&gt; is often a class of neural networks with &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; parameters).
Now, let us sample &lt;span class=&#34;math inline&#34;&gt;\(Z \sim \mathcal N(0, I_\ell)\)&lt;/span&gt; and compute &lt;span class=&#34;math inline&#34;&gt;\(U = G_\theta(Z)\)&lt;/span&gt;.
Then, &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is an observation drawn according to the distribution &lt;span class=&#34;math inline&#34;&gt;\(P_\theta = G_\theta \# N(0, I_\ell)\)&lt;/span&gt; (the push-forward measure of the latent distribution (N(0, I_)) according to (G_)).
In other words, the statistical model for the estimation of &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt; has the form &lt;span class=&#34;math inline&#34;&gt;\(\mathscr P = \left\{ P_\theta = G_\theta \# N(0, I_\ell), \theta \in \Theta \right\}\)&lt;/span&gt; and it is definitely straightforward to sample according to &lt;span class=&#34;math inline&#34;&gt;\(P_\theta\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Assessing the proximity between &lt;span class=&#34;math inline&#34;&gt;\(P_\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt; by comparing two samples &lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n \overset{i.i.d.}{\sim} P^\star\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(U_1, \dots, U_n \overset{i.i.d.}{\sim} P_\theta\)&lt;/span&gt;.
What does comparing mean?
Assume the group of &lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt; is very difficult to ``separate’’ from the group of &lt;span class=&#34;math inline&#34;&gt;\(U_1, \dots, U_n\)&lt;/span&gt;, or put another way,
it is very difficult to distinguish the class of &lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt; from the class of &lt;span class=&#34;math inline&#34;&gt;\(U_1, \dots, U_n\)&lt;/span&gt;.
Would you be convinced that the two distributions &lt;span class=&#34;math inline&#34;&gt;\(P_\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt; are very close (at least for large &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;)?
That is exactly the point.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-distributions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Comparing distributions&lt;/h3&gt;
&lt;p&gt;At this point, Task 2 is still a bit blurry and deserves further details about how to quantify the difficulty (or the ease) of separating the two classes &lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(U_1, \dots, U_n\)&lt;/span&gt;.
This problem is actually closely related to supervised learning, and in particular to classification:
assume that a classifier, let us say &lt;span class=&#34;math inline&#34;&gt;\(h : \mathbb R^d \to \{0, 1\}\)&lt;/span&gt;, manages to perfectly discriminate the two classes: &lt;span class=&#34;math inline&#34;&gt;\(\mathbb P(h(X_1)=1) = \mathbb P(h(U_1)=0) = 1\)&lt;/span&gt;, then we can say that the two distributions &lt;span class=&#34;math inline&#34;&gt;\(P_\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt; are different.
Conversely, if the classifier is fooled, that is &lt;span class=&#34;math inline&#34;&gt;\(\mathbb P(h(X_1)=1) = \mathbb P(h(U_1)=0) = \frac 12\)&lt;/span&gt;, we may accept that the two distributions are identical.&lt;/p&gt;
&lt;p&gt;This classification setting is formalized as following:
let &lt;span class=&#34;math inline&#34;&gt;\((X, Y)\)&lt;/span&gt; be a pair of random variables taking values in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb R^d \times \{0, 1\}\)&lt;/span&gt; such that:
&lt;span class=&#34;math display&#34;&gt;\[
    X|Y=1 \sim P^\star
    \quad \text{and} \quad
    X|Y=0 \sim P_\theta,
    %\tag{M}
\]&lt;/span&gt;
and let &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D = \left \{D_{\alpha} : \mathbb R^d \to [0, 1], \alpha \in \Lambda \right\}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\Lambda \subset \mathbb R^q\)&lt;/span&gt;, be a parametric model of discriminators such that &lt;span class=&#34;math inline&#34;&gt;\(D_\alpha(x)\)&lt;/span&gt; is aimed at estimating &lt;span class=&#34;math inline&#34;&gt;\(\mathbb P(Y=1 | X=x)\)&lt;/span&gt; (put another way, the distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y|X=x\)&lt;/span&gt; is estimated by &lt;span class=&#34;math inline&#34;&gt;\(\mathcal B(D_\alpha(x))\)&lt;/span&gt;).
For a given discriminator &lt;span class=&#34;math inline&#34;&gt;\(D_\alpha\)&lt;/span&gt;, the corresponding classifier is &lt;span class=&#34;math inline&#34;&gt;\(h : x \in \mathbb R^d \mapsto \mathbb 1_{D_\alpha(x) &amp;gt; \frac 12}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The sample ({ (X_1, 1), , (X_n, 1), (U_1, 0), , (U_n, 0) }), previously build by putting together observed and generated data, fits the classification model and can serve for estimating a classifier by maximizing the conditional log-likelihood:
&lt;span class=&#34;math display&#34;&gt;\[
  \hat \alpha \in \operatorname*{arg\,max}_{\alpha \in \Lambda} \hat L(\theta, \alpha),
  \quad \text{where} \quad
  \hat L(\theta, \alpha) = \frac 1n \sum_{i=1}^n \log(D_\alpha(X_i)) + \frac 1n \sum_{i=1}^n \log(1-D_\alpha(U_i)).
\]&lt;/span&gt;
In addition, the maximal log-likelihood &lt;span class=&#34;math inline&#34;&gt;\(\sup_{\alpha \in \Lambda} \hat L(\theta, \alpha)\)&lt;/span&gt; reflects exactly the ease of discrimination of the two classes &lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(U_1, \dots, U_n\)&lt;/span&gt;, that is the proximity between &lt;span class=&#34;math inline&#34;&gt;\(P_\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt;.
Task 2 is thus performed by introducing a class of discriminators (which are often neural networks) and maximizing a log-likelihood.
The latter quantity also helps in adjusting &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; such that the distribution &lt;span class=&#34;math inline&#34;&gt;\(P_{\theta}\)&lt;/span&gt; of the generated data &lt;span class=&#34;math inline&#34;&gt;\(G_\theta(Z)\)&lt;/span&gt; becomes closer and closer to &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In statistical terms, &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt; can be estimated by &lt;span class=&#34;math inline&#34;&gt;\(P_{\hat \theta}\)&lt;/span&gt;, where:
&lt;span class=&#34;math display&#34;&gt;\[
  \hat \theta \in \operatorname*{arg\,min}_{\theta \in \Theta} \sup_{\alpha \in \Lambda} \hat L(\theta, \alpha),
\]&lt;/span&gt;
where, as described previously, the generated data is &lt;span class=&#34;math inline&#34;&gt;\(U_i = G_\theta (Z_i)\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(Z_1, \dots, Z_n \overset{i.i.d.}{\sim} \mathcal N(0, I_\ell)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The story of GANs is not that gleaming since, in practice, we never have access to &lt;span class=&#34;math inline&#34;&gt;\(P_{\hat \theta}\)&lt;/span&gt;, which may be a very complicated object, but only to the generator &lt;span class=&#34;math inline&#34;&gt;\(G_{\hat \theta}\)&lt;/span&gt;.
Anyway, our aim is to sample according to &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt;, which can be achieved (up to the estimation error) thanks to &lt;span class=&#34;math inline&#34;&gt;\(G_{\hat \theta}(Z)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(Z \sim \mathcal N(0, I_\ell)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Actually, in this work, &lt;span class=&#34;math inline&#34;&gt;\(P_\theta\)&lt;/span&gt; is just as a mathematical object helping to understand GANs.
To go into details, the forthcoming results are based on the assumption that all distributions in play are absolutely continuous with respect to a known measure &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; (typically the Hausdorff measure on some submanifold of &lt;span class=&#34;math inline&#34;&gt;\(\mathbb R^d\)&lt;/span&gt;) and probability density functions are noted with lowercase letters (in particular &lt;span class=&#34;math inline&#34;&gt;\(P^\star = p^\star d\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P_\theta = p_\theta d\mu\)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;div id=&#34;concerning-the-comparison-of-distributions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Concerning the comparison of distributions&lt;/h3&gt;
&lt;p&gt;In order to give a mathematical foundation to our intuition in Task 2, it may be useful to analyze the big sample case, where
&lt;span class=&#34;math display&#34;&gt;\[\hat L(\theta, \alpha) \approx \mathbb E [\hat L(\theta, \alpha)] = \mathbb E [\log(D_\alpha(X_1))] + \mathbb E [\log(1-D_\alpha\circ G_\theta(Z_1))].\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If the class of discriminators &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D = \left\{ D_\alpha, \alpha \in \Lambda \right\}\)&lt;/span&gt; is rich enough to contain the ``optimal’’ discriminator &lt;span class=&#34;math inline&#34;&gt;\(D_\theta^\star = \frac{p^\star}{p_\theta + p^\star}\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(\theta \in \Theta\)&lt;/span&gt;,
then
&lt;span class=&#34;math display&#34;&gt;\[\sup_{\alpha \in \Lambda} \mathbb E [\hat L(\theta, \alpha)] = 2 D_{JS}(P^\star, P_\theta) - \log 4,\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(D_{JS}\)&lt;/span&gt; is the Jensen-Shannon divergence &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-EnSc03&#34; role=&#34;doc-biblioref&#34;&gt;Endres and Schindelin 2003&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Two things can be learned from this first result (still assuming that &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D\)&lt;/span&gt; contains ``optimal’’ discriminators):&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Up to the approximation capacity of &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\sup_{\alpha \in \Lambda} \hat L(\theta, \alpha)\)&lt;/span&gt; does reflect the proximity between &lt;span class=&#34;math inline&#34;&gt;\(P_\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt; (thanks to an approximated divergence).&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt; cannot be better than &lt;span class=&#34;math inline&#34;&gt;\(\theta^\star \in \operatorname*{arg\,min}_{\theta \in \Theta} \sup_{\alpha \in \Lambda} \mathbb E [\hat L(\theta, \alpha)] = \operatorname*{arg\,min}_{\theta \in \Theta} D_{JS}(P^\star, P_\theta)\)&lt;/span&gt;, which leads to the approximation &lt;span class=&#34;math inline&#34;&gt;\(P_{\theta^\star}\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(P^\star\)&lt;/span&gt; obtained by minimizing the Jensen-Shannon divergence.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;non-asymptotic-bound-on-jensen-shannon-error&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Non-asymptotic bound on Jensen-Shannon error&lt;/h3&gt;
&lt;p&gt;Thus, GANs drive the world downhill in two directions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A limited approximation capacity for the class of discriminators &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D\)&lt;/span&gt; (which may not contain the ``optimal’’ discriminator &lt;span class=&#34;math inline&#34;&gt;\(D_\theta^\star = \frac{p^\star}{p_\theta + p^\star}\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(\theta \in \Theta\)&lt;/span&gt;): &lt;span class=&#34;math inline&#34;&gt;\(\sup_{\alpha \in \Lambda} \mathbb E [\hat L(\theta, \alpha)] &amp;lt; 2 D_{JS}(P^\star, P_\theta) - \log 4\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;A finite sample approximation: the criterion maximized is &lt;span class=&#34;math inline&#34;&gt;\(\hat L(\theta, \alpha)\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(\mathbb E [\hat L(\theta, \alpha)]\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These limitations introduce two kinds of error in the estimation procedure:
an approximation error (or bias), induced by the richness of &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D\)&lt;/span&gt;,
and an estimation error (or variance) occurring from the finiteness of the sample.&lt;/p&gt;
&lt;p&gt;This can be formalized in the following manner:
assume some regularity conditions of the first order on the models &lt;span class=&#34;math inline&#34;&gt;\(\mathscr P\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mathscr G\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D\)&lt;/span&gt;
and assume that optimal discriminators &lt;span class=&#34;math inline&#34;&gt;\(D_\theta^\star\)&lt;/span&gt; can be approximated by &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D\)&lt;/span&gt; up to an error &lt;span class=&#34;math inline&#34;&gt;\(\epsilon&amp;gt;0\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(L^2\)&lt;/span&gt;-norm.
Then:
&lt;span class=&#34;math display&#34;&gt;\[
  \mathbb E [D_{JS}(P^\star, P_{\hat \theta})] - D_{JS}(P^\star, P_{\theta^\star}) = O \left( \epsilon^2 + \frac{1}{\sqrt n} \right).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This result explains quantitatively that the discriminators in GANs have to be tuned carefully:
on the one hand, poor discriminators induce an uncontrolled gap between &lt;span class=&#34;math inline&#34;&gt;\(\sup_{\alpha \in \Lambda} \mathbb E [\hat L(\theta, \alpha)]\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(D_{JS}(P^\star, P_\theta)\)&lt;/span&gt;;
on the other hand, very flexible discriminators may lead to overfitting the finite sample.&lt;/p&gt;
&lt;p&gt;The first assertion is illustrated in the next figure.
The numerical experiment has been set up with classes of fully connected neural networks for the generators and the discriminators (respectively &lt;span class=&#34;math inline&#34;&gt;\(\mathscr G\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D\)&lt;/span&gt;) and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; sufficiently large.
The depth of the generators is either 2 (blue bars) or 3 (green bars) and the depth of the discriminator ranges from 2 to 5 (from left to right).
As expected, it appears clearly that the more flexible the discriminators are (from left to right), the smaller &lt;span class=&#34;math inline&#34;&gt;\(D_{JS}(P^\star, P_{\hat \theta})\)&lt;/span&gt; is.
Obviously, this is also inversely correlated with the richness of the class of generators &lt;span class=&#34;math inline&#34;&gt;\(\mathscr G\)&lt;/span&gt; (at least in a first regime).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://youngstats.github.io/post/2021-04-27-a-small-step-to-understand-GDA_files/divergences.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;asymptotic-normality&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Asymptotic normality&lt;/h3&gt;
&lt;p&gt;As a second important result, it can be shown that the estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt; is asymptotically normal with convergence rate &lt;span class=&#34;math inline&#34;&gt;\(\sqrt n\)&lt;/span&gt;.
More formally, let us assume &lt;span class=&#34;math inline&#34;&gt;\(\bar \theta \in \operatorname*{arg\,min}_{\theta\in\Theta} \sup_{\alpha \in \Lambda} \mathbb E [\hat L(\theta, \alpha)]\)&lt;/span&gt; exists and is unique.
Let also assume some regularity conditions of the second order on the models &lt;span class=&#34;math inline&#34;&gt;\(\mathscr P\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mathscr G\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathscr D\)&lt;/span&gt;,
well definiteness and smoothness of &lt;span class=&#34;math inline&#34;&gt;\(\theta \mapsto \operatorname*{arg\,max}_{\alpha \in \Lambda} \mathbb E [\hat L(\theta, \alpha)]\)&lt;/span&gt; around &lt;span class=&#34;math inline&#34;&gt;\(\bar \theta\)&lt;/span&gt;.
Then, there exists a covariance matrice &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; such that:
&lt;span class=&#34;math display&#34;&gt;\[
  \sqrt n \left( \hat \theta - \bar \theta \right) \xrightarrow{dist} \mathcal N(0, \Sigma).
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;GANs have been statistically analyzed from the estimation point of view.
Even though some simplifications were made (known dominating measure &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, uniqueness of some quantities) compared to the empirical setting based on deep neural networks,
the theoretical results show the importance of tuning correctly the architecture of the discriminators,
and exhibit an asymptotic behavior similar to that of a standard M-estimator.&lt;/p&gt;
&lt;p&gt;It remains to study the impact of the architecture of neural nets on the performance of GANs, as well as their behavior in an overparametrized regime.
But that’s a different story.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;This post is based on&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;G. Biau, B. Cadre, M. Sangnier and U. Tanielian. 2020. ``Some Theoretical Properties of GANs.’’ The Annals of Statistics 48(3): 1539-1566.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-EnSc03&#34; class=&#34;csl-entry&#34;&gt;
Endres, D. M., and J. E. Schindelin. 2003. &lt;span&gt;“A New Metric for Probability Distributions.”&lt;/span&gt; &lt;em&gt;IEEE Transactions on Information Theory&lt;/em&gt; 49: 1858–60.
&lt;/div&gt;
&lt;div id=&#34;ref-Go16&#34; class=&#34;csl-entry&#34;&gt;
Goodfellow, I. 2016. &lt;em&gt;NIPS 2016 Tutorial: Generative Adversarial Networks&lt;/em&gt;. arXiv:1701.00160.
&lt;/div&gt;
&lt;div id=&#34;ref-GoPoMiXuWaOzCoBe14&#34; class=&#34;csl-entry&#34;&gt;
Goodfellow, I., J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and J. Bengio. 2014. &lt;span&gt;“Generative Adversarial Nets.”&lt;/span&gt; In &lt;em&gt;Advances in Neural Information Processing Systems 27&lt;/em&gt;, edited by Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, 2672–80. Red Hook: Curran Associates, Inc.
&lt;/div&gt;
&lt;div id=&#34;ref-JaLiBo20&#34; class=&#34;csl-entry&#34;&gt;
Jabbar, A., X. Li, and O. Bourahla. 2020. &lt;em&gt;A Survey on Generative Adversarial Networks: Variants, Applications, and Training&lt;/em&gt;. arXiv:2006.05132.
&lt;/div&gt;
&lt;div id=&#34;ref-Karras2018&#34; class=&#34;csl-entry&#34;&gt;
Karras, T., T. Aila, S. Laine, and J. Lehtinen. 2018. &lt;span&gt;“Progressive Growing of GANs for Improved Quality, Stability, and Variation.”&lt;/span&gt; In &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-SaGoZaChRaCg16&#34; class=&#34;csl-entry&#34;&gt;
Salimans, T., I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. 2016. &lt;span&gt;“Improved Techniques for Training &lt;span&gt;GAN&lt;/span&gt;s.”&lt;/span&gt; In &lt;em&gt;Advances in Neural Information Processing Systems 29&lt;/em&gt;, edited by D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, 2234–42. Red Hook: Curran Associates, Inc.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
