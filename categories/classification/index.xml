<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>classification on YoungStatS</title>
    <link>https://youngstats.github.io/categories/classification/</link>
    <description>Recent content in classification on YoungStatS</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 29 Apr 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://youngstats.github.io/categories/classification/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Recent Advances in Functional Data Analysis</title>
      <link>https://youngstats.github.io/post/2021/04/29/fda-webinar/</link>
      <pubDate>Thu, 29 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/04/29/fda-webinar/</guid>
      <description>The fourth &amp;ldquo;One World webinar&amp;rdquo; organized by YoungStatS will take place on June 30th, 2021. The topic of this webinar is on Functional Data Analysis. Selected young European researchers active in this area of research will present their contributions on spherical functional autoregressions, additive models, and clustering methods for functional data, with the focus on both theoretical developments and applications.
When &amp;amp; Where:
  Wednesday, June 30th, 16:30 CEST</description>
    </item>
    
    <item>
      <title>Locally adaptive k-nearest neighbour classification</title>
      <link>https://youngstats.github.io/post/2021/01/31/local-knn/</link>
      <pubDate>Sun, 31 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/01/31/local-knn/</guid>
      <description>AbstractBinary classification is one of the cornerstones of modern data science, but, until recently, our understanding of classical methods such as the k-nn algorithm was limited to settings where feature vectors were compactly supported. Based on a new analysis of this classifier, we propose a variant with significantly lower risk for heavy-tailed distributions.
The \(k\)-nearest neighbour classifierThe basic classifier that we consider here was introduced by Fix and Hodges (1951), and is arguably the simplest and most intuitive nonparametric classifier.</description>
    </item>
    
    <item>
      <title>The Mulitple Latent Block Model for mixed data</title>
      <link>https://youngstats.github.io/post/2021/01/05/mlbm/</link>
      <pubDate>Tue, 05 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2021/01/05/mlbm/</guid>
      <description>body {text-align: justify}AbstractCo-clustering techniques, which group observations and features simultaneously, have proven to be efficient in summarising data sets. They exploit the dualism between rows and columns and the data set is summarized in blocks (the crossing of a row-cluster and a column-cluster). However, in the case of mixed data sets (with features of different kind), it is not easy to define a co-clustering method that takes this heterogeneity into account.</description>
    </item>
    
  </channel>
</rss>
