<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>robust-statistics on YoungStatS</title>
    <link>https://youngstats.github.io/categories/robust-statistics/</link>
    <description>Recent content in robust-statistics on YoungStatS</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Jan 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://youngstats.github.io/categories/robust-statistics/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Universal estimation with Maximum Mean Discrepancy (MMD)</title>
      <link>https://youngstats.github.io/post/2022/01/13/universal-estimation-with-maximum-mean-discrepancy-mmd/</link>
      <pubDate>Thu, 13 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://youngstats.github.io/post/2022/01/13/universal-estimation-with-maximum-mean-discrepancy-mmd/</guid>
      <description>This is an updated version of a blog post on RIKEN AIP Approximate Bayesian Inference team webpage: https://team-approx-bayes.github.io/blog/mmd/
INTRODUCTIONA very old and yet very exciting problem in statistics is the definition of a universal estimator \(\hat{\theta}\). An estimation procedure that would work all the time. Close your eyes, push the button, it works, for any model, in any context.
Formally speaking, we want that for some metric \(d\) on probability distributions, for any statistical model \((P_\theta,\theta\in\Theta)\), given \(X_1,\dots,X_n\) drawn i.</description>
    </item>
    
  </channel>
</rss>
